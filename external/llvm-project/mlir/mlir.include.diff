--- include
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir and include/mlir
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir-c and include/mlir-c
--- include/mlir
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Analysis and include/mlir/Analysis
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/AsmParser and include/mlir/AsmParser
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Bindings and include/mlir/Bindings
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Bytecode and include/mlir/Bytecode
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/CAPI and include/mlir/CAPI
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion and include/mlir/Conversion
Only in ../../../../llvm-project.0/mlir/include/mlir: Debug
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect and include/mlir/Dialect
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine and include/mlir/ExecutionEngine
diff ../../../../llvm-project.0/mlir/include/mlir/InitAllDialects.h include/mlir/InitAllDialects.h
17c17
< #include "mlir/Dialect/AMDGPU/AMDGPUDialect.h"
---
> #include "mlir/Dialect/AMDGPU/IR/AMDGPUDialect.h"
20d19
< #include "mlir/Dialect/Affine/IR/ValueBoundsOpInterfaceImpl.h"
23d21
< #include "mlir/Dialect/Arith/IR/ValueBoundsOpInterfaceImpl.h"
38d35
< #include "mlir/Dialect/IRDL/IR/IRDL.h"
44,45c41
< #include "mlir/Dialect/Linalg/IR/ValueBoundsOpInterfaceImpl.h"
< #include "mlir/Dialect/Linalg/TransformOps/DialectExtension.h"
---
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h"
51d46
< #include "mlir/Dialect/MemRef/IR/ValueBoundsOpInterfaceImpl.h"
53d47
< #include "mlir/Dialect/MemRef/Transforms/BufferizableOpInterfaceImpl.h"
62d55
< #include "mlir/Dialect/SCF/IR/ValueBoundsOpInterfaceImpl.h"
73,74d65
< #include "mlir/Dialect/Tensor/IR/ValueBoundsOpInterfaceImpl.h"
< #include "mlir/Dialect/Tensor/TransformOps/TensorTransformOps.h"
90c81
<                   affine::AffineDialect,
---
>                   AffineDialect,
104d94
<                   irdl::IRDLDialect,
136d125
<   tensor::registerTransformDialectExtension(registry);
140d128
<   affine::registerValueBoundsOpInterfaceExternalModels(registry);
142d129
<   arith::registerValueBoundsOpInterfaceExternalModels(registry);
147,148d133
<   linalg::registerValueBoundsOpInterfaceExternalModels(registry);
<   memref::registerBufferizableOpInterfaceExternalModels(registry);
150d134
<   memref::registerValueBoundsOpInterfaceExternalModels(registry);
152d135
<   scf::registerValueBoundsOpInterfaceExternalModels(registry);
158d140
<   tensor::registerValueBoundsOpInterfaceExternalModels(registry);
diff ../../../../llvm-project.0/mlir/include/mlir/InitAllPasses.h include/mlir/InitAllPasses.h
17a18
> #include "mlir/Dialect/AMDGPU/Transforms/Passes.h"
58c59,60
<   affine::registerAffinePasses();
---
>   registerAffinePasses();
>   amdgpu::registerAMDGPUPasses();
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Interfaces and include/mlir/Interfaces
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/IR and include/mlir/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Parser and include/mlir/Parser
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Pass and include/mlir/Pass
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Reducer and include/mlir/Reducer
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Rewrite and include/mlir/Rewrite
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Support and include/mlir/Support
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/TableGen and include/mlir/TableGen
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target and include/mlir/Target
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools and include/mlir/Tools
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Transforms and include/mlir/Transforms
--- include/mlir/Pass
diff ../../../../llvm-project.0/mlir/include/mlir/Pass/PassManager.h include/mlir/Pass/PassManager.h
23a24,27
> namespace llvm {
> class Any;
> } // namespace llvm
> 
212,215c216,221
<   PassManager(MLIRContext *ctx,
<               StringRef operationName = PassManager::getAnyOpAnchorName(),
<               Nesting nesting = Nesting::Explicit);
<   PassManager(OperationName operationName, Nesting nesting = Nesting::Explicit);
---
>   /// FIXME: We should make the specification of `builtin.module` explicit here,
>   /// so that we can have top-level op-agnostic pass managers.
>   PassManager(MLIRContext *ctx, Nesting nesting = Nesting::Explicit,
>               StringRef operationName = "builtin.module");
>   PassManager(MLIRContext *ctx, StringRef operationName)
>       : PassManager(ctx, Nesting::Explicit, operationName) {}
218,225d223
<   /// Create a new pass manager under the given context with a specific nesting
<   /// style. The created pass manager can schedule operations that match
<   /// `OperationTy`.
<   template <typename OperationTy>
<   static PassManager on(MLIRContext *ctx, Nesting nesting = Nesting::Explicit) {
<     return PassManager(ctx, OperationTy::getOperationName(), nesting);
<   }
< 
443,444c441
<   llvm::hash_code initializationKey =
<       DenseMapInfo<llvm::hash_code>::getTombstoneKey();
---
>   llvm::hash_code initializationKey;
460c457
< LogicalResult applyPassManagerCLOptions(PassManager &pm);
---
> void applyPassManagerCLOptions(PassManager &pm);
--- include/mlir/Pass/PassRegistry.h
--- include/mlir/Pass/PassBase.td
--- include/mlir/Pass/PassInstrumentation.h
--- include/mlir/Pass/PassOptions.h
--- include/mlir/Pass/Pass.h
--- include/mlir/Pass/PassManager.h
23a24,27
> namespace llvm {
> class Any;
> } // namespace llvm
> 
212,215c216,221
<   PassManager(MLIRContext *ctx,
<               StringRef operationName = PassManager::getAnyOpAnchorName(),
<               Nesting nesting = Nesting::Explicit);
<   PassManager(OperationName operationName, Nesting nesting = Nesting::Explicit);
---
>   /// FIXME: We should make the specification of `builtin.module` explicit here,
>   /// so that we can have top-level op-agnostic pass managers.
>   PassManager(MLIRContext *ctx, Nesting nesting = Nesting::Explicit,
>               StringRef operationName = "builtin.module");
>   PassManager(MLIRContext *ctx, StringRef operationName)
>       : PassManager(ctx, Nesting::Explicit, operationName) {}
218,225d223
<   /// Create a new pass manager under the given context with a specific nesting
<   /// style. The created pass manager can schedule operations that match
<   /// `OperationTy`.
<   template <typename OperationTy>
<   static PassManager on(MLIRContext *ctx, Nesting nesting = Nesting::Explicit) {
<     return PassManager(ctx, OperationTy::getOperationName(), nesting);
<   }
< 
443,444c441
<   llvm::hash_code initializationKey =
<       DenseMapInfo<llvm::hash_code>::getTombstoneKey();
---
>   llvm::hash_code initializationKey;
460c457
< LogicalResult applyPassManagerCLOptions(PassManager &pm);
---
> void applyPassManagerCLOptions(PassManager &pm);
--- include/mlir/Pass/AnalysisManager.h
--- include/mlir/ExecutionEngine
Only in include/mlir/ExecutionEngine: CpuSystemDetect.h
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/CRunnerUtils.h include/mlir/ExecutionEngine/CRunnerUtils.h
472,473d471
< extern "C" MLIR_CRUNNERUTILS_EXPORT void printF16(uint16_t bits);  // bits!
< extern "C" MLIR_CRUNNERUTILS_EXPORT void printBF16(uint16_t bits); // bits!
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/ExecutionEngine.h include/mlir/ExecutionEngine/ExecutionEngine.h
111,113c111
<   /// Creates an execution engine for the given MLIR IR. If TargetMachine is
<   /// not provided, default TM is created (i.e. ignoring any command line flags
<   /// that could affect the set-up).
---
>   /// Creates an execution engine for the given MLIR IR.
115,116c113
<   create(Operation *op, const ExecutionEngineOptions &options = {},
<          std::unique_ptr<llvm::TargetMachine> tm = nullptr);
---
>   create(Operation *op, const ExecutionEngineOptions &options = {});
186,190c183,185
<   /// Set the target triple and the data layout for the input module based on
<   /// the input TargetMachine. This is implicitly done when creating the
<   /// engine.
<   static void setupTargetTripleAndDataLayout(llvm::Module *llvmModule,
<                                              llvm::TargetMachine *tm);
---
>   /// Set the target triple on the module. This is implicitly done when creating
>   /// the engine.
>   static bool setupTargetTriple(llvm::Module *llvmModule);
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/MemRefUtils.h include/mlir/ExecutionEngine/MemRefUtils.h
194c194
<     memset(&other.descriptor, 0, sizeof(other.descriptor));
---
>     memset(0, &other.descriptor, sizeof(other.descriptor));
Only in include/mlir/ExecutionEngine: RocmDeviceName.h
Only in include/mlir/ExecutionEngine: RocmSystemDetect.h
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/RunnerUtils.h include/mlir/ExecutionEngine/RunnerUtils.h
37d36
< #include <iomanip>
44,48c43,44
<   // Make the printed pointer format platform independent by casting it to an
<   // integer and manually formatting it to a hex with prefix as tests expect.
<   os << "base@ = " << std::hex << std::showbase
<      << reinterpret_cast<std::intptr_t>(v.data) << std::dec << std::noshowbase
<      << " rank = " << v.rank << " offset = " << v.offset;
---
>   os << "base@ = " << reinterpret_cast<void *>(v.data) << " rank = " << v.rank
>      << " offset = " << v.offset;
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/SparseTensor and include/mlir/ExecutionEngine/SparseTensor
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/SparseTensorRuntime.h include/mlir/ExecutionEngine/SparseTensorRuntime.h
64,65c64,65
<     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType posTp,
<     OverheadType crdTp, PrimaryType valTp, Action action, void *ptr);
---
>     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType ptrTp,
>     OverheadType indTp, PrimaryType valTp, Action action, void *ptr);
77,91c77,91
< /// Tensor-storage method to obtain direct access to the positions array
< /// for the given level.
< #define DECL_SPARSEPOSITIONS(PNAME, P)                                         \
<   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparsePositions##PNAME(           \
<       StridedMemRefType<P, 1> *out, void *tensor, index_type lvl);
< MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEPOSITIONS)
< #undef DECL_SPARSEPOSITIONS
< 
< /// Tensor-storage method to obtain direct access to the coordinates array
< /// for the given level.
< #define DECL_SPARSECOORDINATES(CNAME, C)                                       \
<   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparseCoordinates##CNAME(         \
<       StridedMemRefType<C, 1> *out, void *tensor, index_type lvl);
< MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSECOORDINATES)
< #undef DECL_SPARSECOORDINATES
---
> /// Tensor-storage method to obtain direct access to the pointers array
> /// for the given dimension.
> #define DECL_SPARSEPOINTERS(PNAME, P)                                          \
>   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparsePointers##PNAME(            \
>       StridedMemRefType<P, 1> *out, void *tensor, index_type d);
> MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEPOINTERS)
> #undef DECL_SPARSEPOINTERS
> 
> /// Tensor-storage method to obtain direct access to the indices array
> /// for the given dimension.
> #define DECL_SPARSEINDICES(INAME, I)                                           \
>   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparseIndices##INAME(             \
>       StridedMemRefType<I, 1> *out, void *tensor, index_type d);
> MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEINDICES)
> #undef DECL_SPARSEINDICES
96,98c96,98
<       void *lvlCOO, StridedMemRefType<V, 0> *vref,                             \
<       StridedMemRefType<index_type, 1> *dimCoordsRef,                          \
<       StridedMemRefType<index_type, 1> *dim2lvlRef);
---
>       void *coo, StridedMemRefType<V, 0> *vref,                                \
>       StridedMemRefType<index_type, 1> *iref,                                  \
>       StridedMemRefType<index_type, 1> *pref);
103,104d102
< /// The `cref` argument uses the same coordinate-space as the `iter` (which
< /// can be either dim- or lvl-coords, depending on context).
107c105
<       void *iter, StridedMemRefType<index_type, 1> *cref,                      \
---
>       void *coo, StridedMemRefType<index_type, 1> *iref,                       \
112,113c110
< /// Tensor-storage method to insert elements in lexicographical
< /// level-coordinate order.
---
> /// Tensor-storage method to insert elements in lexicographical index order.
116c113
<       void *tensor, StridedMemRefType<index_type, 1> *lvlCoordsRef,            \
---
>       void *tensor, StridedMemRefType<index_type, 1> *cref,                    \
124c121
<       void *tensor, StridedMemRefType<index_type, 1> *lvlCoordsRef,            \
---
>       void *tensor, StridedMemRefType<index_type, 1> *cref,                    \
187,190c184,186
< ///   coordinates: a flat "nse * rank" array with coordinates for all
< ///            specified elements
< ///   perm:    the permutation of the levels in the storage
< ///   sparse:  the sparsity for the levels
---
> ///   indices: a flat "nse * rank" array with indices for all specified elements
> ///   perm:    the permutation of the dimensions in the storage
> ///   sparse:  the sparsity for the dimensions
200c196
< ///      coordinates = [ 0, 0,  1, 1,  1, 2]
---
> ///      indices = [ 0, 0,  1, 1,  1, 2]
203,204c199,200
<       uint64_t rank, uint64_t nse, uint64_t *dimSizes, V *values,              \
<       uint64_t *dimCoordinates, uint64_t *dim2lvl, uint8_t *lvlTypes);
---
>       uint64_t rank, uint64_t nse, uint64_t *shape, V *values,                 \
>       uint64_t *indices, uint64_t *perm, uint8_t *sparse);
214c210
< ///   shape:   array with size for each dimension
---
> ///   shape:   array with dimension size for each rank
216,217c212
< ///   coordinates: a flat "nse * rank" array with coordinates for all
< ///            specified elements
---
> ///   indices: a flat "nse * rank" array with indices for all specified elements
219c214
< /// The input is a pointer to `SparseTensorStorage<P, C, V>`, typically
---
> /// The input is a pointer to `SparseTensorStorage<P, I, V>`, typically
224c219
<       V **pValues, uint64_t **pCoordinates);
---
>       V **pValues, uint64_t **pIndices);
250,251c245,246
<     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType posTp,
<     OverheadType crdTp, PrimaryType valTp);
---
>     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType ptrTp,
>     OverheadType indTp, PrimaryType valTp);
259,260c254,255
< /// Returns the number of stored elements for the sparse tensor being read.
< MLIR_CRUNNERUTILS_EXPORT index_type getSparseTensorReaderNSE(void *p);
---
> /// Returns the number of non-zero values for the sparse tensor being read.
> MLIR_CRUNNERUTILS_EXPORT index_type getSparseTensorReaderNNZ(void *p);
286c281
<       void *p, StridedMemRefType<index_type, 1> *dimCoordsRef,                 \
---
>       void *p, StridedMemRefType<index_type, 1> *iref,                         \
291,301d285
< /// Reads the sparse tensor, stores the coordinates and values to the given
< /// memrefs. Returns a boolean value to indicate whether the COO elements are
< /// sorted.
< #define DECL_GETNEXT(VNAME, V, CNAME, C)                                       \
<   MLIR_CRUNNERUTILS_EXPORT bool                                                \
<       _mlir_ciface_getSparseTensorReaderRead##CNAME##VNAME(                    \
<           void *p, StridedMemRefType<index_type, 1> *dim2lvlRef,               \
<           StridedMemRefType<C, 1> *iref, StridedMemRefType<V, 1> *vref)        \
<           MLIR_SPARSETENSOR_FOREVERY_V_O(DECL_GETNEXT)
< #undef DECL_GETNEXT
< 
314c298
< /// Outputs the sparse tensor dim-rank, nse, and dim-shape.
---
> /// Outputs the sparse tensor rank, nnz and shape.
316,317c300,301
<     void *p, index_type dimRank, index_type nse,
<     StridedMemRefType<index_type, 1> *dimSizesRef);
---
>     void *p, index_type rank, index_type nnz,
>     StridedMemRefType<index_type, 1> *dref);
322,323c306
<       void *p, index_type dimRank,                                             \
<       StridedMemRefType<index_type, 1> *dimCoordsRef,                          \
---
>       void *p, index_type rank, StridedMemRefType<index_type, 1> *iref,        \
Only in include/mlir/ExecutionEngine: SystemDevices.h
--- include/mlir/ExecutionEngine/CpuSystemDetect.h
--- include/mlir/ExecutionEngine/CRunnerUtils.h
472,473d471
< extern "C" MLIR_CRUNNERUTILS_EXPORT void printF16(uint16_t bits);  // bits!
< extern "C" MLIR_CRUNNERUTILS_EXPORT void printBF16(uint16_t bits); // bits!
--- include/mlir/ExecutionEngine/MemRefUtils.h
194c194
<     memset(&other.descriptor, 0, sizeof(other.descriptor));
---
>     memset(0, &other.descriptor, sizeof(other.descriptor));
--- include/mlir/ExecutionEngine/Msan.h
--- include/mlir/ExecutionEngine/RocmDeviceName.h
--- include/mlir/ExecutionEngine/RocmSystemDetect.h
--- include/mlir/ExecutionEngine/ExecutionEngine.h
111,113c111
<   /// Creates an execution engine for the given MLIR IR. If TargetMachine is
<   /// not provided, default TM is created (i.e. ignoring any command line flags
<   /// that could affect the set-up).
---
>   /// Creates an execution engine for the given MLIR IR.
115,116c113
<   create(Operation *op, const ExecutionEngineOptions &options = {},
<          std::unique_ptr<llvm::TargetMachine> tm = nullptr);
---
>   create(Operation *op, const ExecutionEngineOptions &options = {});
186,190c183,185
<   /// Set the target triple and the data layout for the input module based on
<   /// the input TargetMachine. This is implicitly done when creating the
<   /// engine.
<   static void setupTargetTripleAndDataLayout(llvm::Module *llvmModule,
<                                              llvm::TargetMachine *tm);
---
>   /// Set the target triple on the module. This is implicitly done when creating
>   /// the engine.
>   static bool setupTargetTriple(llvm::Module *llvmModule);
--- include/mlir/ExecutionEngine/RunnerUtils.h
37d36
< #include <iomanip>
44,48c43,44
<   // Make the printed pointer format platform independent by casting it to an
<   // integer and manually formatting it to a hex with prefix as tests expect.
<   os << "base@ = " << std::hex << std::showbase
<      << reinterpret_cast<std::intptr_t>(v.data) << std::dec << std::noshowbase
<      << " rank = " << v.rank << " offset = " << v.offset;
---
>   os << "base@ = " << reinterpret_cast<void *>(v.data) << " rank = " << v.rank
>      << " offset = " << v.offset;
--- include/mlir/ExecutionEngine/OptUtils.h
--- include/mlir/ExecutionEngine/Float16bits.h
--- include/mlir/ExecutionEngine/AsyncRuntime.h
--- include/mlir/ExecutionEngine/SystemDevices.h
--- include/mlir/ExecutionEngine/SparseTensorRuntime.h
64,65c64,65
<     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType posTp,
<     OverheadType crdTp, PrimaryType valTp, Action action, void *ptr);
---
>     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType ptrTp,
>     OverheadType indTp, PrimaryType valTp, Action action, void *ptr);
77,91c77,91
< /// Tensor-storage method to obtain direct access to the positions array
< /// for the given level.
< #define DECL_SPARSEPOSITIONS(PNAME, P)                                         \
<   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparsePositions##PNAME(           \
<       StridedMemRefType<P, 1> *out, void *tensor, index_type lvl);
< MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEPOSITIONS)
< #undef DECL_SPARSEPOSITIONS
< 
< /// Tensor-storage method to obtain direct access to the coordinates array
< /// for the given level.
< #define DECL_SPARSECOORDINATES(CNAME, C)                                       \
<   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparseCoordinates##CNAME(         \
<       StridedMemRefType<C, 1> *out, void *tensor, index_type lvl);
< MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSECOORDINATES)
< #undef DECL_SPARSECOORDINATES
---
> /// Tensor-storage method to obtain direct access to the pointers array
> /// for the given dimension.
> #define DECL_SPARSEPOINTERS(PNAME, P)                                          \
>   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparsePointers##PNAME(            \
>       StridedMemRefType<P, 1> *out, void *tensor, index_type d);
> MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEPOINTERS)
> #undef DECL_SPARSEPOINTERS
> 
> /// Tensor-storage method to obtain direct access to the indices array
> /// for the given dimension.
> #define DECL_SPARSEINDICES(INAME, I)                                           \
>   MLIR_CRUNNERUTILS_EXPORT void _mlir_ciface_sparseIndices##INAME(             \
>       StridedMemRefType<I, 1> *out, void *tensor, index_type d);
> MLIR_SPARSETENSOR_FOREVERY_O(DECL_SPARSEINDICES)
> #undef DECL_SPARSEINDICES
96,98c96,98
<       void *lvlCOO, StridedMemRefType<V, 0> *vref,                             \
<       StridedMemRefType<index_type, 1> *dimCoordsRef,                          \
<       StridedMemRefType<index_type, 1> *dim2lvlRef);
---
>       void *coo, StridedMemRefType<V, 0> *vref,                                \
>       StridedMemRefType<index_type, 1> *iref,                                  \
>       StridedMemRefType<index_type, 1> *pref);
103,104d102
< /// The `cref` argument uses the same coordinate-space as the `iter` (which
< /// can be either dim- or lvl-coords, depending on context).
107c105
<       void *iter, StridedMemRefType<index_type, 1> *cref,                      \
---
>       void *coo, StridedMemRefType<index_type, 1> *iref,                       \
112,113c110
< /// Tensor-storage method to insert elements in lexicographical
< /// level-coordinate order.
---
> /// Tensor-storage method to insert elements in lexicographical index order.
116c113
<       void *tensor, StridedMemRefType<index_type, 1> *lvlCoordsRef,            \
---
>       void *tensor, StridedMemRefType<index_type, 1> *cref,                    \
124c121
<       void *tensor, StridedMemRefType<index_type, 1> *lvlCoordsRef,            \
---
>       void *tensor, StridedMemRefType<index_type, 1> *cref,                    \
187,190c184,186
< ///   coordinates: a flat "nse * rank" array with coordinates for all
< ///            specified elements
< ///   perm:    the permutation of the levels in the storage
< ///   sparse:  the sparsity for the levels
---
> ///   indices: a flat "nse * rank" array with indices for all specified elements
> ///   perm:    the permutation of the dimensions in the storage
> ///   sparse:  the sparsity for the dimensions
200c196
< ///      coordinates = [ 0, 0,  1, 1,  1, 2]
---
> ///      indices = [ 0, 0,  1, 1,  1, 2]
203,204c199,200
<       uint64_t rank, uint64_t nse, uint64_t *dimSizes, V *values,              \
<       uint64_t *dimCoordinates, uint64_t *dim2lvl, uint8_t *lvlTypes);
---
>       uint64_t rank, uint64_t nse, uint64_t *shape, V *values,                 \
>       uint64_t *indices, uint64_t *perm, uint8_t *sparse);
214c210
< ///   shape:   array with size for each dimension
---
> ///   shape:   array with dimension size for each rank
216,217c212
< ///   coordinates: a flat "nse * rank" array with coordinates for all
< ///            specified elements
---
> ///   indices: a flat "nse * rank" array with indices for all specified elements
219c214
< /// The input is a pointer to `SparseTensorStorage<P, C, V>`, typically
---
> /// The input is a pointer to `SparseTensorStorage<P, I, V>`, typically
224c219
<       V **pValues, uint64_t **pCoordinates);
---
>       V **pValues, uint64_t **pIndices);
250,251c245,246
<     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType posTp,
<     OverheadType crdTp, PrimaryType valTp);
---
>     StridedMemRefType<index_type, 1> *dim2lvlRef, OverheadType ptrTp,
>     OverheadType indTp, PrimaryType valTp);
259,260c254,255
< /// Returns the number of stored elements for the sparse tensor being read.
< MLIR_CRUNNERUTILS_EXPORT index_type getSparseTensorReaderNSE(void *p);
---
> /// Returns the number of non-zero values for the sparse tensor being read.
> MLIR_CRUNNERUTILS_EXPORT index_type getSparseTensorReaderNNZ(void *p);
286c281
<       void *p, StridedMemRefType<index_type, 1> *dimCoordsRef,                 \
---
>       void *p, StridedMemRefType<index_type, 1> *iref,                         \
291,301d285
< /// Reads the sparse tensor, stores the coordinates and values to the given
< /// memrefs. Returns a boolean value to indicate whether the COO elements are
< /// sorted.
< #define DECL_GETNEXT(VNAME, V, CNAME, C)                                       \
<   MLIR_CRUNNERUTILS_EXPORT bool                                                \
<       _mlir_ciface_getSparseTensorReaderRead##CNAME##VNAME(                    \
<           void *p, StridedMemRefType<index_type, 1> *dim2lvlRef,               \
<           StridedMemRefType<C, 1> *iref, StridedMemRefType<V, 1> *vref)        \
<           MLIR_SPARSETENSOR_FOREVERY_V_O(DECL_GETNEXT)
< #undef DECL_GETNEXT
< 
314c298
< /// Outputs the sparse tensor dim-rank, nse, and dim-shape.
---
> /// Outputs the sparse tensor rank, nnz and shape.
316,317c300,301
<     void *p, index_type dimRank, index_type nse,
<     StridedMemRefType<index_type, 1> *dimSizesRef);
---
>     void *p, index_type rank, index_type nnz,
>     StridedMemRefType<index_type, 1> *dref);
322,323c306
<       void *p, index_type dimRank,                                             \
<       StridedMemRefType<index_type, 1> *dimCoordsRef,                          \
---
>       void *p, index_type rank, StridedMemRefType<index_type, 1> *iref,        \
--- include/mlir/ExecutionEngine/JitRunner.h
--- include/mlir/ExecutionEngine/SparseTensor
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/SparseTensor/COO.h include/mlir/ExecutionEngine/SparseTensor/COO.h
30,31c30,31
< /// (i.e., a pair of coordinates and value).  For example, a rank-1
< /// vector element would look like
---
> /// (i.e., a pair of indices and value).  For example, a rank-1 vector
> /// element would look like
36,43c36,43
< /// The coordinates are represented as a (non-owning) pointer into
< /// a shared pool of coordinates, rather than being stored directly in
< /// this object.  This significantly improves performance because it:
< /// (1) reduces the per-element memory footprint, and (2) centralizes
< /// the memory management for coordinates.  The only downside is that
< /// the coordinates themselves cannot be retrieved without knowing the
< /// rank of the tensor to which this element belongs (and that rank is
< /// not stored in this object).
---
> /// The indices are represented as a (non-owning) pointer into a shared
> /// pool of indices, rather than being stored directly in this object.
> /// This significantly improves performance because it: (1) reduces
> /// the per-element memory footprint, and (2) centralizes the memory
> /// management for indices.  The only downside is that the indices
> /// themselves cannot be retrieved without knowing the rank of the
> /// tensor to which this element belongs (and that rank is not stored
> /// in this object).
46,47c46,47
<   Element(const uint64_t *coords, V val) : coords(coords), value(val){};
<   const uint64_t *coords; // pointer into shared coordinates pool
---
>   Element(const uint64_t *ind, V val) : indices(ind), value(val){};
>   const uint64_t *indices; // pointer into shared index pool
61c61
<       if (e1.coords[d] == e2.coords[d])
---
>       if (e1.indices[d] == e2.indices[d])
63c63
<       return e1.coords[d] < e2.coords[d];
---
>       return e1.indices[d] < e2.indices[d];
135c135
<       coordinates.reserve(capacity * dimRank);
---
>       indices.reserve(capacity * dimRank);
139c139
<   /// Gets the dimension-rank of the tensor.
---
>   /// Gets the rank of the tensor.
152c152
<   /// `dimCoords` is already associated with a value, it adds it regardless.
---
>   /// `ind` is already associated with a value, it adds it regardless.
159,169c159,168
<   /// * the `dimCoords` is valid for `getRank`.
<   /// * the components of `dimCoords` are valid for `getDimSizes`.
<   void add(const std::vector<uint64_t> &dimCoords, V val) {
<     const uint64_t *base = coordinates.data();
<     const uint64_t size = coordinates.size();
<     const uint64_t dimRank = getRank();
<     assert(dimCoords.size() == dimRank && "Element rank mismatch");
<     for (uint64_t d = 0; d < dimRank; ++d) {
<       assert(dimCoords[d] < dimSizes[d] &&
<              "Coordinate is too large for the dimension");
<       coordinates.push_back(dimCoords[d]);
---
>   /// * the `ind` is valid for `rank`
>   /// * the elements of `ind` are valid for `dimSizes`.
>   void add(const std::vector<uint64_t> &ind, V val) {
>     const uint64_t *base = indices.data();
>     uint64_t size = indices.size();
>     uint64_t rank = getRank();
>     assert(ind.size() == rank && "Element rank mismatch");
>     for (uint64_t r = 0; r < rank; ++r) {
>       assert(ind[r] < dimSizes[r] && "Index is too large for the dimension");
>       indices.push_back(ind[r]);
171,176c170,175
<     // This base only changes if `coordinates` was reallocated.  In which
<     // case, we need to correct all previous pointers into the vector.
<     // Note that this only happens if we did not set the initial capacity
<     // right, and then only for every internal vector reallocation (which
<     // with the doubling rule should only incur an amortized linear overhead).
<     const uint64_t *const newBase = coordinates.data();
---
>     // This base only changes if indices were reallocated. In that case, we
>     // need to correct all previous pointers into the vector. Note that this
>     // only happens if we did not set the initial capacity right, and then only
>     // for every internal vector reallocation (which with the doubling rule
>     // should only incur an amortized linear overhead).
>     const uint64_t *newBase = indices.data();
179c178
<         elements[i].coords = newBase + (elements[i].coords - base);
---
>         elements[i].indices = newBase + (elements[i].indices - base);
183c182
<     const Element<V> addedElem(base + size, val);
---
>     Element<V> addedElem(base + size, val);
192,194c191,192
<   /// Sorts elements lexicographically by coordinates.  If a coordinate
<   /// is mapped to multiple values, then the relative order of those
<   /// values is unspecified.
---
>   /// Sorts elements lexicographically by index.  If an index is mapped to
>   /// multiple values, then the relative order of those values is unspecified.
207c205
<   std::vector<uint64_t> coordinates;    // shared coordinate pool
---
>   std::vector<uint64_t> indices;        // shared index pool
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/SparseTensor/File.h include/mlir/ExecutionEngine/SparseTensor/File.h
49c49
< inline std::enable_if_t<!is_complex<V>::value, V> readValue(char **linePtr) {
---
> inline std::enable_if_t<!is_complex<V>::value, V> readCOOValue(char **linePtr) {
62c62
< inline std::enable_if_t<is_complex<V>::value, V> readValue(char **linePtr) {
---
> inline std::enable_if_t<is_complex<V>::value, V> readCOOValue(char **linePtr) {
75,76c75,76
< /// Returns an element-value.  If `isPattern` is true, then returns an
< /// arbitrary value.  If `isPattern` is false, then reads the value from
---
> /// Returns an element-value.  If `is_pattern` is true, then returns an
> /// arbitrary value.  If `is_pattern` is false, then reads the value from
79,80c79,82
< inline V readValue(char **linePtr, bool isPattern) {
<   return isPattern ? readValue<V, true>(linePtr) : readValue<V, false>(linePtr);
---
> inline V readCOOValue(char **linePtr, bool is_pattern) {
>   if (is_pattern)
>     return readCOOValue<V, true>(linePtr);
>   return readCOOValue<V, false>(linePtr);
172,173c174
<   /// Gets the dimension-rank of the tensor.  Is only valid after parsing
<   /// the header.
---
>   /// Gets the rank of the tensor.  Is only valid after parsing the header.
179,182c180,182
<   /// Gets the number of stored elements.  Is only valid after parsing
<   /// the header.
<   uint64_t getNSE() const {
<     assert(isValid() && "Attempt to getNSE() before readHeader()");
---
>   /// Gets the number of non-zeros.  Is only valid after parsing the header.
>   uint64_t getNNZ() const {
>     assert(isValid() && "Attempt to getNNZ() before readHeader()");
204c204
<   /// to the `dimCoords` array.
---
>   /// to the `indices` array.
206,209c206,209
<   V readElement(uint64_t dimRank, uint64_t *dimCoords) {
<     assert(dimRank == getRank() && "rank mismatch");
<     char *linePtr = readCoords(dimCoords);
<     return detail::readValue<V>(&linePtr, isPattern());
---
>   V readCOOElement(uint64_t rank, uint64_t *indices) {
>     assert(rank == getRank() && "rank mismatch");
>     char *linePtr = readCOOIndices(indices);
>     return detail::readCOOValue<V>(&linePtr, isPattern());
213,214c213,214
<   /// all the elements from the file and applying `dim2lvl` to their
<   /// dim-coordinates, and then closes the file.
---
>   /// all the elements from the file and applying `dim2lvl` to their indices,
>   /// and then closes the file.
219c219,220
<   /// * `dim2lvl` maps `getDimSizes()`-coordinates to `lvlSizes`-coordinates.
---
>   /// * `dim2lvl` maps indices valid for `getDimSizes()` to indices
>   ///   valid for `lvlSizes`.
251,258d251
<   /// Reads the COO tensor from the file, stores the coordinates and values to
<   /// the given buffers, returns a boolean value to indicate whether the COO
<   /// elements are sorted.
<   /// Precondition: the buffers should have enough space to hold the elements.
<   template <typename C, typename V>
<   bool readToBuffers(uint64_t lvlRank, const uint64_t *dim2lvl,
<                      C *lvlCoordinates, V *values);
< 
265c258
<   /// into the `dimCoords` argument.  Returns the position in the `line`
---
>   /// into the `indices` argument.  Returns the position in the `line`
267c260
<   /// has been factored out from `readElement` to minimize code bloat
---
>   /// has been factored out from `readCOOElement` to minimize code bloat
270,283c263,264
<   /// Precondition: `dimCoords` is valid for `getRank()`.
<   template <typename C>
<   char *readCoords(C *dimCoords) {
<     readLine();
<     // Local variable for tracking the parser's position in the `line` buffer.
<     char *linePtr = line;
<     for (uint64_t dimRank = getRank(), d = 0; d < dimRank; ++d) {
<       // Parse the 1-based coordinate.
<       uint64_t c = strtoul(linePtr, &linePtr, 10);
<       // Store the 0-based coordinate.
<       dimCoords[d] = static_cast<C>(c - 1);
<     }
<     return linePtr;
<   }
---
>   /// Precondition: `indices` is valid for `getRank()`.
>   char *readCOOIndices(uint64_t *indices);
286,287c267,268
<   /// `IsPattern` in order to perform LICM without needing to duplicate the
<   /// source code.
---
>   /// `IsPattern` and `IsSymmetric` in order to perform LICM without
>   /// needing to duplicate the source code.
293c274
<   template <typename V, bool IsPattern>
---
>   template <typename V, bool IsPattern, bool IsSymmetric>
297,303d277
<   /// The internal implementation of `readToBuffers`.  We template over
<   /// `IsPattern` in order to perform LICM without needing to duplicate the
<   /// source code.
<   template <typename C, typename V, bool IsPattern>
<   bool readToBuffersLoop(uint64_t lvlRank, detail::PermutationRef dim2lvl,
<                          C *lvlCoordinates, V *values);
< 
336,337c310,311
<   // Prepare a COO object with the number of stored elems as initial capacity.
<   auto *lvlCOO = new SparseTensorCOO<V>(lvlRank, lvlSizes, getNSE());
---
>   // Prepare a COO object with the number of nonzeros as initial capacity.
>   auto *lvlCOO = new SparseTensorCOO<V>(lvlRank, lvlSizes, getNNZ());
340,341c314,320
<   if (IsPattern)
<     readCOOLoop<V, true>(lvlRank, d2l, lvlCOO);
---
>   const bool IsSymmetric = (isSymmetric() && getRank() == 2);
>   if (IsPattern && IsSymmetric)
>     readCOOLoop<V, true, true>(lvlRank, d2l, lvlCOO);
>   else if (IsPattern)
>     readCOOLoop<V, true, false>(lvlRank, d2l, lvlCOO);
>   else if (IsSymmetric)
>     readCOOLoop<V, false, true>(lvlRank, d2l, lvlCOO);
343c322
<     readCOOLoop<V, false>(lvlRank, d2l, lvlCOO);
---
>     readCOOLoop<V, false, false>(lvlRank, d2l, lvlCOO);
349c328
< template <typename V, bool IsPattern>
---
> template <typename V, bool IsPattern, bool IsSymmetric>
354,357c333,336
<   std::vector<uint64_t> dimCoords(dimRank);
<   std::vector<uint64_t> lvlCoords(lvlRank);
<   for (uint64_t nse = getNSE(), k = 0; k < nse; ++k) {
<     // We inline `readElement` here in order to avoid redundant
---
>   std::vector<uint64_t> dimInd(dimRank);
>   std::vector<uint64_t> lvlInd(lvlRank);
>   for (uint64_t nnz = getNNZ(), k = 0; k < nnz; ++k) {
>     // We inline `readCOOElement` here in order to avoid redundant
359,362c338,341
<     // and the construction of `dimCoords` above.
<     char *linePtr = readCoords(dimCoords.data());
<     const V value = detail::readValue<V, IsPattern>(&linePtr);
<     dim2lvl.pushforward(dimRank, dimCoords.data(), lvlCoords.data());
---
>     // and the construction of `dimInd` above.
>     char *linePtr = readCOOIndices(dimInd.data());
>     const V value = detail::readCOOValue<V, IsPattern>(&linePtr);
>     dim2lvl.pushforward(dimRank, dimInd.data(), lvlInd.data());
364,421c343,352
<     lvlCOO->add(lvlCoords, value);
<   }
< }
< 
< template <typename C, typename V>
< bool SparseTensorReader::readToBuffers(uint64_t lvlRank,
<                                        const uint64_t *dim2lvl,
<                                        C *lvlCoordinates, V *values) {
<   assert(isValid() && "Attempt to readCOO() before readHeader()");
<   // Construct a `PermutationRef` for the `pushforward` below.
<   // TODO: This specific implementation does not generalize to arbitrary
<   // mappings, but once we functionalize the `dim2lvl` argument we can
<   // simply use that function instead.
<   const uint64_t dimRank = getRank();
<   assert(lvlRank == dimRank && "Rank mismatch");
<   detail::PermutationRef d2l(dimRank, dim2lvl);
<   // Do some manual LICM, to avoid assertions in the for-loop.
<   bool isSorted =
<       isPattern()
<           ? readToBuffersLoop<C, V, true>(lvlRank, d2l, lvlCoordinates, values)
<           : readToBuffersLoop<C, V, false>(lvlRank, d2l, lvlCoordinates,
<                                            values);
< 
<   // Close the file and return isSorted.
<   closeFile();
<   return isSorted;
< }
< 
< template <typename C, typename V, bool IsPattern>
< bool SparseTensorReader::readToBuffersLoop(uint64_t lvlRank,
<                                            detail::PermutationRef dim2lvl,
<                                            C *lvlCoordinates, V *values) {
<   const uint64_t dimRank = getRank();
<   const uint64_t nse = getNSE();
<   std::vector<C> dimCoords(dimRank);
<   // Read the first element with isSorted=false as a way to avoid accessing its
<   // previous element.
<   bool isSorted = false;
<   char *linePtr;
<   // We inline `readElement` here in order to avoid redundant assertions,
<   // since they're guaranteed by the call to `isValid()` and the construction
<   // of `dimCoords` above.
<   const auto readNextElement = [&]() {
<     linePtr = readCoords<C>(dimCoords.data());
<     dim2lvl.pushforward(dimRank, dimCoords.data(), lvlCoordinates);
<     *values = detail::readValue<V, IsPattern>(&linePtr);
<     if (isSorted) {
<       // Note that isSorted was set to false while reading the first element,
<       // to guarantee the safeness of using prevLvlCoords.
<       C *prevLvlCoords = lvlCoordinates - lvlRank;
<       // TODO: define a new CoordsLT which is like ElementLT but doesn't have
<       // the V parameter, and use it here.
<       for (uint64_t l = 0; l < lvlRank; ++l) {
<         if (prevLvlCoords[l] != lvlCoordinates[l]) {
<           if (prevLvlCoords[l] > lvlCoordinates[l])
<             isSorted = false;
<           break;
<         }
---
>     lvlCOO->add(lvlInd, value);
>     // We currently chose to deal with symmetric matrices by fully
>     // constructing them.  In the future, we may want to make symmetry
>     // implicit for storage reasons.
>     if constexpr (IsSymmetric)
>       if (dimInd[0] != dimInd[1]) {
>         // Must recompute `lvlInd`, since arbitrary maps don't preserve swap.
>         std::swap(dimInd[0], dimInd[1]);
>         dim2lvl.pushforward(dimRank, dimInd.data(), lvlInd.data());
>         lvlCOO->add(lvlInd, value);
423,432c354
<     }
<     lvlCoordinates += lvlRank;
<     ++values;
<   };
<   readNextElement();
<   isSorted = true;
<   for (uint64_t n = 1; n < nse; ++n)
<     readNextElement();
< 
<   return isSorted;
---
>   }
440,443c362,365
<   const auto &dimSizes = coo.getDimSizes();
<   const auto &elements = coo.getElements();
<   const uint64_t dimRank = coo.getRank();
<   const uint64_t nse = elements.size();
---
>   auto &dimSizes = coo.getDimSizes();
>   auto &elements = coo.getElements();
>   const uint64_t rank = coo.getRank();
>   const uint64_t nnz = elements.size();
447,454c369,376
<   file << "; extended FROSTT format\n" << dimRank << " " << nse << std::endl;
<   for (uint64_t d = 0; d < dimRank - 1; ++d)
<     file << dimSizes[d] << " ";
<   file << dimSizes[dimRank - 1] << std::endl;
<   for (uint64_t i = 0; i < nse; ++i) {
<     const auto &coords = elements[i].coords;
<     for (uint64_t d = 0; d < dimRank; ++d)
<       file << (coords[d] + 1) << " ";
---
>   file << "; extended FROSTT format\n" << rank << " " << nnz << std::endl;
>   for (uint64_t r = 0; r < rank - 1; ++r)
>     file << dimSizes[r] << " ";
>   file << dimSizes[rank - 1] << std::endl;
>   for (uint64_t i = 0; i < nnz; ++i) {
>     auto &idx = elements[i].indices;
>     for (uint64_t r = 0; r < rank; ++r)
>       file << (idx[r] + 1) << " ";
diff ../../../../llvm-project.0/mlir/include/mlir/ExecutionEngine/SparseTensor/Storage.h include/mlir/ExecutionEngine/SparseTensor/Storage.h
18c18
< // * `SparseTensorStorage<P, C, V>`
---
> // * `SparseTensorStorage<P, I, V>`
20c20
< // * `SparseTensorEnumerator<P, C, V>`
---
> // * `SparseTensorEnumerator<P, I, V>`
55c55
<   assert(d < getDimRank() && "Dimension is out of bounds");
---
>   assert(d < getDimRank() && "Dimension index is out of bounds");
57c57
<   assert(l < getLvlRank() && "Level is out of bounds");
---
>   assert(l < getLvlRank() && "Level index is out of bounds");
73,74c73,74
< /// Abstract base class for `SparseTensorStorage<P,C,V>`.  This class
< /// takes responsibility for all the `<P,C,V>`-independent aspects
---
> /// Abstract base class for `SparseTensorStorage<P,I,V>`.  This class
> /// takes responsibility for all the `<P,I,V>`-independent aspects
92c92
< /// The *size* of an axis is the cardinality of possible coordinate
---
> /// The *size* of an axis is the cardinality of possible coordinate/index
132c132,133
<   /// * `lvl2dim` must map `lvlSizes`-coordinates to `dimSizes`-coordinates.
---
>   /// * `lvl2dim` must map indices valid for `lvlSizes` to indices valid
>   ///   for `dimSizes`.
215,227c216,227
<   /// Gets positions-overhead storage for the given level.
< #define DECL_GETPOSITIONS(PNAME, P)                                            \
<   virtual void getPositions(std::vector<P> **, uint64_t);
<   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETPOSITIONS)
< #undef DECL_GETPOSITIONS
< 
<   /// Gets coordinates-overhead storage for the given level.
< #define DECL_GETCOORDINATES(INAME, C)                                          \
<   virtual void getCoordinates(std::vector<C> **, uint64_t);
<   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETCOORDINATES)
< #undef DECL_GETCOORDINATES
<   /// Gets the coordinate-value stored at the given level and position.
<   virtual uint64_t getCrd(uint64_t lvl, uint64_t pos) const = 0;
---
>   /// Gets pointers-overhead storage.
> #define DECL_GETPOINTERS(PNAME, P)                                             \
>   virtual void getPointers(std::vector<P> **, uint64_t);
>   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETPOINTERS)
> #undef DECL_GETPOINTERS
> 
>   /// Gets indices-overhead storage.
> #define DECL_GETINDICES(INAME, I)                                              \
>   virtual void getIndices(std::vector<I> **, uint64_t);
>   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETINDICES)
> #undef DECL_GETINDICES
>   virtual uint64_t getIndex(uint64_t l, uint64_t pos) const = 0;
234,235c234,235
<   /// Element-wise insertion in lexicographic coordinate order.  The first
<   /// argument is the level-coordinates for the value being inserted.
---
>   /// Element-wise insertion in lexicographic index order.  The first
>   /// argument is the level-indices for the value being inserted.
237c237
<   // length of `lvlCoords` and check that against `getLvlRank()`.
---
>   // length of `lvlInd` and check that against `getLvlRank()`.
247c247
<   /// * `lvlCoords` the level-coordinates shared by the values being inserted.
---
>   /// * `lvlInd` the level-indices shared by the values being inserted.
272c272
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
276c276
< /// per-level sparse/dense annotations.  This data structure provides
---
> /// per-dimension sparse/dense annotations.  This data structure provides
282c282
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
287c287
<   /// doesn't entail `!(positions[l].empty())`.
---
>   /// doesn't entail `!(pointers[l].empty())`.
295c295
<         positions(lvlRank), coordinates(lvlRank), lvlCursor(lvlRank) {}
---
>         pointers(lvlRank), indices(lvlRank), lvlCursor(lvlRank) {}
342c342
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
346c346
<     return new SparseTensorStorage<P, C, V>(dimRank, dimSizes, lvlRank,
---
>     return new SparseTensorStorage<P, I, V>(dimRank, dimSizes, lvlRank,
368c368
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
378,380c378,380
<   /// * `src2lvl` must be valid for `srcRank`, must map coordinates valid
<   ///    for `source.getDimSizes()` to coordinates valid for `lvlSizes`,
<   ///    and therefore must be the inverse of `lvl2dim`.
---
>   /// * `src2lvl` must be valid for `srcRank`, must map indices valid for
>   ///   `source.getDimSizes()` to indices valid for `lvlSizes`, and therefore
>   ///   must be the inverse of `lvl2dim`.
399c399
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
409c409
<   void getPositions(std::vector<P> **out, uint64_t lvl) final {
---
>   void getPointers(std::vector<P> **out, uint64_t l) final {
411,412c411,412
<     ASSERT_VALID_LVL(lvl);
<     *out = &positions[lvl];
---
>     ASSERT_VALID_LVL(l);
>     *out = &pointers[l];
414c414
<   void getCoordinates(std::vector<C> **out, uint64_t lvl) final {
---
>   void getIndices(std::vector<I> **out, uint64_t l) final {
416,417c416,417
<     ASSERT_VALID_LVL(lvl);
<     *out = &coordinates[lvl];
---
>     ASSERT_VALID_LVL(l);
>     *out = &indices[l];
424,427c424,427
<   uint64_t getCrd(uint64_t lvl, uint64_t pos) const final {
<     ASSERT_COMPRESSED_OR_SINGLETON_LVL(lvl);
<     assert(pos < coordinates[lvl].size() && "Position is out of bounds");
<     return coordinates[lvl][pos]; // Converts the stored `C` into `uint64_t`.
---
>   uint64_t getIndex(uint64_t l, uint64_t pos) const final {
>     ASSERT_COMPRESSED_OR_SINGLETON_LVL(l);
>     assert(pos < indices[l].size() && "Index position is out of bounds");
>     return indices[l][pos]; // Converts the stored `I` into `uint64_t`.
431,432c431,432
<   void lexInsert(const uint64_t *lvlCoords, V val) final {
<     assert(lvlCoords && "Received nullptr for level-coordinates");
---
>   void lexInsert(const uint64_t *lvlInd, V val) final {
>     assert(lvlInd && "Received nullptr for level-indices");
435c435
<     uint64_t full = 0;
---
>     uint64_t topIdx = 0;
437c437
<       diffLvl = lexDiff(lvlCoords);
---
>       diffLvl = lexDiff(lvlInd);
439c439
<       full = lvlCursor[diffLvl] + 1;
---
>       topIdx = lvlCursor[diffLvl] + 1;
442c442
<     insPath(lvlCoords, diffLvl, full, val);
---
>     insPath(lvlInd, diffLvl, topIdx, val);
446c446
<   void expInsert(uint64_t *lvlCoords, V *values, bool *filled, uint64_t *added,
---
>   void expInsert(uint64_t *lvlInd, V *values, bool *filled, uint64_t *added,
448c448
<     assert((lvlCoords && values && filled && added) && "Received nullptr");
---
>     assert((lvlInd && values && filled && added) && "Received nullptr");
455,460c455,460
<     uint64_t c = added[0];
<     assert(filled[c] && "added coordinate is not filled");
<     lvlCoords[lastLvl] = c;
<     lexInsert(lvlCoords, values[c]);
<     values[c] = 0;
<     filled[c] = false;
---
>     uint64_t index = added[0];
>     assert(filled[index] && "added index is not filled");
>     lvlInd[lastLvl] = index;
>     lexInsert(lvlInd, values[index]);
>     values[index] = 0;
>     filled[index] = false;
463,469c463,469
<       assert(c < added[i] && "non-lexicographic insertion");
<       c = added[i];
<       assert(filled[c] && "added coordinate is not filled");
<       lvlCoords[lastLvl] = c;
<       insPath(lvlCoords, lastLvl, added[i - 1] + 1, values[c]);
<       values[c] = 0;
<       filled[c] = false;
---
>       assert(index < added[i] && "non-lexicographic insertion");
>       index = added[i];
>       assert(filled[index] && "added index is not filled");
>       lvlInd[lastLvl] = index;
>       insPath(lvlInd, lastLvl, added[i - 1] + 1, values[index]);
>       values[index] = 0;
>       filled[index] = false;
481,482c481,482
<   /// Allocates a new enumerator for this class's `<P,C,V>` types and
<   /// erase the `<P,C>` parts from the type.  Callers must make sure to
---
>   /// Allocates a new enumerator for this class's `<P,I,V>` types and
>   /// erase the `<P,I>` parts from the type.  Callers must make sure to
488c488
<     *out = new SparseTensorEnumerator<P, C, V>(*this, trgRank, trgSizes,
---
>     *out = new SparseTensorEnumerator<P, I, V>(*this, trgRank, trgSizes,
501c501
<     SparseTensorEnumerator<P, C, V> enumerator(*this, trgRank, trgSizes,
---
>     SparseTensorEnumerator<P, I, V> enumerator(*this, trgRank, trgSizes,
505c505
<         [&coo](const auto &trgCoords, V val) { coo->add(trgCoords, val); });
---
>         [&coo](const auto &trgInd, V val) { coo->add(trgInd, val); });
514c514
<   /// Appends an arbitrary new position to `positions[lvl]`.  This method
---
>   /// Appends an arbitrary new position to `pointers[l]`.  This method
517,519c517,519
<   /// the previous position and smaller than `coordinates[lvl].capacity()`).
<   void appendPos(uint64_t lvl, uint64_t pos, uint64_t count = 1) {
<     ASSERT_COMPRESSED_LVL(lvl);
---
>   /// the previous position and smaller than `indices[l].capacity()`).
>   void appendPointer(uint64_t l, uint64_t pos, uint64_t count = 1) {
>     ASSERT_COMPRESSED_LVL(l);
521,536c521,536
<     // "Position value is too large for the P-type"
<     positions[lvl].insert(positions[lvl].end(), count,
<                           detail::checkOverflowCast<P>(pos));
<   }
< 
<   /// Appends coordinate `crd` to level `lvl`, in the semantically
<   /// general sense.  For non-dense levels, that means appending to the
<   /// `coordinates[lvl]` array, checking that `crd` is representable in
<   /// the `C` type; however, we do not verify other semantic requirements
<   /// (e.g., that `crd` is in bounds for `lvlSizes[lvl]`, and not previously
<   /// occurring in the same segment).  For dense levels, this method instead
<   /// appends the appropriate number of zeros to the `values` array, where
<   /// `full` is the number of "entries" already written to `values` for this
<   /// segment (aka one after the highest coordinate previously appended).
<   void appendCrd(uint64_t lvl, uint64_t full, uint64_t crd) {
<     const auto dlt = getLvlType(lvl); // Avoid redundant bounds checking.
---
>     // "Pointer value is too large for the P-type"
>     pointers[l].insert(pointers[l].end(), count,
>                        detail::checkOverflowCast<P>(pos));
>   }
> 
>   /// Appends index `i` to level `l`, in the semantically general sense.
>   /// For non-dense levels, that means appending to the `indices[l]` array,
>   /// checking that `i` is representable in the `I` type; however, we do
>   /// not verify other semantic requirements (e.g., that `i` is in bounds
>   /// for `lvlSizes[l]`, and not previously occurring in the same segment).
>   /// For dense levels, this method instead appends the appropriate number
>   /// of zeros to the `values` array, where `full` is the number of "entries"
>   /// already written to `values` for this segment (aka one after the highest
>   /// index previously appended).
>   void appendIndex(uint64_t l, uint64_t full, uint64_t i) {
>     const auto dlt = getLvlType(l); // Avoid redundant bounds checking.
539,541c539,541
<       // "Coordinate value is too large for the C-type"
<       coordinates[lvl].push_back(detail::checkOverflowCast<C>(crd));
<     } else { // Dense level.
---
>       // "Index value is too large for the I-type"
>       indices[l].push_back(detail::checkOverflowCast<I>(i));
>     } else { // Dense dimension.
543,544c543,544
<       assert(crd >= full && "Coordinate was already filled");
<       if (crd == full)
---
>       assert(i >= full && "Index was already filled");
>       if (i == full)
546,547c546,547
<       if (lvl + 1 == getLvlRank())
<         values.insert(values.end(), crd - full, 0);
---
>       if (l + 1 == getLvlRank())
>         values.insert(values.end(), i - full, 0);
549c549
<         finalizeSegment(lvl + 1, 0, crd - full);
---
>         finalizeSegment(l + 1, 0, i - full);
553,558c553,558
<   /// Writes the given coordinate to `coordinates[lvl][pos]`.  This method
<   /// checks that `crd` is representable in the `C` type; however, it
<   /// does not check that `crd` is semantically valid (i.e., in bounds
<   /// for `dimSizes[lvl]` and not elsewhere occurring in the same segment).
<   void writeCrd(uint64_t lvl, uint64_t pos, uint64_t crd) {
<     ASSERT_COMPRESSED_OR_SINGLETON_LVL(lvl);
---
>   /// Writes the given coordinate to `indices[l][pos]`.  This method
>   /// checks that `i` is representable in the `I` type; however, it
>   /// does not check that `i` is semantically valid (i.e., in bounds
>   /// for `dimSizes[l]` and not elsewhere occurring in the same segment).
>   void writeIndex(uint64_t l, uint64_t pos, uint64_t i) {
>     ASSERT_COMPRESSED_OR_SINGLETON_LVL(l);
562c562
<     assert(pos < coordinates[lvl].size() && "Position is out of bounds");
---
>     assert(pos < indices[l].size() && "Index position is out of bounds");
564,565c564,565
<     // "Coordinate value is too large for the C-type"
<     coordinates[lvl][pos] = detail::checkOverflowCast<C>(crd);
---
>     // "Index value is too large for the I-type"
>     indices[l][pos] = detail::checkOverflowCast<I>(i);
574c574
<   /// Precondition: the `positions[l]` array must be fully initialized
---
>   /// Precondition: the `pointers[l]` array must be fully initialized
579c579
<       return positions[l][parentSz];
---
>       return pointers[l][parentSz];
589,590c589,590
<   /// tensor in coordinate scheme. This method prepares the positions and
<   /// coordinates arrays under the given per-level dense/sparse annotations.
---
>   /// tensor in coordinate scheme. This method prepares the pointers and
>   /// indices arrays under the given per-dimension dense/sparse annotations.
594c594
<   /// * the coordinates of every element are valid for `getLvlSizes()`
---
>   /// * the indices of every element are valid for `getLvlSizes()`
600c600
<     // Once levels are exhausted, insert the numerical values.
---
>     // Once dimensions are exhausted, insert the numerical values.
609,610c609,610
<       // Find segment in interval with same coordinate at this level.
<       const uint64_t c = lvlElements[lo].coords[l];
---
>       // Find segment in interval with same index elements in this level.
>       const uint64_t i = lvlElements[lo].indices[l];
613c613
<         while (seg < hi && lvlElements[seg].coords[l] == c)
---
>         while (seg < hi && lvlElements[seg].indices[l] == i)
616,617c616,617
<       appendCrd(l, full, c);
<       full = c + 1;
---
>       appendIndex(l, full, i);
>       full = i + 1;
622c622
<     // Finalize the sparse position structure at this level.
---
>     // Finalize the sparse pointer structure at this level.
626c626
<   /// Finalizes the sparse position structure at this level.
---
>   /// Finalizes the sparse pointer structure at this level.
632c632
<       appendPos(l, coordinates[l].size(), count);
---
>       appendPointer(l, indices[l].size(), count);
664,665c664,665
<   /// argument is the level-coordinates for the value being inserted.
<   void insPath(const uint64_t *lvlCoords, uint64_t diffLvl, uint64_t full,
---
>   /// argument is the storage-level indices for the value being inserted.
>   void insPath(const uint64_t *lvlInd, uint64_t diffLvl, uint64_t topIdx,
670,673c670,673
<       const uint64_t c = lvlCoords[l];
<       appendCrd(l, full, c);
<       full = 0;
<       lvlCursor[l] = c;
---
>       const uint64_t i = lvlInd[l];
>       appendIndex(l, topIdx, i);
>       topIdx = 0;
>       lvlCursor[l] = i;
678c678
<   /// Finds the lexicographically first level where the level-coordinates
---
>   /// Finds the lexicographically first level where the level-indices
680c680
<   uint64_t lexDiff(const uint64_t *lvlCoords) const {
---
>   uint64_t lexDiff(const uint64_t *lvlInd) const {
683c683
<       if (lvlCoords[l] > lvlCursor[l])
---
>       if (lvlInd[l] > lvlCursor[l])
686c686
<         assert(lvlCoords[l] == lvlCursor[l] && "non-lexicographic insertion");
---
>         assert(lvlInd[l] == lvlCursor[l] && "non-lexicographic insertion");
694c694
<   friend class SparseTensorEnumerator<P, C, V>;
---
>   friend class SparseTensorEnumerator<P, I, V>;
696,697c696,697
<   std::vector<std::vector<P>> positions;
<   std::vector<std::vector<C>> coordinates;
---
>   std::vector<std::vector<P>> pointers;
>   std::vector<std::vector<I>> indices;
712c712
< /// and applies a permutation to the coordinates before handing
---
> /// and applies a permutation to the coordinates/indices before handing
722,723c722,723
< /// `SparseTensorEnumerator<P,C,V>` is because we need to hide/generalize
< /// the `<P,C>` template parameters from MLIR client code (to simplify the
---
> /// `SparseTensorEnumerator<P,I,V>` is because we need to hide/generalize
> /// the `<P,I>` template parameters from MLIR client code (to simplify the
725c725
< /// reason we define the `SparseTensorEnumerator<P,C,V>` subclasses rather
---
> /// reason we define the `SparseTensorEnumerator<P,I,V>` subclasses rather
738,739c738,739
<   /// * `src2trg` must be valid for `srcRank`, and must map coordinates
<   ///   valid for `src.getDimSizes()` to coordinates valid for `trgSizes`.
---
>   /// * `src2trg` must be valid for `srcRank`, and must map indices
>   ///   valid for `src.getDimSizes()` to indices valid for `trgSizes`.
782c782
<   /// coordinates, and passes the permuted element to the callback.
---
>   /// indices, and passes the permuted element to the callback.
796c796
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
800c800
<   using StorageImpl = SparseTensorStorage<P, C, V>;
---
>   using StorageImpl = SparseTensorStorage<P, I, V>;
833c833
<     // Recover the `<P,C,V>` type parameters of `src`.
---
>     // Recover the `<P,I,V>` type parameters of `src`.
847,854c847,854
<       const std::vector<P> &positionsL = src.positions[l];
<       assert(parentPos + 1 < positionsL.size() &&
<              "Parent position is out of bounds");
<       const uint64_t pstart = static_cast<uint64_t>(positionsL[parentPos]);
<       const uint64_t pstop = static_cast<uint64_t>(positionsL[parentPos + 1]);
<       // Loop-invariant code for looking up the `l`-level coordinates.
<       const std::vector<C> &coordinatesL = src.coordinates[l];
<       assert(pstop <= coordinatesL.size() && "Stop position is out of bounds");
---
>       const std::vector<P> &pointersL = src.pointers[l];
>       assert(parentPos + 1 < pointersL.size() &&
>              "Parent pointer position is out of bounds");
>       const uint64_t pstart = static_cast<uint64_t>(pointersL[parentPos]);
>       const uint64_t pstop = static_cast<uint64_t>(pointersL[parentPos + 1]);
>       // Loop-invariant code for looking up the `l`-level coordinates/indices.
>       const std::vector<I> &indicesL = src.indices[l];
>       assert(pstop <= indicesL.size() && "Index position is out of bounds");
856c856
<         cursorL = static_cast<uint64_t>(coordinatesL[pos]);
---
>         cursorL = static_cast<uint64_t>(indicesL[pos]);
860c860
<       cursorL = src.getCrd(l, parentPos);
---
>       cursorL = src.getIndex(l, parentPos);
862c862
<     } else { // Dense level.
---
>     } else { // Dense dimension.
866,868c866,868
<       for (uint64_t c = 0; c < sz; ++c) {
<         cursorL = c;
<         forallElements(yield, pstart + c, l + 1);
---
>       for (uint64_t i = 0; i < sz; ++i) {
>         cursorL = i;
>         forallElements(yield, pstart + i, l + 1);
916c916
<         [this](const std::vector<uint64_t> &lvlCoords, V) { add(lvlCoords); });
---
>         [this](const std::vector<uint64_t> &ind, V) { add(ind); });
922c922
<   /// Lexicographically enumerates all coordinates for levels strictly
---
>   /// Lexicographically enumerates all indicies for levels strictly
926c926
<   void forallCoords(uint64_t stopLvl, NNZConsumer yield) const;
---
>   void forallIndices(uint64_t stopLvl, NNZConsumer yield) const;
932c932
<   /// to avoid needing to re-assert validity of `lvlCoords` (which is
---
>   /// to avoid needing to re-assert validity of `lvlInd` (which is
934c934
<   void add(const std::vector<uint64_t> &lvlCoords);
---
>   void add(const std::vector<uint64_t> &lvlInd);
936,938c936,938
<   /// Recursive component of the public `forallCoords`.
<   void forallCoords(NNZConsumer yield, uint64_t stopLvl, uint64_t parentPos,
<                     uint64_t l) const;
---
>   /// Recursive component of the public `forallIndices`.
>   void forallIndices(NNZConsumer yield, uint64_t stopLvl, uint64_t parentPos,
>                      uint64_t l) const;
947c947
< // Definitions of the ctors and factories of `SparseTensorStorage<P,C,V>`.
---
> // Definitions of the ctors and factories of `SparseTensorStorage<P,I,V>`.
949,950c949,950
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V> *SparseTensorStorage<P, C, V>::newFromCOO(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V> *SparseTensorStorage<P, I, V>::newFromCOO(
969c969
<   return new SparseTensorStorage<P, C, V>(dimRank, dimSizes.data(), lvlRank,
---
>   return new SparseTensorStorage<P, I, V>(dimRank, dimSizes.data(), lvlRank,
973,974c973,974
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V> *SparseTensorStorage<P, C, V>::newFromSparseTensor(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V> *SparseTensorStorage<P, I, V>::newFromSparseTensor(
992c992
<   auto *tensor = new SparseTensorStorage<P, C, V>(
---
>   auto *tensor = new SparseTensorStorage<P, I, V>(
998,999c998,999
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage(
1005c1005
<   // Provide hints on capacity of positions and coordinates.
---
>   // Provide hints on capacity of pointers and indices.
1007,1009c1007,1009
<   // we reserve position/coordinate space based on all previous dense
<   // levels, which works well up to first sparse level; but we should
<   // really use nnz and dense/sparse distribution.
---
>   //       we reserve pointer/index space based on all previous dense
>   //       dimensions, which works well up to first sparse dim; but
>   //       we should really use nnz and dense/sparse distribution.
1017,1019c1017,1019
<       positions[l].reserve(sz + 1);
<       positions[l].push_back(0);
<       coordinates[l].reserve(sz);
---
>       pointers[l].reserve(sz + 1);
>       pointers[l].push_back(0);
>       indices[l].reserve(sz);
1023c1023
<       coordinates[l].reserve(sz);
---
>       indices[l].reserve(sz);
1026c1026
<     } else { // Dense level.
---
>     } else { // Dense dimension.
1035,1036c1035,1036
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage( // NOLINT
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage( // NOLINT
1049,1051c1049,1051
<   const uint64_t nse = elements.size();
<   values.reserve(nse);
<   fromCOO(elements, 0, nse, 0);
---
>   uint64_t nnz = elements.size();
>   values.reserve(nnz);
>   fromCOO(elements, 0, nnz, 0);
1054,1055c1054,1055
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage(
1067c1067
<     // Initialize "positions" overhead (and allocate "coordinates", "values").
---
>     // Initialize "pointers" overhead (and allocate "indices", "values").
1072,1073c1072,1073
<         positions[l].reserve(parentSz + 1);
<         positions[l].push_back(0);
---
>         pointers[l].reserve(parentSz + 1);
>         pointers[l].push_back(0);
1075c1075
<         nnz.forallCoords(l, [this, &currentPos, l](uint64_t n) {
---
>         nnz.forallIndices(l, [this, &currentPos, l](uint64_t n) {
1077c1077
<           appendPos(l, currentPos);
---
>           appendPointer(l, currentPos);
1079,1080c1079,1080
<         assert(positions[l].size() == parentSz + 1 &&
<                "Final positions size doesn't match allocated size");
---
>         assert(pointers[l].size() == parentSz + 1 &&
>                "Final pointers size doesn't match allocated size");
1082c1082
<         // is now in a valid state.  That is, `positions[l][parentSz]`
---
>         // is now in a valid state.  That is, `pointers[l][parentSz]`
1084c1084
<         // correct assembled-size for `coordinates[l]`.
---
>         // correct assembled-size for `indices[l]`.
1088c1088
<       // Ideally we need only `coordinates[l].reserve(parentSz)`, however
---
>       // Ideally we need only `indices[l].reserve(parentSz)`, however
1091c1091
<       // to `coordinates[l]`; however, `std::vector`'s subscript-assignment
---
>       // to `indices[l]`; however, `std::vector`'s subscript-assignment
1094c1094
<         coordinates[l].resize(parentSz, 0);
---
>         indices[l].resize(parentSz, 0);
1101c1101
<   lvlEnumerator.forallElements([this](const auto &lvlCoords, V val) {
---
>   lvlEnumerator.forallElements([this](const auto &lvlInd, V val) {
1108c1108
<         // does not represent a segment of `coordinates[l]`.  Moreover, that
---
>         // does not represent a segment of `indices[l]`.  Moreover, that
1110,1111c1110,1111
<         assert(parentPos < parentSz && "Parent position is out of bounds");
<         const uint64_t currentPos = positions[l][parentPos];
---
>         assert(parentPos < parentSz && "Pointers position is out of bounds");
>         const uint64_t currentPos = pointers[l][parentPos];
1113c1113
<         // exceed the original value of `positions[l][parentPos+1]`
---
>         // exceed the original value of `pointers[l][parentPos+1]`
1116,1117c1116,1117
<         positions[l][parentPos]++;
<         writeCrd(l, currentPos, lvlCoords[l]);
---
>         pointers[l][parentPos]++;
>         writeIndex(l, currentPos, lvlInd[l]);
1120c1120
<         writeCrd(l, parentPos, lvlCoords[l]);
---
>         writeIndex(l, parentPos, lvlInd[l]);
1122c1122
<       } else { // Dense level.
---
>       } else { // Dense dimension.
1124c1124
<         parentPos = parentPos * getLvlSizes()[l] + lvlCoords[l];
---
>         parentPos = parentPos * getLvlSizes()[l] + lvlInd[l];
1135,1136c1135,1136
<       assert(parentSz == positions[l].size() - 1 &&
<              "Actual positions size doesn't match the expected size");
---
>       assert(parentSz == pointers[l].size() - 1 &&
>              "Actual pointers size doesn't match the expected size");
1138,1139c1138,1139
<       assert(positions[l][parentSz - 1] == positions[l][parentSz] &&
<              "Positions got corrupted");
---
>       assert(pointers[l][parentSz - 1] == pointers[l][parentSz] &&
>              "Pointers got corrupted");
1143c1143
<         positions[l][parentPos] = positions[l][parentPos - 1];
---
>         pointers[l][parentPos] = pointers[l][parentPos - 1];
1145c1145
<       positions[l][0] = 0;
---
>       pointers[l][0] = 0;
--- include/mlir/ExecutionEngine/SparseTensor/COO.h
30,31c30,31
< /// (i.e., a pair of coordinates and value).  For example, a rank-1
< /// vector element would look like
---
> /// (i.e., a pair of indices and value).  For example, a rank-1 vector
> /// element would look like
36,43c36,43
< /// The coordinates are represented as a (non-owning) pointer into
< /// a shared pool of coordinates, rather than being stored directly in
< /// this object.  This significantly improves performance because it:
< /// (1) reduces the per-element memory footprint, and (2) centralizes
< /// the memory management for coordinates.  The only downside is that
< /// the coordinates themselves cannot be retrieved without knowing the
< /// rank of the tensor to which this element belongs (and that rank is
< /// not stored in this object).
---
> /// The indices are represented as a (non-owning) pointer into a shared
> /// pool of indices, rather than being stored directly in this object.
> /// This significantly improves performance because it: (1) reduces
> /// the per-element memory footprint, and (2) centralizes the memory
> /// management for indices.  The only downside is that the indices
> /// themselves cannot be retrieved without knowing the rank of the
> /// tensor to which this element belongs (and that rank is not stored
> /// in this object).
46,47c46,47
<   Element(const uint64_t *coords, V val) : coords(coords), value(val){};
<   const uint64_t *coords; // pointer into shared coordinates pool
---
>   Element(const uint64_t *ind, V val) : indices(ind), value(val){};
>   const uint64_t *indices; // pointer into shared index pool
61c61
<       if (e1.coords[d] == e2.coords[d])
---
>       if (e1.indices[d] == e2.indices[d])
63c63
<       return e1.coords[d] < e2.coords[d];
---
>       return e1.indices[d] < e2.indices[d];
135c135
<       coordinates.reserve(capacity * dimRank);
---
>       indices.reserve(capacity * dimRank);
139c139
<   /// Gets the dimension-rank of the tensor.
---
>   /// Gets the rank of the tensor.
152c152
<   /// `dimCoords` is already associated with a value, it adds it regardless.
---
>   /// `ind` is already associated with a value, it adds it regardless.
159,169c159,168
<   /// * the `dimCoords` is valid for `getRank`.
<   /// * the components of `dimCoords` are valid for `getDimSizes`.
<   void add(const std::vector<uint64_t> &dimCoords, V val) {
<     const uint64_t *base = coordinates.data();
<     const uint64_t size = coordinates.size();
<     const uint64_t dimRank = getRank();
<     assert(dimCoords.size() == dimRank && "Element rank mismatch");
<     for (uint64_t d = 0; d < dimRank; ++d) {
<       assert(dimCoords[d] < dimSizes[d] &&
<              "Coordinate is too large for the dimension");
<       coordinates.push_back(dimCoords[d]);
---
>   /// * the `ind` is valid for `rank`
>   /// * the elements of `ind` are valid for `dimSizes`.
>   void add(const std::vector<uint64_t> &ind, V val) {
>     const uint64_t *base = indices.data();
>     uint64_t size = indices.size();
>     uint64_t rank = getRank();
>     assert(ind.size() == rank && "Element rank mismatch");
>     for (uint64_t r = 0; r < rank; ++r) {
>       assert(ind[r] < dimSizes[r] && "Index is too large for the dimension");
>       indices.push_back(ind[r]);
171,176c170,175
<     // This base only changes if `coordinates` was reallocated.  In which
<     // case, we need to correct all previous pointers into the vector.
<     // Note that this only happens if we did not set the initial capacity
<     // right, and then only for every internal vector reallocation (which
<     // with the doubling rule should only incur an amortized linear overhead).
<     const uint64_t *const newBase = coordinates.data();
---
>     // This base only changes if indices were reallocated. In that case, we
>     // need to correct all previous pointers into the vector. Note that this
>     // only happens if we did not set the initial capacity right, and then only
>     // for every internal vector reallocation (which with the doubling rule
>     // should only incur an amortized linear overhead).
>     const uint64_t *newBase = indices.data();
179c178
<         elements[i].coords = newBase + (elements[i].coords - base);
---
>         elements[i].indices = newBase + (elements[i].indices - base);
183c182
<     const Element<V> addedElem(base + size, val);
---
>     Element<V> addedElem(base + size, val);
192,194c191,192
<   /// Sorts elements lexicographically by coordinates.  If a coordinate
<   /// is mapped to multiple values, then the relative order of those
<   /// values is unspecified.
---
>   /// Sorts elements lexicographically by index.  If an index is mapped to
>   /// multiple values, then the relative order of those values is unspecified.
207c205
<   std::vector<uint64_t> coordinates;    // shared coordinate pool
---
>   std::vector<uint64_t> indices;        // shared index pool
--- include/mlir/ExecutionEngine/SparseTensor/ArithmeticUtils.h
--- include/mlir/ExecutionEngine/SparseTensor/Storage.h
18c18
< // * `SparseTensorStorage<P, C, V>`
---
> // * `SparseTensorStorage<P, I, V>`
20c20
< // * `SparseTensorEnumerator<P, C, V>`
---
> // * `SparseTensorEnumerator<P, I, V>`
55c55
<   assert(d < getDimRank() && "Dimension is out of bounds");
---
>   assert(d < getDimRank() && "Dimension index is out of bounds");
57c57
<   assert(l < getLvlRank() && "Level is out of bounds");
---
>   assert(l < getLvlRank() && "Level index is out of bounds");
73,74c73,74
< /// Abstract base class for `SparseTensorStorage<P,C,V>`.  This class
< /// takes responsibility for all the `<P,C,V>`-independent aspects
---
> /// Abstract base class for `SparseTensorStorage<P,I,V>`.  This class
> /// takes responsibility for all the `<P,I,V>`-independent aspects
92c92
< /// The *size* of an axis is the cardinality of possible coordinate
---
> /// The *size* of an axis is the cardinality of possible coordinate/index
132c132,133
<   /// * `lvl2dim` must map `lvlSizes`-coordinates to `dimSizes`-coordinates.
---
>   /// * `lvl2dim` must map indices valid for `lvlSizes` to indices valid
>   ///   for `dimSizes`.
215,227c216,227
<   /// Gets positions-overhead storage for the given level.
< #define DECL_GETPOSITIONS(PNAME, P)                                            \
<   virtual void getPositions(std::vector<P> **, uint64_t);
<   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETPOSITIONS)
< #undef DECL_GETPOSITIONS
< 
<   /// Gets coordinates-overhead storage for the given level.
< #define DECL_GETCOORDINATES(INAME, C)                                          \
<   virtual void getCoordinates(std::vector<C> **, uint64_t);
<   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETCOORDINATES)
< #undef DECL_GETCOORDINATES
<   /// Gets the coordinate-value stored at the given level and position.
<   virtual uint64_t getCrd(uint64_t lvl, uint64_t pos) const = 0;
---
>   /// Gets pointers-overhead storage.
> #define DECL_GETPOINTERS(PNAME, P)                                             \
>   virtual void getPointers(std::vector<P> **, uint64_t);
>   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETPOINTERS)
> #undef DECL_GETPOINTERS
> 
>   /// Gets indices-overhead storage.
> #define DECL_GETINDICES(INAME, I)                                              \
>   virtual void getIndices(std::vector<I> **, uint64_t);
>   MLIR_SPARSETENSOR_FOREVERY_FIXED_O(DECL_GETINDICES)
> #undef DECL_GETINDICES
>   virtual uint64_t getIndex(uint64_t l, uint64_t pos) const = 0;
234,235c234,235
<   /// Element-wise insertion in lexicographic coordinate order.  The first
<   /// argument is the level-coordinates for the value being inserted.
---
>   /// Element-wise insertion in lexicographic index order.  The first
>   /// argument is the level-indices for the value being inserted.
237c237
<   // length of `lvlCoords` and check that against `getLvlRank()`.
---
>   // length of `lvlInd` and check that against `getLvlRank()`.
247c247
<   /// * `lvlCoords` the level-coordinates shared by the values being inserted.
---
>   /// * `lvlInd` the level-indices shared by the values being inserted.
272c272
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
276c276
< /// per-level sparse/dense annotations.  This data structure provides
---
> /// per-dimension sparse/dense annotations.  This data structure provides
282c282
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
287c287
<   /// doesn't entail `!(positions[l].empty())`.
---
>   /// doesn't entail `!(pointers[l].empty())`.
295c295
<         positions(lvlRank), coordinates(lvlRank), lvlCursor(lvlRank) {}
---
>         pointers(lvlRank), indices(lvlRank), lvlCursor(lvlRank) {}
342c342
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
346c346
<     return new SparseTensorStorage<P, C, V>(dimRank, dimSizes, lvlRank,
---
>     return new SparseTensorStorage<P, I, V>(dimRank, dimSizes, lvlRank,
368c368
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
378,380c378,380
<   /// * `src2lvl` must be valid for `srcRank`, must map coordinates valid
<   ///    for `source.getDimSizes()` to coordinates valid for `lvlSizes`,
<   ///    and therefore must be the inverse of `lvl2dim`.
---
>   /// * `src2lvl` must be valid for `srcRank`, must map indices valid for
>   ///   `source.getDimSizes()` to indices valid for `lvlSizes`, and therefore
>   ///   must be the inverse of `lvl2dim`.
399c399
<   static SparseTensorStorage<P, C, V> *
---
>   static SparseTensorStorage<P, I, V> *
409c409
<   void getPositions(std::vector<P> **out, uint64_t lvl) final {
---
>   void getPointers(std::vector<P> **out, uint64_t l) final {
411,412c411,412
<     ASSERT_VALID_LVL(lvl);
<     *out = &positions[lvl];
---
>     ASSERT_VALID_LVL(l);
>     *out = &pointers[l];
414c414
<   void getCoordinates(std::vector<C> **out, uint64_t lvl) final {
---
>   void getIndices(std::vector<I> **out, uint64_t l) final {
416,417c416,417
<     ASSERT_VALID_LVL(lvl);
<     *out = &coordinates[lvl];
---
>     ASSERT_VALID_LVL(l);
>     *out = &indices[l];
424,427c424,427
<   uint64_t getCrd(uint64_t lvl, uint64_t pos) const final {
<     ASSERT_COMPRESSED_OR_SINGLETON_LVL(lvl);
<     assert(pos < coordinates[lvl].size() && "Position is out of bounds");
<     return coordinates[lvl][pos]; // Converts the stored `C` into `uint64_t`.
---
>   uint64_t getIndex(uint64_t l, uint64_t pos) const final {
>     ASSERT_COMPRESSED_OR_SINGLETON_LVL(l);
>     assert(pos < indices[l].size() && "Index position is out of bounds");
>     return indices[l][pos]; // Converts the stored `I` into `uint64_t`.
431,432c431,432
<   void lexInsert(const uint64_t *lvlCoords, V val) final {
<     assert(lvlCoords && "Received nullptr for level-coordinates");
---
>   void lexInsert(const uint64_t *lvlInd, V val) final {
>     assert(lvlInd && "Received nullptr for level-indices");
435c435
<     uint64_t full = 0;
---
>     uint64_t topIdx = 0;
437c437
<       diffLvl = lexDiff(lvlCoords);
---
>       diffLvl = lexDiff(lvlInd);
439c439
<       full = lvlCursor[diffLvl] + 1;
---
>       topIdx = lvlCursor[diffLvl] + 1;
442c442
<     insPath(lvlCoords, diffLvl, full, val);
---
>     insPath(lvlInd, diffLvl, topIdx, val);
446c446
<   void expInsert(uint64_t *lvlCoords, V *values, bool *filled, uint64_t *added,
---
>   void expInsert(uint64_t *lvlInd, V *values, bool *filled, uint64_t *added,
448c448
<     assert((lvlCoords && values && filled && added) && "Received nullptr");
---
>     assert((lvlInd && values && filled && added) && "Received nullptr");
455,460c455,460
<     uint64_t c = added[0];
<     assert(filled[c] && "added coordinate is not filled");
<     lvlCoords[lastLvl] = c;
<     lexInsert(lvlCoords, values[c]);
<     values[c] = 0;
<     filled[c] = false;
---
>     uint64_t index = added[0];
>     assert(filled[index] && "added index is not filled");
>     lvlInd[lastLvl] = index;
>     lexInsert(lvlInd, values[index]);
>     values[index] = 0;
>     filled[index] = false;
463,469c463,469
<       assert(c < added[i] && "non-lexicographic insertion");
<       c = added[i];
<       assert(filled[c] && "added coordinate is not filled");
<       lvlCoords[lastLvl] = c;
<       insPath(lvlCoords, lastLvl, added[i - 1] + 1, values[c]);
<       values[c] = 0;
<       filled[c] = false;
---
>       assert(index < added[i] && "non-lexicographic insertion");
>       index = added[i];
>       assert(filled[index] && "added index is not filled");
>       lvlInd[lastLvl] = index;
>       insPath(lvlInd, lastLvl, added[i - 1] + 1, values[index]);
>       values[index] = 0;
>       filled[index] = false;
481,482c481,482
<   /// Allocates a new enumerator for this class's `<P,C,V>` types and
<   /// erase the `<P,C>` parts from the type.  Callers must make sure to
---
>   /// Allocates a new enumerator for this class's `<P,I,V>` types and
>   /// erase the `<P,I>` parts from the type.  Callers must make sure to
488c488
<     *out = new SparseTensorEnumerator<P, C, V>(*this, trgRank, trgSizes,
---
>     *out = new SparseTensorEnumerator<P, I, V>(*this, trgRank, trgSizes,
501c501
<     SparseTensorEnumerator<P, C, V> enumerator(*this, trgRank, trgSizes,
---
>     SparseTensorEnumerator<P, I, V> enumerator(*this, trgRank, trgSizes,
505c505
<         [&coo](const auto &trgCoords, V val) { coo->add(trgCoords, val); });
---
>         [&coo](const auto &trgInd, V val) { coo->add(trgInd, val); });
514c514
<   /// Appends an arbitrary new position to `positions[lvl]`.  This method
---
>   /// Appends an arbitrary new position to `pointers[l]`.  This method
517,519c517,519
<   /// the previous position and smaller than `coordinates[lvl].capacity()`).
<   void appendPos(uint64_t lvl, uint64_t pos, uint64_t count = 1) {
<     ASSERT_COMPRESSED_LVL(lvl);
---
>   /// the previous position and smaller than `indices[l].capacity()`).
>   void appendPointer(uint64_t l, uint64_t pos, uint64_t count = 1) {
>     ASSERT_COMPRESSED_LVL(l);
521,536c521,536
<     // "Position value is too large for the P-type"
<     positions[lvl].insert(positions[lvl].end(), count,
<                           detail::checkOverflowCast<P>(pos));
<   }
< 
<   /// Appends coordinate `crd` to level `lvl`, in the semantically
<   /// general sense.  For non-dense levels, that means appending to the
<   /// `coordinates[lvl]` array, checking that `crd` is representable in
<   /// the `C` type; however, we do not verify other semantic requirements
<   /// (e.g., that `crd` is in bounds for `lvlSizes[lvl]`, and not previously
<   /// occurring in the same segment).  For dense levels, this method instead
<   /// appends the appropriate number of zeros to the `values` array, where
<   /// `full` is the number of "entries" already written to `values` for this
<   /// segment (aka one after the highest coordinate previously appended).
<   void appendCrd(uint64_t lvl, uint64_t full, uint64_t crd) {
<     const auto dlt = getLvlType(lvl); // Avoid redundant bounds checking.
---
>     // "Pointer value is too large for the P-type"
>     pointers[l].insert(pointers[l].end(), count,
>                        detail::checkOverflowCast<P>(pos));
>   }
> 
>   /// Appends index `i` to level `l`, in the semantically general sense.
>   /// For non-dense levels, that means appending to the `indices[l]` array,
>   /// checking that `i` is representable in the `I` type; however, we do
>   /// not verify other semantic requirements (e.g., that `i` is in bounds
>   /// for `lvlSizes[l]`, and not previously occurring in the same segment).
>   /// For dense levels, this method instead appends the appropriate number
>   /// of zeros to the `values` array, where `full` is the number of "entries"
>   /// already written to `values` for this segment (aka one after the highest
>   /// index previously appended).
>   void appendIndex(uint64_t l, uint64_t full, uint64_t i) {
>     const auto dlt = getLvlType(l); // Avoid redundant bounds checking.
539,541c539,541
<       // "Coordinate value is too large for the C-type"
<       coordinates[lvl].push_back(detail::checkOverflowCast<C>(crd));
<     } else { // Dense level.
---
>       // "Index value is too large for the I-type"
>       indices[l].push_back(detail::checkOverflowCast<I>(i));
>     } else { // Dense dimension.
543,544c543,544
<       assert(crd >= full && "Coordinate was already filled");
<       if (crd == full)
---
>       assert(i >= full && "Index was already filled");
>       if (i == full)
546,547c546,547
<       if (lvl + 1 == getLvlRank())
<         values.insert(values.end(), crd - full, 0);
---
>       if (l + 1 == getLvlRank())
>         values.insert(values.end(), i - full, 0);
549c549
<         finalizeSegment(lvl + 1, 0, crd - full);
---
>         finalizeSegment(l + 1, 0, i - full);
553,558c553,558
<   /// Writes the given coordinate to `coordinates[lvl][pos]`.  This method
<   /// checks that `crd` is representable in the `C` type; however, it
<   /// does not check that `crd` is semantically valid (i.e., in bounds
<   /// for `dimSizes[lvl]` and not elsewhere occurring in the same segment).
<   void writeCrd(uint64_t lvl, uint64_t pos, uint64_t crd) {
<     ASSERT_COMPRESSED_OR_SINGLETON_LVL(lvl);
---
>   /// Writes the given coordinate to `indices[l][pos]`.  This method
>   /// checks that `i` is representable in the `I` type; however, it
>   /// does not check that `i` is semantically valid (i.e., in bounds
>   /// for `dimSizes[l]` and not elsewhere occurring in the same segment).
>   void writeIndex(uint64_t l, uint64_t pos, uint64_t i) {
>     ASSERT_COMPRESSED_OR_SINGLETON_LVL(l);
562c562
<     assert(pos < coordinates[lvl].size() && "Position is out of bounds");
---
>     assert(pos < indices[l].size() && "Index position is out of bounds");
564,565c564,565
<     // "Coordinate value is too large for the C-type"
<     coordinates[lvl][pos] = detail::checkOverflowCast<C>(crd);
---
>     // "Index value is too large for the I-type"
>     indices[l][pos] = detail::checkOverflowCast<I>(i);
574c574
<   /// Precondition: the `positions[l]` array must be fully initialized
---
>   /// Precondition: the `pointers[l]` array must be fully initialized
579c579
<       return positions[l][parentSz];
---
>       return pointers[l][parentSz];
589,590c589,590
<   /// tensor in coordinate scheme. This method prepares the positions and
<   /// coordinates arrays under the given per-level dense/sparse annotations.
---
>   /// tensor in coordinate scheme. This method prepares the pointers and
>   /// indices arrays under the given per-dimension dense/sparse annotations.
594c594
<   /// * the coordinates of every element are valid for `getLvlSizes()`
---
>   /// * the indices of every element are valid for `getLvlSizes()`
600c600
<     // Once levels are exhausted, insert the numerical values.
---
>     // Once dimensions are exhausted, insert the numerical values.
609,610c609,610
<       // Find segment in interval with same coordinate at this level.
<       const uint64_t c = lvlElements[lo].coords[l];
---
>       // Find segment in interval with same index elements in this level.
>       const uint64_t i = lvlElements[lo].indices[l];
613c613
<         while (seg < hi && lvlElements[seg].coords[l] == c)
---
>         while (seg < hi && lvlElements[seg].indices[l] == i)
616,617c616,617
<       appendCrd(l, full, c);
<       full = c + 1;
---
>       appendIndex(l, full, i);
>       full = i + 1;
622c622
<     // Finalize the sparse position structure at this level.
---
>     // Finalize the sparse pointer structure at this level.
626c626
<   /// Finalizes the sparse position structure at this level.
---
>   /// Finalizes the sparse pointer structure at this level.
632c632
<       appendPos(l, coordinates[l].size(), count);
---
>       appendPointer(l, indices[l].size(), count);
664,665c664,665
<   /// argument is the level-coordinates for the value being inserted.
<   void insPath(const uint64_t *lvlCoords, uint64_t diffLvl, uint64_t full,
---
>   /// argument is the storage-level indices for the value being inserted.
>   void insPath(const uint64_t *lvlInd, uint64_t diffLvl, uint64_t topIdx,
670,673c670,673
<       const uint64_t c = lvlCoords[l];
<       appendCrd(l, full, c);
<       full = 0;
<       lvlCursor[l] = c;
---
>       const uint64_t i = lvlInd[l];
>       appendIndex(l, topIdx, i);
>       topIdx = 0;
>       lvlCursor[l] = i;
678c678
<   /// Finds the lexicographically first level where the level-coordinates
---
>   /// Finds the lexicographically first level where the level-indices
680c680
<   uint64_t lexDiff(const uint64_t *lvlCoords) const {
---
>   uint64_t lexDiff(const uint64_t *lvlInd) const {
683c683
<       if (lvlCoords[l] > lvlCursor[l])
---
>       if (lvlInd[l] > lvlCursor[l])
686c686
<         assert(lvlCoords[l] == lvlCursor[l] && "non-lexicographic insertion");
---
>         assert(lvlInd[l] == lvlCursor[l] && "non-lexicographic insertion");
694c694
<   friend class SparseTensorEnumerator<P, C, V>;
---
>   friend class SparseTensorEnumerator<P, I, V>;
696,697c696,697
<   std::vector<std::vector<P>> positions;
<   std::vector<std::vector<C>> coordinates;
---
>   std::vector<std::vector<P>> pointers;
>   std::vector<std::vector<I>> indices;
712c712
< /// and applies a permutation to the coordinates before handing
---
> /// and applies a permutation to the coordinates/indices before handing
722,723c722,723
< /// `SparseTensorEnumerator<P,C,V>` is because we need to hide/generalize
< /// the `<P,C>` template parameters from MLIR client code (to simplify the
---
> /// `SparseTensorEnumerator<P,I,V>` is because we need to hide/generalize
> /// the `<P,I>` template parameters from MLIR client code (to simplify the
725c725
< /// reason we define the `SparseTensorEnumerator<P,C,V>` subclasses rather
---
> /// reason we define the `SparseTensorEnumerator<P,I,V>` subclasses rather
738,739c738,739
<   /// * `src2trg` must be valid for `srcRank`, and must map coordinates
<   ///   valid for `src.getDimSizes()` to coordinates valid for `trgSizes`.
---
>   /// * `src2trg` must be valid for `srcRank`, and must map indices
>   ///   valid for `src.getDimSizes()` to indices valid for `trgSizes`.
782c782
<   /// coordinates, and passes the permuted element to the callback.
---
>   /// indices, and passes the permuted element to the callback.
796c796
< template <typename P, typename C, typename V>
---
> template <typename P, typename I, typename V>
800c800
<   using StorageImpl = SparseTensorStorage<P, C, V>;
---
>   using StorageImpl = SparseTensorStorage<P, I, V>;
833c833
<     // Recover the `<P,C,V>` type parameters of `src`.
---
>     // Recover the `<P,I,V>` type parameters of `src`.
847,854c847,854
<       const std::vector<P> &positionsL = src.positions[l];
<       assert(parentPos + 1 < positionsL.size() &&
<              "Parent position is out of bounds");
<       const uint64_t pstart = static_cast<uint64_t>(positionsL[parentPos]);
<       const uint64_t pstop = static_cast<uint64_t>(positionsL[parentPos + 1]);
<       // Loop-invariant code for looking up the `l`-level coordinates.
<       const std::vector<C> &coordinatesL = src.coordinates[l];
<       assert(pstop <= coordinatesL.size() && "Stop position is out of bounds");
---
>       const std::vector<P> &pointersL = src.pointers[l];
>       assert(parentPos + 1 < pointersL.size() &&
>              "Parent pointer position is out of bounds");
>       const uint64_t pstart = static_cast<uint64_t>(pointersL[parentPos]);
>       const uint64_t pstop = static_cast<uint64_t>(pointersL[parentPos + 1]);
>       // Loop-invariant code for looking up the `l`-level coordinates/indices.
>       const std::vector<I> &indicesL = src.indices[l];
>       assert(pstop <= indicesL.size() && "Index position is out of bounds");
856c856
<         cursorL = static_cast<uint64_t>(coordinatesL[pos]);
---
>         cursorL = static_cast<uint64_t>(indicesL[pos]);
860c860
<       cursorL = src.getCrd(l, parentPos);
---
>       cursorL = src.getIndex(l, parentPos);
862c862
<     } else { // Dense level.
---
>     } else { // Dense dimension.
866,868c866,868
<       for (uint64_t c = 0; c < sz; ++c) {
<         cursorL = c;
<         forallElements(yield, pstart + c, l + 1);
---
>       for (uint64_t i = 0; i < sz; ++i) {
>         cursorL = i;
>         forallElements(yield, pstart + i, l + 1);
916c916
<         [this](const std::vector<uint64_t> &lvlCoords, V) { add(lvlCoords); });
---
>         [this](const std::vector<uint64_t> &ind, V) { add(ind); });
922c922
<   /// Lexicographically enumerates all coordinates for levels strictly
---
>   /// Lexicographically enumerates all indicies for levels strictly
926c926
<   void forallCoords(uint64_t stopLvl, NNZConsumer yield) const;
---
>   void forallIndices(uint64_t stopLvl, NNZConsumer yield) const;
932c932
<   /// to avoid needing to re-assert validity of `lvlCoords` (which is
---
>   /// to avoid needing to re-assert validity of `lvlInd` (which is
934c934
<   void add(const std::vector<uint64_t> &lvlCoords);
---
>   void add(const std::vector<uint64_t> &lvlInd);
936,938c936,938
<   /// Recursive component of the public `forallCoords`.
<   void forallCoords(NNZConsumer yield, uint64_t stopLvl, uint64_t parentPos,
<                     uint64_t l) const;
---
>   /// Recursive component of the public `forallIndices`.
>   void forallIndices(NNZConsumer yield, uint64_t stopLvl, uint64_t parentPos,
>                      uint64_t l) const;
947c947
< // Definitions of the ctors and factories of `SparseTensorStorage<P,C,V>`.
---
> // Definitions of the ctors and factories of `SparseTensorStorage<P,I,V>`.
949,950c949,950
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V> *SparseTensorStorage<P, C, V>::newFromCOO(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V> *SparseTensorStorage<P, I, V>::newFromCOO(
969c969
<   return new SparseTensorStorage<P, C, V>(dimRank, dimSizes.data(), lvlRank,
---
>   return new SparseTensorStorage<P, I, V>(dimRank, dimSizes.data(), lvlRank,
973,974c973,974
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V> *SparseTensorStorage<P, C, V>::newFromSparseTensor(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V> *SparseTensorStorage<P, I, V>::newFromSparseTensor(
992c992
<   auto *tensor = new SparseTensorStorage<P, C, V>(
---
>   auto *tensor = new SparseTensorStorage<P, I, V>(
998,999c998,999
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage(
1005c1005
<   // Provide hints on capacity of positions and coordinates.
---
>   // Provide hints on capacity of pointers and indices.
1007,1009c1007,1009
<   // we reserve position/coordinate space based on all previous dense
<   // levels, which works well up to first sparse level; but we should
<   // really use nnz and dense/sparse distribution.
---
>   //       we reserve pointer/index space based on all previous dense
>   //       dimensions, which works well up to first sparse dim; but
>   //       we should really use nnz and dense/sparse distribution.
1017,1019c1017,1019
<       positions[l].reserve(sz + 1);
<       positions[l].push_back(0);
<       coordinates[l].reserve(sz);
---
>       pointers[l].reserve(sz + 1);
>       pointers[l].push_back(0);
>       indices[l].reserve(sz);
1023c1023
<       coordinates[l].reserve(sz);
---
>       indices[l].reserve(sz);
1026c1026
<     } else { // Dense level.
---
>     } else { // Dense dimension.
1035,1036c1035,1036
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage( // NOLINT
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage( // NOLINT
1049,1051c1049,1051
<   const uint64_t nse = elements.size();
<   values.reserve(nse);
<   fromCOO(elements, 0, nse, 0);
---
>   uint64_t nnz = elements.size();
>   values.reserve(nnz);
>   fromCOO(elements, 0, nnz, 0);
1054,1055c1054,1055
< template <typename P, typename C, typename V>
< SparseTensorStorage<P, C, V>::SparseTensorStorage(
---
> template <typename P, typename I, typename V>
> SparseTensorStorage<P, I, V>::SparseTensorStorage(
1067c1067
<     // Initialize "positions" overhead (and allocate "coordinates", "values").
---
>     // Initialize "pointers" overhead (and allocate "indices", "values").
1072,1073c1072,1073
<         positions[l].reserve(parentSz + 1);
<         positions[l].push_back(0);
---
>         pointers[l].reserve(parentSz + 1);
>         pointers[l].push_back(0);
1075c1075
<         nnz.forallCoords(l, [this, &currentPos, l](uint64_t n) {
---
>         nnz.forallIndices(l, [this, &currentPos, l](uint64_t n) {
1077c1077
<           appendPos(l, currentPos);
---
>           appendPointer(l, currentPos);
1079,1080c1079,1080
<         assert(positions[l].size() == parentSz + 1 &&
<                "Final positions size doesn't match allocated size");
---
>         assert(pointers[l].size() == parentSz + 1 &&
>                "Final pointers size doesn't match allocated size");
1082c1082
<         // is now in a valid state.  That is, `positions[l][parentSz]`
---
>         // is now in a valid state.  That is, `pointers[l][parentSz]`
1084c1084
<         // correct assembled-size for `coordinates[l]`.
---
>         // correct assembled-size for `indices[l]`.
1088c1088
<       // Ideally we need only `coordinates[l].reserve(parentSz)`, however
---
>       // Ideally we need only `indices[l].reserve(parentSz)`, however
1091c1091
<       // to `coordinates[l]`; however, `std::vector`'s subscript-assignment
---
>       // to `indices[l]`; however, `std::vector`'s subscript-assignment
1094c1094
<         coordinates[l].resize(parentSz, 0);
---
>         indices[l].resize(parentSz, 0);
1101c1101
<   lvlEnumerator.forallElements([this](const auto &lvlCoords, V val) {
---
>   lvlEnumerator.forallElements([this](const auto &lvlInd, V val) {
1108c1108
<         // does not represent a segment of `coordinates[l]`.  Moreover, that
---
>         // does not represent a segment of `indices[l]`.  Moreover, that
1110,1111c1110,1111
<         assert(parentPos < parentSz && "Parent position is out of bounds");
<         const uint64_t currentPos = positions[l][parentPos];
---
>         assert(parentPos < parentSz && "Pointers position is out of bounds");
>         const uint64_t currentPos = pointers[l][parentPos];
1113c1113
<         // exceed the original value of `positions[l][parentPos+1]`
---
>         // exceed the original value of `pointers[l][parentPos+1]`
1116,1117c1116,1117
<         positions[l][parentPos]++;
<         writeCrd(l, currentPos, lvlCoords[l]);
---
>         pointers[l][parentPos]++;
>         writeIndex(l, currentPos, lvlInd[l]);
1120c1120
<         writeCrd(l, parentPos, lvlCoords[l]);
---
>         writeIndex(l, parentPos, lvlInd[l]);
1122c1122
<       } else { // Dense level.
---
>       } else { // Dense dimension.
1124c1124
<         parentPos = parentPos * getLvlSizes()[l] + lvlCoords[l];
---
>         parentPos = parentPos * getLvlSizes()[l] + lvlInd[l];
1135,1136c1135,1136
<       assert(parentSz == positions[l].size() - 1 &&
<              "Actual positions size doesn't match the expected size");
---
>       assert(parentSz == pointers[l].size() - 1 &&
>              "Actual pointers size doesn't match the expected size");
1138,1139c1138,1139
<       assert(positions[l][parentSz - 1] == positions[l][parentSz] &&
<              "Positions got corrupted");
---
>       assert(pointers[l][parentSz - 1] == pointers[l][parentSz] &&
>              "Pointers got corrupted");
1143c1143
<         positions[l][parentPos] = positions[l][parentPos - 1];
---
>         pointers[l][parentPos] = pointers[l][parentPos - 1];
1145c1145
<       positions[l][0] = 0;
---
>       pointers[l][0] = 0;
--- include/mlir/ExecutionEngine/SparseTensor/ErrorHandling.h
--- include/mlir/ExecutionEngine/SparseTensor/Attributes.h
--- include/mlir/ExecutionEngine/SparseTensor/File.h
49c49
< inline std::enable_if_t<!is_complex<V>::value, V> readValue(char **linePtr) {
---
> inline std::enable_if_t<!is_complex<V>::value, V> readCOOValue(char **linePtr) {
62c62
< inline std::enable_if_t<is_complex<V>::value, V> readValue(char **linePtr) {
---
> inline std::enable_if_t<is_complex<V>::value, V> readCOOValue(char **linePtr) {
75,76c75,76
< /// Returns an element-value.  If `isPattern` is true, then returns an
< /// arbitrary value.  If `isPattern` is false, then reads the value from
---
> /// Returns an element-value.  If `is_pattern` is true, then returns an
> /// arbitrary value.  If `is_pattern` is false, then reads the value from
79,80c79,82
< inline V readValue(char **linePtr, bool isPattern) {
<   return isPattern ? readValue<V, true>(linePtr) : readValue<V, false>(linePtr);
---
> inline V readCOOValue(char **linePtr, bool is_pattern) {
>   if (is_pattern)
>     return readCOOValue<V, true>(linePtr);
>   return readCOOValue<V, false>(linePtr);
172,173c174
<   /// Gets the dimension-rank of the tensor.  Is only valid after parsing
<   /// the header.
---
>   /// Gets the rank of the tensor.  Is only valid after parsing the header.
179,182c180,182
<   /// Gets the number of stored elements.  Is only valid after parsing
<   /// the header.
<   uint64_t getNSE() const {
<     assert(isValid() && "Attempt to getNSE() before readHeader()");
---
>   /// Gets the number of non-zeros.  Is only valid after parsing the header.
>   uint64_t getNNZ() const {
>     assert(isValid() && "Attempt to getNNZ() before readHeader()");
204c204
<   /// to the `dimCoords` array.
---
>   /// to the `indices` array.
206,209c206,209
<   V readElement(uint64_t dimRank, uint64_t *dimCoords) {
<     assert(dimRank == getRank() && "rank mismatch");
<     char *linePtr = readCoords(dimCoords);
<     return detail::readValue<V>(&linePtr, isPattern());
---
>   V readCOOElement(uint64_t rank, uint64_t *indices) {
>     assert(rank == getRank() && "rank mismatch");
>     char *linePtr = readCOOIndices(indices);
>     return detail::readCOOValue<V>(&linePtr, isPattern());
213,214c213,214
<   /// all the elements from the file and applying `dim2lvl` to their
<   /// dim-coordinates, and then closes the file.
---
>   /// all the elements from the file and applying `dim2lvl` to their indices,
>   /// and then closes the file.
219c219,220
<   /// * `dim2lvl` maps `getDimSizes()`-coordinates to `lvlSizes`-coordinates.
---
>   /// * `dim2lvl` maps indices valid for `getDimSizes()` to indices
>   ///   valid for `lvlSizes`.
251,258d251
<   /// Reads the COO tensor from the file, stores the coordinates and values to
<   /// the given buffers, returns a boolean value to indicate whether the COO
<   /// elements are sorted.
<   /// Precondition: the buffers should have enough space to hold the elements.
<   template <typename C, typename V>
<   bool readToBuffers(uint64_t lvlRank, const uint64_t *dim2lvl,
<                      C *lvlCoordinates, V *values);
< 
265c258
<   /// into the `dimCoords` argument.  Returns the position in the `line`
---
>   /// into the `indices` argument.  Returns the position in the `line`
267c260
<   /// has been factored out from `readElement` to minimize code bloat
---
>   /// has been factored out from `readCOOElement` to minimize code bloat
270,283c263,264
<   /// Precondition: `dimCoords` is valid for `getRank()`.
<   template <typename C>
<   char *readCoords(C *dimCoords) {
<     readLine();
<     // Local variable for tracking the parser's position in the `line` buffer.
<     char *linePtr = line;
<     for (uint64_t dimRank = getRank(), d = 0; d < dimRank; ++d) {
<       // Parse the 1-based coordinate.
<       uint64_t c = strtoul(linePtr, &linePtr, 10);
<       // Store the 0-based coordinate.
<       dimCoords[d] = static_cast<C>(c - 1);
<     }
<     return linePtr;
<   }
---
>   /// Precondition: `indices` is valid for `getRank()`.
>   char *readCOOIndices(uint64_t *indices);
286,287c267,268
<   /// `IsPattern` in order to perform LICM without needing to duplicate the
<   /// source code.
---
>   /// `IsPattern` and `IsSymmetric` in order to perform LICM without
>   /// needing to duplicate the source code.
293c274
<   template <typename V, bool IsPattern>
---
>   template <typename V, bool IsPattern, bool IsSymmetric>
297,303d277
<   /// The internal implementation of `readToBuffers`.  We template over
<   /// `IsPattern` in order to perform LICM without needing to duplicate the
<   /// source code.
<   template <typename C, typename V, bool IsPattern>
<   bool readToBuffersLoop(uint64_t lvlRank, detail::PermutationRef dim2lvl,
<                          C *lvlCoordinates, V *values);
< 
336,337c310,311
<   // Prepare a COO object with the number of stored elems as initial capacity.
<   auto *lvlCOO = new SparseTensorCOO<V>(lvlRank, lvlSizes, getNSE());
---
>   // Prepare a COO object with the number of nonzeros as initial capacity.
>   auto *lvlCOO = new SparseTensorCOO<V>(lvlRank, lvlSizes, getNNZ());
340,341c314,320
<   if (IsPattern)
<     readCOOLoop<V, true>(lvlRank, d2l, lvlCOO);
---
>   const bool IsSymmetric = (isSymmetric() && getRank() == 2);
>   if (IsPattern && IsSymmetric)
>     readCOOLoop<V, true, true>(lvlRank, d2l, lvlCOO);
>   else if (IsPattern)
>     readCOOLoop<V, true, false>(lvlRank, d2l, lvlCOO);
>   else if (IsSymmetric)
>     readCOOLoop<V, false, true>(lvlRank, d2l, lvlCOO);
343c322
<     readCOOLoop<V, false>(lvlRank, d2l, lvlCOO);
---
>     readCOOLoop<V, false, false>(lvlRank, d2l, lvlCOO);
349c328
< template <typename V, bool IsPattern>
---
> template <typename V, bool IsPattern, bool IsSymmetric>
354,357c333,336
<   std::vector<uint64_t> dimCoords(dimRank);
<   std::vector<uint64_t> lvlCoords(lvlRank);
<   for (uint64_t nse = getNSE(), k = 0; k < nse; ++k) {
<     // We inline `readElement` here in order to avoid redundant
---
>   std::vector<uint64_t> dimInd(dimRank);
>   std::vector<uint64_t> lvlInd(lvlRank);
>   for (uint64_t nnz = getNNZ(), k = 0; k < nnz; ++k) {
>     // We inline `readCOOElement` here in order to avoid redundant
359,362c338,341
<     // and the construction of `dimCoords` above.
<     char *linePtr = readCoords(dimCoords.data());
<     const V value = detail::readValue<V, IsPattern>(&linePtr);
<     dim2lvl.pushforward(dimRank, dimCoords.data(), lvlCoords.data());
---
>     // and the construction of `dimInd` above.
>     char *linePtr = readCOOIndices(dimInd.data());
>     const V value = detail::readCOOValue<V, IsPattern>(&linePtr);
>     dim2lvl.pushforward(dimRank, dimInd.data(), lvlInd.data());
364,421c343,352
<     lvlCOO->add(lvlCoords, value);
<   }
< }
< 
< template <typename C, typename V>
< bool SparseTensorReader::readToBuffers(uint64_t lvlRank,
<                                        const uint64_t *dim2lvl,
<                                        C *lvlCoordinates, V *values) {
<   assert(isValid() && "Attempt to readCOO() before readHeader()");
<   // Construct a `PermutationRef` for the `pushforward` below.
<   // TODO: This specific implementation does not generalize to arbitrary
<   // mappings, but once we functionalize the `dim2lvl` argument we can
<   // simply use that function instead.
<   const uint64_t dimRank = getRank();
<   assert(lvlRank == dimRank && "Rank mismatch");
<   detail::PermutationRef d2l(dimRank, dim2lvl);
<   // Do some manual LICM, to avoid assertions in the for-loop.
<   bool isSorted =
<       isPattern()
<           ? readToBuffersLoop<C, V, true>(lvlRank, d2l, lvlCoordinates, values)
<           : readToBuffersLoop<C, V, false>(lvlRank, d2l, lvlCoordinates,
<                                            values);
< 
<   // Close the file and return isSorted.
<   closeFile();
<   return isSorted;
< }
< 
< template <typename C, typename V, bool IsPattern>
< bool SparseTensorReader::readToBuffersLoop(uint64_t lvlRank,
<                                            detail::PermutationRef dim2lvl,
<                                            C *lvlCoordinates, V *values) {
<   const uint64_t dimRank = getRank();
<   const uint64_t nse = getNSE();
<   std::vector<C> dimCoords(dimRank);
<   // Read the first element with isSorted=false as a way to avoid accessing its
<   // previous element.
<   bool isSorted = false;
<   char *linePtr;
<   // We inline `readElement` here in order to avoid redundant assertions,
<   // since they're guaranteed by the call to `isValid()` and the construction
<   // of `dimCoords` above.
<   const auto readNextElement = [&]() {
<     linePtr = readCoords<C>(dimCoords.data());
<     dim2lvl.pushforward(dimRank, dimCoords.data(), lvlCoordinates);
<     *values = detail::readValue<V, IsPattern>(&linePtr);
<     if (isSorted) {
<       // Note that isSorted was set to false while reading the first element,
<       // to guarantee the safeness of using prevLvlCoords.
<       C *prevLvlCoords = lvlCoordinates - lvlRank;
<       // TODO: define a new CoordsLT which is like ElementLT but doesn't have
<       // the V parameter, and use it here.
<       for (uint64_t l = 0; l < lvlRank; ++l) {
<         if (prevLvlCoords[l] != lvlCoordinates[l]) {
<           if (prevLvlCoords[l] > lvlCoordinates[l])
<             isSorted = false;
<           break;
<         }
---
>     lvlCOO->add(lvlInd, value);
>     // We currently chose to deal with symmetric matrices by fully
>     // constructing them.  In the future, we may want to make symmetry
>     // implicit for storage reasons.
>     if constexpr (IsSymmetric)
>       if (dimInd[0] != dimInd[1]) {
>         // Must recompute `lvlInd`, since arbitrary maps don't preserve swap.
>         std::swap(dimInd[0], dimInd[1]);
>         dim2lvl.pushforward(dimRank, dimInd.data(), lvlInd.data());
>         lvlCOO->add(lvlInd, value);
423,432c354
<     }
<     lvlCoordinates += lvlRank;
<     ++values;
<   };
<   readNextElement();
<   isSorted = true;
<   for (uint64_t n = 1; n < nse; ++n)
<     readNextElement();
< 
<   return isSorted;
---
>   }
440,443c362,365
<   const auto &dimSizes = coo.getDimSizes();
<   const auto &elements = coo.getElements();
<   const uint64_t dimRank = coo.getRank();
<   const uint64_t nse = elements.size();
---
>   auto &dimSizes = coo.getDimSizes();
>   auto &elements = coo.getElements();
>   const uint64_t rank = coo.getRank();
>   const uint64_t nnz = elements.size();
447,454c369,376
<   file << "; extended FROSTT format\n" << dimRank << " " << nse << std::endl;
<   for (uint64_t d = 0; d < dimRank - 1; ++d)
<     file << dimSizes[d] << " ";
<   file << dimSizes[dimRank - 1] << std::endl;
<   for (uint64_t i = 0; i < nse; ++i) {
<     const auto &coords = elements[i].coords;
<     for (uint64_t d = 0; d < dimRank; ++d)
<       file << (coords[d] + 1) << " ";
---
>   file << "; extended FROSTT format\n" << rank << " " << nnz << std::endl;
>   for (uint64_t r = 0; r < rank - 1; ++r)
>     file << dimSizes[r] << " ";
>   file << dimSizes[rank - 1] << std::endl;
>   for (uint64_t i = 0; i < nnz; ++i) {
>     auto &idx = elements[i].indices;
>     for (uint64_t r = 0; r < rank; ++r)
>       file << (idx[r] + 1) << " ";
--- include/mlir/ExecutionEngine/SparseTensor/PermutationRef.h
--- include/mlir/Target
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/Cpp and include/mlir/Target/Cpp
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR and include/mlir/Target/LLVMIR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/SPIRV and include/mlir/Target/SPIRV
--- include/mlir/Target/LLVMIR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect and include/mlir/Target/LLVMIR/Dialect
diff ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/ModuleImport.h include/mlir/Target/LLVMIR/ModuleImport.h
35d34
< class DataLayoutImporter;
37d35
< class LoopAnnotationImporter;
62,65d59
<   /// Converts the data layout of the LLVM module to an MLIR data layout
<   /// specification.
<   LogicalResult convertDataLayout();
< 
127,131d120
<   /// Converts an LLVM metadata value to an MLIR value, or returns failure if
<   /// the conversion fails. Uses the `convertConstant` method to translate
<   /// constant LLVM values.
<   FailureOr<Value> convertMetadataValue(llvm::Value *value);
< 
143,147d131
<   /// Converts `value` to an array of symbol references pointing to alias scope
<   /// operations, or returns failure if the conversion fails.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   matchAliasScopeAttrs(llvm::Value *value);
< 
168,174c152,154
<   /// Converts all LLVM metadata nodes that translate to operations nested in a
<   /// global metadata operation, such as alias analysis or access group
<   /// metadata, and builds a map from the metadata nodes to the symbols pointing
<   /// to the converted operations. Returns success if all conversions succeed
<   /// and failure otherwise.
<   // Note: All metadata is nested inside a single global metadata operation to
<   // minimize the number of symbols that pollute the global namespace.
---
>   /// Converts LLVM metadata to corresponding MLIR representation,
>   /// e.g. metadata nodes referenced via !tbaa are converted to
>   /// TBAA operations hosted inside a MetadataOp.
177,179c157,159
<   /// Returns the MLIR symbol reference mapped to the given LLVM TBAA
<   /// metadata `node`.
<   SymbolRefAttr lookupTBAAAttr(const llvm::MDNode *node) const {
---
>   /// Returns SymbolRefAttr representing TBAA metadata `node`
>   /// in `tbaaMapping`.
>   SymbolRefAttr lookupTBAAAttr(const llvm::MDNode *node) {
183,199d162
<   /// Returns the symbol references pointing to the access group operations that
<   /// map to the access group nodes starting from the access group metadata
<   /// `node`. Returns failure, if any of the symbol references cannot be found.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   lookupAccessGroupAttrs(const llvm::MDNode *node) const;
< 
<   /// Returns the loop annotation attribute that corresponds to the given LLVM
<   /// loop metadata `node`.
<   LoopAnnotationAttr translateLoopAnnotationAttr(const llvm::MDNode *node,
<                                                  Location loc) const;
< 
<   /// Returns the symbol references pointing to the alias scope operations that
<   /// map to the alias scope nodes starting from the metadata `node`. Returns
<   /// failure, if any of the symbol references cannot be found.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   lookupAliasScopeAttrs(const llvm::MDNode *node) const;
< 
250,257d212
<   /// Converts the parameter attributes attached to `func` and adds them to the
<   /// `funcOp`.
<   void convertParameterAttributes(llvm::Function *func, LLVMFuncOp funcOp,
<                                   OpBuilder &builder);
<   /// Converts the AttributeSet of one parameter in LLVM IR to a corresponding
<   /// DictionaryAttr for the LLVM dialect.
<   DictionaryAttr convertParameterAttribute(llvm::AttributeSet llvmParamAttrs,
<                                            OpBuilder &builder);
277,280c232,239
<   /// Returns a global metadata operation that serves as a container for LLVM
<   /// metadata that converts to MLIR operations. Creates the global metadata
<   /// operation on the first invocation.
<   MetadataOp getGlobalMetadataOp();
---
>   /// Returns symbol name to be used for MetadataOp containing
>   /// TBAA metadata operations. It must not conflict with the user
>   /// name space.
>   StringRef getTBAAMetadataOpName() const { return "__tbaa"; }
>   /// Returns a terminated MetadataOp into which TBAA metadata
>   /// operations can be placed. The MetadataOp is created
>   /// on the first invocation of this function.
>   MetadataOp getTBAAMetadataOp();
287,297c246,250
<   /// Converts all LLVM access groups starting from `node` to MLIR access group
<   /// operations and stores a mapping from every nested access group node to the
<   /// symbol pointing to the translated operation. Returns success if all
<   /// conversions succeed and failure otherwise.
<   LogicalResult processAccessGroupMetadata(const llvm::MDNode *node);
<   /// Converts all LLVM alias scopes and domains starting from `node` to MLIR
<   /// alias scope and domain operations and stores a mapping from every nested
<   /// alias scope or alias domain node to the symbol pointing to the translated
<   /// operation. Returns success if all conversions succeed and failure
<   /// otherwise.
<   LogicalResult processAliasScopeMetadata(const llvm::MDNode *node);
---
>   /// Returns unique string name of a symbol that may be used
>   /// for a TBAA metadata operation. The name will contain
>   /// the provided `basename` and will be uniqued via
>   /// tbaaNodeCounter (see below).
>   std::string getNewTBAANodeName(StringRef basename);
307,308d259
<   /// Operation to insert metadata operations into.
<   MetadataOp globalMetadataOp = nullptr;
328,333d278
<   /// Mapping between LLVM alias scope and domain metadata nodes and symbol
<   /// references to the LLVM dialect operations corresponding to these nodes.
<   DenseMap<const llvm::MDNode *, SymbolRefAttr> aliasScopeMapping;
<   /// Mapping between LLVM TBAA metadata nodes and symbol references to the LLVM
<   /// dialect TBAA operations corresponding to these nodes.
<   DenseMap<const llvm::MDNode *, SymbolRefAttr> tbaaMapping;
338,339c283,292
<   /// Loop annotation importer.
<   std::unique_ptr<detail::LoopAnnotationImporter> loopAnnotationImporter;
---
>   /// A terminated MetadataOp where TBAA metadata operations
>   /// can be inserted.
>   MetadataOp tbaaMetadataOp{};
>   /// Mapping between LLVM TBAA metadata nodes and symbol references
>   /// to the LLVMIR dialect TBAA operations corresponding to these
>   /// nodes.
>   DenseMap<const llvm::MDNode *, SymbolRefAttr> tbaaMapping;
>   /// A counter to be used as a unique suffix for symbols
>   /// defined by TBAA operations.
>   unsigned tbaaNodeCounter = 0;
diff ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/ModuleTranslation.h include/mlir/Target/LLVMIR/ModuleTranslation.h
17d16
< #include "mlir/Dialect/LLVMIR/LLVMInterfaces.h"
19d17
< #include "mlir/IR/SymbolTable.h"
20a19
> #include "mlir/IR/SymbolTable.h"
44d42
< class LoopAnnotationTranslation;
124,131c122,143
<   /// Returns the LLVM metadata corresponding to a symbol reference to an mlir
<   /// LLVM dialect alias scope operation.
<   llvm::MDNode *getAliasScope(Operation *op, SymbolRefAttr aliasScopeRef) const;
< 
<   /// Returns the LLVM metadata corresponding to an array of symbol references
<   /// to mlir LLVM dialect alias scope operations.
<   llvm::MDNode *getAliasScopes(Operation *op,
<                                ArrayRef<SymbolRefAttr> aliasScopeRefs) const;
---
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect access group operation.
>   llvm::MDNode *getAccessGroup(Operation &opInst,
>                                SymbolRefAttr accessGroupRef) const;
> 
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect alias scope operation
>   llvm::MDNode *getAliasScope(Operation &opInst,
>                               SymbolRefAttr aliasScopeRef) const;
> 
>   /// Returns the LLVM metadata corresponding to a llvm loop's codegen
>   /// options attribute.
>   llvm::MDNode *lookupLoopOptionsMetadata(Attribute options) const {
>     return loopOptionsMetadataMapping.lookup(options);
>   }
> 
>   void mapLoopOptionsMetadata(Attribute options, llvm::MDNode *metadata) {
>     auto result = loopOptionsMetadataMapping.try_emplace(options, metadata);
>     (void)result;
>     assert(result.second &&
>            "attempting to map loop options that was already mapped");
>   }
134,135c146
<   void setAccessGroupsMetadata(AccessGroupOpInterface op,
<                                llvm::Instruction *inst);
---
>   void setAccessGroupsMetadata(Operation *op, llvm::Instruction *inst);
138,139c149
<   void setAliasScopeMetadata(AliasAnalysisOpInterface op,
<                              llvm::Instruction *inst);
---
>   void setAliasScopeMetadata(Operation *op, llvm::Instruction *inst);
141,146c151,153
<   /// Sets LLVM TBAA metadata for memory operations that have TBAA attributes.
<   void setTBAAMetadata(AliasAnalysisOpInterface op, llvm::Instruction *inst);
< 
<   /// Sets LLVM loop metadata for branch operations that have a loop annotation
<   /// attribute.
<   void setLoopMetadata(Operation *op, llvm::Instruction *inst);
---
>   /// Sets LLVM TBAA metadata for memory operations that have
>   /// TBAA attributes.
>   void setTBAAMetadata(Operation *op, llvm::Instruction *inst);
165,168c172,178
<   llvm::OpenMPIRBuilder *getOpenMPBuilder();
< 
<   /// Returns the LLVM module in which the IR is being constructed.
<   llvm::Module *getLLVMModule() { return llvmModule.get(); }
---
>   llvm::OpenMPIRBuilder *getOpenMPBuilder() {
>     if (!ompBuilder) {
>       ompBuilder = std::make_unique<llvm::OpenMPIRBuilder>(*llvmModule);
>       ompBuilder->initialize();
>     }
>     return ompBuilder.get();
>   }
266c276
<   SymbolTableCollection &symbolTable() { return symbolTableCollection; }
---
>   SymbolTableCollection& symbolTable() { return symbolTableCollection; }
288,290c298,300
<   /// Returns the LLVM metadata corresponding to a symbol reference to an mlir
<   /// LLVM dialect TBAATagOp operation.
<   llvm::MDNode *getTBAANode(Operation *op, SymbolRefAttr tagRef) const;
---
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect TBAATagOp operation.
>   llvm::MDNode *getTBAANode(Operation &memOp, SymbolRefAttr tagRef) const;
299,301d308
<   /// Translates parameter attributes and adds them to the returned AttrBuilder.
<   llvm::AttrBuilder convertParameterAttrs(DictionaryAttr paramAttrs);
< 
308,310d314
<   /// A converter for translating loop annotations.
<   std::unique_ptr<detail::LoopAnnotationTranslation> loopAnnotationTranslation;
< 
332a337,346
> 
>   /// Mapping from an access group metadata operation to its LLVM metadata.
>   /// This map is populated on module entry and is used to annotate loops (as
>   /// identified via their branches) and contained memory accesses.
>   DenseMap<Operation *, llvm::MDNode *> accessGroupMetadataMapping;
> 
>   /// Mapping from an attribute describing loop codegen options to its LLVM
>   /// metadata. The metadata is attached to Latch block branches with this
>   /// attribute.
>   DenseMap<Attribute, llvm::MDNode *> loopOptionsMetadataMapping;
--- include/mlir/Target/LLVMIR/TypeFromLLVM.h
--- include/mlir/Target/LLVMIR/LLVMImportInterface.h
--- include/mlir/Target/LLVMIR/Export.h
--- include/mlir/Target/LLVMIR/LLVMTranslationInterface.h
--- include/mlir/Target/LLVMIR/TypeToLLVM.h
--- include/mlir/Target/LLVMIR/Import.h
--- include/mlir/Target/LLVMIR/ModuleImport.h
35d34
< class DataLayoutImporter;
37d35
< class LoopAnnotationImporter;
62,65d59
<   /// Converts the data layout of the LLVM module to an MLIR data layout
<   /// specification.
<   LogicalResult convertDataLayout();
< 
127,131d120
<   /// Converts an LLVM metadata value to an MLIR value, or returns failure if
<   /// the conversion fails. Uses the `convertConstant` method to translate
<   /// constant LLVM values.
<   FailureOr<Value> convertMetadataValue(llvm::Value *value);
< 
143,147d131
<   /// Converts `value` to an array of symbol references pointing to alias scope
<   /// operations, or returns failure if the conversion fails.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   matchAliasScopeAttrs(llvm::Value *value);
< 
168,174c152,154
<   /// Converts all LLVM metadata nodes that translate to operations nested in a
<   /// global metadata operation, such as alias analysis or access group
<   /// metadata, and builds a map from the metadata nodes to the symbols pointing
<   /// to the converted operations. Returns success if all conversions succeed
<   /// and failure otherwise.
<   // Note: All metadata is nested inside a single global metadata operation to
<   // minimize the number of symbols that pollute the global namespace.
---
>   /// Converts LLVM metadata to corresponding MLIR representation,
>   /// e.g. metadata nodes referenced via !tbaa are converted to
>   /// TBAA operations hosted inside a MetadataOp.
177,179c157,159
<   /// Returns the MLIR symbol reference mapped to the given LLVM TBAA
<   /// metadata `node`.
<   SymbolRefAttr lookupTBAAAttr(const llvm::MDNode *node) const {
---
>   /// Returns SymbolRefAttr representing TBAA metadata `node`
>   /// in `tbaaMapping`.
>   SymbolRefAttr lookupTBAAAttr(const llvm::MDNode *node) {
183,199d162
<   /// Returns the symbol references pointing to the access group operations that
<   /// map to the access group nodes starting from the access group metadata
<   /// `node`. Returns failure, if any of the symbol references cannot be found.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   lookupAccessGroupAttrs(const llvm::MDNode *node) const;
< 
<   /// Returns the loop annotation attribute that corresponds to the given LLVM
<   /// loop metadata `node`.
<   LoopAnnotationAttr translateLoopAnnotationAttr(const llvm::MDNode *node,
<                                                  Location loc) const;
< 
<   /// Returns the symbol references pointing to the alias scope operations that
<   /// map to the alias scope nodes starting from the metadata `node`. Returns
<   /// failure, if any of the symbol references cannot be found.
<   FailureOr<SmallVector<SymbolRefAttr>>
<   lookupAliasScopeAttrs(const llvm::MDNode *node) const;
< 
250,257d212
<   /// Converts the parameter attributes attached to `func` and adds them to the
<   /// `funcOp`.
<   void convertParameterAttributes(llvm::Function *func, LLVMFuncOp funcOp,
<                                   OpBuilder &builder);
<   /// Converts the AttributeSet of one parameter in LLVM IR to a corresponding
<   /// DictionaryAttr for the LLVM dialect.
<   DictionaryAttr convertParameterAttribute(llvm::AttributeSet llvmParamAttrs,
<                                            OpBuilder &builder);
277,280c232,239
<   /// Returns a global metadata operation that serves as a container for LLVM
<   /// metadata that converts to MLIR operations. Creates the global metadata
<   /// operation on the first invocation.
<   MetadataOp getGlobalMetadataOp();
---
>   /// Returns symbol name to be used for MetadataOp containing
>   /// TBAA metadata operations. It must not conflict with the user
>   /// name space.
>   StringRef getTBAAMetadataOpName() const { return "__tbaa"; }
>   /// Returns a terminated MetadataOp into which TBAA metadata
>   /// operations can be placed. The MetadataOp is created
>   /// on the first invocation of this function.
>   MetadataOp getTBAAMetadataOp();
287,297c246,250
<   /// Converts all LLVM access groups starting from `node` to MLIR access group
<   /// operations and stores a mapping from every nested access group node to the
<   /// symbol pointing to the translated operation. Returns success if all
<   /// conversions succeed and failure otherwise.
<   LogicalResult processAccessGroupMetadata(const llvm::MDNode *node);
<   /// Converts all LLVM alias scopes and domains starting from `node` to MLIR
<   /// alias scope and domain operations and stores a mapping from every nested
<   /// alias scope or alias domain node to the symbol pointing to the translated
<   /// operation. Returns success if all conversions succeed and failure
<   /// otherwise.
<   LogicalResult processAliasScopeMetadata(const llvm::MDNode *node);
---
>   /// Returns unique string name of a symbol that may be used
>   /// for a TBAA metadata operation. The name will contain
>   /// the provided `basename` and will be uniqued via
>   /// tbaaNodeCounter (see below).
>   std::string getNewTBAANodeName(StringRef basename);
307,308d259
<   /// Operation to insert metadata operations into.
<   MetadataOp globalMetadataOp = nullptr;
328,333d278
<   /// Mapping between LLVM alias scope and domain metadata nodes and symbol
<   /// references to the LLVM dialect operations corresponding to these nodes.
<   DenseMap<const llvm::MDNode *, SymbolRefAttr> aliasScopeMapping;
<   /// Mapping between LLVM TBAA metadata nodes and symbol references to the LLVM
<   /// dialect TBAA operations corresponding to these nodes.
<   DenseMap<const llvm::MDNode *, SymbolRefAttr> tbaaMapping;
338,339c283,292
<   /// Loop annotation importer.
<   std::unique_ptr<detail::LoopAnnotationImporter> loopAnnotationImporter;
---
>   /// A terminated MetadataOp where TBAA metadata operations
>   /// can be inserted.
>   MetadataOp tbaaMetadataOp{};
>   /// Mapping between LLVM TBAA metadata nodes and symbol references
>   /// to the LLVMIR dialect TBAA operations corresponding to these
>   /// nodes.
>   DenseMap<const llvm::MDNode *, SymbolRefAttr> tbaaMapping;
>   /// A counter to be used as a unique suffix for symbols
>   /// defined by TBAA operations.
>   unsigned tbaaNodeCounter = 0;
--- include/mlir/Target/LLVMIR/Dialect
diff ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/All.h include/mlir/Target/LLVMIR/Dialect/All.h
20,21d19
< #include "mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h"
< #include "mlir/Target/LLVMIR/Dialect/GPU/GPUToLLVMIRTranslation.h"
39,40d36
<   registerBuiltinDialectTranslation(registry);
<   registerGPUDialectTranslation(registry);
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/AMX and include/mlir/Target/LLVMIR/Dialect/AMX
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/ArmNeon and include/mlir/Target/LLVMIR/Dialect/ArmNeon
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/ArmSVE and include/mlir/Target/LLVMIR/Dialect/ArmSVE
Only in ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect: Builtin
Only in ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect: GPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/LLVMIR and include/mlir/Target/LLVMIR/Dialect/LLVMIR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/NVVM and include/mlir/Target/LLVMIR/Dialect/NVVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/OpenACC and include/mlir/Target/LLVMIR/Dialect/OpenACC
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/OpenMP and include/mlir/Target/LLVMIR/Dialect/OpenMP
Only in ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect: OpenMPCommon.h
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/ROCDL and include/mlir/Target/LLVMIR/Dialect/ROCDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Target/LLVMIR/Dialect/X86Vector and include/mlir/Target/LLVMIR/Dialect/X86Vector
--- include/mlir/Target/LLVMIR/Dialect/LLVMIR
--- include/mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMIRToLLVMTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/AMX
--- include/mlir/Target/LLVMIR/Dialect/AMX/AMXToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/All.h
20,21d19
< #include "mlir/Target/LLVMIR/Dialect/Builtin/BuiltinToLLVMIRTranslation.h"
< #include "mlir/Target/LLVMIR/Dialect/GPU/GPUToLLVMIRTranslation.h"
39,40d36
<   registerBuiltinDialectTranslation(registry);
<   registerGPUDialectTranslation(registry);
--- include/mlir/Target/LLVMIR/Dialect/ArmNeon
--- include/mlir/Target/LLVMIR/Dialect/ArmNeon/ArmNeonToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/OpenACC
--- include/mlir/Target/LLVMIR/Dialect/OpenACC/OpenACCToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/OpenMP
--- include/mlir/Target/LLVMIR/Dialect/OpenMP/OpenMPToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/ROCDL
--- include/mlir/Target/LLVMIR/Dialect/ROCDL/ROCDLToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/NVVM
--- include/mlir/Target/LLVMIR/Dialect/NVVM/NVVMToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/ArmSVE
--- include/mlir/Target/LLVMIR/Dialect/ArmSVE/ArmSVEToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/Dialect/X86Vector
--- include/mlir/Target/LLVMIR/Dialect/X86Vector/X86VectorToLLVMIRTranslation.h
--- include/mlir/Target/LLVMIR/ModuleTranslation.h
17d16
< #include "mlir/Dialect/LLVMIR/LLVMInterfaces.h"
19d17
< #include "mlir/IR/SymbolTable.h"
20a19
> #include "mlir/IR/SymbolTable.h"
44d42
< class LoopAnnotationTranslation;
124,131c122,143
<   /// Returns the LLVM metadata corresponding to a symbol reference to an mlir
<   /// LLVM dialect alias scope operation.
<   llvm::MDNode *getAliasScope(Operation *op, SymbolRefAttr aliasScopeRef) const;
< 
<   /// Returns the LLVM metadata corresponding to an array of symbol references
<   /// to mlir LLVM dialect alias scope operations.
<   llvm::MDNode *getAliasScopes(Operation *op,
<                                ArrayRef<SymbolRefAttr> aliasScopeRefs) const;
---
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect access group operation.
>   llvm::MDNode *getAccessGroup(Operation &opInst,
>                                SymbolRefAttr accessGroupRef) const;
> 
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect alias scope operation
>   llvm::MDNode *getAliasScope(Operation &opInst,
>                               SymbolRefAttr aliasScopeRef) const;
> 
>   /// Returns the LLVM metadata corresponding to a llvm loop's codegen
>   /// options attribute.
>   llvm::MDNode *lookupLoopOptionsMetadata(Attribute options) const {
>     return loopOptionsMetadataMapping.lookup(options);
>   }
> 
>   void mapLoopOptionsMetadata(Attribute options, llvm::MDNode *metadata) {
>     auto result = loopOptionsMetadataMapping.try_emplace(options, metadata);
>     (void)result;
>     assert(result.second &&
>            "attempting to map loop options that was already mapped");
>   }
134,135c146
<   void setAccessGroupsMetadata(AccessGroupOpInterface op,
<                                llvm::Instruction *inst);
---
>   void setAccessGroupsMetadata(Operation *op, llvm::Instruction *inst);
138,139c149
<   void setAliasScopeMetadata(AliasAnalysisOpInterface op,
<                              llvm::Instruction *inst);
---
>   void setAliasScopeMetadata(Operation *op, llvm::Instruction *inst);
141,146c151,153
<   /// Sets LLVM TBAA metadata for memory operations that have TBAA attributes.
<   void setTBAAMetadata(AliasAnalysisOpInterface op, llvm::Instruction *inst);
< 
<   /// Sets LLVM loop metadata for branch operations that have a loop annotation
<   /// attribute.
<   void setLoopMetadata(Operation *op, llvm::Instruction *inst);
---
>   /// Sets LLVM TBAA metadata for memory operations that have
>   /// TBAA attributes.
>   void setTBAAMetadata(Operation *op, llvm::Instruction *inst);
165,168c172,178
<   llvm::OpenMPIRBuilder *getOpenMPBuilder();
< 
<   /// Returns the LLVM module in which the IR is being constructed.
<   llvm::Module *getLLVMModule() { return llvmModule.get(); }
---
>   llvm::OpenMPIRBuilder *getOpenMPBuilder() {
>     if (!ompBuilder) {
>       ompBuilder = std::make_unique<llvm::OpenMPIRBuilder>(*llvmModule);
>       ompBuilder->initialize();
>     }
>     return ompBuilder.get();
>   }
266c276
<   SymbolTableCollection &symbolTable() { return symbolTableCollection; }
---
>   SymbolTableCollection& symbolTable() { return symbolTableCollection; }
288,290c298,300
<   /// Returns the LLVM metadata corresponding to a symbol reference to an mlir
<   /// LLVM dialect TBAATagOp operation.
<   llvm::MDNode *getTBAANode(Operation *op, SymbolRefAttr tagRef) const;
---
>   /// Returns the LLVM metadata corresponding to a reference to an mlir LLVM
>   /// dialect TBAATagOp operation.
>   llvm::MDNode *getTBAANode(Operation &memOp, SymbolRefAttr tagRef) const;
299,301d308
<   /// Translates parameter attributes and adds them to the returned AttrBuilder.
<   llvm::AttrBuilder convertParameterAttrs(DictionaryAttr paramAttrs);
< 
308,310d314
<   /// A converter for translating loop annotations.
<   std::unique_ptr<detail::LoopAnnotationTranslation> loopAnnotationTranslation;
< 
332a337,346
> 
>   /// Mapping from an access group metadata operation to its LLVM metadata.
>   /// This map is populated on module entry and is used to annotate loops (as
>   /// identified via their branches) and contained memory accesses.
>   DenseMap<Operation *, llvm::MDNode *> accessGroupMetadataMapping;
> 
>   /// Mapping from an attribute describing loop codegen options to its LLVM
>   /// metadata. The metadata is attached to Latch block branches with this
>   /// attribute.
>   DenseMap<Attribute, llvm::MDNode *> loopOptionsMetadataMapping;
--- include/mlir/Target/Cpp
--- include/mlir/Target/Cpp/CppEmitter.h
--- include/mlir/Target/SPIRV
--- include/mlir/Target/SPIRV/Serialization.h
--- include/mlir/Target/SPIRV/Deserialization.h
--- include/mlir/Target/SPIRV/SPIRVBinaryUtils.h
--- include/mlir/AsmParser
diff ../../../../llvm-project.0/mlir/include/mlir/AsmParser/AsmParser.h include/mlir/AsmParser/AsmParser.h
46,53c46,59
< /// This parses a single MLIR attribute to an MLIR context if it was valid. If
< /// not, an error diagnostic is emitted to the context and a null value is
< /// returned.
< /// If `numRead` is provided, it is set to the number of consumed characters on
< /// succesful parse. Otherwise, parsing fails if the entire string is not
< /// consumed.
< /// Some internal copying can be skipped if the source string is known to be
< /// null terminated.
---
> /// This parses a single MLIR attribute to an MLIR context if it was valid.  If
> /// not, an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `attrStr`. If the passed `attrStr` has additional tokens that were not part
> /// of the type, an error is emitted.
> // TODO: Improve diagnostic reporting.
> Attribute parseAttribute(llvm::StringRef attrStr, MLIRContext *context);
> Attribute parseAttribute(llvm::StringRef attrStr, Type type);
> 
> /// This parses a single MLIR attribute to an MLIR context if it was valid.  If
> /// not, an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `attrStr`. The number of characters of `attrStr` parsed in the process is
> /// returned in `numRead`.
55,56c61,70
<                          Type type = {}, size_t *numRead = nullptr,
<                          bool isKnownNullTerminated = false);
---
>                          size_t &numRead);
> Attribute parseAttribute(llvm::StringRef attrStr, Type type, size_t &numRead);
> 
> /// This parses a single MLIR type to an MLIR context if it was valid.  If not,
> /// an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `typeStr`. If the passed `typeStr` has additional tokens that were not part
> /// of the type, an error is emitted.
> // TODO: Improve diagnostic reporting.
> Type parseType(llvm::StringRef typeStr, MLIRContext *context);
58,66c72,77
< /// This parses a single MLIR type to an MLIR context if it was valid. If not,
< /// an error diagnostic is emitted to the context.
< /// If `numRead` is provided, it is set to the number of consumed characters on
< /// succesful parse. Otherwise, parsing fails if the entire string is not
< /// consumed.
< /// Some internal copying can be skipped if the source string is known to be
< /// null terminated.
< Type parseType(llvm::StringRef typeStr, MLIRContext *context,
<                size_t *numRead = nullptr, bool isKnownNullTerminated = false);
---
> /// This parses a single MLIR type to an MLIR context if it was valid.  If not,
> /// an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `typeStr`. The number of characters of `typeStr` parsed in the process is
> /// returned in `numRead`.
> Type parseType(llvm::StringRef typeStr, MLIRContext *context, size_t &numRead);
--- include/mlir/AsmParser/AsmParserState.h
--- include/mlir/AsmParser/AsmParser.h
46,53c46,59
< /// This parses a single MLIR attribute to an MLIR context if it was valid. If
< /// not, an error diagnostic is emitted to the context and a null value is
< /// returned.
< /// If `numRead` is provided, it is set to the number of consumed characters on
< /// succesful parse. Otherwise, parsing fails if the entire string is not
< /// consumed.
< /// Some internal copying can be skipped if the source string is known to be
< /// null terminated.
---
> /// This parses a single MLIR attribute to an MLIR context if it was valid.  If
> /// not, an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `attrStr`. If the passed `attrStr` has additional tokens that were not part
> /// of the type, an error is emitted.
> // TODO: Improve diagnostic reporting.
> Attribute parseAttribute(llvm::StringRef attrStr, MLIRContext *context);
> Attribute parseAttribute(llvm::StringRef attrStr, Type type);
> 
> /// This parses a single MLIR attribute to an MLIR context if it was valid.  If
> /// not, an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `attrStr`. The number of characters of `attrStr` parsed in the process is
> /// returned in `numRead`.
55,56c61,70
<                          Type type = {}, size_t *numRead = nullptr,
<                          bool isKnownNullTerminated = false);
---
>                          size_t &numRead);
> Attribute parseAttribute(llvm::StringRef attrStr, Type type, size_t &numRead);
> 
> /// This parses a single MLIR type to an MLIR context if it was valid.  If not,
> /// an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `typeStr`. If the passed `typeStr` has additional tokens that were not part
> /// of the type, an error is emitted.
> // TODO: Improve diagnostic reporting.
> Type parseType(llvm::StringRef typeStr, MLIRContext *context);
58,66c72,77
< /// This parses a single MLIR type to an MLIR context if it was valid. If not,
< /// an error diagnostic is emitted to the context.
< /// If `numRead` is provided, it is set to the number of consumed characters on
< /// succesful parse. Otherwise, parsing fails if the entire string is not
< /// consumed.
< /// Some internal copying can be skipped if the source string is known to be
< /// null terminated.
< Type parseType(llvm::StringRef typeStr, MLIRContext *context,
<                size_t *numRead = nullptr, bool isKnownNullTerminated = false);
---
> /// This parses a single MLIR type to an MLIR context if it was valid.  If not,
> /// an error message is emitted through a new SourceMgrDiagnosticHandler
> /// constructed from a new SourceMgr with a single a MemoryBuffer wrapping
> /// `typeStr`. The number of characters of `typeStr` parsed in the process is
> /// returned in `numRead`.
> Type parseType(llvm::StringRef typeStr, MLIRContext *context, size_t &numRead);
--- include/mlir/AsmParser/CodeComplete.h
--- include/mlir/Bindings
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Bindings/Python and include/mlir/Bindings/Python
--- include/mlir/Bindings/Python
diff ../../../../llvm-project.0/mlir/include/mlir/Bindings/Python/PybindAdaptors.h include/mlir/Bindings/Python/PybindAdaptors.h
456,511d455
< /// Creates a custom subclass of mlir.ir.Value, implementing a casting
< /// constructor and type checking methods.
< class mlir_value_subclass : public pure_subclass {
< public:
<   using IsAFunctionTy = bool (*)(MlirValue);
< 
<   /// Subclasses by looking up the super-class dynamically.
<   mlir_value_subclass(py::handle scope, const char *valueClassName,
<                       IsAFunctionTy isaFunction)
<       : mlir_value_subclass(
<             scope, valueClassName, isaFunction,
<             py::module::import(MAKE_MLIR_PYTHON_QUALNAME("ir")).attr("Value")) {
<   }
< 
<   /// Subclasses with a provided mlir.ir.Value super-class. This must
<   /// be used if the subclass is being defined in the same extension module
<   /// as the mlir.ir class (otherwise, it will trigger a recursive
<   /// initialization).
<   mlir_value_subclass(py::handle scope, const char *valueClassName,
<                       IsAFunctionTy isaFunction, const py::object &superCls)
<       : pure_subclass(scope, valueClassName, superCls) {
<     // Casting constructor. Note that it hard, if not impossible, to properly
<     // call chain to parent `__init__` in pybind11 due to its special handling
<     // for init functions that don't have a fully constructed self-reference,
<     // which makes it impossible to forward it to `__init__` of a superclass.
<     // Instead, provide a custom `__new__` and call that of a superclass, which
<     // eventually calls `__init__` of the superclass. Since attribute subclasses
<     // have no additional members, we can just return the instance thus created
<     // without amending it.
<     std::string captureValueName(
<         valueClassName); // As string in case if valueClassName is not static.
<     py::cpp_function newCf(
<         [superCls, isaFunction, captureValueName](py::object cls,
<                                                   py::object otherValue) {
<           MlirValue rawValue = py::cast<MlirValue>(otherValue);
<           if (!isaFunction(rawValue)) {
<             auto origRepr = py::repr(otherValue).cast<std::string>();
<             throw std::invalid_argument((llvm::Twine("Cannot cast value to ") +
<                                          captureValueName + " (from " +
<                                          origRepr + ")")
<                                             .str());
<           }
<           py::object self = superCls.attr("__new__")(cls, otherValue);
<           return self;
<         },
<         py::name("__new__"), py::arg("cls"), py::arg("cast_from_value"));
<     thisClass.attr("__new__") = newCf;
< 
<     // 'isinstance' method.
<     def_staticmethod(
<         "isinstance",
<         [isaFunction](MlirValue other) { return isaFunction(other); },
<         py::arg("other_value"));
<   }
< };
< 
--- include/mlir/Bindings/Python/Attributes.td
--- include/mlir/Bindings/Python/PybindAdaptors.h
456,511d455
< /// Creates a custom subclass of mlir.ir.Value, implementing a casting
< /// constructor and type checking methods.
< class mlir_value_subclass : public pure_subclass {
< public:
<   using IsAFunctionTy = bool (*)(MlirValue);
< 
<   /// Subclasses by looking up the super-class dynamically.
<   mlir_value_subclass(py::handle scope, const char *valueClassName,
<                       IsAFunctionTy isaFunction)
<       : mlir_value_subclass(
<             scope, valueClassName, isaFunction,
<             py::module::import(MAKE_MLIR_PYTHON_QUALNAME("ir")).attr("Value")) {
<   }
< 
<   /// Subclasses with a provided mlir.ir.Value super-class. This must
<   /// be used if the subclass is being defined in the same extension module
<   /// as the mlir.ir class (otherwise, it will trigger a recursive
<   /// initialization).
<   mlir_value_subclass(py::handle scope, const char *valueClassName,
<                       IsAFunctionTy isaFunction, const py::object &superCls)
<       : pure_subclass(scope, valueClassName, superCls) {
<     // Casting constructor. Note that it hard, if not impossible, to properly
<     // call chain to parent `__init__` in pybind11 due to its special handling
<     // for init functions that don't have a fully constructed self-reference,
<     // which makes it impossible to forward it to `__init__` of a superclass.
<     // Instead, provide a custom `__new__` and call that of a superclass, which
<     // eventually calls `__init__` of the superclass. Since attribute subclasses
<     // have no additional members, we can just return the instance thus created
<     // without amending it.
<     std::string captureValueName(
<         valueClassName); // As string in case if valueClassName is not static.
<     py::cpp_function newCf(
<         [superCls, isaFunction, captureValueName](py::object cls,
<                                                   py::object otherValue) {
<           MlirValue rawValue = py::cast<MlirValue>(otherValue);
<           if (!isaFunction(rawValue)) {
<             auto origRepr = py::repr(otherValue).cast<std::string>();
<             throw std::invalid_argument((llvm::Twine("Cannot cast value to ") +
<                                          captureValueName + " (from " +
<                                          origRepr + ")")
<                                             .str());
<           }
<           py::object self = superCls.attr("__new__")(cls, otherValue);
<           return self;
<         },
<         py::name("__new__"), py::arg("cls"), py::arg("cast_from_value"));
<     thisClass.attr("__new__") = newCf;
< 
<     // 'isinstance' method.
<     def_staticmethod(
<         "isinstance",
<         [isaFunction](MlirValue other) { return isaFunction(other); },
<         py::arg("other_value"));
<   }
< };
< 
--- include/mlir/Bytecode
diff ../../../../llvm-project.0/mlir/include/mlir/Bytecode/BytecodeImplementation.h include/mlir/Bytecode/BytecodeImplementation.h
236,249d235
< 
<   /// Return the bytecode version being emitted for.
<   virtual int64_t getBytecodeVersion() const = 0;
< };
< 
< //===--------------------------------------------------------------------===//
< // Dialect Version Interface.
< //===--------------------------------------------------------------------===//
< 
< /// This class is used to represent the version of a dialect, for the purpose
< /// of polymorphic destruction.
< class DialectVersion {
< public:
<   virtual ~DialectVersion() = default;
273,285d258
<   /// Read a versioned attribute encoding belonging to this dialect from the
<   /// given reader. This method should return null in the case of failure, and
<   /// falls back to the non-versioned reader in case the dialect implements
<   /// versioning but it does not support versioned custom encodings for the
<   /// attributes.
<   virtual Attribute readAttribute(DialectBytecodeReader &reader,
<                                   const DialectVersion &version) const {
<     reader.emitError()
<         << "dialect " << getDialect()->getNamespace()
<         << " does not support reading versioned attributes from bytecode";
<     return Attribute();
<   }
< 
294,306d266
<   /// Read a versioned type encoding belonging to this dialect from the given
<   /// reader. This method should return null in the case of failure, and
<   /// falls back to the non-versioned reader in case the dialect implements
<   /// versioning but it does not support versioned custom encodings for the
<   /// types.
<   virtual Type readType(DialectBytecodeReader &reader,
<                         const DialectVersion &version) const {
<     reader.emitError()
<         << "dialect " << getDialect()->getNamespace()
<         << " does not support reading versioned types from bytecode";
<     return Type();
<   }
< 
328,348d287
< 
<   /// Write the version of this dialect to the given writer.
<   virtual void writeVersion(DialectBytecodeWriter &writer) const {}
< 
<   // Read the version of this dialect from the provided reader and return it as
<   // a `unique_ptr` to a dialect version object.
<   virtual std::unique_ptr<DialectVersion>
<   readVersion(DialectBytecodeReader &reader) const {
<     reader.emitError("Dialect does not support versioning");
<     return nullptr;
<   }
< 
<   /// Hook invoked after parsing completed, if a version directive was present
<   /// and included an entry for the current dialect. This hook offers the
<   /// opportunity to the dialect to visit the IR and upgrades constructs emitted
<   /// by the version of the dialect corresponding to the provided version.
<   virtual LogicalResult
<   upgradeFromVersion(Operation *topLevelOp,
<                      const DialectVersion &version) const {
<     return success();
<   }
350,380d288
< 
< /// Helper for resource handle reading that returns LogicalResult.
< template <typename T, typename... Ts>
< static LogicalResult readResourceHandle(DialectBytecodeReader &reader,
<                                         FailureOr<T> &value, Ts &&...params) {
<   FailureOr<T> handle = reader.readResourceHandle<T>();
<   if (failed(handle))
<     return failure();
<   if (auto *result = dyn_cast<T>(&*handle)) {
<     value = std::move(*result);
<     return success();
<   }
<   return failure();
< }
< 
< /// Helper method that injects context only if needed, this helps unify some of
< /// the attribute construction methods.
< template <typename T, typename... Ts>
< auto get(MLIRContext *context, Ts &&...params) {
<   // Prefer a direct `get` method if one exists.
<   if constexpr (llvm::is_detected<detail::has_get_method, T, Ts...>::value) {
<     (void)context;
<     return T::get(std::forward<Ts>(params)...);
<   } else if constexpr (llvm::is_detected<detail::has_get_method, T,
<                                          MLIRContext *, Ts...>::value) {
<     return T::get(context, std::forward<Ts>(params)...);
<   } else {
<     // Otherwise, pass to the base get.
<     return T::Base::get(context, std::forward<Ts>(params)...);
<   }
< }
diff ../../../../llvm-project.0/mlir/include/mlir/Bytecode/BytecodeWriter.h include/mlir/Bytecode/BytecodeWriter.h
43,48d42
<   /// Set the desired bytecode version to emit. This function clamps the version
<   /// to the existing version if larger than existing. The desired version may
<   /// not be used depending on the features used and the actual version required
<   /// is returned by bytecode writer entry point.
<   void setDesiredBytecodeVersion(int64_t bytecodeVersion);
< 
84,86c78,79
< /// It only ever fails if setDesiredByteCodeVersion can't be honored.
< LogicalResult writeBytecodeToFile(Operation *op, raw_ostream &os,
<                                   const BytecodeWriterConfig &config = {});
---
> void writeBytecodeToFile(Operation *op, raw_ostream &os,
>                          const BytecodeWriterConfig &config = {});
--- include/mlir/Bytecode/BytecodeImplementation.h
236,249d235
< 
<   /// Return the bytecode version being emitted for.
<   virtual int64_t getBytecodeVersion() const = 0;
< };
< 
< //===--------------------------------------------------------------------===//
< // Dialect Version Interface.
< //===--------------------------------------------------------------------===//
< 
< /// This class is used to represent the version of a dialect, for the purpose
< /// of polymorphic destruction.
< class DialectVersion {
< public:
<   virtual ~DialectVersion() = default;
273,285d258
<   /// Read a versioned attribute encoding belonging to this dialect from the
<   /// given reader. This method should return null in the case of failure, and
<   /// falls back to the non-versioned reader in case the dialect implements
<   /// versioning but it does not support versioned custom encodings for the
<   /// attributes.
<   virtual Attribute readAttribute(DialectBytecodeReader &reader,
<                                   const DialectVersion &version) const {
<     reader.emitError()
<         << "dialect " << getDialect()->getNamespace()
<         << " does not support reading versioned attributes from bytecode";
<     return Attribute();
<   }
< 
294,306d266
<   /// Read a versioned type encoding belonging to this dialect from the given
<   /// reader. This method should return null in the case of failure, and
<   /// falls back to the non-versioned reader in case the dialect implements
<   /// versioning but it does not support versioned custom encodings for the
<   /// types.
<   virtual Type readType(DialectBytecodeReader &reader,
<                         const DialectVersion &version) const {
<     reader.emitError()
<         << "dialect " << getDialect()->getNamespace()
<         << " does not support reading versioned types from bytecode";
<     return Type();
<   }
< 
328,348d287
< 
<   /// Write the version of this dialect to the given writer.
<   virtual void writeVersion(DialectBytecodeWriter &writer) const {}
< 
<   // Read the version of this dialect from the provided reader and return it as
<   // a `unique_ptr` to a dialect version object.
<   virtual std::unique_ptr<DialectVersion>
<   readVersion(DialectBytecodeReader &reader) const {
<     reader.emitError("Dialect does not support versioning");
<     return nullptr;
<   }
< 
<   /// Hook invoked after parsing completed, if a version directive was present
<   /// and included an entry for the current dialect. This hook offers the
<   /// opportunity to the dialect to visit the IR and upgrades constructs emitted
<   /// by the version of the dialect corresponding to the provided version.
<   virtual LogicalResult
<   upgradeFromVersion(Operation *topLevelOp,
<                      const DialectVersion &version) const {
<     return success();
<   }
350,380d288
< 
< /// Helper for resource handle reading that returns LogicalResult.
< template <typename T, typename... Ts>
< static LogicalResult readResourceHandle(DialectBytecodeReader &reader,
<                                         FailureOr<T> &value, Ts &&...params) {
<   FailureOr<T> handle = reader.readResourceHandle<T>();
<   if (failed(handle))
<     return failure();
<   if (auto *result = dyn_cast<T>(&*handle)) {
<     value = std::move(*result);
<     return success();
<   }
<   return failure();
< }
< 
< /// Helper method that injects context only if needed, this helps unify some of
< /// the attribute construction methods.
< template <typename T, typename... Ts>
< auto get(MLIRContext *context, Ts &&...params) {
<   // Prefer a direct `get` method if one exists.
<   if constexpr (llvm::is_detected<detail::has_get_method, T, Ts...>::value) {
<     (void)context;
<     return T::get(std::forward<Ts>(params)...);
<   } else if constexpr (llvm::is_detected<detail::has_get_method, T,
<                                          MLIRContext *, Ts...>::value) {
<     return T::get(context, std::forward<Ts>(params)...);
<   } else {
<     // Otherwise, pass to the base get.
<     return T::Base::get(context, std::forward<Ts>(params)...);
<   }
< }
--- include/mlir/Bytecode/BytecodeWriter.h
43,48d42
<   /// Set the desired bytecode version to emit. This function clamps the version
<   /// to the existing version if larger than existing. The desired version may
<   /// not be used depending on the features used and the actual version required
<   /// is returned by bytecode writer entry point.
<   void setDesiredBytecodeVersion(int64_t bytecodeVersion);
< 
84,86c78,79
< /// It only ever fails if setDesiredByteCodeVersion can't be honored.
< LogicalResult writeBytecodeToFile(Operation *op, raw_ostream &os,
<                                   const BytecodeWriterConfig &config = {});
---
> void writeBytecodeToFile(Operation *op, raw_ostream &os,
>                          const BytecodeWriterConfig &config = {});
--- include/mlir/Bytecode/BytecodeReader.h
--- include/mlir/Reducer
--- include/mlir/Reducer/Passes.h
--- include/mlir/Reducer/Passes.td
--- include/mlir/Reducer/Tester.h
--- include/mlir/Reducer/CMakeLists.txt
--- include/mlir/Reducer/ReductionPatternInterface.h
--- include/mlir/Reducer/ReductionNode.h
--- include/mlir/TableGen
diff ../../../../llvm-project.0/mlir/include/mlir/TableGen/Builder.h include/mlir/TableGen/Builder.h
73,76d72
<   /// Return the deprecation message of the builder.
<   /// Empty optional if the builder is not deprecated.
<   std::optional<StringRef> getDeprecatedMessage() const;
< 
diff ../../../../llvm-project.0/mlir/include/mlir/TableGen/Class.h include/mlir/TableGen/Class.h
155,157d154
<   /// Get the return type of the method
<   StringRef getReturnType() const { return returnType; }
< 
332,336d328
<   /// Sets or removes the deprecation message of the method.
<   void setDeprecated(std::optional<StringRef> message) {
<     this->deprecationMessage = message;
<   }
< 
355,357d346
<   /// Returns the return type of this method
<   StringRef getReturnType() const { return methodSignature.getReturnType(); }
< 
377,378d365
<   /// Deprecation message if the method is deprecated.
<   std::optional<std::string> deprecationMessage;
diff ../../../../llvm-project.0/mlir/include/mlir/TableGen/Dialect.h include/mlir/TableGen/Dialect.h
88a89,97
>   enum class FolderAPI {
>     RawAttributes = 0, /// fold method with ArrayRef<Attribute>.
>     FolderAdaptor = 1, /// fold method with the operation's FoldAdaptor.
>   };
> 
>   /// Returns the folder API that should be emitted for operations in this
>   /// dialect.
>   FolderAPI getFolderAPI() const;
> 
--- include/mlir/TableGen/Attribute.h
--- include/mlir/TableGen/Successor.h
--- include/mlir/TableGen/Builder.h
73,76d72
<   /// Return the deprecation message of the builder.
<   /// Empty optional if the builder is not deprecated.
<   std::optional<StringRef> getDeprecatedMessage() const;
< 
--- include/mlir/TableGen/AttrOrTypeDef.h
--- include/mlir/TableGen/Class.h
155,157d154
<   /// Get the return type of the method
<   StringRef getReturnType() const { return returnType; }
< 
332,336d328
<   /// Sets or removes the deprecation message of the method.
<   void setDeprecated(std::optional<StringRef> message) {
<     this->deprecationMessage = message;
<   }
< 
355,357d346
<   /// Returns the return type of this method
<   StringRef getReturnType() const { return methodSignature.getReturnType(); }
< 
377,378d365
<   /// Deprecation message if the method is deprecated.
<   std::optional<std::string> deprecationMessage;
--- include/mlir/TableGen/Pattern.h
--- include/mlir/TableGen/Format.h
--- include/mlir/TableGen/Constraint.h
--- include/mlir/TableGen/Dialect.h
88a89,97
>   enum class FolderAPI {
>     RawAttributes = 0, /// fold method with ArrayRef<Attribute>.
>     FolderAdaptor = 1, /// fold method with the operation's FoldAdaptor.
>   };
> 
>   /// Returns the folder API that should be emitted for operations in this
>   /// dialect.
>   FolderAPI getFolderAPI() const;
> 
--- include/mlir/TableGen/GenNameParser.h
--- include/mlir/TableGen/SideEffects.h
--- include/mlir/TableGen/Argument.h
--- include/mlir/TableGen/CodeGenHelpers.h
--- include/mlir/TableGen/Region.h
--- include/mlir/TableGen/Type.h
--- include/mlir/TableGen/Interfaces.h
--- include/mlir/TableGen/GenInfo.h
--- include/mlir/TableGen/Trait.h
--- include/mlir/TableGen/Operator.h
--- include/mlir/TableGen/Pass.h
--- include/mlir/TableGen/Predicate.h
--- include/mlir/CMakeLists.txt
--- include/mlir/InitAllPasses.h
17a18
> #include "mlir/Dialect/AMDGPU/Transforms/Passes.h"
58c59,60
<   affine::registerAffinePasses();
---
>   registerAffinePasses();
>   amdgpu::registerAMDGPUPasses();
--- include/mlir/CAPI
diff ../../../../llvm-project.0/mlir/include/mlir/CAPI/IR.h include/mlir/CAPI/IR.h
18d17
< #include "mlir/Bytecode/BytecodeWriter.h"
24d22
< DEFINE_C_API_PTR_METHODS(MlirBytecodeWriterConfig, mlir::BytecodeWriterConfig)
--- include/mlir/CAPI/AffineMap.h
--- include/mlir/CAPI/Diagnostics.h
--- include/mlir/CAPI/AffineExpr.h
--- include/mlir/CAPI/ExecutionEngine.h
--- include/mlir/CAPI/Wrap.h
--- include/mlir/CAPI/Utils.h
--- include/mlir/CAPI/Interfaces.h
--- include/mlir/CAPI/Support.h
--- include/mlir/CAPI/IntegerSet.h
--- include/mlir/CAPI/Registration.h
--- include/mlir/CAPI/Pass.h
--- include/mlir/CAPI/IR.h
18d17
< #include "mlir/Bytecode/BytecodeWriter.h"
24d22
< DEFINE_C_API_PTR_METHODS(MlirBytecodeWriterConfig, mlir::BytecodeWriterConfig)
--- include/mlir/InitAllTranslations.h
--- include/mlir/Interfaces
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/CallInterfaces.h include/mlir/Interfaces/CallInterfaces.h
22a23,26
> 
> namespace func {
> class FuncOp;
> }
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/CallInterfaces.td include/mlir/Interfaces/CallInterfaces.td
55c55
<     >,  
---
>     >,
62,69c62
<       }],
<       /*retTy=*/"::mlir::CallInterfaceCallable",
<       /*methodName=*/"getCallableForCallee",
<       /*args=*/(ins),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return $_op->template getAttrOfType<SymbolRefAttr>("callee");
<       }]
---
>       }], "::mlir::CallInterfaceCallable", "getCallableForCallee"
76,82c69
<       /*retTy=*/"::mlir::Operation::operand_range",
<       /*methodName=*/"getCallOperands",
<       /*args=*/(ins),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return $_op->getOperands();
<       }]
---
>       "::mlir::Operation::operand_range", "getArgOperands"
111d97
<         In the default case, there should not be any segments.
121c107
<     >
---
>     >,
160,171d145
<     InterfaceMethod<[{
<         Returns the argument attributes for all callable region arguments or
<         null if there are none.
<       }],
<       "::mlir::ArrayAttr", "getCallableArgAttrs"
<     >,
<     InterfaceMethod<[{
<         Returns the result attributes for all callable region results or null
<         if there are none.
<       }],
<       "::mlir::ArrayAttr", "getCallableResAttrs"
<     >
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/CMakeLists.txt include/mlir/Interfaces/CMakeLists.txt
10d9
< add_mlir_interface(Mem2RegInterfaces)
16d14
< add_mlir_interface(ValueBoundsOpInterface)
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/ControlFlowInterfaces.td include/mlir/Interfaces/ControlFlowInterfaces.td
70c70
<         some successor, or std::nullopt if `operandIndex` isn't a successor operand
---
>         some successor, or None if `operandIndex` isn't a successor operand
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/DataLayoutInterfaces.h include/mlir/Interfaces/DataLayoutInterfaces.h
59,66d58
< /// Default handler for alloca memory space request. Dispatches to the
< /// DataLayoutInterface if specified, otherwise returns the default.
< Attribute getDefaultAllocaMemorySpace(DataLayoutEntryInterface entry);
< 
< /// Default handler for the stack alignment request. Dispatches to the
< /// DataLayoutInterface if specified, otherwise returns the default.
< unsigned getDefaultStackAlignment(DataLayoutEntryInterface entry);
< 
170,178d161
<   /// Returns the memory space used for AllocaOps.
<   Attribute getAllocaMemorySpace() const;
< 
<   /// Returns the natural alignment of the stack in bits. Alignment promotion of
<   /// stack variables should be limited to the natural stack alignment to
<   /// prevent dynamic stack alignment. Returns zero if the stack alignment is
<   /// unspecified.
<   unsigned getStackAlignment() const;
< 
200,205d182
< 
<   /// Cache for alloca memory space.
<   mutable std::optional<Attribute> allocaMemorySpace;
< 
<   /// Cache for stack alignment.
<   mutable std::optional<unsigned> stackAlignment;
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/DataLayoutInterfaces.td include/mlir/Interfaces/DataLayoutInterfaces.td
109,120d108
<     InterfaceMethod<
<       /*description=*/"Returns the alloca memory space identifier.",
<       /*retTy=*/"::mlir::StringAttr",
<       /*methodName=*/"getAllocaMemorySpaceIdentifier",
<       /*args=*/(ins "::mlir::MLIRContext *":$context)
<     >,
<     InterfaceMethod<
<       /*description=*/"Returns the stack alignment identifier.",
<       /*retTy=*/"::mlir::StringAttr",
<       /*methodName=*/"getStackAlignmentIdentifier",
<       /*args=*/(ins "::mlir::MLIRContext *":$context)
<     >,
269,292d256
<       }]
<     >,
<     StaticInterfaceMethod<
<       /*description=*/"Returns the memory space used by the ABI computed "
<                       "using the relevant entries. The data layout object "
<                       "can be used for recursive queries.",
<       /*retTy=*/"::mlir::Attribute",
<       /*methodName=*/"getAllocaMemorySpace",
<       /*args=*/(ins "::mlir::DataLayoutEntryInterface":$entry),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return ::mlir::detail::getDefaultAllocaMemorySpace(entry);
<       }]
<     >,
<     StaticInterfaceMethod<
<       /*description=*/"Returns the natural stack alignment in bits computed "
<                       "using the relevant entries. The data layout object "
<                       "can be used for recursive queries.",
<       /*retTy=*/"unsigned",
<       /*methodName=*/"getStackAlignment",
<       /*args=*/(ins "::mlir::DataLayoutEntryInterface":$entry),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return ::mlir::detail::getDefaultStackAlignment(entry);
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/DestinationStyleOpInterface.td include/mlir/Interfaces/DestinationStyleOpInterface.td
88c88
<       /*retTy=*/"::mlir::OpOperandVector",
---
>       /*retTy=*/"OpOperandVector",
95c95
<         ::mlir::OpOperandVector result;
---
>         OpOperandVector result;
104c104
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
118c118
<       /*args=*/(ins "int64_t":$i, "::mlir::Value":$value),
---
>       /*args=*/(ins "int64_t":$i, "Value":$value),
138c138
<       /*retTy=*/"::mlir::OpOperandVector",
---
>       /*retTy=*/"OpOperandVector",
147c147
<         ::mlir::OpOperandVector result;
---
>         OpOperandVector result;
159c159
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
176c176
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
188c188
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
200c200
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
209c209
<       /*retTy=*/"::mlir::OpResult",
---
>       /*retTy=*/"OpResult",
211c211
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
225c225
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
227c227
<       /*args=*/(ins "::mlir::OpResult":$opResult),
---
>       /*args=*/(ins "OpResult":$opResult),
245,246c245,246
<           ::llvm::all_of($_op->getOpOperands(),
<             [&](::mlir::OpOperand &opOperand) {
---
>           llvm::all_of($_op->getOpOperands(),
>             [&](OpOperand &opOperand) {
248c248
<                      opOperand.get().getType().template isa<::mlir::MemRefType>();
---
>                      opOperand.get().getType().template isa<MemRefType>();
259,260c259,260
<         return ::llvm::all_of($_op->getOpOperands(),
<           [&](::mlir::OpOperand &opOperand) {
---
>         return llvm::all_of($_op->getOpOperands(),
>           [&](OpOperand &opOperand) {
262c262
<                    opOperand.get().getType().template isa<::mlir::RankedTensorType>();
---
>                    opOperand.get().getType().template isa<RankedTensorType>();
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/InferTypeOpInterface.h include/mlir/Interfaces/InferTypeOpInterface.h
29,35c29
< using ReifiedRankedShapedTypeDims = SmallVector<SmallVector<OpFoldResult>>;
< 
< /// Reify the shape of the result of an operation (typically in terms of the
< /// shape of its operands).
< LogicalResult
< reifyResultShapes(OpBuilder &b, Operation *op,
<                   ReifiedRankedShapedTypeDims &reifiedReturnShapes);
---
> using ReifiedRankedShapedTypeDims = SmallVector<SmallVector<Value>>;
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/InferTypeOpInterface.td include/mlir/Interfaces/InferTypeOpInterface.td
214,215c214,215
<         Reify the shape of the result of an operation (typically in terms of the
<         shape of its operands).
---
>         Reify the shape of the result of an operation (typically in
>         terms of shape of its operands)
217,223c217,222
<         `reifiedReturnShapes` is populated with one vector per op result. Each
<         of those vectors contains an OpFoldResult for each dimension of the
<         shaped type. In case a dimension in the type is static, the
<         corresponding entry is an IntegerAttr. Otherwise, it is a Value. The
<         given builder may be used to insert ops that compute result shapes.
< 
<         If the shape of a particular result cannot be computed it must be empty.
---
>         Insert operations using the given `OpBuilder` that computes
>         the result shape. The `reifiedReturnShapes` is expected to be
>         populated with as many vectors as the number of results of the
>         op. Each of these vectors is expected to be of size equal to
>         rank of the corresponding result. If the shape of a particular
>         result cannot be computed it must be empty.
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/LoopLikeInterface.td include/mlir/Interfaces/LoopLikeInterface.td
100,106d99
< 
<   let extraClassDeclaration = [{
<     /// Returns if a block is inside a loop (within the current function). This
<     /// can either be because the block is nested inside a LoopLikeInterface, or
<     /// because the control flow graph is cyclic
<     static bool blockIsInLoop(Block *block);
<   }];
Only in ../../../../llvm-project.0/mlir/include/mlir/Interfaces: Mem2RegInterfaces.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Interfaces: Mem2RegInterfaces.td
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/SideEffectInterfaceBase.td include/mlir/Interfaces/SideEffectInterfaceBase.td
64c64
<       ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>       SmallVectorImpl<::mlir::SideEffects::EffectInstance<
67,68c67,68
<       ::llvm::erase_if(effects, [&](auto &it) {
<         return !::llvm::isa<Effect>(it.getEffect());
---
>       llvm::erase_if(effects, [&](auto &it) {
>         return !llvm::isa<Effect>(it.getEffect());
74,75c74
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
77,78c76,77
<       return ::llvm::any_of(effects, [](const auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect());
---
>       return llvm::any_of(effects, [](const auto &it) {
>         return llvm::isa<Effect>(it.getEffect());
84,85c83
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
87,88c85,86
<       return !effects.empty() && ::llvm::all_of(effects, [](const auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect());
---
>       return !effects.empty() && llvm::all_of(effects, [](const auto &it) {
>         return isa<Effect>(it.getEffect());
94,95c92
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<::mlir::SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
103c100
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
106c103
<       ::llvm::erase_if(effects, [&](auto &it) { return it.getValue() != value; });
---
>       llvm::erase_if(effects, [&](auto &it) { return it.getValue() != value; });
114c111
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
---
>       llvm::SmallVector<::mlir::SideEffects::EffectInstance<
117,118c114,115
<       auto it = ::llvm::find_if(effects, [&](auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect()) && it.getValue() == value;
---
>       auto it = llvm::find_if(effects, [&](auto &it) {
>         return isa<Effect>(it.getEffect()) && it.getValue() == value;
128c125
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
131c128
<       ::llvm::erase_if(effects, [&](auto &it) {
---
>       llvm::erase_if(effects, [&](auto &it) {
139c136
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
142c139
<       ::llvm::erase_if(effects, [&](auto &it) {
---
>       llvm::erase_if(effects, [&](auto &it) {
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/TilingInterface.h include/mlir/Interfaces/TilingInterface.h
24,37d23
< namespace mlir {
< 
< /// Container for result values of tiling.
< /// - `tiledOps` contains operations created by the tiling implementation that
< /// are returned to the caller for further transformations.
< /// - `tiledValues` contains the tiled value corresponding to the result of the
< /// untiled operation.
< struct TilingResult {
<   SmallVector<Operation *> tiledOps;
<   SmallVector<Value> tiledValues;
< };
< 
< } // namespace mlir
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/TilingInterface.td include/mlir/Interfaces/TilingInterface.td
66c66
<         /*retType=*/"FailureOr<TilingResult>",
---
>         /*retType=*/"SmallVector<Operation *>",
122c122
<         /*retType=*/"FailureOr<TilingResult>",
---
>         /*retType=*/"FailureOr<Value>",
Only in ../../../../llvm-project.0/mlir/include/mlir/Interfaces: Utils
Only in ../../../../llvm-project.0/mlir/include/mlir/Interfaces: ValueBoundsOpInterface.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Interfaces: ValueBoundsOpInterface.td
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/VectorInterfaces.td include/mlir/Interfaces/VectorInterfaces.td
28c28
<         `targetShape`. Return `std::nullopt` if the op cannot be unrolled to the target
---
>         `targetShape`. Return `None` if the op cannot be unrolled to the target
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/ViewLikeInterface.h include/mlir/Interfaces/ViewLikeInterface.h
51,54c51,52
< void printDynamicIndexList(
<     OpAsmPrinter &printer, Operation *op, OperandRange values,
<     ArrayRef<int64_t> integers,
<     AsmParser::Delimiter delimiter = AsmParser::Delimiter::Square);
---
> void printDynamicIndexList(OpAsmPrinter &printer, Operation *op,
>                            OperandRange values, ArrayRef<int64_t> integers);
69,73c67,70
< ParseResult parseDynamicIndexList(
<     OpAsmParser &parser,
<     SmallVectorImpl<OpAsmParser::UnresolvedOperand> &values,
<     DenseI64ArrayAttr &integers,
<     AsmParser::Delimiter delimiter = AsmParser::Delimiter::Square);
---
> ParseResult
> parseDynamicIndexList(OpAsmParser &parser,
>                       SmallVectorImpl<OpAsmParser::UnresolvedOperand> &values,
>                       DenseI64ArrayAttr &integers);
diff ../../../../llvm-project.0/mlir/include/mlir/Interfaces/ViewLikeInterface.td include/mlir/Interfaces/ViewLikeInterface.td
53,54c53,55
<          `ShapedType::kDynamic`, then the corresponding entry is a dynamic
<          offset (resp. size, stride).
---
>          `ShapedType::kDynamic` (resp. `ShapedType::kDynamic`,
>          `ShapedType::kDynamic`), then the corresponding entry is
>          a dynamic offset (resp. size, stride).
--- include/mlir/Interfaces/InferIntRangeInterface.h
--- include/mlir/Interfaces/DerivedAttributeOpInterface.h
--- include/mlir/Interfaces/LoopLikeInterface.h
--- include/mlir/Interfaces/InferTypeOpInterface.h
29,35c29
< using ReifiedRankedShapedTypeDims = SmallVector<SmallVector<OpFoldResult>>;
< 
< /// Reify the shape of the result of an operation (typically in terms of the
< /// shape of its operands).
< LogicalResult
< reifyResultShapes(OpBuilder &b, Operation *op,
<                   ReifiedRankedShapedTypeDims &reifiedReturnShapes);
---
> using ReifiedRankedShapedTypeDims = SmallVector<SmallVector<Value>>;
--- include/mlir/Interfaces/ViewLikeInterface.td
53,54c53,55
<          `ShapedType::kDynamic`, then the corresponding entry is a dynamic
<          offset (resp. size, stride).
---
>          `ShapedType::kDynamic` (resp. `ShapedType::kDynamic`,
>          `ShapedType::kDynamic`), then the corresponding entry is
>          a dynamic offset (resp. size, stride).
--- include/mlir/Interfaces/SideEffectInterfaces.td
--- include/mlir/Interfaces/CopyOpInterface.h
--- include/mlir/Interfaces/DestinationStyleOpInterface.td
88c88
<       /*retTy=*/"::mlir::OpOperandVector",
---
>       /*retTy=*/"OpOperandVector",
95c95
<         ::mlir::OpOperandVector result;
---
>         OpOperandVector result;
104c104
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
118c118
<       /*args=*/(ins "int64_t":$i, "::mlir::Value":$value),
---
>       /*args=*/(ins "int64_t":$i, "Value":$value),
138c138
<       /*retTy=*/"::mlir::OpOperandVector",
---
>       /*retTy=*/"OpOperandVector",
147c147
<         ::mlir::OpOperandVector result;
---
>         OpOperandVector result;
159c159
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
176c176
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
188c188
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
200c200
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
209c209
<       /*retTy=*/"::mlir::OpResult",
---
>       /*retTy=*/"OpResult",
211c211
<       /*args=*/(ins "::mlir::OpOperand *":$opOperand),
---
>       /*args=*/(ins "OpOperand *":$opOperand),
225c225
<       /*retTy=*/"::mlir::OpOperand *",
---
>       /*retTy=*/"OpOperand *",
227c227
<       /*args=*/(ins "::mlir::OpResult":$opResult),
---
>       /*args=*/(ins "OpResult":$opResult),
245,246c245,246
<           ::llvm::all_of($_op->getOpOperands(),
<             [&](::mlir::OpOperand &opOperand) {
---
>           llvm::all_of($_op->getOpOperands(),
>             [&](OpOperand &opOperand) {
248c248
<                      opOperand.get().getType().template isa<::mlir::MemRefType>();
---
>                      opOperand.get().getType().template isa<MemRefType>();
259,260c259,260
<         return ::llvm::all_of($_op->getOpOperands(),
<           [&](::mlir::OpOperand &opOperand) {
---
>         return llvm::all_of($_op->getOpOperands(),
>           [&](OpOperand &opOperand) {
262c262
<                    opOperand.get().getType().template isa<::mlir::RankedTensorType>();
---
>                    opOperand.get().getType().template isa<RankedTensorType>();
--- include/mlir/Interfaces/InferTypeOpInterface.td
214,215c214,215
<         Reify the shape of the result of an operation (typically in terms of the
<         shape of its operands).
---
>         Reify the shape of the result of an operation (typically in
>         terms of shape of its operands)
217,223c217,222
<         `reifiedReturnShapes` is populated with one vector per op result. Each
<         of those vectors contains an OpFoldResult for each dimension of the
<         shaped type. In case a dimension in the type is static, the
<         corresponding entry is an IntegerAttr. Otherwise, it is a Value. The
<         given builder may be used to insert ops that compute result shapes.
< 
<         If the shape of a particular result cannot be computed it must be empty.
---
>         Insert operations using the given `OpBuilder` that computes
>         the result shape. The `reifiedReturnShapes` is expected to be
>         populated with as many vectors as the number of results of the
>         op. Each of these vectors is expected to be of size equal to
>         rank of the corresponding result. If the shape of a particular
>         result cannot be computed it must be empty.
--- include/mlir/Interfaces/ControlFlowInterfaces.h
--- include/mlir/Interfaces/VectorInterfaces.h
--- include/mlir/Interfaces/CallInterfaces.h
22a23,26
> 
> namespace func {
> class FuncOp;
> }
--- include/mlir/Interfaces/LoopLikeInterface.td
100,106d99
< 
<   let extraClassDeclaration = [{
<     /// Returns if a block is inside a loop (within the current function). This
<     /// can either be because the block is nested inside a LoopLikeInterface, or
<     /// because the control flow graph is cyclic
<     static bool blockIsInLoop(Block *block);
<   }];
--- include/mlir/Interfaces/InferIntRangeInterface.td
--- include/mlir/Interfaces/CallInterfaces.td
55c55
<     >,  
---
>     >,
62,69c62
<       }],
<       /*retTy=*/"::mlir::CallInterfaceCallable",
<       /*methodName=*/"getCallableForCallee",
<       /*args=*/(ins),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return $_op->template getAttrOfType<SymbolRefAttr>("callee");
<       }]
---
>       }], "::mlir::CallInterfaceCallable", "getCallableForCallee"
76,82c69
<       /*retTy=*/"::mlir::Operation::operand_range",
<       /*methodName=*/"getCallOperands",
<       /*args=*/(ins),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return $_op->getOperands();
<       }]
---
>       "::mlir::Operation::operand_range", "getArgOperands"
111d97
<         In the default case, there should not be any segments.
121c107
<     >
---
>     >,
160,171d145
<     InterfaceMethod<[{
<         Returns the argument attributes for all callable region arguments or
<         null if there are none.
<       }],
<       "::mlir::ArrayAttr", "getCallableArgAttrs"
<     >,
<     InterfaceMethod<[{
<         Returns the result attributes for all callable region results or null
<         if there are none.
<       }],
<       "::mlir::ArrayAttr", "getCallableResAttrs"
<     >
--- include/mlir/Interfaces/TilingInterface.td
66c66
<         /*retType=*/"FailureOr<TilingResult>",
---
>         /*retType=*/"SmallVector<Operation *>",
122c122
<         /*retType=*/"FailureOr<TilingResult>",
---
>         /*retType=*/"FailureOr<Value>",
--- include/mlir/Interfaces/SideEffectInterfaceBase.td
64c64
<       ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>       SmallVectorImpl<::mlir::SideEffects::EffectInstance<
67,68c67,68
<       ::llvm::erase_if(effects, [&](auto &it) {
<         return !::llvm::isa<Effect>(it.getEffect());
---
>       llvm::erase_if(effects, [&](auto &it) {
>         return !llvm::isa<Effect>(it.getEffect());
74,75c74
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
77,78c76,77
<       return ::llvm::any_of(effects, [](const auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect());
---
>       return llvm::any_of(effects, [](const auto &it) {
>         return llvm::isa<Effect>(it.getEffect());
84,85c83
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
87,88c85,86
<       return !effects.empty() && ::llvm::all_of(effects, [](const auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect());
---
>       return !effects.empty() && llvm::all_of(effects, [](const auto &it) {
>         return isa<Effect>(it.getEffect());
94,95c92
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
<                                             }] # baseEffect # [{>, 4> effects;
---
>       SmallVector<::mlir::SideEffects::EffectInstance<}] # baseEffect # [{>, 4> effects;
103c100
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
106c103
<       ::llvm::erase_if(effects, [&](auto &it) { return it.getValue() != value; });
---
>       llvm::erase_if(effects, [&](auto &it) { return it.getValue() != value; });
114c111
<       ::llvm::SmallVector<::mlir::SideEffects::EffectInstance<
---
>       llvm::SmallVector<::mlir::SideEffects::EffectInstance<
117,118c114,115
<       auto it = ::llvm::find_if(effects, [&](auto &it) {
<         return ::llvm::isa<Effect>(it.getEffect()) && it.getValue() == value;
---
>       auto it = llvm::find_if(effects, [&](auto &it) {
>         return isa<Effect>(it.getEffect()) && it.getValue() == value;
128c125
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
131c128
<       ::llvm::erase_if(effects, [&](auto &it) {
---
>       llvm::erase_if(effects, [&](auto &it) {
139c136
<               ::llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
---
>               llvm::SmallVectorImpl<::mlir::SideEffects::EffectInstance<
142c139
<       ::llvm::erase_if(effects, [&](auto &it) {
---
>       llvm::erase_if(effects, [&](auto &it) {
--- include/mlir/Interfaces/RuntimeVerifiableOpInterface.td
--- include/mlir/Interfaces/ViewLikeInterface.h
51,54c51,52
< void printDynamicIndexList(
<     OpAsmPrinter &printer, Operation *op, OperandRange values,
<     ArrayRef<int64_t> integers,
<     AsmParser::Delimiter delimiter = AsmParser::Delimiter::Square);
---
> void printDynamicIndexList(OpAsmPrinter &printer, Operation *op,
>                            OperandRange values, ArrayRef<int64_t> integers);
69,73c67,70
< ParseResult parseDynamicIndexList(
<     OpAsmParser &parser,
<     SmallVectorImpl<OpAsmParser::UnresolvedOperand> &values,
<     DenseI64ArrayAttr &integers,
<     AsmParser::Delimiter delimiter = AsmParser::Delimiter::Square);
---
> ParseResult
> parseDynamicIndexList(OpAsmParser &parser,
>                       SmallVectorImpl<OpAsmParser::UnresolvedOperand> &values,
>                       DenseI64ArrayAttr &integers);
--- include/mlir/Interfaces/CMakeLists.txt
10d9
< add_mlir_interface(Mem2RegInterfaces)
16d14
< add_mlir_interface(ValueBoundsOpInterface)
--- include/mlir/Interfaces/ParallelCombiningOpInterface.h
--- include/mlir/Interfaces/DestinationStyleOpInterface.h
--- include/mlir/Interfaces/CastInterfaces.td
--- include/mlir/Interfaces/ControlFlowInterfaces.td
70c70
<         some successor, or std::nullopt if `operandIndex` isn't a successor operand
---
>         some successor, or None if `operandIndex` isn't a successor operand
--- include/mlir/Interfaces/CopyOpInterface.td
--- include/mlir/Interfaces/SideEffectInterfaces.h
--- include/mlir/Interfaces/ShapedOpInterfaces.td
--- include/mlir/Interfaces/RuntimeVerifiableOpInterface.h
--- include/mlir/Interfaces/ShapedOpInterfaces.h
--- include/mlir/Interfaces/DataLayoutInterfaces.td
109,120d108
<     InterfaceMethod<
<       /*description=*/"Returns the alloca memory space identifier.",
<       /*retTy=*/"::mlir::StringAttr",
<       /*methodName=*/"getAllocaMemorySpaceIdentifier",
<       /*args=*/(ins "::mlir::MLIRContext *":$context)
<     >,
<     InterfaceMethod<
<       /*description=*/"Returns the stack alignment identifier.",
<       /*retTy=*/"::mlir::StringAttr",
<       /*methodName=*/"getStackAlignmentIdentifier",
<       /*args=*/(ins "::mlir::MLIRContext *":$context)
<     >,
269,292d256
<       }]
<     >,
<     StaticInterfaceMethod<
<       /*description=*/"Returns the memory space used by the ABI computed "
<                       "using the relevant entries. The data layout object "
<                       "can be used for recursive queries.",
<       /*retTy=*/"::mlir::Attribute",
<       /*methodName=*/"getAllocaMemorySpace",
<       /*args=*/(ins "::mlir::DataLayoutEntryInterface":$entry),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return ::mlir::detail::getDefaultAllocaMemorySpace(entry);
<       }]
<     >,
<     StaticInterfaceMethod<
<       /*description=*/"Returns the natural stack alignment in bits computed "
<                       "using the relevant entries. The data layout object "
<                       "can be used for recursive queries.",
<       /*retTy=*/"unsigned",
<       /*methodName=*/"getStackAlignment",
<       /*args=*/(ins "::mlir::DataLayoutEntryInterface":$entry),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         return ::mlir::detail::getDefaultStackAlignment(entry);
--- include/mlir/Interfaces/DerivedAttributeOpInterface.td
--- include/mlir/Interfaces/TilingInterface.h
24,37d23
< namespace mlir {
< 
< /// Container for result values of tiling.
< /// - `tiledOps` contains operations created by the tiling implementation that
< /// are returned to the caller for further transformations.
< /// - `tiledValues` contains the tiled value corresponding to the result of the
< /// untiled operation.
< struct TilingResult {
<   SmallVector<Operation *> tiledOps;
<   SmallVector<Value> tiledValues;
< };
< 
< } // namespace mlir
< 
--- include/mlir/Interfaces/CastInterfaces.h
--- include/mlir/Interfaces/FoldInterfaces.h
--- include/mlir/Interfaces/VectorInterfaces.td
28c28
<         `targetShape`. Return `std::nullopt` if the op cannot be unrolled to the target
---
>         `targetShape`. Return `None` if the op cannot be unrolled to the target
--- include/mlir/Interfaces/ParallelCombiningOpInterface.td
--- include/mlir/Interfaces/DataLayoutInterfaces.h
59,66d58
< /// Default handler for alloca memory space request. Dispatches to the
< /// DataLayoutInterface if specified, otherwise returns the default.
< Attribute getDefaultAllocaMemorySpace(DataLayoutEntryInterface entry);
< 
< /// Default handler for the stack alignment request. Dispatches to the
< /// DataLayoutInterface if specified, otherwise returns the default.
< unsigned getDefaultStackAlignment(DataLayoutEntryInterface entry);
< 
170,178d161
<   /// Returns the memory space used for AllocaOps.
<   Attribute getAllocaMemorySpace() const;
< 
<   /// Returns the natural alignment of the stack in bits. Alignment promotion of
<   /// stack variables should be limited to the natural stack alignment to
<   /// prevent dynamic stack alignment. Returns zero if the stack alignment is
<   /// unspecified.
<   unsigned getStackAlignment() const;
< 
200,205d182
< 
<   /// Cache for alloca memory space.
<   mutable std::optional<Attribute> allocaMemorySpace;
< 
<   /// Cache for stack alignment.
<   mutable std::optional<unsigned> stackAlignment;
--- include/mlir/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/DialectConversion.h include/mlir/Transforms/DialectConversion.h
24d23
< class Attribute;
91,118d89
<   /// The general result of a type attribute conversion callback, allowing
<   /// for early termination. The default constructor creates the na case.
<   class AttributeConversionResult {
<   public:
<     constexpr AttributeConversionResult() : impl() {}
<     AttributeConversionResult(Attribute attr) : impl(attr, resultTag) {}
< 
<     static AttributeConversionResult result(Attribute attr);
<     static AttributeConversionResult na();
<     static AttributeConversionResult abort();
< 
<     bool hasResult() const;
<     bool isNa() const;
<     bool isAbort() const;
< 
<     Attribute getResult() const;
< 
<   private:
<     AttributeConversionResult(Attribute attr, unsigned tag) : impl(attr, tag) {}
< 
<     llvm::PointerIntPair<Attribute, 2> impl;
<     // Note that na is 0 so that we can use PointerIntPair's default
<     // constructor.
<     static constexpr unsigned naTag = 0;
<     static constexpr unsigned resultTag = 1;
<     static constexpr unsigned abortTag = 2;
<   };
< 
188,215d158
<   /// Register a conversion function for attributes within types. Type
<   /// converters may call this function in order to allow hoking into the
<   /// translation of attributes that exist within types. For example, a type
<   /// converter for the `memref` type could use these conversions to convert
<   /// memory spaces or layouts in an extensible way.
<   ///
<   /// The conversion functions take a non-null Type or subclass of Type and a
<   /// non-null Attribute (or subclass of Attribute), and returns a
<   /// `AttributeConversionResult`. This result can either contan an `Attribute`,
<   /// which may be `nullptr`, representing the conversion's success,
<   /// `AttributeConversionResult::na()` (the default empty value), indicating
<   /// that the conversion function did not apply and that further conversion
<   /// functions should be checked, or `AttributeConversionResult::abort()`
<   /// indicating that the conversion process should be aborted.
<   ///
<   /// Registered conversion functions are callled in the reverse of the order in
<   /// which they were registered.
<   template <
<       typename FnT,
<       typename T =
<           typename llvm::function_traits<std::decay_t<FnT>>::template arg_t<0>,
<       typename A =
<           typename llvm::function_traits<std::decay_t<FnT>>::template arg_t<1>>
<   void addTypeAttributeConversion(FnT &&callback) {
<     registerTypeAttributeConversion(
<         wrapTypeAttributeConversion<T, A>(std::forward<FnT>(callback)));
<   }
< 
226,233d168
<   /// Attempts a 1-1 type conversion, expecting the result type to be
<   /// `TargetType`. Returns the converted type cast to `TargetType` on success,
<   /// and a null type on conversion or cast failure.
<   template <typename TargetType>
<   TargetType convertType(Type t) {
<     return dyn_cast_or_null<TargetType>(convertType(t));
<   }
< 
294,299d228
<   /// Convert an attribute present `attr` from within the type `type` using
<   /// the registered conversion functions. If no applicable conversion has been
<   /// registered, return std::nullopt. Note that the empty attribute/`nullptr`
<   /// is a valid return value for this function.
<   std::optional<Attribute> convertTypeAttribute(Type type, Attribute attr);
< 
311,314d239
<   /// The signature of the callback used to convert a type attribute.
<   using TypeAttributeConversionCallbackFn =
<       std::function<AttributeConversionResult(Type, Attribute)>;
< 
339,340c264,266
<   /// With callback of form: `std::optional<LogicalResult>(
<   ///     T, SmallVectorImpl<Type> &)`.
---
>   /// With callback of form: `std::optional<LogicalResult>(T,
>   /// SmallVectorImpl<Type>
>   /// &)`
351,352c277,279
<   /// With callback of form: `std::optional<LogicalResult>(
<   ///     T, SmallVectorImpl<Type> &, ArrayRef<Type>)`.
---
>   /// With callback of form: `std::optional<LogicalResult>(T,
>   /// SmallVectorImpl<Type>
>   /// &, ArrayRef<Type>)`.
389,414d315
<   /// Generate a wrapper for the given memory space conversion callback. The
<   /// callback may take any subclass of `Attribute` and the wrapper will check
<   /// for the target attribute to be of the expected class before calling the
<   /// callback.
<   template <typename T, typename A, typename FnT>
<   TypeAttributeConversionCallbackFn
<   wrapTypeAttributeConversion(FnT &&callback) {
<     return [callback = std::forward<FnT>(callback)](
<                Type type, Attribute attr) -> AttributeConversionResult {
<       if (T derivedType = type.dyn_cast<T>()) {
<         if (A derivedAttr = attr.dyn_cast_or_null<A>())
<           return callback(derivedType, derivedAttr);
<       }
<       return AttributeConversionResult::na();
<     };
<   }
< 
<   /// Register a memory space conversion, clearing caches.
<   void
<   registerTypeAttributeConversion(TypeAttributeConversionCallbackFn callback) {
<     typeAttributeConversions.emplace_back(std::move(callback));
<     // Clear type conversions in case a memory space is lingering inside.
<     cachedDirectConversions.clear();
<     cachedMultiConversions.clear();
<   }
< 
423,425d323
<   /// The list of registered type attribute conversion functions.
<   SmallVector<TypeAttributeConversionCallbackFn, 2> typeAttributeConversions;
< 
629,630c527
< class ConversionPatternRewriter final : public PatternRewriter,
<                                         public RewriterBase::Listener {
---
> class ConversionPatternRewriter final : public PatternRewriter {
713,716c610,611
<   /// PatternRewriter hook for inlining the ops of a block into another block.
<   void inlineBlockBefore(Block *source, Block *dest, Block::iterator before,
<                          ValueRange argValues = std::nullopt) override;
<   using PatternRewriter::inlineBlockBefore;
---
>   /// PatternRewriter hook for merging a block into another.
>   void mergeBlocks(Block *source, Block *dest, ValueRange argValues) override;
756,758d650
<   using OpBuilder::getListener;
<   using OpBuilder::setListener;
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/FoldUtils.h include/mlir/Transforms/FoldUtils.h
20d19
< #include "mlir/IR/PatternMatch.h"
35,36c34
<   OperationFolder(MLIRContext *ctx, OpBuilder::Listener *listener = nullptr)
<       : interfaces(ctx), rewriter(ctx, listener) {}
---
>   OperationFolder(MLIRContext *ctx) : interfaces(ctx) {}
40c38,40
<   /// folded results, and returns success. If the op was completely folded it is
---
>   /// folded results, and returns success. `preReplaceAction` is invoked on `op`
>   /// before it is replaced. 'processGeneratedConstants' is invoked for any new
>   /// operations generated when folding. If the op was completely folded it is
42c42,46
<   LogicalResult tryToFold(Operation *op, bool *inPlaceUpdate = nullptr);
---
>   LogicalResult
>   tryToFold(Operation *op,
>             function_ref<void(Operation *)> processGeneratedConstants = nullptr,
>             function_ref<void(Operation *)> preReplaceAction = nullptr,
>             bool *inPlaceUpdate = nullptr);
60a65,108
>   /// Create an operation of specific op type with the given builder,
>   /// and immediately try to fold it. This function populates 'results' with
>   /// the results after folding the operation.
>   template <typename OpTy, typename... Args>
>   void create(OpBuilder &builder, SmallVectorImpl<Value> &results,
>               Location location, Args &&...args) {
>     // The op needs to be inserted only if the fold (below) fails, or the number
>     // of results produced by the successful folding is zero (which is treated
>     // as an in-place fold). Using create methods of the builder will insert the
>     // op, so not using it here.
>     OperationState state(location, OpTy::getOperationName());
>     OpTy::build(builder, state, std::forward<Args>(args)...);
>     Operation *op = Operation::create(state);
> 
>     if (failed(tryToFold(builder, op, results)) || results.empty()) {
>       builder.insert(op);
>       results.assign(op->result_begin(), op->result_end());
>       return;
>     }
>     op->destroy();
>   }
> 
>   /// Overload to create or fold a single result operation.
>   template <typename OpTy, typename... Args>
>   std::enable_if_t<OpTy::template hasTrait<OpTrait::OneResult>(), Value>
>   create(OpBuilder &builder, Location location, Args &&...args) {
>     SmallVector<Value, 1> results;
>     create<OpTy>(builder, results, location, std::forward<Args>(args)...);
>     return results.front();
>   }
> 
>   /// Overload to create or fold a zero result operation.
>   template <typename OpTy, typename... Args>
>   std::enable_if_t<OpTy::template hasTrait<OpTrait::ZeroResults>(), OpTy>
>   create(OpBuilder &builder, Location location, Args &&...args) {
>     auto op = builder.create<OpTy>(location, std::forward<Args>(args)...);
>     SmallVector<Value, 0> unused;
>     (void)tryToFold(op.getOperation(), unused);
> 
>     // Folding cannot remove a zero-result operation, so for convenience we
>     // continue to return it.
>     return op;
>   }
> 
64,68c112,115
<   /// Get or create a constant for use in the specified block. The constant may
<   /// be created in a parent block. On success this returns the constant
<   /// operation, nullptr otherwise.
<   Value getOrCreateConstant(Block *block, Dialect *dialect, Attribute value,
<                             Type type, Location loc);
---
>   /// Get or create a constant using the given builder. On success this returns
>   /// the constant operation, nullptr otherwise.
>   Value getOrCreateConstant(OpBuilder &builder, Dialect *dialect,
>                             Attribute value, Type type, Location loc);
84,90c131,141
<   LogicalResult tryToFold(Operation *op, SmallVectorImpl<Value> &results);
< 
<   /// Try to process a set of fold results. Populates `results` on success,
<   /// otherwise leaves it unchanged.
<   LogicalResult processFoldResults(Operation *op,
<                                    SmallVectorImpl<Value> &results,
<                                    ArrayRef<OpFoldResult> foldResults);
---
>   LogicalResult tryToFold(
>       OpBuilder &builder, Operation *op, SmallVectorImpl<Value> &results,
>       function_ref<void(Operation *)> processGeneratedConstants = nullptr);
> 
>   /// Try to process a set of fold results, generating constants as necessary.
>   /// Populates `results` on success, otherwise leaves it unchanged.
>   LogicalResult
>   processFoldResults(OpBuilder &builder, Operation *op,
>                      SmallVectorImpl<Value> &results,
>                      ArrayRef<OpFoldResult> foldResults,
>                      function_ref<void(Operation *)> processGeneratedConstants);
95,96c146,147
<                                     Dialect *dialect, Attribute value,
<                                     Type type, Location loc);
---
>                                     Dialect *dialect, OpBuilder &builder,
>                                     Attribute value, Type type, Location loc);
108,110d158
< 
<   /// A rewriter that performs all IR modifications.
<   IRRewriter rewriter;
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/GreedyPatternRewriteDriver.h include/mlir/Transforms/GreedyPatternRewriteDriver.h
21,31d20
< /// This enum controls which ops are put on the worklist during a greedy
< /// pattern rewrite.
< enum class GreedyRewriteStrictness {
<   /// No restrictions wrt. which ops are processed.
<   AnyOp,
<   /// Only pre-existing and newly created ops are processed.
<   ExistingAndNewOps,
<   /// Only pre-existing ops are processed.
<   ExistingOps
< };
< 
40,41d28
<   ///
<   /// Note: Only applicable when simplifying entire regions.
44,47c31,32
<   /// Perform control flow optimizations to the region tree after applying all
<   /// patterns.
<   ///
<   /// Note: Only applicable when simplifying entire regions.
---
>   // Perform control flow optimizations to the region tree after applying all
>   // patterns.
53,54d37
<   ///
<   /// Note: Only applicable when simplifying entire regions.
62,81d44
< 
<   /// Only ops within the scope are added to the worklist. If no scope is
<   /// specified, the closest enclosing region around the initial list of ops
<   /// is used as a scope.
<   Region *scope = nullptr;
< 
<   /// Strict mode can restrict the ops that are added to the worklist during
<   /// the rewrite.
<   ///
<   /// * GreedyRewriteStrictness::AnyOp: No ops are excluded.
<   /// * GreedyRewriteStrictness::ExistingAndNewOps: Only pre-existing ops (that
<   ///   were on the worklist at the very beginning) and newly created ops are
<   ///   enqueued. All other ops are excluded.
<   /// * GreedyRewriteStrictness::ExistingOps: Only pre-existing ops (that were
<   ///   were on the worklist at the very beginning) enqueued. All other ops are
<   ///   excluded.
<   GreedyRewriteStrictness strictMode = GreedyRewriteStrictness::AnyOp;
< 
<   /// An optional listener that should be notified about IR modifications.
<   RewriterBase::Listener *listener = nullptr;
88,90c51,53
< /// Rewrite ops in the given region, which must be isolated from above, by
< /// repeatedly applying the highest benefit patterns in a greedy work-list
< /// driven manner.
---
> /// Rewrite the regions of the specified operation, which must be isolated from
> /// above, by repeatedly applying the highest benefit patterns in a greedy
> /// work-list driven manner.
105c68
<     Region &region, const FrozenRewritePatternSet &patterns,
---
>     MutableArrayRef<Region> regions, const FrozenRewritePatternSet &patterns,
108,109c71
< /// Rewrite ops in all regions of the given op, which must be isolated from
< /// above.
---
> /// Rewrite the given regions, which must be isolated from above.
113,116c75
<   bool failed = false;
<   for (Region &region : op->getRegions())
<     failed |= applyPatternsAndFoldGreedily(region, patterns, config).failed();
<   return failure(failed);
---
>   return applyPatternsAndFoldGreedily(op->getRegions(), patterns, config);
119,120c78,82
< /// Applies the specified rewrite patterns on `ops` while also trying to fold
< /// these ops.
---
> /// Applies the specified patterns on `op` alone while also trying to fold it,
> /// by selecting the highest benefits patterns in a greedy manner. Returns
> /// success if no more patterns can be matched. `erased` is set to true if `op`
> /// was folded away or erased as a result of becoming dead. Note: This does not
> /// apply any patterns recursively to the regions of `op`.
122,135c84,99
< /// Newly created ops and other pre-existing ops that use results of rewritten
< /// ops or supply operands to such ops are simplified, unless such ops are
< /// excluded via `config.strictMode`. Any other ops remain unmodified (i.e.,
< /// regardless of `strictMode`).
< ///
< /// In addition to strictness, a region scope can be specified. Only ops within
< /// the scope are simplified. This is similar to `applyPatternsAndFoldGreedily`,
< /// where only ops within the given regions are simplified. If no scope is
< /// specified, it is assumed to be the first common enclosing region of the
< /// given ops.
< ///
< /// Note that ops in `ops` could be erased as result of folding, becoming dead,
< /// or via pattern rewrites. If more far reaching simplification is desired,
< /// applyPatternsAndFoldGreedily should be used.
---
> /// Returns success if the iterative process converged and no more patterns can
> /// be matched.
> LogicalResult applyOpPatternsAndFold(Operation *op,
>                                      const FrozenRewritePatternSet &patterns,
>                                      bool *erased = nullptr);
> 
> /// Applies the specified rewrite patterns on `ops` while also trying to fold
> /// these ops as well as any other ops that were in turn created due to such
> /// rewrites. Furthermore, any pre-existing ops in the IR outside of `ops`
> /// remain completely unmodified if `strict` is set to true. If `strict` is
> /// false, other operations that use results of rewritten ops or supply operands
> /// to such ops are in turn simplified; any other ops still remain unmodified
> /// (i.e., regardless of `strict`). Note that ops in `ops` could be erased as a
> /// result of folding, becoming dead, or via pattern rewrites. If more far
> /// reaching simplification is desired, applyPatternsAndFoldGreedily should be
> /// used.
139,144c103,105
< /// `allOpsErased` is set to true if all ops in `ops` were erased.
< LogicalResult
< applyOpPatternsAndFold(ArrayRef<Operation *> ops,
<                        const FrozenRewritePatternSet &patterns,
<                        GreedyRewriteConfig config = GreedyRewriteConfig(),
<                        bool *changed = nullptr, bool *allErased = nullptr);
---
> LogicalResult applyOpPatternsAndFold(ArrayRef<Operation *> ops,
>                                      const FrozenRewritePatternSet &patterns,
>                                      bool strict, bool *changed = nullptr);
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/InliningUtils.h include/mlir/Transforms/InliningUtils.h
16d15
< #include "mlir/IR/BuiltinAttributes.h"
145,173d143
<   /// Hook to transform the call arguments before using them to replace the
<   /// callee arguments. Returns a value of the same type or the `argument`
<   /// itself if nothing changed. The `argumentAttrs` dictionary is non-null even
<   /// if no attribute is present. The hook is called after converting the
<   /// callsite argument types using the materializeCallConversion callback, and
<   /// right before inlining the callee region. Any operations created using the
<   /// provided `builder` are inserted right before the inlined callee region. An
<   /// example use case is the insertion of copies for by value arguments.
<   virtual Value handleArgument(OpBuilder &builder, Operation *call,
<                                Operation *callable, Value argument,
<                                DictionaryAttr argumentAttrs) const {
<     return argument;
<   }
< 
<   /// Hook to transform the callee results before using them to replace the call
<   /// results. Returns a value of the same type or the `result` itself if
<   /// nothing changed. The `resultAttrs` dictionary is non-null even if no
<   /// attribute is present. The hook is called right before handling
<   /// terminators, and obtains the callee result before converting its type
<   /// using the `materializeCallConversion` callback. Any operations created
<   /// using the provided `builder` are inserted right after the inlined callee
<   /// region. An example use case is the insertion of copies for by value
<   /// results. NOTE: This hook is invoked after inlining the `callable` region.
<   virtual Value handleResult(OpBuilder &builder, Operation *call,
<                              Operation *callable, Value result,
<                              DictionaryAttr resultAttrs) const {
<     return result;
<   }
< 
216,223d185
< 
<   virtual Value handleArgument(OpBuilder &builder, Operation *call,
<                                Operation *callable, Value argument,
<                                DictionaryAttr argumentAttrs) const;
<   virtual Value handleResult(OpBuilder &builder, Operation *call,
<                              Operation *callable, Value result,
<                              DictionaryAttr resultAttrs) const;
< 
Only in ../../../../llvm-project.0/mlir/include/mlir/Transforms: Mem2Reg.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Transforms: OneToNTypeConversion.h
Only in include/mlir/Transforms: OutlinerUtils.h
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/Passes.h include/mlir/Transforms/Passes.h
34d33
< #define GEN_PASS_DECL_INLINER
36,38d34
< #define GEN_PASS_DECL_MEM2REG
< #define GEN_PASS_DECL_PRINTIRPASS
< #define GEN_PASS_DECL_PRINTOPSTATS
39a36,37
> #define GEN_PASS_DECL_PRINTOPSTATS
> #define GEN_PASS_DECL_INLINER
68,70d65
< 
< /// Creates a pass to print IR on the debug stream.
< std::unique_ptr<Pass> createPrintIRPass(const PrintIRPassOptions & = {});
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/Passes.td include/mlir/Transforms/Passes.td
88,99d87
< def PrintIRPass : Pass<"print-ir"> {
<   let summary = "Print IR on the debug stream";
<   let description = [{
<     Print the entire IR on the debug stream. This is meant for debugging
<     purposes to inspect the IR at a specific point in the pipeline.
<   }];
<   let constructor = "mlir::createPrintIRPass()";
<   let options = [
<     Option<"label", "label", "std::string", /*default=*/"", "Label">,
<   ];
< }
< 
115c103
<            /*default=*/"\"canonicalize\"", "The default optimizer pipeline used for callables">,
---
>            /*default=*/"", "The default optimizer pipeline used for callables">,
148c136
<     of a [`Name Location`](Dialects/Builtin.md/#nameloc) with the specified tag.
---
>     of a [`Name Location`](Diagnostics.md#name-location) with the specified tag.
175,193d162
< def Mem2Reg : Pass<"mem2reg"> {
<   let summary = "Promotes memory slots into values.";
<   let description = [{
<     This pass removes loads out of and stores into a memory slot, and turns
<     them into direct uses of SSA values. This is done generically using the
<     `PromoteAllocationOpInterface`, `PromoteOpInterface` and
<     `PromoteMemOpInterface` interfaces.
< 
<     This pass will attempt to compute which definitions of the content of
<     the memory slot reach operations that use the memory slot pointer. It
<     will rewire or remove operations that use the slot pointer so they no
<     longer use it. If any of this is not possible, the IR will be left
<     without mutation.
< 
<     This pass only supports unstructured control-flow. Promotion of operations
<     within subregions will not happen.
<   }];
< }
< 
221c190
<     operation locations with [`unknown`](Dialects/Builtin.md/#unknownloc).
---
>     operation locations with [`unknown`](Diagnostics.md#unknown-location).
233c202
<     [visibility](SymbolsAndSymbolTables.md/#symbol-visibility) that extends
---
>     [visibility](SymbolsAndSymbolTables.md#symbol-visibility) that extends
diff ../../../../llvm-project.0/mlir/include/mlir/Transforms/RegionUtils.h include/mlir/Transforms/RegionUtils.h
54,71d53
< /// Make a region isolated from above
< /// - Capture the values that are defined above the region and used within it.
< /// - Append to the entry block arguments that represent the captured values
< /// (one per captured value).
< /// - Replace all uses within the region of the captured values with the
< ///   newly added arguments.
< /// - `cloneOperationIntoRegion` is a callback that allows caller to specify
< ///   if the operation defining an `OpOperand` needs to be cloned into the
< ///   region. Then the operands of this operation become part of the captured
< ///   values set (unless the operations that define the operands themeselves
< ///   are to be cloned). The cloned operations are added to the entry block
< ///   of the region.
< /// Return the set of captured values for the operation.
< SmallVector<Value> makeRegionIsolatedFromAbove(
<     RewriterBase &rewriter, Region &region,
<     llvm::function_ref<bool(Operation *)> cloneOperationIntoRegion =
<         [](Operation *) { return false; });
< 
--- include/mlir/Transforms/LoopInvariantCodeMotionUtils.h
--- include/mlir/Transforms/DialectConversion.h
24d23
< class Attribute;
91,118d89
<   /// The general result of a type attribute conversion callback, allowing
<   /// for early termination. The default constructor creates the na case.
<   class AttributeConversionResult {
<   public:
<     constexpr AttributeConversionResult() : impl() {}
<     AttributeConversionResult(Attribute attr) : impl(attr, resultTag) {}
< 
<     static AttributeConversionResult result(Attribute attr);
<     static AttributeConversionResult na();
<     static AttributeConversionResult abort();
< 
<     bool hasResult() const;
<     bool isNa() const;
<     bool isAbort() const;
< 
<     Attribute getResult() const;
< 
<   private:
<     AttributeConversionResult(Attribute attr, unsigned tag) : impl(attr, tag) {}
< 
<     llvm::PointerIntPair<Attribute, 2> impl;
<     // Note that na is 0 so that we can use PointerIntPair's default
<     // constructor.
<     static constexpr unsigned naTag = 0;
<     static constexpr unsigned resultTag = 1;
<     static constexpr unsigned abortTag = 2;
<   };
< 
188,215d158
<   /// Register a conversion function for attributes within types. Type
<   /// converters may call this function in order to allow hoking into the
<   /// translation of attributes that exist within types. For example, a type
<   /// converter for the `memref` type could use these conversions to convert
<   /// memory spaces or layouts in an extensible way.
<   ///
<   /// The conversion functions take a non-null Type or subclass of Type and a
<   /// non-null Attribute (or subclass of Attribute), and returns a
<   /// `AttributeConversionResult`. This result can either contan an `Attribute`,
<   /// which may be `nullptr`, representing the conversion's success,
<   /// `AttributeConversionResult::na()` (the default empty value), indicating
<   /// that the conversion function did not apply and that further conversion
<   /// functions should be checked, or `AttributeConversionResult::abort()`
<   /// indicating that the conversion process should be aborted.
<   ///
<   /// Registered conversion functions are callled in the reverse of the order in
<   /// which they were registered.
<   template <
<       typename FnT,
<       typename T =
<           typename llvm::function_traits<std::decay_t<FnT>>::template arg_t<0>,
<       typename A =
<           typename llvm::function_traits<std::decay_t<FnT>>::template arg_t<1>>
<   void addTypeAttributeConversion(FnT &&callback) {
<     registerTypeAttributeConversion(
<         wrapTypeAttributeConversion<T, A>(std::forward<FnT>(callback)));
<   }
< 
226,233d168
<   /// Attempts a 1-1 type conversion, expecting the result type to be
<   /// `TargetType`. Returns the converted type cast to `TargetType` on success,
<   /// and a null type on conversion or cast failure.
<   template <typename TargetType>
<   TargetType convertType(Type t) {
<     return dyn_cast_or_null<TargetType>(convertType(t));
<   }
< 
294,299d228
<   /// Convert an attribute present `attr` from within the type `type` using
<   /// the registered conversion functions. If no applicable conversion has been
<   /// registered, return std::nullopt. Note that the empty attribute/`nullptr`
<   /// is a valid return value for this function.
<   std::optional<Attribute> convertTypeAttribute(Type type, Attribute attr);
< 
311,314d239
<   /// The signature of the callback used to convert a type attribute.
<   using TypeAttributeConversionCallbackFn =
<       std::function<AttributeConversionResult(Type, Attribute)>;
< 
339,340c264,266
<   /// With callback of form: `std::optional<LogicalResult>(
<   ///     T, SmallVectorImpl<Type> &)`.
---
>   /// With callback of form: `std::optional<LogicalResult>(T,
>   /// SmallVectorImpl<Type>
>   /// &)`
351,352c277,279
<   /// With callback of form: `std::optional<LogicalResult>(
<   ///     T, SmallVectorImpl<Type> &, ArrayRef<Type>)`.
---
>   /// With callback of form: `std::optional<LogicalResult>(T,
>   /// SmallVectorImpl<Type>
>   /// &, ArrayRef<Type>)`.
389,414d315
<   /// Generate a wrapper for the given memory space conversion callback. The
<   /// callback may take any subclass of `Attribute` and the wrapper will check
<   /// for the target attribute to be of the expected class before calling the
<   /// callback.
<   template <typename T, typename A, typename FnT>
<   TypeAttributeConversionCallbackFn
<   wrapTypeAttributeConversion(FnT &&callback) {
<     return [callback = std::forward<FnT>(callback)](
<                Type type, Attribute attr) -> AttributeConversionResult {
<       if (T derivedType = type.dyn_cast<T>()) {
<         if (A derivedAttr = attr.dyn_cast_or_null<A>())
<           return callback(derivedType, derivedAttr);
<       }
<       return AttributeConversionResult::na();
<     };
<   }
< 
<   /// Register a memory space conversion, clearing caches.
<   void
<   registerTypeAttributeConversion(TypeAttributeConversionCallbackFn callback) {
<     typeAttributeConversions.emplace_back(std::move(callback));
<     // Clear type conversions in case a memory space is lingering inside.
<     cachedDirectConversions.clear();
<     cachedMultiConversions.clear();
<   }
< 
423,425d323
<   /// The list of registered type attribute conversion functions.
<   SmallVector<TypeAttributeConversionCallbackFn, 2> typeAttributeConversions;
< 
629,630c527
< class ConversionPatternRewriter final : public PatternRewriter,
<                                         public RewriterBase::Listener {
---
> class ConversionPatternRewriter final : public PatternRewriter {
713,716c610,611
<   /// PatternRewriter hook for inlining the ops of a block into another block.
<   void inlineBlockBefore(Block *source, Block *dest, Block::iterator before,
<                          ValueRange argValues = std::nullopt) override;
<   using PatternRewriter::inlineBlockBefore;
---
>   /// PatternRewriter hook for merging a block into another.
>   void mergeBlocks(Block *source, Block *dest, ValueRange argValues) override;
756,758d650
<   using OpBuilder::getListener;
<   using OpBuilder::setListener;
< 
--- include/mlir/Transforms/ControlFlowSinkUtils.h
--- include/mlir/Transforms/Passes.h
34d33
< #define GEN_PASS_DECL_INLINER
36,38d34
< #define GEN_PASS_DECL_MEM2REG
< #define GEN_PASS_DECL_PRINTIRPASS
< #define GEN_PASS_DECL_PRINTOPSTATS
39a36,37
> #define GEN_PASS_DECL_PRINTOPSTATS
> #define GEN_PASS_DECL_INLINER
68,70d65
< 
< /// Creates a pass to print IR on the debug stream.
< std::unique_ptr<Pass> createPrintIRPass(const PrintIRPassOptions & = {});
--- include/mlir/Transforms/Passes.td
88,99d87
< def PrintIRPass : Pass<"print-ir"> {
<   let summary = "Print IR on the debug stream";
<   let description = [{
<     Print the entire IR on the debug stream. This is meant for debugging
<     purposes to inspect the IR at a specific point in the pipeline.
<   }];
<   let constructor = "mlir::createPrintIRPass()";
<   let options = [
<     Option<"label", "label", "std::string", /*default=*/"", "Label">,
<   ];
< }
< 
115c103
<            /*default=*/"\"canonicalize\"", "The default optimizer pipeline used for callables">,
---
>            /*default=*/"", "The default optimizer pipeline used for callables">,
148c136
<     of a [`Name Location`](Dialects/Builtin.md/#nameloc) with the specified tag.
---
>     of a [`Name Location`](Diagnostics.md#name-location) with the specified tag.
175,193d162
< def Mem2Reg : Pass<"mem2reg"> {
<   let summary = "Promotes memory slots into values.";
<   let description = [{
<     This pass removes loads out of and stores into a memory slot, and turns
<     them into direct uses of SSA values. This is done generically using the
<     `PromoteAllocationOpInterface`, `PromoteOpInterface` and
<     `PromoteMemOpInterface` interfaces.
< 
<     This pass will attempt to compute which definitions of the content of
<     the memory slot reach operations that use the memory slot pointer. It
<     will rewire or remove operations that use the slot pointer so they no
<     longer use it. If any of this is not possible, the IR will be left
<     without mutation.
< 
<     This pass only supports unstructured control-flow. Promotion of operations
<     within subregions will not happen.
<   }];
< }
< 
221c190
<     operation locations with [`unknown`](Dialects/Builtin.md/#unknownloc).
---
>     operation locations with [`unknown`](Diagnostics.md#unknown-location).
233c202
<     [visibility](SymbolsAndSymbolTables.md/#symbol-visibility) that extends
---
>     [visibility](SymbolsAndSymbolTables.md#symbol-visibility) that extends
--- include/mlir/Transforms/RegionUtils.h
54,71d53
< /// Make a region isolated from above
< /// - Capture the values that are defined above the region and used within it.
< /// - Append to the entry block arguments that represent the captured values
< /// (one per captured value).
< /// - Replace all uses within the region of the captured values with the
< ///   newly added arguments.
< /// - `cloneOperationIntoRegion` is a callback that allows caller to specify
< ///   if the operation defining an `OpOperand` needs to be cloned into the
< ///   region. Then the operands of this operation become part of the captured
< ///   values set (unless the operations that define the operands themeselves
< ///   are to be cloned). The cloned operations are added to the entry block
< ///   of the region.
< /// Return the set of captured values for the operation.
< SmallVector<Value> makeRegionIsolatedFromAbove(
<     RewriterBase &rewriter, Region &region,
<     llvm::function_ref<bool(Operation *)> cloneOperationIntoRegion =
<         [](Operation *) { return false; });
< 
--- include/mlir/Transforms/ViewOpGraph.h
--- include/mlir/Transforms/GreedyPatternRewriteDriver.h
21,31d20
< /// This enum controls which ops are put on the worklist during a greedy
< /// pattern rewrite.
< enum class GreedyRewriteStrictness {
<   /// No restrictions wrt. which ops are processed.
<   AnyOp,
<   /// Only pre-existing and newly created ops are processed.
<   ExistingAndNewOps,
<   /// Only pre-existing ops are processed.
<   ExistingOps
< };
< 
40,41d28
<   ///
<   /// Note: Only applicable when simplifying entire regions.
44,47c31,32
<   /// Perform control flow optimizations to the region tree after applying all
<   /// patterns.
<   ///
<   /// Note: Only applicable when simplifying entire regions.
---
>   // Perform control flow optimizations to the region tree after applying all
>   // patterns.
53,54d37
<   ///
<   /// Note: Only applicable when simplifying entire regions.
62,81d44
< 
<   /// Only ops within the scope are added to the worklist. If no scope is
<   /// specified, the closest enclosing region around the initial list of ops
<   /// is used as a scope.
<   Region *scope = nullptr;
< 
<   /// Strict mode can restrict the ops that are added to the worklist during
<   /// the rewrite.
<   ///
<   /// * GreedyRewriteStrictness::AnyOp: No ops are excluded.
<   /// * GreedyRewriteStrictness::ExistingAndNewOps: Only pre-existing ops (that
<   ///   were on the worklist at the very beginning) and newly created ops are
<   ///   enqueued. All other ops are excluded.
<   /// * GreedyRewriteStrictness::ExistingOps: Only pre-existing ops (that were
<   ///   were on the worklist at the very beginning) enqueued. All other ops are
<   ///   excluded.
<   GreedyRewriteStrictness strictMode = GreedyRewriteStrictness::AnyOp;
< 
<   /// An optional listener that should be notified about IR modifications.
<   RewriterBase::Listener *listener = nullptr;
88,90c51,53
< /// Rewrite ops in the given region, which must be isolated from above, by
< /// repeatedly applying the highest benefit patterns in a greedy work-list
< /// driven manner.
---
> /// Rewrite the regions of the specified operation, which must be isolated from
> /// above, by repeatedly applying the highest benefit patterns in a greedy
> /// work-list driven manner.
105c68
<     Region &region, const FrozenRewritePatternSet &patterns,
---
>     MutableArrayRef<Region> regions, const FrozenRewritePatternSet &patterns,
108,109c71
< /// Rewrite ops in all regions of the given op, which must be isolated from
< /// above.
---
> /// Rewrite the given regions, which must be isolated from above.
113,116c75
<   bool failed = false;
<   for (Region &region : op->getRegions())
<     failed |= applyPatternsAndFoldGreedily(region, patterns, config).failed();
<   return failure(failed);
---
>   return applyPatternsAndFoldGreedily(op->getRegions(), patterns, config);
119,120c78,82
< /// Applies the specified rewrite patterns on `ops` while also trying to fold
< /// these ops.
---
> /// Applies the specified patterns on `op` alone while also trying to fold it,
> /// by selecting the highest benefits patterns in a greedy manner. Returns
> /// success if no more patterns can be matched. `erased` is set to true if `op`
> /// was folded away or erased as a result of becoming dead. Note: This does not
> /// apply any patterns recursively to the regions of `op`.
122,135c84,99
< /// Newly created ops and other pre-existing ops that use results of rewritten
< /// ops or supply operands to such ops are simplified, unless such ops are
< /// excluded via `config.strictMode`. Any other ops remain unmodified (i.e.,
< /// regardless of `strictMode`).
< ///
< /// In addition to strictness, a region scope can be specified. Only ops within
< /// the scope are simplified. This is similar to `applyPatternsAndFoldGreedily`,
< /// where only ops within the given regions are simplified. If no scope is
< /// specified, it is assumed to be the first common enclosing region of the
< /// given ops.
< ///
< /// Note that ops in `ops` could be erased as result of folding, becoming dead,
< /// or via pattern rewrites. If more far reaching simplification is desired,
< /// applyPatternsAndFoldGreedily should be used.
---
> /// Returns success if the iterative process converged and no more patterns can
> /// be matched.
> LogicalResult applyOpPatternsAndFold(Operation *op,
>                                      const FrozenRewritePatternSet &patterns,
>                                      bool *erased = nullptr);
> 
> /// Applies the specified rewrite patterns on `ops` while also trying to fold
> /// these ops as well as any other ops that were in turn created due to such
> /// rewrites. Furthermore, any pre-existing ops in the IR outside of `ops`
> /// remain completely unmodified if `strict` is set to true. If `strict` is
> /// false, other operations that use results of rewritten ops or supply operands
> /// to such ops are in turn simplified; any other ops still remain unmodified
> /// (i.e., regardless of `strict`). Note that ops in `ops` could be erased as a
> /// result of folding, becoming dead, or via pattern rewrites. If more far
> /// reaching simplification is desired, applyPatternsAndFoldGreedily should be
> /// used.
139,144c103,105
< /// `allOpsErased` is set to true if all ops in `ops` were erased.
< LogicalResult
< applyOpPatternsAndFold(ArrayRef<Operation *> ops,
<                        const FrozenRewritePatternSet &patterns,
<                        GreedyRewriteConfig config = GreedyRewriteConfig(),
<                        bool *changed = nullptr, bool *allErased = nullptr);
---
> LogicalResult applyOpPatternsAndFold(ArrayRef<Operation *> ops,
>                                      const FrozenRewritePatternSet &patterns,
>                                      bool strict, bool *changed = nullptr);
--- include/mlir/Transforms/DialectConversion.pdll
--- include/mlir/Transforms/CommutativityUtils.h
--- include/mlir/Transforms/FoldUtils.h
20d19
< #include "mlir/IR/PatternMatch.h"
35,36c34
<   OperationFolder(MLIRContext *ctx, OpBuilder::Listener *listener = nullptr)
<       : interfaces(ctx), rewriter(ctx, listener) {}
---
>   OperationFolder(MLIRContext *ctx) : interfaces(ctx) {}
40c38,40
<   /// folded results, and returns success. If the op was completely folded it is
---
>   /// folded results, and returns success. `preReplaceAction` is invoked on `op`
>   /// before it is replaced. 'processGeneratedConstants' is invoked for any new
>   /// operations generated when folding. If the op was completely folded it is
42c42,46
<   LogicalResult tryToFold(Operation *op, bool *inPlaceUpdate = nullptr);
---
>   LogicalResult
>   tryToFold(Operation *op,
>             function_ref<void(Operation *)> processGeneratedConstants = nullptr,
>             function_ref<void(Operation *)> preReplaceAction = nullptr,
>             bool *inPlaceUpdate = nullptr);
60a65,108
>   /// Create an operation of specific op type with the given builder,
>   /// and immediately try to fold it. This function populates 'results' with
>   /// the results after folding the operation.
>   template <typename OpTy, typename... Args>
>   void create(OpBuilder &builder, SmallVectorImpl<Value> &results,
>               Location location, Args &&...args) {
>     // The op needs to be inserted only if the fold (below) fails, or the number
>     // of results produced by the successful folding is zero (which is treated
>     // as an in-place fold). Using create methods of the builder will insert the
>     // op, so not using it here.
>     OperationState state(location, OpTy::getOperationName());
>     OpTy::build(builder, state, std::forward<Args>(args)...);
>     Operation *op = Operation::create(state);
> 
>     if (failed(tryToFold(builder, op, results)) || results.empty()) {
>       builder.insert(op);
>       results.assign(op->result_begin(), op->result_end());
>       return;
>     }
>     op->destroy();
>   }
> 
>   /// Overload to create or fold a single result operation.
>   template <typename OpTy, typename... Args>
>   std::enable_if_t<OpTy::template hasTrait<OpTrait::OneResult>(), Value>
>   create(OpBuilder &builder, Location location, Args &&...args) {
>     SmallVector<Value, 1> results;
>     create<OpTy>(builder, results, location, std::forward<Args>(args)...);
>     return results.front();
>   }
> 
>   /// Overload to create or fold a zero result operation.
>   template <typename OpTy, typename... Args>
>   std::enable_if_t<OpTy::template hasTrait<OpTrait::ZeroResults>(), OpTy>
>   create(OpBuilder &builder, Location location, Args &&...args) {
>     auto op = builder.create<OpTy>(location, std::forward<Args>(args)...);
>     SmallVector<Value, 0> unused;
>     (void)tryToFold(op.getOperation(), unused);
> 
>     // Folding cannot remove a zero-result operation, so for convenience we
>     // continue to return it.
>     return op;
>   }
> 
64,68c112,115
<   /// Get or create a constant for use in the specified block. The constant may
<   /// be created in a parent block. On success this returns the constant
<   /// operation, nullptr otherwise.
<   Value getOrCreateConstant(Block *block, Dialect *dialect, Attribute value,
<                             Type type, Location loc);
---
>   /// Get or create a constant using the given builder. On success this returns
>   /// the constant operation, nullptr otherwise.
>   Value getOrCreateConstant(OpBuilder &builder, Dialect *dialect,
>                             Attribute value, Type type, Location loc);
84,90c131,141
<   LogicalResult tryToFold(Operation *op, SmallVectorImpl<Value> &results);
< 
<   /// Try to process a set of fold results. Populates `results` on success,
<   /// otherwise leaves it unchanged.
<   LogicalResult processFoldResults(Operation *op,
<                                    SmallVectorImpl<Value> &results,
<                                    ArrayRef<OpFoldResult> foldResults);
---
>   LogicalResult tryToFold(
>       OpBuilder &builder, Operation *op, SmallVectorImpl<Value> &results,
>       function_ref<void(Operation *)> processGeneratedConstants = nullptr);
> 
>   /// Try to process a set of fold results, generating constants as necessary.
>   /// Populates `results` on success, otherwise leaves it unchanged.
>   LogicalResult
>   processFoldResults(OpBuilder &builder, Operation *op,
>                      SmallVectorImpl<Value> &results,
>                      ArrayRef<OpFoldResult> foldResults,
>                      function_ref<void(Operation *)> processGeneratedConstants);
95,96c146,147
<                                     Dialect *dialect, Attribute value,
<                                     Type type, Location loc);
---
>                                     Dialect *dialect, OpBuilder &builder,
>                                     Attribute value, Type type, Location loc);
108,110d158
< 
<   /// A rewriter that performs all IR modifications.
<   IRRewriter rewriter;
--- include/mlir/Transforms/CMakeLists.txt
--- include/mlir/Transforms/TopologicalSortUtils.h
--- include/mlir/Transforms/OutlinerUtils.h
--- include/mlir/Transforms/LocationSnapshot.h
--- include/mlir/Transforms/InliningUtils.h
16d15
< #include "mlir/IR/BuiltinAttributes.h"
145,173d143
<   /// Hook to transform the call arguments before using them to replace the
<   /// callee arguments. Returns a value of the same type or the `argument`
<   /// itself if nothing changed. The `argumentAttrs` dictionary is non-null even
<   /// if no attribute is present. The hook is called after converting the
<   /// callsite argument types using the materializeCallConversion callback, and
<   /// right before inlining the callee region. Any operations created using the
<   /// provided `builder` are inserted right before the inlined callee region. An
<   /// example use case is the insertion of copies for by value arguments.
<   virtual Value handleArgument(OpBuilder &builder, Operation *call,
<                                Operation *callable, Value argument,
<                                DictionaryAttr argumentAttrs) const {
<     return argument;
<   }
< 
<   /// Hook to transform the callee results before using them to replace the call
<   /// results. Returns a value of the same type or the `result` itself if
<   /// nothing changed. The `resultAttrs` dictionary is non-null even if no
<   /// attribute is present. The hook is called right before handling
<   /// terminators, and obtains the callee result before converting its type
<   /// using the `materializeCallConversion` callback. Any operations created
<   /// using the provided `builder` are inserted right after the inlined callee
<   /// region. An example use case is the insertion of copies for by value
<   /// results. NOTE: This hook is invoked after inlining the `callable` region.
<   virtual Value handleResult(OpBuilder &builder, Operation *call,
<                              Operation *callable, Value result,
<                              DictionaryAttr resultAttrs) const {
<     return result;
<   }
< 
216,223d185
< 
<   virtual Value handleArgument(OpBuilder &builder, Operation *call,
<                                Operation *callable, Value argument,
<                                DictionaryAttr argumentAttrs) const;
<   virtual Value handleResult(OpBuilder &builder, Operation *call,
<                              Operation *callable, Value result,
<                              DictionaryAttr resultAttrs) const;
< 
--- include/mlir/Tools
Only in ../../../../llvm-project.0/mlir/include/mlir/Tools: lsp-server-support
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-lsp-server and include/mlir/Tools/mlir-lsp-server
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-opt and include/mlir/Tools/mlir-opt
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-pdll-lsp-server and include/mlir/Tools/mlir-pdll-lsp-server
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-reduce and include/mlir/Tools/mlir-reduce
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-tblgen and include/mlir/Tools/mlir-tblgen
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-translate and include/mlir/Tools/mlir-translate
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL and include/mlir/Tools/PDLL
Only in ../../../../llvm-project.0/mlir/include/mlir/Tools: Plugins
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/tblgen-lsp-server and include/mlir/Tools/tblgen-lsp-server
--- include/mlir/Tools/ParseUtilities.h
--- include/mlir/Tools/tblgen-lsp-server
--- include/mlir/Tools/tblgen-lsp-server/TableGenLspServerMain.h
--- include/mlir/Tools/mlir-lsp-server
--- include/mlir/Tools/mlir-lsp-server/MlirLspServerMain.h
--- include/mlir/Tools/mlir-translate
--- include/mlir/Tools/mlir-translate/MlirTranslateMain.h
--- include/mlir/Tools/mlir-translate/Translation.h
--- include/mlir/Tools/PDLL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL/AST and include/mlir/Tools/PDLL/AST
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL/CodeGen and include/mlir/Tools/PDLL/CodeGen
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL/ODS and include/mlir/Tools/PDLL/ODS
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL/Parser and include/mlir/Tools/PDLL/Parser
--- include/mlir/Tools/PDLL/CodeGen
--- include/mlir/Tools/PDLL/CodeGen/CPPGen.h
--- include/mlir/Tools/PDLL/CodeGen/MLIRGen.h
--- include/mlir/Tools/PDLL/Parser
--- include/mlir/Tools/PDLL/Parser/CodeComplete.h
--- include/mlir/Tools/PDLL/Parser/Parser.h
--- include/mlir/Tools/PDLL/ODS
--- include/mlir/Tools/PDLL/ODS/Context.h
--- include/mlir/Tools/PDLL/ODS/Constraint.h
--- include/mlir/Tools/PDLL/ODS/Dialect.h
--- include/mlir/Tools/PDLL/ODS/Operation.h
--- include/mlir/Tools/PDLL/AST
diff ../../../../llvm-project.0/mlir/include/mlir/Tools/PDLL/AST/Nodes.h include/mlir/Tools/PDLL/AST/Nodes.h
1055,1056c1055
<   /// The benefit of the pattern if it was explicitly specified, std::nullopt
<   /// otherwise.
---
>   /// The benefit of the pattern if it was explicitly specified, None otherwise.
--- include/mlir/Tools/PDLL/AST/Context.h
--- include/mlir/Tools/PDLL/AST/Types.h
--- include/mlir/Tools/PDLL/AST/Nodes.h
1055,1056c1055
<   /// The benefit of the pattern if it was explicitly specified, std::nullopt
<   /// otherwise.
---
>   /// The benefit of the pattern if it was explicitly specified, None otherwise.
--- include/mlir/Tools/PDLL/AST/Diagnostic.h
--- include/mlir/Tools/mlir-pdll-lsp-server
--- include/mlir/Tools/mlir-pdll-lsp-server/MlirPdllLspServerMain.h
--- include/mlir/Tools/mlir-opt
diff ../../../../llvm-project.0/mlir/include/mlir/Tools/mlir-opt/MlirOptMain.h include/mlir/Tools/mlir-opt/MlirOptMain.h
16d15
< #include "mlir/Debug/CLOptionsSetup.h"
21d19
< #include <functional>
34,203d31
< /// Configuration options for the mlir-opt tool.
< /// This is intended to help building tools like mlir-opt by collecting the
< /// supported options.
< /// The API is fluent, and the options are sorted in alphabetical order below.
< /// The options can be exposed to the LLVM command line by registering them
< /// with `MlirOptMainConfig::registerCLOptions(DialectRegistry &);` and creating
< /// a config using `auto config = MlirOptMainConfig::createFromCLOptions();`.
< class MlirOptMainConfig {
< public:
<   /// Register the options as global LLVM command line options.
<   static void registerCLOptions(DialectRegistry &dialectRegistry);
< 
<   /// Create a new config with the default set from the CL options.
<   static MlirOptMainConfig createFromCLOptions();
< 
<   ///
<   /// Options.
<   ///
< 
<   /// Allow operation with no registered dialects.
<   /// This option is for convenience during testing only and discouraged in
<   /// general.
<   MlirOptMainConfig &allowUnregisteredDialects(bool allow) {
<     allowUnregisteredDialectsFlag = allow;
<     return *this;
<   }
<   bool shouldAllowUnregisteredDialects() const {
<     return allowUnregisteredDialectsFlag;
<   }
< 
<   /// Set the debug configuration to use.
<   MlirOptMainConfig &setDebugConfig(tracing::DebugConfig config) {
<     debugConfig = std::move(config);
<     return *this;
<   }
<   tracing::DebugConfig &getDebugConfig() { return debugConfig; }
<   const tracing::DebugConfig &getDebugConfig() const { return debugConfig; }
< 
<   /// Print the pass-pipeline as text before executing.
<   MlirOptMainConfig &dumpPassPipeline(bool dump) {
<     dumpPassPipelineFlag = dump;
<     return *this;
<   }
<   bool shouldDumpPassPipeline() const { return dumpPassPipelineFlag; }
< 
<   /// Set the output format to bytecode instead of textual IR.
<   MlirOptMainConfig &emitBytecode(bool emit) {
<     emitBytecodeFlag = emit;
<     return *this;
<   }
<   bool shouldEmitBytecode() const { return emitBytecodeFlag; }
< 
<   /// Set the IRDL file to load before processing the input.
<   MlirOptMainConfig &setIrdlFile(StringRef file) {
<     irdlFileFlag = file;
<     return *this;
<   }
<   StringRef getIrdlFile() const { return irdlFileFlag; }
< 
<   /// Set the bytecode version to emit.
<   MlirOptMainConfig &setEmitBytecodeVersion(int64_t version) {
<     emitBytecodeVersion = version;
<     return *this;
<   }
<   std::optional<int64_t> bytecodeVersionToEmit() const {
<     return emitBytecodeVersion;
<   }
< 
<   /// Set the callback to populate the pass manager.
<   MlirOptMainConfig &
<   setPassPipelineSetupFn(std::function<LogicalResult(PassManager &)> callback) {
<     passPipelineCallback = std::move(callback);
<     return *this;
<   }
< 
<   /// Set the parser to use to populate the pass manager.
<   MlirOptMainConfig &setPassPipelineParser(const PassPipelineCLParser &parser);
< 
<   /// Populate the passmanager, if any callback was set.
<   LogicalResult setupPassPipeline(PassManager &pm) const {
<     if (passPipelineCallback)
<       return passPipelineCallback(pm);
<     return success();
<   }
< 
<   /// Show the registered dialects before trying to load the input file.
<   MlirOptMainConfig &showDialects(bool show) {
<     showDialectsFlag = show;
<     return *this;
<   }
<   bool shouldShowDialects() const { return showDialectsFlag; }
< 
<   /// Set whether to split the input file based on the `// -----` marker into
<   /// pieces and process each chunk independently.
<   MlirOptMainConfig &splitInputFile(bool split = true) {
<     splitInputFileFlag = split;
<     return *this;
<   }
<   bool shouldSplitInputFile() const { return splitInputFileFlag; }
< 
<   /// Disable implicit addition of a top-level module op during parsing.
<   MlirOptMainConfig &useExplicitModule(bool useExplicitModule) {
<     useExplicitModuleFlag = useExplicitModule;
<     return *this;
<   }
<   bool shouldUseExplicitModule() const { return useExplicitModuleFlag; }
< 
<   /// Set whether to check that emitted diagnostics match `expected-*` lines on
<   /// the corresponding line. This is meant for implementing diagnostic tests.
<   MlirOptMainConfig &verifyDiagnostics(bool verify) {
<     verifyDiagnosticsFlag = verify;
<     return *this;
<   }
<   bool shouldVerifyDiagnostics() const { return verifyDiagnosticsFlag; }
< 
<   /// Set whether to run the verifier after each transformation pass.
<   MlirOptMainConfig &verifyPasses(bool verify) {
<     verifyPassesFlag = verify;
<     return *this;
<   }
<   bool shouldVerifyPasses() const { return verifyPassesFlag; }
< 
< protected:
<   /// Allow operation with no registered dialects.
<   /// This option is for convenience during testing only and discouraged in
<   /// general.
<   bool allowUnregisteredDialectsFlag = false;
< 
<   /// Configuration for the debugging hooks.
<   tracing::DebugConfig debugConfig;
< 
<   /// Print the pipeline that will be run.
<   bool dumpPassPipelineFlag = false;
< 
<   /// Emit bytecode instead of textual assembly when generating output.
<   bool emitBytecodeFlag = false;
< 
<   /// Enable the Debugger action hook: Debugger can intercept MLIR Actions.
<   bool enableDebuggerActionHookFlag = false;
< 
<   /// IRDL file to register before processing the input.
<   std::string irdlFileFlag = "";
< 
<   /// Location Breakpoints to filter the action logging.
<   std::vector<tracing::BreakpointManager *> logActionLocationFilter;
< 
<   /// Emit bytecode at given version.
<   std::optional<int64_t> emitBytecodeVersion = std::nullopt;
< 
<   /// The callback to populate the pass manager.
<   std::function<LogicalResult(PassManager &)> passPipelineCallback;
< 
<   /// Show the registered dialects before trying to load the input file.
<   bool showDialectsFlag = false;
< 
<   /// Split the input file based on the `// -----` marker into pieces and
<   /// process each chunk independently.
<   bool splitInputFileFlag = false;
< 
<   /// Use an explicit top-level module op during parsing.
<   bool useExplicitModuleFlag = false;
< 
<   /// Set whether to check that emitted diagnostics match `expected-*` lines on
<   /// the corresponding line. This is meant for implementing diagnostic tests.
<   bool verifyDiagnosticsFlag = false;
< 
<   /// Run the verifier after each transformation pass.
<   bool verifyPassesFlag = true;
< };
< 
209c37
< /// Perform the core processing behind `mlir-opt`.
---
> /// Perform the core processing behind `mlir-opt`:
211a40
> /// - passPipeline is the specification of the pipeline that will be applied.
213,217c42,74
< /// - config contains the configuration options for the tool.
< LogicalResult MlirOptMain(llvm::raw_ostream &outputStream,
<                           std::unique_ptr<llvm::MemoryBuffer> buffer,
<                           DialectRegistry &registry,
<                           const MlirOptMainConfig &config);
---
> /// - splitInputFile will look for a "-----" marker in the input file, and load
> /// each chunk in an individual ModuleOp processed separately.
> /// - verifyDiagnostics enables a verification mode where comments starting with
> /// "expected-(error|note|remark|warning)" are parsed in the input and matched
> /// against emitted diagnostics.
> /// - verifyPasses enables the IR verifier in-between each pass in the pipeline.
> /// - allowUnregisteredDialects allows to parse and create operation without
> /// registering the Dialect in the MLIRContext.
> /// - preloadDialectsInContext will trigger the upfront loading of all
> ///   dialects from the global registry in the MLIRContext. This option is
> ///   deprecated and will be removed soon.
> /// - emitBytecode will generate bytecode output instead of text.
> /// - implicitModule will enable implicit addition of a top-level
> /// 'builtin.module' if one doesn't already exist.
> /// - dumpPassPipeline will dump the pipeline being run to stderr
> LogicalResult
> MlirOptMain(llvm::raw_ostream &outputStream,
>             std::unique_ptr<llvm::MemoryBuffer> buffer,
>             const PassPipelineCLParser &passPipeline, DialectRegistry &registry,
>             bool splitInputFile, bool verifyDiagnostics, bool verifyPasses,
>             bool allowUnregisteredDialects,
>             bool preloadDialectsInContext = false, bool emitBytecode = false,
>             bool implicitModule = false, bool dumpPassPipeline = false);
> 
> /// Support a callback to setup the pass manager.
> /// - passManagerSetupFn is the callback invoked to setup the pass manager to
> ///   apply on the loaded IR.
> LogicalResult MlirOptMain(
>     llvm::raw_ostream &outputStream, std::unique_ptr<llvm::MemoryBuffer> buffer,
>     PassPipelineFn passManagerSetupFn, DialectRegistry &registry,
>     bool splitInputFile, bool verifyDiagnostics, bool verifyPasses,
>     bool allowUnregisteredDialects, bool preloadDialectsInContext = false,
>     bool emitBytecode = false, bool implicitModule = false);
221a79,81
> /// - preloadDialectsInContext will trigger the upfront loading of all
> ///   dialects from the global registry in the MLIRContext. This option is
> ///   deprecated and will be removed soon.
223c83,84
<                           DialectRegistry &registry);
---
>                           DialectRegistry &registry,
>                           bool preloadDialectsInContext = false);
--- include/mlir/Tools/mlir-opt/MlirOptMain.h
16d15
< #include "mlir/Debug/CLOptionsSetup.h"
21d19
< #include <functional>
34,203d31
< /// Configuration options for the mlir-opt tool.
< /// This is intended to help building tools like mlir-opt by collecting the
< /// supported options.
< /// The API is fluent, and the options are sorted in alphabetical order below.
< /// The options can be exposed to the LLVM command line by registering them
< /// with `MlirOptMainConfig::registerCLOptions(DialectRegistry &);` and creating
< /// a config using `auto config = MlirOptMainConfig::createFromCLOptions();`.
< class MlirOptMainConfig {
< public:
<   /// Register the options as global LLVM command line options.
<   static void registerCLOptions(DialectRegistry &dialectRegistry);
< 
<   /// Create a new config with the default set from the CL options.
<   static MlirOptMainConfig createFromCLOptions();
< 
<   ///
<   /// Options.
<   ///
< 
<   /// Allow operation with no registered dialects.
<   /// This option is for convenience during testing only and discouraged in
<   /// general.
<   MlirOptMainConfig &allowUnregisteredDialects(bool allow) {
<     allowUnregisteredDialectsFlag = allow;
<     return *this;
<   }
<   bool shouldAllowUnregisteredDialects() const {
<     return allowUnregisteredDialectsFlag;
<   }
< 
<   /// Set the debug configuration to use.
<   MlirOptMainConfig &setDebugConfig(tracing::DebugConfig config) {
<     debugConfig = std::move(config);
<     return *this;
<   }
<   tracing::DebugConfig &getDebugConfig() { return debugConfig; }
<   const tracing::DebugConfig &getDebugConfig() const { return debugConfig; }
< 
<   /// Print the pass-pipeline as text before executing.
<   MlirOptMainConfig &dumpPassPipeline(bool dump) {
<     dumpPassPipelineFlag = dump;
<     return *this;
<   }
<   bool shouldDumpPassPipeline() const { return dumpPassPipelineFlag; }
< 
<   /// Set the output format to bytecode instead of textual IR.
<   MlirOptMainConfig &emitBytecode(bool emit) {
<     emitBytecodeFlag = emit;
<     return *this;
<   }
<   bool shouldEmitBytecode() const { return emitBytecodeFlag; }
< 
<   /// Set the IRDL file to load before processing the input.
<   MlirOptMainConfig &setIrdlFile(StringRef file) {
<     irdlFileFlag = file;
<     return *this;
<   }
<   StringRef getIrdlFile() const { return irdlFileFlag; }
< 
<   /// Set the bytecode version to emit.
<   MlirOptMainConfig &setEmitBytecodeVersion(int64_t version) {
<     emitBytecodeVersion = version;
<     return *this;
<   }
<   std::optional<int64_t> bytecodeVersionToEmit() const {
<     return emitBytecodeVersion;
<   }
< 
<   /// Set the callback to populate the pass manager.
<   MlirOptMainConfig &
<   setPassPipelineSetupFn(std::function<LogicalResult(PassManager &)> callback) {
<     passPipelineCallback = std::move(callback);
<     return *this;
<   }
< 
<   /// Set the parser to use to populate the pass manager.
<   MlirOptMainConfig &setPassPipelineParser(const PassPipelineCLParser &parser);
< 
<   /// Populate the passmanager, if any callback was set.
<   LogicalResult setupPassPipeline(PassManager &pm) const {
<     if (passPipelineCallback)
<       return passPipelineCallback(pm);
<     return success();
<   }
< 
<   /// Show the registered dialects before trying to load the input file.
<   MlirOptMainConfig &showDialects(bool show) {
<     showDialectsFlag = show;
<     return *this;
<   }
<   bool shouldShowDialects() const { return showDialectsFlag; }
< 
<   /// Set whether to split the input file based on the `// -----` marker into
<   /// pieces and process each chunk independently.
<   MlirOptMainConfig &splitInputFile(bool split = true) {
<     splitInputFileFlag = split;
<     return *this;
<   }
<   bool shouldSplitInputFile() const { return splitInputFileFlag; }
< 
<   /// Disable implicit addition of a top-level module op during parsing.
<   MlirOptMainConfig &useExplicitModule(bool useExplicitModule) {
<     useExplicitModuleFlag = useExplicitModule;
<     return *this;
<   }
<   bool shouldUseExplicitModule() const { return useExplicitModuleFlag; }
< 
<   /// Set whether to check that emitted diagnostics match `expected-*` lines on
<   /// the corresponding line. This is meant for implementing diagnostic tests.
<   MlirOptMainConfig &verifyDiagnostics(bool verify) {
<     verifyDiagnosticsFlag = verify;
<     return *this;
<   }
<   bool shouldVerifyDiagnostics() const { return verifyDiagnosticsFlag; }
< 
<   /// Set whether to run the verifier after each transformation pass.
<   MlirOptMainConfig &verifyPasses(bool verify) {
<     verifyPassesFlag = verify;
<     return *this;
<   }
<   bool shouldVerifyPasses() const { return verifyPassesFlag; }
< 
< protected:
<   /// Allow operation with no registered dialects.
<   /// This option is for convenience during testing only and discouraged in
<   /// general.
<   bool allowUnregisteredDialectsFlag = false;
< 
<   /// Configuration for the debugging hooks.
<   tracing::DebugConfig debugConfig;
< 
<   /// Print the pipeline that will be run.
<   bool dumpPassPipelineFlag = false;
< 
<   /// Emit bytecode instead of textual assembly when generating output.
<   bool emitBytecodeFlag = false;
< 
<   /// Enable the Debugger action hook: Debugger can intercept MLIR Actions.
<   bool enableDebuggerActionHookFlag = false;
< 
<   /// IRDL file to register before processing the input.
<   std::string irdlFileFlag = "";
< 
<   /// Location Breakpoints to filter the action logging.
<   std::vector<tracing::BreakpointManager *> logActionLocationFilter;
< 
<   /// Emit bytecode at given version.
<   std::optional<int64_t> emitBytecodeVersion = std::nullopt;
< 
<   /// The callback to populate the pass manager.
<   std::function<LogicalResult(PassManager &)> passPipelineCallback;
< 
<   /// Show the registered dialects before trying to load the input file.
<   bool showDialectsFlag = false;
< 
<   /// Split the input file based on the `// -----` marker into pieces and
<   /// process each chunk independently.
<   bool splitInputFileFlag = false;
< 
<   /// Use an explicit top-level module op during parsing.
<   bool useExplicitModuleFlag = false;
< 
<   /// Set whether to check that emitted diagnostics match `expected-*` lines on
<   /// the corresponding line. This is meant for implementing diagnostic tests.
<   bool verifyDiagnosticsFlag = false;
< 
<   /// Run the verifier after each transformation pass.
<   bool verifyPassesFlag = true;
< };
< 
209c37
< /// Perform the core processing behind `mlir-opt`.
---
> /// Perform the core processing behind `mlir-opt`:
211a40
> /// - passPipeline is the specification of the pipeline that will be applied.
213,217c42,74
< /// - config contains the configuration options for the tool.
< LogicalResult MlirOptMain(llvm::raw_ostream &outputStream,
<                           std::unique_ptr<llvm::MemoryBuffer> buffer,
<                           DialectRegistry &registry,
<                           const MlirOptMainConfig &config);
---
> /// - splitInputFile will look for a "-----" marker in the input file, and load
> /// each chunk in an individual ModuleOp processed separately.
> /// - verifyDiagnostics enables a verification mode where comments starting with
> /// "expected-(error|note|remark|warning)" are parsed in the input and matched
> /// against emitted diagnostics.
> /// - verifyPasses enables the IR verifier in-between each pass in the pipeline.
> /// - allowUnregisteredDialects allows to parse and create operation without
> /// registering the Dialect in the MLIRContext.
> /// - preloadDialectsInContext will trigger the upfront loading of all
> ///   dialects from the global registry in the MLIRContext. This option is
> ///   deprecated and will be removed soon.
> /// - emitBytecode will generate bytecode output instead of text.
> /// - implicitModule will enable implicit addition of a top-level
> /// 'builtin.module' if one doesn't already exist.
> /// - dumpPassPipeline will dump the pipeline being run to stderr
> LogicalResult
> MlirOptMain(llvm::raw_ostream &outputStream,
>             std::unique_ptr<llvm::MemoryBuffer> buffer,
>             const PassPipelineCLParser &passPipeline, DialectRegistry &registry,
>             bool splitInputFile, bool verifyDiagnostics, bool verifyPasses,
>             bool allowUnregisteredDialects,
>             bool preloadDialectsInContext = false, bool emitBytecode = false,
>             bool implicitModule = false, bool dumpPassPipeline = false);
> 
> /// Support a callback to setup the pass manager.
> /// - passManagerSetupFn is the callback invoked to setup the pass manager to
> ///   apply on the loaded IR.
> LogicalResult MlirOptMain(
>     llvm::raw_ostream &outputStream, std::unique_ptr<llvm::MemoryBuffer> buffer,
>     PassPipelineFn passManagerSetupFn, DialectRegistry &registry,
>     bool splitInputFile, bool verifyDiagnostics, bool verifyPasses,
>     bool allowUnregisteredDialects, bool preloadDialectsInContext = false,
>     bool emitBytecode = false, bool implicitModule = false);
221a79,81
> /// - preloadDialectsInContext will trigger the upfront loading of all
> ///   dialects from the global registry in the MLIRContext. This option is
> ///   deprecated and will be removed soon.
223c83,84
<                           DialectRegistry &registry);
---
>                           DialectRegistry &registry,
>                           bool preloadDialectsInContext = false);
--- include/mlir/Tools/mlir-reduce
--- include/mlir/Tools/mlir-reduce/MlirReduceMain.h
--- include/mlir/Tools/mlir-tblgen
--- include/mlir/Tools/mlir-tblgen/MlirTblgenMain.h
--- include/mlir/IR
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: Action.h
diff ../../../../llvm-project.0/mlir/include/mlir/IR/AffineExpr.h include/mlir/IR/AffineExpr.h
323a324,330
> template <typename AffineExprTy>
> void bindSymbolsList(MLIRContext *ctx, SmallVectorImpl<AffineExprTy> &exprs) {
>   int idx = 0;
>   for (AffineExprTy &e : exprs)
>     e = getAffineSymbolExpr(idx++, ctx);
> }
> 
333,339d339
< template <typename AffineExprTy>
< void bindDimsList(MLIRContext *ctx, MutableArrayRef<AffineExprTy> exprs) {
<   int idx = 0;
<   for (AffineExprTy &e : exprs)
<     e = getAffineDimExpr(idx++, ctx);
< }
< 
345,351d344
< }
< 
< template <typename AffineExprTy>
< void bindSymbolsList(MLIRContext *ctx, MutableArrayRef<AffineExprTy> exprs) {
<   int idx = 0;
<   for (AffineExprTy &e : exprs)
<     e = getAffineSymbolExpr(idx++, ctx);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/AffineExprVisitor.h include/mlir/IR/AffineExprVisitor.h
327c327
<   // IntegerRelation::addLocalFloorDiv).
---
>   // FlatAffineConstraints::addLocalFloorDiv).
diff ../../../../llvm-project.0/mlir/include/mlir/IR/AffineMap.h include/mlir/IR/AffineMap.h
252,254c252
<   AffineMap dropResult(int64_t pos) const {
<     return dropResults(ArrayRef({pos}));
<   }
---
>   AffineMap dropResult(int64_t pos) { return dropResults({pos}); }
257,258c255,256
<   // results in `positions` dropped.
<   AffineMap dropResults(ArrayRef<int64_t> positions) const {
---
>   // positions in `positions` dropped from results.
>   AffineMap dropResults(ArrayRef<int64_t> positions) {
268,271d265
<   // Returns a new AffineMap with the same number of dims and symbols, but all
<   // results in `positions` dropped.
<   AffineMap dropResults(const llvm::SmallBitVector &positions) const;
< 
274c268
<   AffineMap insertResult(AffineExpr expr, unsigned pos) const {
---
>   AffineMap insertResult(AffineExpr expr, unsigned pos) {
412,414d405
< /// Drop the dims that are listed in `unusedDims`.
< AffineMap compressDims(AffineMap map, const llvm::SmallBitVector &unusedDims);
< 
423,425c414,415
< /// Drop the symbols that are listed in `unusedSymbols`.
< AffineMap compressSymbols(AffineMap map,
<                           const llvm::SmallBitVector &unusedSymbols);
---
> /// Drop the dims that are not listed in `unusedDims`.
> AffineMap compressDims(AffineMap map, const llvm::SmallBitVector &unusedDims);
434a425,428
> /// Drop the symbols that are not listed in `unusedSymbols`.
> AffineMap compressSymbols(AffineMap map,
>                           const llvm::SmallBitVector &unusedSymbols);
> 
478c472
< /// Prerequisites: `map` must be a projected permutation.
---
> /// Prerequisites: `map` must be a projected permuation.
568,578c562
< /// This function also compresses the dims when the boolean flag is true.
< AffineMap projectDims(AffineMap map,
<                       const llvm::SmallBitVector &projectedDimensions,
<                       bool compressDimsFlag = false);
< /// Symbol counterpart of `projectDims`.
< /// This function also compresses the symbols when the boolean flag is true.
< AffineMap projectSymbols(AffineMap map,
<                          const llvm::SmallBitVector &projectedSymbols,
<                          bool compressSymbolsFlag = false);
< /// Calls `projectDims(map, projectedDimensions, compressDimsFlag)`.
< /// If `compressSymbolsFlag` is true, additionally call `compressUnusedSymbols`.
---
> /// This function also compresses unused symbols away.
580,605c564
<                           const llvm::SmallBitVector &projectedDimensions,
<                           bool compressDimsFlag = true,
<                           bool compressSymbolsFlag = true);
< 
< // Return a bitvector where each bit set indicates a dimension that is not used
< // by any of the maps in the input array `maps`.
< llvm::SmallBitVector getUnusedDimsBitVector(ArrayRef<AffineMap> maps);
< 
< // Return a bitvector where each bit set indicates a symbol that is not used
< // by any of the maps in the input array `maps`.
< llvm::SmallBitVector getUnusedSymbolsBitVector(ArrayRef<AffineMap> maps);
< 
< /// Expand `map` to operate on `rank` dims while projecting out the dims in
< /// `projectedDimensions`. This amounts to composing `map` with
< /// `id(rank).dropResults(projectedDimensions)`.
< AffineMap expandDimsToRank(AffineMap map, int64_t rank,
<                            const llvm::SmallBitVector &projectedDimensions);
< 
< inline raw_ostream &operator<<(raw_ostream &os, AffineMap map) {
<   map.print(os);
<   return os;
< }
< 
< //===----------------------------------------------------------------------===//
< // Templated helper functions.
< //===----------------------------------------------------------------------===//
---
>                           const llvm::SmallBitVector &projectedDimensions);
628c587
< /// Calculates maximum dimension and symbol positions from the expressions
---
> /// Calculates maxmimum dimension and symbol positions from the expressions
643a603,611
> 
> inline raw_ostream &operator<<(raw_ostream &os, AffineMap map) {
>   map.print(os);
>   return os;
> }
> 
> // Return a bitvector where each bit set indicates a dimension that is not used
> // by any of the maps in the input array `maps`.
> llvm::SmallBitVector getUnusedDimsBitVector(ArrayRef<AffineMap> maps);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Attributes.h include/mlir/IR/Attributes.h
62a63,65
>   // Support dyn_cast'ing Attribute to itself.
>   static bool classof(Attribute) { return true; }
> 
101,142d103
<   /// Walk all of the immediately nested sub-attributes and sub-types. This
<   /// method does not recurse into sub elements.
<   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const {
<     getAbstractAttribute().walkImmediateSubElements(*this, walkAttrsFn,
<                                                     walkTypesFn);
<   }
< 
<   /// Replace the immediately nested sub-attributes and sub-types with those
<   /// provided. The order of the provided elements is derived from the order of
<   /// the elements returned by the callbacks of `walkImmediateSubElements`. The
<   /// element at index 0 would replace the very first attribute given by
<   /// `walkImmediateSubElements`. On success, the new instance with the values
<   /// replaced is returned. If replacement fails, nullptr is returned.
<   auto replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const {
<     return getAbstractAttribute().replaceImmediateSubElements(*this, replAttrs,
<                                                               replTypes);
<   }
< 
<   /// Walk this attribute and all attibutes/types nested within using the
<   /// provided walk functions. See `AttrTypeWalker` for information on the
<   /// supported walk function types.
<   template <WalkOrder Order = WalkOrder::PostOrder, typename... WalkFns>
<   auto walk(WalkFns &&...walkFns) {
<     AttrTypeWalker walker;
<     (walker.addWalk(std::forward<WalkFns>(walkFns)), ...);
<     return walker.walk<Order>(*this);
<   }
< 
<   /// Recursively replace all of the nested sub-attributes and sub-types using
<   /// the provided map functions. Returns nullptr in the case of failure. See
<   /// `AttrTypeReplacer` for information on the support replacement function
<   /// types.
<   template <typename... ReplacementFns>
<   auto replace(ReplacementFns &&...replacementFns) {
<     AttrTypeReplacer replacer;
<     (replacer.addReplacement(std::forward<ReplacementFns>(replacementFns)),
<      ...);
<     return replacer.replace(*this);
<   }
< 
243,258d203
< /// Allow walking and replacing the subelements of a NamedAttribute.
< template <>
< struct AttrTypeSubElementHandler<NamedAttribute> {
<   template <typename T>
<   static void walk(T param, AttrTypeImmediateSubElementWalker &walker) {
<     walker.walk(param.getName());
<     walker.walk(param.getValue());
<   }
<   template <typename T>
<   static T replace(T param, AttrSubElementReplacements &attrRepls,
<                    TypeSubElementReplacements &typeRepls) {
<     ArrayRef<Attribute> paramRepls = attrRepls.take_front(2);
<     return T(cast<decltype(param.getName())>(paramRepls[0]), paramRepls[1]);
<   }
< };
< 
394,399c339,340
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     }
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/AttributeSupport.h include/mlir/IR/AttributeSupport.h
22a23,25
> class MLIRContext;
> class Type;
> 
32,35d34
<   using WalkImmediateSubElementsFn = function_ref<void(
<       Attribute, function_ref<void(Attribute)>, function_ref<void(Type)>)>;
<   using ReplaceImmediateSubElementsFn =
<       function_ref<Attribute(Attribute, ArrayRef<Attribute>, ArrayRef<Type>)>;
46,47d44
<                              T::getWalkImmediateSubElementsFn(),
<                              T::getReplaceImmediateSubElementsFn(),
55,60c52,54
<   static AbstractAttribute
<   get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
<       HasTraitFn &&hasTrait,
<       WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<       ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<       TypeID typeID) {
---
>   static AbstractAttribute get(Dialect &dialect,
>                                detail::InterfaceMap &&interfaceMap,
>                                HasTraitFn &&hasTrait, TypeID typeID) {
62,63c56
<                              std::move(hasTrait), walkImmediateSubElementsFn,
<                              replaceImmediateSubElementsFn, typeID);
---
>                              std::move(hasTrait), typeID);
92,101d84
<   /// Walk the immediate sub-elements of this attribute.
<   void walkImmediateSubElements(Attribute attr,
<                                 function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const;
< 
<   /// Replace the immediate sub-elements of this attribute.
<   Attribute replaceImmediateSubElements(Attribute attr,
<                                         ArrayRef<Attribute> replAttrs,
<                                         ArrayRef<Type> replTypes) const;
< 
107,110c90
<                     HasTraitFn &&hasTraitFn,
<                     WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<                     ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<                     TypeID typeID)
---
>                     HasTraitFn &&hasTrait, TypeID typeID)
112,115c92
<         hasTraitFn(std::move(hasTraitFn)),
<         walkImmediateSubElementsFn(walkImmediateSubElementsFn),
<         replaceImmediateSubElementsFn(replaceImmediateSubElementsFn),
<         typeID(typeID) {}
---
>         hasTraitFn(std::move(hasTrait)), typeID(typeID) {}
136,141d112
<   /// Function to walk the immediate sub-elements of this attribute.
<   WalkImmediateSubElementsFn walkImmediateSubElementsFn;
< 
<   /// Function to replace the immediate sub-elements of this attribute.
<   ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn;
< 
296,308d266
< 
< // Internal function called by ODS generated code.
< // Default initializes the type within a FailureOr<T> if T is default
< // constructible and returns a reference to the instance.
< // Otherwise, returns a reference to the FailureOr<T>.
< template <class T>
< decltype(auto) unwrapForCustomParse(FailureOr<T> &failureOr) {
<   if constexpr (std::is_default_constructible_v<T>)
<     return failureOr.emplace();
<   else
<     return failureOr;
< }
< 
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: AttrTypeSubElements.h
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Block.h include/mlir/IR/Block.h
261,264c261,263
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
270,271c269
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
274c272
<     return walk<Order, Iterator>(begin(), end(), std::forward<FnT>(callback));
---
>     return walk<Order>(begin(), end(), std::forward<FnT>(callback));
279,281c277,278
<   /// depending on the callback provided. The order in which regions, blocks and
<   /// operations at the same nesting level are visited (e.g., lexicographical or
<   /// reverse lexicographical order) is determined by 'Iterator'. The walk order
---
>   /// depending on the callback provided. Regions, blocks and operations at the
>   /// same nesting level are visited in lexicographical order. The walk order
288,289c285
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
294c290
<       detail::walk<Order, Iterator>(&op, callback);
---
>       detail::walk<Order>(&op, callback);
299,301c295,296
<   /// depending on the callback provided. The order in which regions, blocks and
<   /// operations at the same nesting level are visited (e.g., lexicographical or
<   /// reverse lexicographical order) is determined by 'Iterator'. The walk order
---
>   /// depending on the callback provided. Regions, blocks and operations at the
>   /// same nesting level are visited in lexicographical order. The walk order
309,310c304
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
315c309
<       if (detail::walk<Order, Iterator>(&op, callback).wasInterrupted())
---
>       if (detail::walk<Order>(&op, callback).wasInterrupted())
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Builders.h include/mlir/IR/Builders.h
67d66
<   FloatType getFloat8E4M3B11FNUZType();
119c118
<   TypedAttr getZeroAttr(Type type);
---
>   Attribute getZeroAttr(Type type);
257,276d255
<   /// Base class for listeners.
<   struct ListenerBase {
<     /// The kind of listener.
<     enum class Kind {
<       /// OpBuilder::Listener or user-derived class.
<       OpBuilderListener = 0,
< 
<       /// RewriterBase::Listener or user-derived class.
<       RewriterBaseListener = 1
<     };
< 
<     Kind getKind() const { return kind; }
< 
<   protected:
<     ListenerBase(Kind kind) : kind(kind) {}
< 
<   private:
<     const Kind kind;
<   };
< 
279,282c258,259
<   struct Listener : public ListenerBase {
<     Listener() : ListenerBase(ListenerBase::Kind::OpBuilderListener) {}
< 
<     virtual ~Listener() = default;
---
>   struct Listener {
>     virtual ~Listener();
291,293d267
< 
<   protected:
<     Listener(Kind kind) : ListenerBase(kind) {}
566,569d539
< protected:
<   /// The optional listener for events of this builder.
<   Listener *listener;
< 
575a546,547
>   /// The optional listener for events of this builder.
>   Listener *listener;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinAttributeInterfaces.td include/mlir/IR/BuiltinAttributeInterfaces.td
19,37d18
< // TypedAttrInterface
< //===----------------------------------------------------------------------===//
< 
< def TypedAttrInterface : AttrInterface<"TypedAttr"> {
<   let cppNamespace = "::mlir";
< 
<   let description = [{
<     This interface is used for attributes that have a type. The type of an
<     attribute is understood to represent the type of the data contained in the
<     attribute and is often used as the type of a value with this data.
<   }];
< 
<   let methods = [InterfaceMethod<
<     "Get the attribute's type",
<     "::mlir::Type", "getType"
<   >];
< }
< 
< //===----------------------------------------------------------------------===//
41c22
< def ElementsAttrInterface : AttrInterface<"ElementsAttr", [TypedAttrInterface]> {
---
> def ElementsAttrInterface : AttrInterface<"ElementsAttr"> {
100c81
<       mlir::Type elementType = getShapedType().getElementType();
---
>       mlir::Type elementType = getType().getElementType();
176c157
<     }], "bool", "isSplat", (ins), [{}], /*defaultImplementation=*/[{
---
>     }], "bool", "isSplat", (ins), /*defaultImplementation=*/[{}], [{
182,184c163
<     }], "::mlir::ShapedType", "getShapedType", (ins), [{}], /*defaultImplementation=*/[{
<         return $_attr.getType();
<     }]>
---
>     }], "::mlir::ShapedType", "getType">
349c328
<       return getFlattenedIndex(elementsAttr.getShapedType(), index);
---
>       return getFlattenedIndex(elementsAttr.getType(), index);
381c360
<       return {getShapedType(), value_begin<T>(), value_end<T>()};
---
>       return {getType(), value_begin<T>(), value_end<T>()};
401c380
<       return {getShapedType(), llvm::map_range(getValues<Attribute>(),
---
>       return {getType(), llvm::map_range(getValues<Attribute>(),
421c400
<         return iterator_range<T>(getShapedType(), *beginIt, value_end<T>());
---
>         return iterator_range<T>(getType(), *beginIt, value_end<T>());
437c416
<         getShapedType(),
---
>         getType(),
495a475,493
> }
> 
> //===----------------------------------------------------------------------===//
> // TypedAttrInterface
> //===----------------------------------------------------------------------===//
> 
> def TypedAttrInterface : AttrInterface<"TypedAttr"> {
>   let cppNamespace = "::mlir";
> 
>   let description = [{
>     This interface is used for attributes that have a type. The type of an
>     attribute is understood to represent the type of the data contained in the
>     attribute and is often used as the type of a value with this data.
>   }];
> 
>   let methods = [InterfaceMethod<
>     "Get the attribute's type",
>     "::mlir::Type", "getType"
>   >];
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinAttributes.h include/mlir/IR/BuiltinAttributes.h
12a13
> #include "mlir/IR/SubElementInterfaces.h"
81,83c82,84
<   operator ElementsAttr() const { return cast_if_present<ElementsAttr>(*this); }
<   /// Allow implicit conversion to TypedAttr.
<   operator TypedAttr() const { return ElementsAttr(*this); }
---
>   operator ElementsAttr() const {
>     return *this ? cast<ElementsAttr>() : nullptr;
>   }
845,846c846,847
<   /// Enable conversion to IntegerAttr and its interfaces. This uses conversion
<   /// vs. inheritance to avoid bringing in all of IntegerAttrs methods.
---
>   /// Enable conversion to IntegerAttr. This uses conversion vs. inheritance to
>   /// avoid bringing in all of IntegerAttrs methods.
848d848
<   operator TypedAttr() const { return IntegerAttr(impl); }
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinAttributes.td include/mlir/IR/BuiltinAttributes.td
20a21
> include "mlir/IR/SubElementInterfaces.td"
73c74,76
< def Builtin_ArrayAttr : Builtin_Attr<"Array"> {
---
> def Builtin_ArrayAttr : Builtin_Attr<"Array", [
>     SubElementAttrInterface
>   ]> {
221c224
<     "DenseIntOrFPElements", [ElementsAttrInterface],
---
>     "DenseIntOrFPElements", [ElementsAttrInterface, TypedAttrInterface],
362c365
<     "DenseStringElements", [ElementsAttrInterface],
---
>     "DenseStringElements", [ElementsAttrInterface, TypedAttrInterface],
433c436
<     ElementsAttrInterface
---
>     ElementsAttrInterface, TypedAttrInterface
491c494,496
< def Builtin_DictionaryAttr : Builtin_Attr<"Dictionary"> {
---
> def Builtin_DictionaryAttr : Builtin_Attr<"Dictionary", [
>     SubElementAttrInterface
>   ]> {
807c812
<     "SparseElements", [ElementsAttrInterface]
---
>     "SparseElements", [ElementsAttrInterface, TypedAttrInterface]
1094c1099,1101
< def Builtin_SymbolRefAttr : Builtin_Attr<"SymbolRef"> {
---
> def Builtin_SymbolRefAttr : Builtin_Attr<"SymbolRef", [
>     SubElementAttrInterface
>   ]> {
1109a1117,1123
>     This attribute can only be held internally by
>     [array attributes](#array-attribute),
>     [dictionary attributes](#dictionary-attribute)(including the top-level
>     operation attribute dictionary) as well as attributes exposing it via
>     the `SubElementAttrInterface` interface. Symbol reference attributes
>     nested in types are currently not supported.
> 
1160c1174,1176
< def Builtin_TypeAttr : Builtin_Attr<"Type"> {
---
> def Builtin_TypeAttr : Builtin_Attr<"Type", [
>     SubElementAttrInterface
>   ]> {
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: BuiltinDialectBytecode.td
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinDialect.td include/mlir/IR/BuiltinDialect.td
37a38
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinLocationAttributes.td include/mlir/IR/BuiltinLocationAttributes.td
17a18
> include "mlir/IR/SubElementInterfaces.td"
30c31,33
< def CallSiteLoc : Builtin_LocationAttr<"CallSiteLoc"> {
---
> def CallSiteLoc : Builtin_LocationAttr<"CallSiteLoc", [
>     SubElementAttrInterface
>   ]> {
107c110,112
< def FusedLoc : Builtin_LocationAttr<"FusedLoc"> {
---
> def FusedLoc : Builtin_LocationAttr<"FusedLoc", [
>     SubElementAttrInterface
>   ]> {
146c151,153
< def NameLoc : Builtin_LocationAttr<"NameLoc"> {
---
> def NameLoc : Builtin_LocationAttr<"NameLoc", [
>     SubElementAttrInterface
>   ]> {
183c190,192
< def OpaqueLoc : Builtin_LocationAttr<"OpaqueLoc"> {
---
> def OpaqueLoc : Builtin_LocationAttr<"OpaqueLoc", [
>     SubElementAttrInterface
>   ]> {
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinTypeInterfaces.td include/mlir/IR/BuiltinTypeInterfaces.td
101a102,110
> 
>     /// Returns the total amount of bits occupied by a value of this type. This
>     /// does not take into account any memory layout or widening constraints,
>     /// e.g. a vector<3xi57> may report to occupy 3x57=171 bit, even though in
>     /// practice it will likely be stored as in a 4xi64 vector register. Fails
>     /// with an assertion if the size cannot be computed statically, e.g. if the
>     /// type has a dynamic shape or if its elemental type does not have a known
>     /// bit width.
>     int64_t getSizeInBits() const;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinTypes.h include/mlir/IR/BuiltinTypes.h
13a14
> #include "mlir/IR/SubElementInterfaces.h"
52d52
<   static FloatType getFloat8E4M3B11FNUZ(MLIRContext *ctx);
94c94
<   /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>   /// provided shape is `None`, the current shape of the type is used.
130c130
<   /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>   /// provided shape is `None`, the current shape of the type is used.
380,383c380,382
<   return type
<       .isa<Float8E5M2Type, Float8E4M3FNType, Float8E5M2FNUZType,
<            Float8E4M3FNUZType, Float8E4M3B11FNUZType, BFloat16Type, Float16Type,
<            Float32Type, Float64Type, Float80Type, Float128Type>();
---
>   return type.isa<Float8E5M2Type, Float8E4M3FNType, Float8E5M2FNUZType,
>                   Float8E4M3FNUZType, BFloat16Type, Float16Type, Float32Type,
>                   Float64Type, Float80Type, Float128Type>();
400,403d398
< }
< 
< inline FloatType FloatType::getFloat8E4M3B11FNUZ(MLIRContext *ctx) {
<   return Float8E4M3B11FNUZType::get(ctx);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/BuiltinTypes.td include/mlir/IR/BuiltinTypes.td
19a20
> include "mlir/IR/SubElementInterfaces.td"
166,187d166
< // Float8E4M3B11FNUZType
< 
< def Builtin_Float8E4M3B11FNUZ : Builtin_FloatType<"Float8E4M3B11FNUZ"> {
<   let summary = "8-bit floating point with 3 bit mantissa";
<   let description = [{
<     An 8-bit floating point type with 1 sign bit, 4 bits exponent and 3 bits
<     mantissa. This is not a standard type as defined by IEEE-754, but it follows
<     similar conventions, with the exception that there are no infinity values,
<     no negative zero, and only one NaN representation. This type has the
<     following characteristics:
< 
<       * bit encoding: S1E4M3
<       * exponent bias: 11
<       * infinities: Not supported
<       * NaNs: Supported with sign bit set to 1, exponent bits and mantissa bits set to all 0s
<       * denormals when exponent is 0
< 
<     Related to: https://dl.acm.org/doi/10.5555/3454287.3454728
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
233c212,214
< def Builtin_Function : Builtin_Type<"Function"> {
---
> def Builtin_Function : Builtin_Type<"Function", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>
>   ]> {
380c361
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
715c696
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
735c716
<     low level buffer access, MLIR has a [`memref` type](#memreftype). This
---
>     low level buffer access, MLIR has a [`memref` type](#memref-type). This
819c800,802
< def Builtin_Tuple : Builtin_Type<"Tuple"> {
---
> def Builtin_Tuple : Builtin_Type<"Tuple", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>
>   ]> {
887c870
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
959c942
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
1007c990,992
< def Builtin_Vector : Builtin_Type<"Vector", [ShapedTypeInterface], "Type"> {
---
> def Builtin_Vector : Builtin_Type<"Vector", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
>   ], "Type"> {
1045c1030
<     // A 2D scalable-length vector that contains a multiple of 2x8 f32 elements.
---
>     // A 2D scalable-length vector that contains a multiple of 2x8 i8 elements.
1092c1077
<     /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>     /// provided shape is `None`, the current shape of the type is used.
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: BytecodeBase.td
diff ../../../../llvm-project.0/mlir/include/mlir/IR/CMakeLists.txt include/mlir/IR/CMakeLists.txt
20,23d19
< set(LLVM_TARGET_DEFINITIONS BuiltinDialectBytecode.td)
< mlir_tablegen(BuiltinDialectBytecode.cpp.inc -gen-bytecode -bytecode-dialect="Builtin")
< add_public_tablegen_target(MLIRBuiltinDialectBytecodeIncGen)
< 
48a45,51
> 
> set(LLVM_TARGET_DEFINITIONS SubElementInterfaces.td)
> mlir_tablegen(SubElementAttrInterfaces.h.inc -gen-attr-interface-decls)
> mlir_tablegen(SubElementAttrInterfaces.cpp.inc -gen-attr-interface-defs)
> mlir_tablegen(SubElementTypeInterfaces.h.inc -gen-type-interface-decls)
> mlir_tablegen(SubElementTypeInterfaces.cpp.inc -gen-type-interface-defs)
> add_public_tablegen_target(MLIRSubElementInterfacesIncGen)
diff ../../../../llvm-project.0/mlir/include/mlir/IR/DialectBase.td include/mlir/IR/DialectBase.td
16,19c16,17
< // Helper for marking deprecated classes or defs in TableGen. To mark a def as
< // deprecated, mix in the `Deprecate` class with a reason.
< // Usage of a deprecated def within TableGen will cause a warning with the
< // given message.
---
> // Helper for marking deprecated classes or defs. To mark a def as deprecated,
> // mix in the `Deprecate` class with a reason.
24,33d21
< // Helper for marking entities in ODS generated C++ as deprecated.
< // Usage of such an entity from C++ code will cause a warning being emitted by
< // the C++ compiler with the given message.
< //
< // Note: Support has to be implemented by the code generator of a given
< // entity.
< class CppDeprecated<string reason> {
<   string odsCppDeprecated = reason;
< }
< 
37a26,38
> class EmitFolderBase;
> // Generate 'fold' method with 'ArrayRef<Attribute>' parameter.
> // New code should prefer using 'kEmitFoldAdaptorFolder' and
> // consider 'kEmitRawAttributesFolder' deprecated and to be
> // removed in the future.
> def kEmitRawAttributesFolder : EmitFolderBase, Deprecated<
>   "'useFoldAPI' of 'kEmitRawAttributesFolder' (default) has been deprecated "
>   # "and is pending removal. Please switch to 'kEmitFoldAdaptorFolder'. See "
>   # "https://discourse.llvm.org/t/psa-new-improved-fold-method-signature-has-landed-please-update-your-downstream-projects/67618"
> > {}
> // Generate 'fold' method with 'FoldAdaptor' parameter.
> def kEmitFoldAdaptorFolder : EmitFolderBase {}
> 
105a107,109
> 
>   // Fold API to use for operations in this dialect.
>   EmitFolderBase useFoldAPI = kEmitRawAttributesFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/DialectImplementation.h include/mlir/IR/DialectImplementation.h
143,148d142
< namespace detail {
< template <typename T>
< using has_push_back_t = decltype(std::declval<T>().push_back(
<     std::declval<typename T::value_type &&>()));
< } // namespace detail
< 
151,154c145,148
< struct FieldParser<ContainerT,
<                    std::enable_if_t<llvm::is_detected<detail::has_push_back_t,
<                                                       ContainerT>::value,
<                                     ContainerT>> {
---
> struct FieldParser<
>     ContainerT, std::enable_if_t<std::is_member_function_pointer<
>                                      decltype(&ContainerT::push_back)>::value,
>                                  ContainerT>> {
162c156
<       elements.push_back(std::move(*element));
---
>       elements.push_back(*element);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/ExtensibleDialect.h include/mlir/IR/ExtensibleDialect.h
72,82d71
<   /// Sets the verifier function for this attribute. It should emits an error
<   /// message and returns failure if a problem is detected, or returns success
<   /// if everything is ok.
<   void setVerifyFn(VerifierFn &&verify) { verifier = std::move(verify); }
< 
<   /// Sets the static hook for parsing this attribute assembly.
<   void setParseFn(ParserFn &&parse) { parser = std::move(parse); }
< 
<   /// Sets the static hook for printing this attribute assembly.
<   void setPrintFn(PrinterFn &&print) { printer = std::move(print); }
< 
227,237d215
< 
<   /// Sets the verifier function for this type. It should emits an error
<   /// message and returns failure if a problem is detected, or returns success
<   /// if everything is ok.
<   void setVerifyFn(VerifierFn &&verify) { verifier = std::move(verify); }
< 
<   /// Sets the static hook for parsing this type assembly.
<   void setParseFn(ParserFn &&parse) { parser = std::move(parse); }
< 
<   /// Sets the static hook for printing this type assembly.
<   void setPrintFn(PrinterFn &&print) { printer = std::move(print); }
diff ../../../../llvm-project.0/mlir/include/mlir/IR/FunctionInterfaces.td include/mlir/IR/FunctionInterfaces.td
57,60d56
<       Returns the symbol name of the function.
<     }],
<     "StringRef", "getSymName">,
<     InterfaceMethod<[{
282c278
<       function_interface_impl::setFunctionType($_op, newType);
---
>       function_interface_impl::setFunctionType(this->getOperation(), newType);
323c319
<           $_op, argIndices, argTypes, argAttrs, argLocs,
---
>           this->getOperation(), argIndices, argTypes, argAttrs, argLocs,
343c339
<           $_op, resultIndices, resultTypes, resultAttrs,
---
>           this->getOperation(), resultIndices, resultTypes, resultAttrs,
358c354
<         $_op, argIndices, newType);
---
>         this->getOperation(), argIndices, newType);
372c368
<           $_op, resultIndices, newType);
---
>           this->getOperation(), resultIndices, newType);
421c417
<       return function_interface_impl::getArgAttrs($_op, index);
---
>       return function_interface_impl::getArgAttrs(this->getOperation(), index);
471c467
<       function_interface_impl::setAllArgAttrDicts($_op, attributes);
---
>       function_interface_impl::setAllArgAttrDicts(this->getOperation(), attributes);
475c471
<       function_interface_impl::setAllArgAttrDicts($_op, attributes);
---
>       function_interface_impl::setAllArgAttrDicts(this->getOperation(), attributes);
510c506
<       return function_interface_impl::getResultAttrs($_op, index);
---
>       return function_interface_impl::getResultAttrs(this->getOperation(), index);
561c557
<         $_op, attributes);
---
>         this->getOperation(), attributes);
566c562
<         $_op, attributes);
---
>         this->getOperation(), attributes);
596c592
<       return function_interface_impl::getArgAttrDict($_op, index);
---
>       return function_interface_impl::getArgAttrDict(this->getOperation(), index);
604c600
<       return function_interface_impl::getResultAttrDict($_op, index);
---
>       return function_interface_impl::getResultAttrDict(this->getOperation(), index);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/IntegerSet.h include/mlir/IR/IntegerSet.h
20c20
< // transformation. For the latter, use FlatAffineValueConstraints.
---
> // transformation. For the latter, use FlatAffineConstraints.
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: Iterators.h
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Location.h include/mlir/IR/Location.h
17a18
> #include "mlir/IR/SubElementInterfaces.h"
174c175
< // SubElements
---
> // SubElementInterfaces
180c181
<   static void walk(Location param, AttrTypeImmediateSubElementWalker &walker) {
---
>   static void walk(Location param, AttrTypeSubElementWalker &walker) {
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Matchers.h include/mlir/IR/Matchers.h
55,70d54
< /// The matcher that matches operations that have the specified op name.
< struct NameOpMatcher {
<   NameOpMatcher(StringRef name) : name(name) {}
<   bool match(Operation *op) { return op->getName().getStringRef() == name; }
< 
<   StringRef name;
< };
< 
< /// The matcher that matches operations that have the specified attribute name.
< struct AttrOpMatcher {
<   AttrOpMatcher(StringRef attrName) : attrName(attrName) {}
<   bool match(Operation *op) { return op->hasAttr(attrName); }
< 
<   StringRef attrName;
< };
< 
102,124d85
< /// The matcher that matches operations that have the specified attribute
< /// name, and binds the attribute value.
< template <typename AttrT>
< struct AttrOpBinder {
<   /// Creates a matcher instance that binds the attribute value to
<   /// bind_value if match succeeds.
<   AttrOpBinder(StringRef attrName, AttrT *bindValue)
<       : attrName(attrName), bindValue(bindValue) {}
<   /// Creates a matcher instance that doesn't bind if match succeeds.
<   AttrOpBinder(StringRef attrName) : attrName(attrName), bindValue(nullptr) {}
< 
<   bool match(Operation *op) {
<     if (auto attr = op->getAttrOfType<AttrT>(attrName)) {
<       if (bindValue)
<         *bindValue = attr;
<       return true;
<     }
<     return false;
<   }
<   StringRef attrName;
<   AttrT *bindValue;
< };
< 
291,300d251
< /// Matches a named attribute operation.
< inline detail::AttrOpMatcher m_Attr(StringRef attrName) {
<   return detail::AttrOpMatcher(attrName);
< }
< 
< /// Matches a named operation.
< inline detail::NameOpMatcher m_Op(StringRef opName) {
<   return detail::NameOpMatcher(opName);
< }
< 
306,312d256
< }
< 
< /// Matches a named attribute operation and writes the value to bind_value.
< template <typename AttrT>
< inline detail::AttrOpBinder<AttrT> m_Attr(StringRef attrName,
<                                           AttrT *bindValue) {
<   return detail::AttrOpBinder<AttrT>(attrName, bindValue);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/MLIRContext.h include/mlir/IR/MLIRContext.h
14d13
< #include "llvm/ADT/ArrayRef.h"
24,26c23
< namespace tracing {
< class Action;
< }
---
> class DebugActionManager;
36d32
< class IRUnit;
139,142d134
<   /// This option is **heavily discouraged**: it is convenient during testing
<   /// but it is not a good practice to use it in production code. Some system
<   /// invariants can be broken (like loading a dialect after creating
<   ///  operations) without being caught by assertions or other means.
220a213,215
>   /// Returns the manager of debug actions within the context.
>   DebugActionManager &getDebugActionManager();
> 
243,277d237
<   //===--------------------------------------------------------------------===//
<   // Action API
<   //===--------------------------------------------------------------------===//
< 
<   /// Signatures for the action handler that can be registered with the context.
<   using HandlerTy =
<       std::function<void(function_ref<void()>, const tracing::Action &)>;
< 
<   /// Register a handler for handling actions that are dispatched through this
<   /// context. A nullptr handler can be set to disable a previously set handler.
<   void registerActionHandler(HandlerTy handler);
< 
<   /// Return true if a valid ActionHandler is set.
<   bool hasActionHandler();
< 
<   /// Dispatch the provided action to the handler if any, or just execute it.
<   void executeAction(function_ref<void()> actionFn,
<                      const tracing::Action &action) {
<     if (LLVM_UNLIKELY(hasActionHandler()))
<       executeActionInternal(actionFn, action);
<     else
<       actionFn();
<   }
< 
<   /// Dispatch the provided action to the handler if any, or just execute it.
<   template <typename ActionTy, typename... Args>
<   void executeAction(function_ref<void()> actionFn, ArrayRef<IRUnit> irUnits,
<                      Args &&...args) {
<     if (LLVM_UNLIKELY(hasActionHandler()))
<       executeActionInternal<ActionTy, Args...>(actionFn, irUnits,
<                                                std::forward<Args>(args)...);
<     else
<       actionFn();
<   }
< 
281,295d240
< 
<   /// Internal helper for the dispatch method.
<   void executeActionInternal(function_ref<void()> actionFn,
<                              const tracing::Action &action);
< 
<   /// Internal helper for the dispatch method. We get here after checking that
<   /// there is a handler, for the purpose of keeping this code out-of-line. and
<   /// avoid calling the ctor for the Action unnecessarily.
<   template <typename ActionTy, typename... Args>
<   LLVM_ATTRIBUTE_NOINLINE void
<   executeActionInternal(function_ref<void()> actionFn, ArrayRef<IRUnit> irUnits,
<                         Args &&...args) {
<     executeActionInternal(actionFn,
<                           ActionTy(irUnits, std::forward<Args>(args)...));
<   }
diff ../../../../llvm-project.0/mlir/include/mlir/IR/OpBase.td include/mlir/IR/OpBase.td
243,246d242
< // Whether a type is a RankedTensorType
< def IsRankedTensorTypePred
<         : CPred<"$_self.isa<::mlir::RankedTensorType>()">;
< 
497,498d492
< def F8E4M3B11FNUZ : Type<CPred<"$_self.isFloat8E4M3B11FNUZ()">, "f8E4M3B11FNUZ type">,
<                  BuildableType<"$_builder.getFloat8E4M3B11FNUZType()">;
561,566d554
< // Whether a shaped type has a rank greater than or equal of the specified rank.
< class HasRankGreaterOrEqualPred<int rank> : And<[
<     HasRankPred,
<     CPred<[{$_self.cast<::mlir::ShapedType>().getRank() >= }] # rank>
< ]>;
< 
728,742c716,720
< // Unranked tensor type whose element type is from the given `allowedTypes`
< // list, and which additionally satisfies an optional list of predicates.
< class UnrankedTensorOf<list<Type> allowedTypes, list<Pred> preds = [],
<                        string summary = "unranked tensor">
<   : ShapedContainerType<
<       allowedTypes, And<!listconcat([IsUnrankedTensorTypePred], preds)>,
<       summary, "::mlir::UnrankedTensorType">;
< 
< // Ranked tensor type whose element type is from the given `allowedTypes` list,
< // and which additionally satisfies an optional list of predicates.
< class RankedTensorOf<list<Type> allowedTypes, list<Pred> preds = [],
<                      string summary = "ranked tensor">
<   : ShapedContainerType<
<       allowedTypes, And<!listconcat([IsRankedTensorTypePred], preds)>,
<       summary, "::mlir::RankedTensorType">;
---
> // Unranked tensor type whose element type is from the given
> // `allowedTypes` list.
> class UnrankedTensorOf<list<Type> allowedTypes>
>   : ShapedContainerType<allowedTypes, IsUnrankedTensorTypePred,
>       "unranked.tensor", "::mlir::UnrankedTensorType">;
771,773c749,753
< class Non0RankedTensorOf<list<Type> allowedTypes>
<   : TensorOf<allowedTypes, [HasRankGreaterOrEqualPred<1>],
<       "non-0-ranked.tensor">;
---
> class RankedTensorOf<
>     list<Type> allowedTypes,
>     list<Pred> preds = [],
>     string summary = "ranked tensor">
>   : TensorOf<allowedTypes, !listconcat([HasRankPred], preds), summary>;
776,781d755
< def AnyNon0RankedTensor  : Non0RankedTensorOf<[AnyType]>;
< def AnyUnrankedTensor  : UnrankedTensorOf<[AnyType]>;
< 
< def AnyNon0RankedOrUnrankedTensor
<   : AnyTypeOf<[AnyUnrankedTensor, AnyNon0RankedTensor],
<               "non-0-ranked or unranked tensor", "::mlir::TensorType">;
785c759
<   : RankedTensorOf<allowedTypes,
---
>   : TensorOf<allowedTypes,
796,797c770
<   : RankedTensorOf<allowedTypes, [HasStaticShapePred],
<                    "statically shaped tensor">;
---
>   : TensorOf<allowedTypes, [HasStaticShapePred], "statically shaped tensor">;
804c777
< // Any unranked memref whose element type is from the given `allowedTypes` list.
---
> // Unranked Memref type
812c785
< // Any ranked memref whose element type is from the given `allowedTypes` list.
---
> // Memrefs are blocks of data with fixed type and rank.
817,821d789
< class Non0RankedMemRefOf<list<Type> allowedTypes> :
<     ConfinedType<MemRefOf<allowedTypes>, [HasRankGreaterOrEqualPred<1>],
<          "non-0-ranked." # MemRefOf<allowedTypes>.summary,
<          "::mlir::MemRefType">;
< 
823d790
< def AnyNon0RankedMemRef : Non0RankedMemRefOf<[AnyType]>;
825,834c792,793
< // Any memref (ranked or unranked) whose element type is from the given
< // `allowedTypes` list, and which additionally satisfies an optional list of
< // predicates.
< class RankedOrUnrankedMemRefOf<
<     list<Type> allowedTypes,
<     list<Pred> preds = [],
<     string summary = "ranked or unranked memref">
<   : ShapedContainerType<allowedTypes,
<       And<!listconcat([IsBaseMemRefTypePred], preds)>,
<       summary, "::mlir::BaseMemRefType">;
---
> class RankedOrUnrankedMemRefOf<list<Type> allowedTypes>:
>     AnyTypeOf<[UnrankedMemRefOf<allowedTypes>, MemRefOf<allowedTypes>]>;
836,838c795
< def AnyRankedOrUnrankedMemRef  : RankedOrUnrankedMemRefOf<[AnyType]>;
< def AnyNon0RankedOrUnrankedMemRef:
<     AnyTypeOf<[AnyUnrankedMemRef, AnyNon0RankedMemRef]>;
---
> def AnyRankedOrUnrankedMemRef: AnyTypeOf<[AnyUnrankedMemRef, AnyMemRef]>;
897c854
<                 "::llvm::all_of(" # elementTypesCall # ", [](::mlir::Type t) { "
---
>                 "::llvm::all_of(" # elementTypesCall # ", [](Type t) { "
1128,1130d1084
< def LocationAttr : Attr<CPred<"$_self.isa<::mlir::LocationAttr>()">,
<                         "location attribute">;
< 
1299,1300c1253
< class TypeAttrBase<string retType, string summary,
<                         Pred typePred = CPred<"true">> :
---
> class TypeAttrBase<string retType, string summary> :
1304,1306c1257
<             # retType # ">()">,
<       SubstLeaves<"$_self",
<                     "$_self.cast<::mlir::TypeAttr>().getValue()", typePred>]>,
---
>             # retType # ">()">]>,
1319,1320c1270
<    : TypeAttrBase<ty.cppClassName, "type attribute of " # ty.summary,
<                     ty.predicate> {
---
>    : TypeAttrBase<ty.cppClassName, "type attribute of " # ty.summary> {
1534,1536d1483
< def LocationArrayAttr : TypedArrayAttrBase<LocationAttr,
<                                            "location array attribute">;
< 
1771,1788d1717
< class IntArrayNthElemMaxValue<int index, int max> : AttrConstraint<
<     And<[
<       CPred<"$_self.cast<::mlir::ArrayAttr>().size() > " # index>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() <= " # max>
<         ]>,
<     "whose " # index # "-th element must be at most " # max>;
< 
< class IntArrayNthElemInRange<int index, int min, int max> : AttrConstraint<
<     And<[
<       CPred<"$_self.cast<::mlir::ArrayAttr>().size() > " # index>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() >= " # min>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() <= " # max>
<         ]>,
<     "whose " # index # "-th element must be at least " # min # " and at most " # max>;
< 
1812,1816d1740
< // A region with at most the given number of blocks.
< class MaxSizedRegion<int numBlocks> : Region<
<   CPred<"::llvm::hasNItemsOrLess($_self, " # numBlocks # ")">,
<   "region with at most " # numBlocks # " blocks">;
< 
2266,2271d2189
< 
< // OpBuilder like the above, but the emitted 'build' method is marked as
< // deprecated in C++. Use of it will emit a warning by the C++ compiler
< // with the given reason.
< class DeprecatedOpBuilder<string reason, dag p, code b = "">
<   : OpBuilder<p, b>, CppDeprecated<reason>;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/OpDefinition.h include/mlir/IR/OpDefinition.h
26d25
< #include <optional>
27a27
> #include <optional>
135,138c135,137
<   /// The order in which regions, blocks and operations the same nesting level
<   /// are visited (e.g., lexicographical or reverse lexicographical order) is
<   /// determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
144,145c143
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
150c148
<     return state->walk<Order, Iterator>(std::forward<FnT>(callback));
---
>     return state->walk<Order>(std::forward<FnT>(callback));
605a604,609
>   Value getResult() { return this->getOperation()->getResult(0); }
> 
>   /// If the operation returns a single value, then the Op can be implicitly
>   /// converted to an Value. This yields the value of the only result.
>   operator Value() { return getResult(); }
> 
610c614
<     this->getOperation()->getResult(0).replaceAllUsesWith(newValue);
---
>     getResult().replaceAllUsesWith(newValue);
636,638c640,642
<     mlir::TypedValue<ResultType> getResult() {
<       return cast<mlir::TypedValue<ResultType>>(
<           this->getOperation()->getResult(0));
---
>     ResultType getType() {
>       auto resultTy = this->getOperation()->getResult(0).getType();
>       return resultTy.template cast<ResultType>();
640,645d643
< 
<     /// If the operation returns a single value, then the Op can be implicitly
<     /// converted to a Value. This yields the value of the only result.
<     operator mlir::TypedValue<ResultType>() { return getResult(); }
< 
<     ResultType getType() { return getResult().getType(); }
1257,1264d1254
<     }
< 
<     template <typename ParentOpType =
<                   std::tuple_element_t<0, std::tuple<ParentOpTypes...>>>
<     std::enable_if_t<sizeof...(ParentOpTypes) == 1, ParentOpType>
<     getParentOp() {
<       Operation *parent = this->getOperation()->getParentOp();
<       return llvm::cast<ParentOpType>(parent);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Operation.h include/mlir/IR/Operation.h
77,79c77
<   /// Create a new Operation with the specific fields. This constructor
<   /// populates the provided attribute list with default attributes if
<   /// necessary.
---
>   /// Create a new Operation with the specific fields.
85,91d82
<   /// Create a new Operation with the specific fields. This constructor uses an
<   /// existing attribute dictionary to avoid uniquing a list of attributes.
<   static Operation *create(Location location, OperationName name,
<                            TypeRange resultTypes, ValueRange operands,
<                            DictionaryAttr attributes, BlockRange successors,
<                            unsigned numRegions);
< 
260,268d250
<   /// Replace uses of results of this operation with the provided `values` if
<   /// the given callback returns true.
<   template <typename ValuesT>
<   void replaceUsesWithIf(ValuesT &&values,
<                          function_ref<bool(OpOperand &)> shouldReplace) {
<     getResults().replaceUsesWithIf(std::forward<ValuesT>(values),
<                                    shouldReplace);
<   }
< 
527,529c509,511
<     NamedAttrList attrs(getAttrDictionary());
<     name.populateDefaultAttrs(attrs);
<     setAttrs(attrs.getDictionary(getContext()));
---
>       NamedAttrList attrs(getAttrDictionary());
>       name.populateDefaultAttrs(attrs);
>       setAttrs(attrs.getDictionary(getContext()));
610,613c592,594
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
635,636c616
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
641c621
<     return detail::walk<Order, Iterator>(this, std::forward<FnT>(callback));
---
>     return detail::walk<Order>(this, std::forward<FnT>(callback));
807,808d786
<     assert(resultNumber < getNumResults() &&
<            "Result number is out of range for operation");
diff ../../../../llvm-project.0/mlir/include/mlir/IR/OperationSupport.h include/mlir/IR/OperationSupport.h
156c156
<     OperationName::ParseAssemblyFn getParseAssemblyFn() final;
---
>     virtual OperationName::ParseAssemblyFn getParseAssemblyFn() final;
325,327d324
<   /// Return the context this operation is associated with.
<   MLIRContext *getContext() { return getIdentifier().getContext(); }
< 
837,839d833
<   /// Skip printing regions.
<   OpPrintingFlags &skipRegions(bool skip = true);
< 
867,869d860
<   /// Return if regions should be skipped.
<   bool shouldSkipRegions() const;
< 
891,893d881
<   /// Always skip Regions.
<   bool skipRegionsFlag : 1;
< 
938,951c926,930
<   /// Compare two operations (including their regions) and return if they are
<   /// equivalent.
<   ///
<   /// * `checkEquivalent` is a callback to check if two values are equivalent.
<   ///   For two operations to be equivalent, their operands must be the same SSA
<   ///   value or this callback must return `success`.
<   /// * `markEquivalent` is a callback to inform the caller that the analysis
<   ///   determined that two values are equivalent.
<   ///
<   /// Note: Additional information regarding value equivalence can be injected
<   /// into the analysis via `checkEquivalent`. Typically, callers may want
<   /// values that were determined to be equivalent as per `markEquivalent` to be
<   /// reflected in `checkEquivalent`, unless `exactValueMatch` or a different
<   /// equivalence relationship is desired.
---
>   /// Compare two operations and return if they are equivalent.
>   /// `mapOperands` and `mapResults` are optional callbacks that allows the
>   /// caller to check the mapping of SSA value between the lhs and rhs
>   /// operations. It is expected to return success if the mapping is valid and
>   /// failure if it conflicts with a previous mapping.
954,955c933,934
<                  function_ref<LogicalResult(Value, Value)> checkEquivalent,
<                  function_ref<void(Value, Value)> markEquivalent = nullptr,
---
>                  function_ref<LogicalResult(Value, Value)> mapOperands,
>                  function_ref<LogicalResult(Value, Value)> mapResults,
958,974c937,938
<   /// Compare two operations and return if they are equivalent.
<   static bool isEquivalentTo(Operation *lhs, Operation *rhs, Flags flags);
< 
<   /// Compare two regions (including their subregions) and return if they are
<   /// equivalent. See also `isEquivalentTo` for details.
<   static bool isRegionEquivalentTo(
<       Region *lhs, Region *rhs,
<       function_ref<LogicalResult(Value, Value)> checkEquivalent,
<       function_ref<void(Value, Value)> markEquivalent,
<       OperationEquivalence::Flags flags);
< 
<   /// Compare two regions and return if they are equivalent.
<   static bool isRegionEquivalentTo(Region *lhs, Region *rhs,
<                                    OperationEquivalence::Flags flags);
< 
<   /// Helper that can be used with `isEquivalentTo` above to consider ops
<   /// equivalent even if their operands are not equivalent.
---
>   /// Helper that can be used with `isEquivalentTo` above to ignore operation
>   /// operands/result mapping.
978,979c942,943
<   /// Helper that can be used with `isEquivalentTo` above to consider ops
<   /// equivalent only if their operands are the exact same SSA values.
---
>   /// Helper that can be used with `isEquivalentTo` above to ignore operation
>   /// operands/result mapping.
diff ../../../../llvm-project.0/mlir/include/mlir/IR/OpImplementation.h include/mlir/IR/OpImplementation.h
144,146d143
< 
<     raw_ostream &os = getStream();
<     uint64_t posPrior = os.tell();
148,153d144
<     if (posPrior != os.tell())
<       return;
< 
<     // Fallback to printing with prefix if the above failed to write anything
<     // to the output stream.
<     *this << attrOrType;
314d304
< 
322,333d311
< 
< // Prevent matching the TypeRange version above for ValueRange
< // printing through base AsmPrinter. This is needed so that the
< // ValueRange printing behaviour does not change from printing
< // the SSA values to printing the types for the operands when
< // using AsmPrinter instead of OpAsmPrinter.
< template <typename AsmPrinterT, typename T>
< inline std::enable_if_t<std::is_same<AsmPrinter, AsmPrinterT>::value &&
<                             std::is_convertible<T &, ValueRange>::value,
<                         AsmPrinterT &>
< operator<<(AsmPrinterT &p, const T &other) = delete;
< 
diff ../../../../llvm-project.0/mlir/include/mlir/IR/PatternMatch.h include/mlir/IR/PatternMatch.h
399c399
< class RewriterBase : public OpBuilder {
---
> class RewriterBase : public OpBuilder, public OpBuilder::Listener {
401,431d400
<   struct Listener : public OpBuilder::Listener {
<     Listener()
<         : OpBuilder::Listener(ListenerBase::Kind::RewriterBaseListener) {}
< 
<     /// Notify the listener that the specified operation was modified in-place.
<     virtual void notifyOperationModified(Operation *op) {}
< 
<     /// Notify the listener that the specified operation is about to be replaced
<     /// with the set of values potentially produced by new operations. This is
<     /// called before the uses of the operation have been changed.
<     virtual void notifyOperationReplaced(Operation *op,
<                                          ValueRange replacement) {}
< 
<     /// This is called on an operation that a rewrite is removing, right before
<     /// the operation is deleted. At this point, the operation has zero uses.
<     virtual void notifyOperationRemoved(Operation *op) {}
< 
<     /// Notify the listener that the pattern failed to match the given
<     /// operation, and provide a callback to populate a diagnostic with the
<     /// reason why the failure occurred. This method allows for derived
<     /// listeners to optionally hook into the reason why a rewrite failed, and
<     /// display it to users.
<     virtual LogicalResult
<     notifyMatchFailure(Location loc,
<                        function_ref<void(Diagnostic &)> reasonCallback) {
<       return failure();
<     }
< 
<     static bool classof(const OpBuilder::Listener *base);
<   };
< 
494,523c463,473
<   /// Inline the operations of block 'source' into block 'dest' before the given
<   /// position. The source block will be deleted and must have no uses.
<   /// 'argValues' is used to replace the block arguments of 'source'.
<   ///
<   /// If the source block is inserted at the end of the dest block, the dest
<   /// block must have no successors. Similarly, if the source block is inserted
<   /// somewhere in the middle (or beginning) of the dest block, the source block
<   /// must have no successors. Otherwise, the resulting IR would have
<   /// unreachable operations.
<   virtual void inlineBlockBefore(Block *source, Block *dest,
<                                  Block::iterator before,
<                                  ValueRange argValues = std::nullopt);
< 
<   /// Inline the operations of block 'source' before the operation 'op'. The
<   /// source block will be deleted and must have no uses. 'argValues' is used to
<   /// replace the block arguments of 'source'
<   ///
<   /// The source block must have no successors. Otherwise, the resulting IR
<   /// would have unreachable operations.
<   void inlineBlockBefore(Block *source, Operation *op,
<                          ValueRange argValues = std::nullopt);
< 
<   /// Inline the operations of block 'source' into the end of block 'dest'. The
<   /// source block will be deleted and must have no uses. 'argValues' is used to
<   /// replace the block arguments of 'source'
<   ///
<   /// The dest block must have no successors. Otherwise, the resulting IR would
<   /// have unreachable operation.
<   void mergeBlocks(Block *source, Block *dest,
<                    ValueRange argValues = std::nullopt);
---
>   /// Merge the operations of block 'source' into the end of block 'dest'.
>   /// 'source's predecessors must either be empty or only contain 'dest`.
>   /// 'argValues' is used to replace the block arguments of 'source' after
>   /// merging.
>   virtual void mergeBlocks(Block *source, Block *dest,
>                            ValueRange argValues = std::nullopt);
> 
>   // Merge the operations of block 'source' before the operation 'op'. Source
>   // block should not have existing predecessors or successors.
>   void mergeBlockBefore(Block *source, Operation *op,
>                         ValueRange argValues = std::nullopt);
539c489
<   virtual void finalizeRootUpdate(Operation *op);
---
>   virtual void finalizeRootUpdate(Operation *op) {}
558,572c508
<   void replaceAllUsesWith(Value from, Value to) {
<     return replaceAllUsesWith(from.getImpl(), to);
<   }
<   template <typename OperandType, typename ValueT>
<   void replaceAllUsesWith(IRObjectWithUseList<OperandType> *from, ValueT &&to) {
<     for (OperandType &operand : llvm::make_early_inc_range(from->getUses())) {
<       Operation *op = operand.getOwner();
<       updateRootInPlace(op, [&]() { operand.set(to); });
<     }
<   }
<   void replaceAllUsesWith(ValueRange from, ValueRange to) {
<     assert(from.size() == to.size() && "incorrect number of replacements");
<     for (auto it : llvm::zip(from, to))
<       replaceAllUsesWith(std::get<0>(it), std::get<1>(it));
<   }
---
>   void replaceAllUsesWith(Value from, Value to);
577,578c513,514
<   void replaceUsesWithIf(Value from, Value to,
<                          function_ref<bool(OpOperand &)> functor);
---
>   void replaceUseIf(Value from, Value to,
>                     llvm::unique_function<bool(OpOperand &) const> functor);
584c520
<     return replaceUsesWithIf(from, to, [&](OpOperand &use) {
---
>     return replaceUseIf(from, to, [&](OpOperand &use) {
599,602c535,536
<     if (auto *rewriteListener = dyn_cast_if_present<Listener>(listener))
<       return rewriteListener->notifyMatchFailure(
<           loc, function_ref<void(Diagnostic &)>(reasonCallback));
<     return failure();
---
>     return notifyMatchFailure(loc,
>                               function_ref<void(Diagnostic &)>(reasonCallback));
610,613c544,545
<     if (auto *rewriteListener = dyn_cast_if_present<Listener>(listener))
<       return rewriteListener->notifyMatchFailure(
<           op->getLoc(), function_ref<void(Diagnostic &)>(reasonCallback));
<     return failure();
---
>     return notifyMatchFailure(op->getLoc(),
>                               function_ref<void(Diagnostic &)>(reasonCallback));
626,629c558,559
<   /// Initialize the builder.
<   explicit RewriterBase(MLIRContext *ctx,
<                         OpBuilder::Listener *listener = nullptr)
<       : OpBuilder(ctx, listener) {}
---
>   /// Initialize the builder with this rewriter as the listener.
>   explicit RewriterBase(MLIRContext *ctx) : OpBuilder(ctx, /*listener=*/this) {}
631,632c561,586
<       : OpBuilder(otherBuilder) {}
<   virtual ~RewriterBase();
---
>       : OpBuilder(otherBuilder) {
>     setListener(this);
>   }
>   ~RewriterBase() override;
> 
>   /// These are the callback methods that subclasses can choose to implement if
>   /// they would like to be notified about certain types of mutations.
> 
>   /// Notify the rewriter that the specified operation is about to be replaced
>   /// with the set of values potentially produced by new operations. This is
>   /// called before the uses of the operation have been changed.
>   virtual void notifyRootReplaced(Operation *op, ValueRange replacement) {}
> 
>   /// This is called on an operation that a rewrite is removing, right before
>   /// the operation is deleted. At this point, the operation has zero uses.
>   virtual void notifyOperationRemoved(Operation *op) {}
> 
>   /// Notify the rewriter that the pattern failed to match the given operation,
>   /// and provide a callback to populate a diagnostic with the reason why the
>   /// failure occurred. This method allows for derived rewriters to optionally
>   /// hook into the reason why a rewrite failed, and display it to users.
>   virtual LogicalResult
>   notifyMatchFailure(Location loc,
>                      function_ref<void(Diagnostic &)> reasonCallback) {
>     return failure();
>   }
653,654c607
<   explicit IRRewriter(MLIRContext *ctx, OpBuilder::Listener *listener = nullptr)
<       : RewriterBase(ctx, listener) {}
---
>   explicit IRRewriter(MLIRContext *ctx) : RewriterBase(ctx) {}
diff ../../../../llvm-project.0/mlir/include/mlir/IR/RegionGraphTraits.h include/mlir/IR/RegionGraphTraits.h
53,86d52
< struct GraphTraits<const mlir::Block *> {
<   using ChildIteratorType = mlir::Block::succ_iterator;
<   using Node = const mlir::Block;
<   using NodeRef = Node *;
< 
<   static NodeRef getEntryNode(NodeRef node) { return node; }
< 
<   static ChildIteratorType child_begin(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->succ_begin();
<   }
<   static ChildIteratorType child_end(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->succ_end();
<   }
< };
< 
< template <>
< struct GraphTraits<Inverse<const mlir::Block *>> {
<   using ChildIteratorType = mlir::Block::pred_iterator;
<   using Node = const mlir::Block;
<   using NodeRef = Node *;
< 
<   static NodeRef getEntryNode(Inverse<NodeRef> inverseGraph) {
<     return inverseGraph.Graph;
<   }
< 
<   static ChildIteratorType child_begin(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->pred_begin();
<   }
<   static ChildIteratorType child_end(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->pred_end();
<   }
< };
< 
< template <>
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Region.h include/mlir/IR/Region.h
268,271c268,270
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
276,277c275
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
281c279
<       block.walk<Order, Iterator>(callback);
---
>       block.walk<Order>(callback);
286,289c284,286
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
296,297c293
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
302c298
<       if (block.walk<Order, Iterator>(callback).wasInterrupted())
---
>       if (block.walk<Order>(callback).wasInterrupted())
diff ../../../../llvm-project.0/mlir/include/mlir/IR/RegionKindInterface.h include/mlir/IR/RegionKindInterface.h
41,45d40
< /// Return "true" if the given region may have SSA dominance. This function also
< /// returns "true" in case the owner op is an unregistered op or an op that does
< /// not implement the RegionKindInterface.
< bool mayHaveSSADominance(Region &region);
< 
diff ../../../../llvm-project.0/mlir/include/mlir/IR/StorageUniquerSupport.h include/mlir/IR/StorageUniquerSupport.h
16d15
< #include "mlir/IR/AttrTypeSubElements.h"
127,146d125
<     };
<   }
< 
<   /// Returns a function that walks immediate sub elements of a given instance
<   /// of the storage user.
<   static auto getWalkImmediateSubElementsFn() {
<     return [](auto instance, function_ref<void(Attribute)> walkAttrsFn,
<               function_ref<void(Type)> walkTypesFn) {
<       ::mlir::detail::walkImmediateSubElementsImpl(
<           llvm::cast<ConcreteT>(instance), walkAttrsFn, walkTypesFn);
<     };
<   }
< 
<   /// Returns a function that replaces immediate sub elements of a given
<   /// instance of the storage user.
<   static auto getReplaceImmediateSubElementsFn() {
<     return [](auto instance, ArrayRef<Attribute> replAttrs,
<               ArrayRef<Type> replTypes) {
<       return ::mlir::detail::replaceImmediateSubElementsImpl(
<           llvm::cast<ConcreteT>(instance), replAttrs, replTypes);
Only in include/mlir/IR: SubElementInterfaces.h
Only in include/mlir/IR: SubElementInterfaces.td
diff ../../../../llvm-project.0/mlir/include/mlir/IR/SymbolTable.h include/mlir/IR/SymbolTable.h
16d15
< #include "llvm/Support/RWMutex.h"
285,286d283
<   friend class LockedSymbolTableCollection;
< 
289,342d285
< };
< 
< //===----------------------------------------------------------------------===//
< // LockedSymbolTableCollection
< //===----------------------------------------------------------------------===//
< 
< /// This class implements a lock-based shared wrapper around a symbol table
< /// collection that allows shared access to the collection of symbol tables.
< /// This class does not protect shared access to individual symbol tables.
< /// `SymbolTableCollection` lazily instantiates `SymbolTable` instances for
< /// symbol table operations, making read operations not thread-safe. This class
< /// provides a thread-safe `lookupSymbolIn` implementation by synchronizing the
< /// lazy `SymbolTable` lookup.
< class LockedSymbolTableCollection : public SymbolTableCollection {
< public:
<   explicit LockedSymbolTableCollection(SymbolTableCollection &collection)
<       : collection(collection) {}
< 
<   /// Look up a symbol with the specified name within the specified symbol table
<   /// operation, returning null if no such name exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, StringAttr symbol);
<   /// Look up a symbol with the specified name within the specified symbol table
<   /// operation, returning null if no such name exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, FlatSymbolRefAttr symbol);
<   /// Look up a potentially nested symbol within the specified symbol table
<   /// operation, returning null if no such symbol exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, SymbolRefAttr name);
< 
<   /// Lookup a symbol of a particular kind within the specified symbol table,
<   /// returning null if the symbol was not found.
<   template <typename T, typename NameT>
<   T lookupSymbolIn(Operation *symbolTableOp, NameT &&name) {
<     return dyn_cast_or_null<T>(
<         lookupSymbolIn(symbolTableOp, std::forward<NameT>(name)));
<   }
< 
<   /// A variant of 'lookupSymbolIn' that returns all of the symbols referenced
<   /// by a given SymbolRefAttr when resolved within the provided symbol table
<   /// operation. Returns failure if any of the nested references could not be
<   /// resolved.
<   LogicalResult lookupSymbolIn(Operation *symbolTableOp, SymbolRefAttr name,
<                                SmallVectorImpl<Operation *> &symbols);
< 
< private:
<   /// Get the symbol table for the symbol table operation, constructing if it
<   /// does not exist. This function provides thread safety over `collection`
<   /// by locking when performing the lookup and when inserting
<   /// lazily-constructed symbol tables.
<   SymbolTable &getSymbolTable(Operation *symbolTableOp);
< 
<   /// The symbol tables to manage.
<   SymbolTableCollection &collection;
<   /// The mutex protecting access to the symbol table collection.
<   llvm::sys::SmartRWMutex<true> mutex;
diff ../../../../llvm-project.0/mlir/include/mlir/IR/TypeRange.h include/mlir/IR/TypeRange.h
169c169
< // SubElements
---
> // SubElementInterfaces
175c175
<   static void walk(TypeRange param, AttrTypeImmediateSubElementWalker &walker) {
---
>   static void walk(TypeRange param, AttrTypeSubElementWalker &walker) {
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Types.h include/mlir/IR/Types.h
109a110,112
>   // Support type casting Type to itself.
>   static bool classof(Type) { return true; }
> 
127d129
<   bool isFloat8E4M3B11FNUZ() const;
189c191
<   const AbstractTy &getAbstractType() const { return impl->getAbstractType(); }
---
>   const AbstractTy &getAbstractType() { return impl->getAbstractType(); }
194,234d195
<   /// Walk all of the immediately nested sub-attributes and sub-types. This
<   /// method does not recurse into sub elements.
<   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const {
<     getAbstractType().walkImmediateSubElements(*this, walkAttrsFn, walkTypesFn);
<   }
< 
<   /// Replace the immediately nested sub-attributes and sub-types with those
<   /// provided. The order of the provided elements is derived from the order of
<   /// the elements returned by the callbacks of `walkImmediateSubElements`. The
<   /// element at index 0 would replace the very first attribute given by
<   /// `walkImmediateSubElements`. On success, the new instance with the values
<   /// replaced is returned. If replacement fails, nullptr is returned.
<   auto replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const {
<     return getAbstractType().replaceImmediateSubElements(*this, replAttrs,
<                                                          replTypes);
<   }
< 
<   /// Walk this type and all attibutes/types nested within using the
<   /// provided walk functions. See `AttrTypeWalker` for information on the
<   /// supported walk function types.
<   template <WalkOrder Order = WalkOrder::PostOrder, typename... WalkFns>
<   auto walk(WalkFns &&...walkFns) {
<     AttrTypeWalker walker;
<     (walker.addWalk(std::forward<WalkFns>(walkFns)), ...);
<     return walker.walk<Order>(*this);
<   }
< 
<   /// Recursively replace all of the nested sub-attributes and sub-types using
<   /// the provided map functions. Returns nullptr in the case of failure. See
<   /// `AttrTypeReplacer` for information on the support replacement function
<   /// types.
<   template <typename... ReplacementFns>
<   auto replace(ReplacementFns &&...replacementFns) {
<     AttrTypeReplacer replacer;
<     (replacer.addReplacement(std::forward<ReplacementFns>(replacementFns)),
<      ...);
<     return replacer.replace(*this);
<   }
< 
390,395c351,352
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     };
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/TypeSupport.h include/mlir/IR/TypeSupport.h
33,36d32
<   using WalkImmediateSubElementsFn = function_ref<void(
<       Type, function_ref<void(Attribute)>, function_ref<void(Type)>)>;
<   using ReplaceImmediateSubElementsFn =
<       function_ref<Type(Type, ArrayRef<Attribute>, ArrayRef<Type>)>;
47,48c43
<                         T::getWalkImmediateSubElementsFn(),
<                         T::getReplaceImmediateSubElementsFn(), T::getTypeID());
---
>                         T::getTypeID());
55,60c50,51
<   static AbstractType
<   get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
<       HasTraitFn &&hasTrait,
<       WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<       ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<       TypeID typeID) {
---
>   static AbstractType get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
>                           HasTraitFn &&hasTrait, TypeID typeID) {
62,63c53
<                         walkImmediateSubElementsFn,
<                         replaceImmediateSubElementsFn, typeID);
---
>                         typeID);
91,99d80
<   /// Walk the immediate sub-elements of the given type.
<   void walkImmediateSubElements(Type type,
<                                 function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const;
< 
<   /// Replace the immediate sub-elements of the given type.
<   Type replaceImmediateSubElements(Type type, ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const;
< 
105,108c86
<                HasTraitFn &&hasTrait,
<                WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<                ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<                TypeID typeID)
---
>                HasTraitFn &&hasTrait, TypeID typeID)
110,113c88
<         hasTraitFn(std::move(hasTrait)),
<         walkImmediateSubElementsFn(walkImmediateSubElementsFn),
<         replaceImmediateSubElementsFn(replaceImmediateSubElementsFn),
<         typeID(typeID) {}
---
>         hasTraitFn(std::move(hasTrait)), typeID(typeID) {}
133,138d107
< 
<   /// Function to walk the immediate sub-elements of this type.
<   WalkImmediateSubElementsFn walkImmediateSubElementsFn;
< 
<   /// Function to replace the immediate sub-elements of this type.
<   ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn;
Only in ../../../../llvm-project.0/mlir/include/mlir/IR: Unit.h
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Value.h include/mlir/IR/Value.h
430,433d429
<   using Value::Value;
< 
<   static bool classof(Value value) { return llvm::isa<Ty>(value.getType()); }
< 
436c432,444
<   void setType(Ty ty) { Value::setType(ty); }
---
>   void setType(mlir::Type ty) {
>     assert(ty.template isa<Ty>());
>     Value::setType(ty);
>   }
> 
>   TypedValue(Value val) : Value(val) {
>     assert(!val || val.getType().template isa<Ty>());
>   }
>   TypedValue &operator=(const Value &other) {
>     assert(!other || other.getType().template isa<Ty>());
>     Value::operator=(other);
>     return *this;
>   }
578,583c586,587
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     }
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
diff ../../../../llvm-project.0/mlir/include/mlir/IR/ValueRange.h include/mlir/IR/ValueRange.h
282,301d281
<   /// Replace uses of results of this range with the provided 'values' if the
<   /// given callback returns true. The size of `values` must match the size of
<   /// this range.
<   template <typename ValuesT>
<   std::enable_if_t<!std::is_convertible<ValuesT, Operation *>::value>
<   replaceUsesWithIf(ValuesT &&values,
<                     function_ref<bool(OpOperand &)> shouldReplace) {
<     assert(static_cast<size_t>(std::distance(values.begin(), values.end())) ==
<                size() &&
<            "expected 'values' to correspond 1-1 with the number of results");
< 
<     for (auto it : llvm::zip(*this, values))
<       std::get<0>(it).replaceUsesWithIf(std::get<1>(it), shouldReplace);
<   }
< 
<   /// Replace uses of results of this range with results of `op` if the given
<   /// callback returns true.
<   void replaceUsesWithIf(Operation *op,
<                          function_ref<bool(OpOperand &)> shouldReplace);
< 
diff ../../../../llvm-project.0/mlir/include/mlir/IR/Visitors.h include/mlir/IR/Visitors.h
38c38
<   WalkResult(ResultEnum result = Advance) : result(result) {}
---
>   WalkResult(ResultEnum result) : result(result) {}
65,76d64
< /// This iterator enumerates the elements in "forward" order.
< struct ForwardIterator {
<   /// Make operations iterable: return the list of regions.
<   static MutableArrayRef<Region> makeIterable(Operation &range);
< 
<   /// Regions and block are already iterable.
<   template <typename T>
<   static constexpr T &makeIterable(T &range) {
<     return range;
<   }
< };
< 
128,132c116,119
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order'. These methods are invoked for void-returning
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order'. These methods are invoked for void-returning
136d122
< template <typename Iterator>
138,170c124,125
<           WalkOrder order) {
<   // We don't use early increment for regions because they can't be erased from
<   // a callback.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     if (order == WalkOrder::PreOrder)
<       callback(&region);
<     for (auto &block : Iterator::makeIterable(region)) {
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         walk<Iterator>(&nestedOp, callback, order);
<     }
<     if (order == WalkOrder::PostOrder)
<       callback(&region);
<   }
< }
< 
< template <typename Iterator>
< void walk(Operation *op, function_ref<void(Block *)> callback,
<           WalkOrder order) {
<   for (auto &region : Iterator::makeIterable(*op)) {
<     // Early increment here in the case where the block is erased.
<     for (auto &block :
<          llvm::make_early_inc_range(Iterator::makeIterable(region))) {
<       if (order == WalkOrder::PreOrder)
<         callback(&block);
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         walk<Iterator>(&nestedOp, callback, order);
<       if (order == WalkOrder::PostOrder)
<         callback(&block);
<     }
<   }
< }
< 
< template <typename Iterator>
---
>           WalkOrder order);
> void walk(Operation *op, function_ref<void(Block *)> callback, WalkOrder order);
172,189c127
<           WalkOrder order) {
<   if (order == WalkOrder::PreOrder)
<     callback(op);
< 
<   // TODO: This walk should be iterative over the operations.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     for (auto &block : Iterator::makeIterable(region)) {
<       // Early increment here in the case where the operation is erased.
<       for (auto &nestedOp :
<            llvm::make_early_inc_range(Iterator::makeIterable(block)))
<         walk<Iterator>(&nestedOp, callback, order);
<     }
<   }
< 
<   if (order == WalkOrder::PostOrder)
<     callback(op);
< }
< 
---
>           WalkOrder order);
191,197c129,134
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order'. This method is invoked for skippable or
< /// interruptible callbacks. A callback on a block or operation is allowed to
< /// erase that block or operation if either:
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order'. This method is invoked for skippable or interruptible
> /// callbacks. A callback on a block or operation is allowed to erase that block
> /// or operation if either:
200d136
< template <typename Iterator>
202,228c138
<                 WalkOrder order) {
<   // We don't use early increment for regions because they can't be erased from
<   // a callback.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     if (order == WalkOrder::PreOrder) {
<       WalkResult result = callback(&region);
<       if (result.wasSkipped())
<         continue;
<       if (result.wasInterrupted())
<         return WalkResult::interrupt();
<     }
<     for (auto &block : Iterator::makeIterable(region)) {
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<     }
<     if (order == WalkOrder::PostOrder) {
<       if (callback(&region).wasInterrupted())
<         return WalkResult::interrupt();
<       // We don't check if this region was skipped because its walk already
<       // finished and the walk will continue with the next region.
<     }
<   }
<   return WalkResult::advance();
< }
< 
< template <typename Iterator>
---
>                 WalkOrder order);
230,256c140
<                 WalkOrder order) {
<   for (auto &region : Iterator::makeIterable(*op)) {
<     // Early increment here in the case where the block is erased.
<     for (auto &block :
<          llvm::make_early_inc_range(Iterator::makeIterable(region))) {
<       if (order == WalkOrder::PreOrder) {
<         WalkResult result = callback(&block);
<         if (result.wasSkipped())
<           continue;
<         if (result.wasInterrupted())
<           return WalkResult::interrupt();
<       }
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<       if (order == WalkOrder::PostOrder) {
<         if (callback(&block).wasInterrupted())
<           return WalkResult::interrupt();
<         // We don't check if this block was skipped because its walk already
<         // finished and the walk will continue with the next block.
<       }
<     }
<   }
<   return WalkResult::advance();
< }
< 
< template <typename Iterator>
---
>                 WalkOrder order);
258,283c142
<                 WalkOrder order) {
<   if (order == WalkOrder::PreOrder) {
<     WalkResult result = callback(op);
<     // If skipped, caller will continue the walk on the next operation.
<     if (result.wasSkipped())
<       return WalkResult::advance();
<     if (result.wasInterrupted())
<       return WalkResult::interrupt();
<   }
< 
<   // TODO: This walk should be iterative over the operations.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     for (auto &block : Iterator::makeIterable(region)) {
<       // Early increment here in the case where the operation is erased.
<       for (auto &nestedOp :
<            llvm::make_early_inc_range(Iterator::makeIterable(block))) {
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<       }
<     }
<   }
< 
<   if (order == WalkOrder::PostOrder)
<     return callback(op);
<   return WalkResult::advance();
< }
---
>                 WalkOrder order);
291,295c150,153
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'Order' (post-order by default). A callback on a block or
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'Order' (post-order by default). A callback on a block or
307,308c165,166
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
313c171
<   return detail::walk<Iterator>(op, function_ref<RetT(ArgT)>(callback), Order);
---
>   return detail::walk(op, function_ref<RetT(ArgT)>(callback), Order);
317,321c175,178
< /// given operation. The order in which regions, blocks and operations at
< /// the same nesting are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order' (post-order by default). This method is selected for
---
> /// given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order' (post-order by default). This method is selected for
329,330c186,187
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
341,342c198
<   return detail::walk<Iterator>(op, function_ref<RetT(Operation *)>(wrapperFn),
<                                 Order);
---
>   return detail::walk(op, function_ref<RetT(Operation *)>(wrapperFn), Order);
346,353c202,208
< /// given operation. The order in which regions, blocks and operations at
< /// the same nesting are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'Order' (post-order by default). This method is selected for
< /// WalkReturn returning skippable or interruptible callbacks that operate on a
< /// specific derived operation type. A callback on an operation is allowed to
< /// erase that operation if either:
---
> /// given operation. Regions, blocks and operations at the same nesting level
> /// are visited in lexicographical order. The walk order for enclosing regions,
> /// blocks and operations with respect to their nested ones is specified by
> /// 'Order' (post-order by default). This method is selected for WalkReturn
> /// returning skippable or interruptible callbacks that operate on a specific
> /// derived operation type. A callback on an operation is allowed to erase that
> /// operation if either:
366,367c221,222
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
379,380c234
<   return detail::walk<Iterator>(op, function_ref<RetT(Operation *)>(wrapperFn),
<                                 Order);
---
>   return detail::walk(op, function_ref<RetT(Operation *)>(wrapperFn), Order);
--- include/mlir/IR/DialectRegistry.h
--- include/mlir/IR/BuiltinDialect.td
37a38
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/IR/DialectBase.td
16,19c16,17
< // Helper for marking deprecated classes or defs in TableGen. To mark a def as
< // deprecated, mix in the `Deprecate` class with a reason.
< // Usage of a deprecated def within TableGen will cause a warning with the
< // given message.
---
> // Helper for marking deprecated classes or defs. To mark a def as deprecated,
> // mix in the `Deprecate` class with a reason.
24,33d21
< // Helper for marking entities in ODS generated C++ as deprecated.
< // Usage of such an entity from C++ code will cause a warning being emitted by
< // the C++ compiler with the given message.
< //
< // Note: Support has to be implemented by the code generator of a given
< // entity.
< class CppDeprecated<string reason> {
<   string odsCppDeprecated = reason;
< }
< 
37a26,38
> class EmitFolderBase;
> // Generate 'fold' method with 'ArrayRef<Attribute>' parameter.
> // New code should prefer using 'kEmitFoldAdaptorFolder' and
> // consider 'kEmitRawAttributesFolder' deprecated and to be
> // removed in the future.
> def kEmitRawAttributesFolder : EmitFolderBase, Deprecated<
>   "'useFoldAPI' of 'kEmitRawAttributesFolder' (default) has been deprecated "
>   # "and is pending removal. Please switch to 'kEmitFoldAdaptorFolder'. See "
>   # "https://discourse.llvm.org/t/psa-new-improved-fold-method-signature-has-landed-please-update-your-downstream-projects/67618"
> > {}
> // Generate 'fold' method with 'FoldAdaptor' parameter.
> def kEmitFoldAdaptorFolder : EmitFolderBase {}
> 
105a107,109
> 
>   // Fold API to use for operations in this dialect.
>   EmitFolderBase useFoldAPI = kEmitRawAttributesFolder;
--- include/mlir/IR/Threading.h
--- include/mlir/IR/OwningOpRef.h
--- include/mlir/IR/BuiltinAttributes.h
12a13
> #include "mlir/IR/SubElementInterfaces.h"
81,83c82,84
<   operator ElementsAttr() const { return cast_if_present<ElementsAttr>(*this); }
<   /// Allow implicit conversion to TypedAttr.
<   operator TypedAttr() const { return ElementsAttr(*this); }
---
>   operator ElementsAttr() const {
>     return *this ? cast<ElementsAttr>() : nullptr;
>   }
845,846c846,847
<   /// Enable conversion to IntegerAttr and its interfaces. This uses conversion
<   /// vs. inheritance to avoid bringing in all of IntegerAttrs methods.
---
>   /// Enable conversion to IntegerAttr. This uses conversion vs. inheritance to
>   /// avoid bringing in all of IntegerAttrs methods.
848d848
<   operator TypedAttr() const { return IntegerAttr(impl); }
--- include/mlir/IR/PatternMatch.h
399c399
< class RewriterBase : public OpBuilder {
---
> class RewriterBase : public OpBuilder, public OpBuilder::Listener {
401,431d400
<   struct Listener : public OpBuilder::Listener {
<     Listener()
<         : OpBuilder::Listener(ListenerBase::Kind::RewriterBaseListener) {}
< 
<     /// Notify the listener that the specified operation was modified in-place.
<     virtual void notifyOperationModified(Operation *op) {}
< 
<     /// Notify the listener that the specified operation is about to be replaced
<     /// with the set of values potentially produced by new operations. This is
<     /// called before the uses of the operation have been changed.
<     virtual void notifyOperationReplaced(Operation *op,
<                                          ValueRange replacement) {}
< 
<     /// This is called on an operation that a rewrite is removing, right before
<     /// the operation is deleted. At this point, the operation has zero uses.
<     virtual void notifyOperationRemoved(Operation *op) {}
< 
<     /// Notify the listener that the pattern failed to match the given
<     /// operation, and provide a callback to populate a diagnostic with the
<     /// reason why the failure occurred. This method allows for derived
<     /// listeners to optionally hook into the reason why a rewrite failed, and
<     /// display it to users.
<     virtual LogicalResult
<     notifyMatchFailure(Location loc,
<                        function_ref<void(Diagnostic &)> reasonCallback) {
<       return failure();
<     }
< 
<     static bool classof(const OpBuilder::Listener *base);
<   };
< 
494,523c463,473
<   /// Inline the operations of block 'source' into block 'dest' before the given
<   /// position. The source block will be deleted and must have no uses.
<   /// 'argValues' is used to replace the block arguments of 'source'.
<   ///
<   /// If the source block is inserted at the end of the dest block, the dest
<   /// block must have no successors. Similarly, if the source block is inserted
<   /// somewhere in the middle (or beginning) of the dest block, the source block
<   /// must have no successors. Otherwise, the resulting IR would have
<   /// unreachable operations.
<   virtual void inlineBlockBefore(Block *source, Block *dest,
<                                  Block::iterator before,
<                                  ValueRange argValues = std::nullopt);
< 
<   /// Inline the operations of block 'source' before the operation 'op'. The
<   /// source block will be deleted and must have no uses. 'argValues' is used to
<   /// replace the block arguments of 'source'
<   ///
<   /// The source block must have no successors. Otherwise, the resulting IR
<   /// would have unreachable operations.
<   void inlineBlockBefore(Block *source, Operation *op,
<                          ValueRange argValues = std::nullopt);
< 
<   /// Inline the operations of block 'source' into the end of block 'dest'. The
<   /// source block will be deleted and must have no uses. 'argValues' is used to
<   /// replace the block arguments of 'source'
<   ///
<   /// The dest block must have no successors. Otherwise, the resulting IR would
<   /// have unreachable operation.
<   void mergeBlocks(Block *source, Block *dest,
<                    ValueRange argValues = std::nullopt);
---
>   /// Merge the operations of block 'source' into the end of block 'dest'.
>   /// 'source's predecessors must either be empty or only contain 'dest`.
>   /// 'argValues' is used to replace the block arguments of 'source' after
>   /// merging.
>   virtual void mergeBlocks(Block *source, Block *dest,
>                            ValueRange argValues = std::nullopt);
> 
>   // Merge the operations of block 'source' before the operation 'op'. Source
>   // block should not have existing predecessors or successors.
>   void mergeBlockBefore(Block *source, Operation *op,
>                         ValueRange argValues = std::nullopt);
539c489
<   virtual void finalizeRootUpdate(Operation *op);
---
>   virtual void finalizeRootUpdate(Operation *op) {}
558,572c508
<   void replaceAllUsesWith(Value from, Value to) {
<     return replaceAllUsesWith(from.getImpl(), to);
<   }
<   template <typename OperandType, typename ValueT>
<   void replaceAllUsesWith(IRObjectWithUseList<OperandType> *from, ValueT &&to) {
<     for (OperandType &operand : llvm::make_early_inc_range(from->getUses())) {
<       Operation *op = operand.getOwner();
<       updateRootInPlace(op, [&]() { operand.set(to); });
<     }
<   }
<   void replaceAllUsesWith(ValueRange from, ValueRange to) {
<     assert(from.size() == to.size() && "incorrect number of replacements");
<     for (auto it : llvm::zip(from, to))
<       replaceAllUsesWith(std::get<0>(it), std::get<1>(it));
<   }
---
>   void replaceAllUsesWith(Value from, Value to);
577,578c513,514
<   void replaceUsesWithIf(Value from, Value to,
<                          function_ref<bool(OpOperand &)> functor);
---
>   void replaceUseIf(Value from, Value to,
>                     llvm::unique_function<bool(OpOperand &) const> functor);
584c520
<     return replaceUsesWithIf(from, to, [&](OpOperand &use) {
---
>     return replaceUseIf(from, to, [&](OpOperand &use) {
599,602c535,536
<     if (auto *rewriteListener = dyn_cast_if_present<Listener>(listener))
<       return rewriteListener->notifyMatchFailure(
<           loc, function_ref<void(Diagnostic &)>(reasonCallback));
<     return failure();
---
>     return notifyMatchFailure(loc,
>                               function_ref<void(Diagnostic &)>(reasonCallback));
610,613c544,545
<     if (auto *rewriteListener = dyn_cast_if_present<Listener>(listener))
<       return rewriteListener->notifyMatchFailure(
<           op->getLoc(), function_ref<void(Diagnostic &)>(reasonCallback));
<     return failure();
---
>     return notifyMatchFailure(op->getLoc(),
>                               function_ref<void(Diagnostic &)>(reasonCallback));
626,629c558,559
<   /// Initialize the builder.
<   explicit RewriterBase(MLIRContext *ctx,
<                         OpBuilder::Listener *listener = nullptr)
<       : OpBuilder(ctx, listener) {}
---
>   /// Initialize the builder with this rewriter as the listener.
>   explicit RewriterBase(MLIRContext *ctx) : OpBuilder(ctx, /*listener=*/this) {}
631,632c561,586
<       : OpBuilder(otherBuilder) {}
<   virtual ~RewriterBase();
---
>       : OpBuilder(otherBuilder) {
>     setListener(this);
>   }
>   ~RewriterBase() override;
> 
>   /// These are the callback methods that subclasses can choose to implement if
>   /// they would like to be notified about certain types of mutations.
> 
>   /// Notify the rewriter that the specified operation is about to be replaced
>   /// with the set of values potentially produced by new operations. This is
>   /// called before the uses of the operation have been changed.
>   virtual void notifyRootReplaced(Operation *op, ValueRange replacement) {}
> 
>   /// This is called on an operation that a rewrite is removing, right before
>   /// the operation is deleted. At this point, the operation has zero uses.
>   virtual void notifyOperationRemoved(Operation *op) {}
> 
>   /// Notify the rewriter that the pattern failed to match the given operation,
>   /// and provide a callback to populate a diagnostic with the reason why the
>   /// failure occurred. This method allows for derived rewriters to optionally
>   /// hook into the reason why a rewrite failed, and display it to users.
>   virtual LogicalResult
>   notifyMatchFailure(Location loc,
>                      function_ref<void(Diagnostic &)> reasonCallback) {
>     return failure();
>   }
653,654c607
<   explicit IRRewriter(MLIRContext *ctx, OpBuilder::Listener *listener = nullptr)
<       : RewriterBase(ctx, listener) {}
---
>   explicit IRRewriter(MLIRContext *ctx) : RewriterBase(ctx) {}
--- include/mlir/IR/OpImplementation.h
144,146d143
< 
<     raw_ostream &os = getStream();
<     uint64_t posPrior = os.tell();
148,153d144
<     if (posPrior != os.tell())
<       return;
< 
<     // Fallback to printing with prefix if the above failed to write anything
<     // to the output stream.
<     *this << attrOrType;
314d304
< 
322,333d311
< 
< // Prevent matching the TypeRange version above for ValueRange
< // printing through base AsmPrinter. This is needed so that the
< // ValueRange printing behaviour does not change from printing
< // the SSA values to printing the types for the operands when
< // using AsmPrinter instead of OpAsmPrinter.
< template <typename AsmPrinterT, typename T>
< inline std::enable_if_t<std::is_same<AsmPrinter, AsmPrinterT>::value &&
<                             std::is_convertible<T &, ValueRange>::value,
<                         AsmPrinterT &>
< operator<<(AsmPrinterT &p, const T &other) = delete;
< 
--- include/mlir/IR/Builders.h
67d66
<   FloatType getFloat8E4M3B11FNUZType();
119c118
<   TypedAttr getZeroAttr(Type type);
---
>   Attribute getZeroAttr(Type type);
257,276d255
<   /// Base class for listeners.
<   struct ListenerBase {
<     /// The kind of listener.
<     enum class Kind {
<       /// OpBuilder::Listener or user-derived class.
<       OpBuilderListener = 0,
< 
<       /// RewriterBase::Listener or user-derived class.
<       RewriterBaseListener = 1
<     };
< 
<     Kind getKind() const { return kind; }
< 
<   protected:
<     ListenerBase(Kind kind) : kind(kind) {}
< 
<   private:
<     const Kind kind;
<   };
< 
279,282c258,259
<   struct Listener : public ListenerBase {
<     Listener() : ListenerBase(ListenerBase::Kind::OpBuilderListener) {}
< 
<     virtual ~Listener() = default;
---
>   struct Listener {
>     virtual ~Listener();
291,293d267
< 
<   protected:
<     Listener(Kind kind) : ListenerBase(kind) {}
566,569d539
< protected:
<   /// The optional listener for events of this builder.
<   Listener *listener;
< 
575a546,547
>   /// The optional listener for events of this builder.
>   Listener *listener;
--- include/mlir/IR/BuiltinOps.h
--- include/mlir/IR/FunctionImplementation.h
--- include/mlir/IR/Dominance.h
--- include/mlir/IR/OpDefinition.h
26d25
< #include <optional>
27a27
> #include <optional>
135,138c135,137
<   /// The order in which regions, blocks and operations the same nesting level
<   /// are visited (e.g., lexicographical or reverse lexicographical order) is
<   /// determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
144,145c143
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
150c148
<     return state->walk<Order, Iterator>(std::forward<FnT>(callback));
---
>     return state->walk<Order>(std::forward<FnT>(callback));
605a604,609
>   Value getResult() { return this->getOperation()->getResult(0); }
> 
>   /// If the operation returns a single value, then the Op can be implicitly
>   /// converted to an Value. This yields the value of the only result.
>   operator Value() { return getResult(); }
> 
610c614
<     this->getOperation()->getResult(0).replaceAllUsesWith(newValue);
---
>     getResult().replaceAllUsesWith(newValue);
636,638c640,642
<     mlir::TypedValue<ResultType> getResult() {
<       return cast<mlir::TypedValue<ResultType>>(
<           this->getOperation()->getResult(0));
---
>     ResultType getType() {
>       auto resultTy = this->getOperation()->getResult(0).getType();
>       return resultTy.template cast<ResultType>();
640,645d643
< 
<     /// If the operation returns a single value, then the Op can be implicitly
<     /// converted to a Value. This yields the value of the only result.
<     operator mlir::TypedValue<ResultType>() { return getResult(); }
< 
<     ResultType getType() { return getResult().getType(); }
1257,1264d1254
<     }
< 
<     template <typename ParentOpType =
<                   std::tuple_element_t<0, std::tuple<ParentOpTypes...>>>
<     std::enable_if_t<sizeof...(ParentOpTypes) == 1, ParentOpType>
<     getParentOp() {
<       Operation *parent = this->getOperation()->getParentOp();
<       return llvm::cast<ParentOpType>(parent);
--- include/mlir/IR/BuiltinTypeInterfaces.h
--- include/mlir/IR/AffineMap.h
252,254c252
<   AffineMap dropResult(int64_t pos) const {
<     return dropResults(ArrayRef({pos}));
<   }
---
>   AffineMap dropResult(int64_t pos) { return dropResults({pos}); }
257,258c255,256
<   // results in `positions` dropped.
<   AffineMap dropResults(ArrayRef<int64_t> positions) const {
---
>   // positions in `positions` dropped from results.
>   AffineMap dropResults(ArrayRef<int64_t> positions) {
268,271d265
<   // Returns a new AffineMap with the same number of dims and symbols, but all
<   // results in `positions` dropped.
<   AffineMap dropResults(const llvm::SmallBitVector &positions) const;
< 
274c268
<   AffineMap insertResult(AffineExpr expr, unsigned pos) const {
---
>   AffineMap insertResult(AffineExpr expr, unsigned pos) {
412,414d405
< /// Drop the dims that are listed in `unusedDims`.
< AffineMap compressDims(AffineMap map, const llvm::SmallBitVector &unusedDims);
< 
423,425c414,415
< /// Drop the symbols that are listed in `unusedSymbols`.
< AffineMap compressSymbols(AffineMap map,
<                           const llvm::SmallBitVector &unusedSymbols);
---
> /// Drop the dims that are not listed in `unusedDims`.
> AffineMap compressDims(AffineMap map, const llvm::SmallBitVector &unusedDims);
434a425,428
> /// Drop the symbols that are not listed in `unusedSymbols`.
> AffineMap compressSymbols(AffineMap map,
>                           const llvm::SmallBitVector &unusedSymbols);
> 
478c472
< /// Prerequisites: `map` must be a projected permutation.
---
> /// Prerequisites: `map` must be a projected permuation.
568,578c562
< /// This function also compresses the dims when the boolean flag is true.
< AffineMap projectDims(AffineMap map,
<                       const llvm::SmallBitVector &projectedDimensions,
<                       bool compressDimsFlag = false);
< /// Symbol counterpart of `projectDims`.
< /// This function also compresses the symbols when the boolean flag is true.
< AffineMap projectSymbols(AffineMap map,
<                          const llvm::SmallBitVector &projectedSymbols,
<                          bool compressSymbolsFlag = false);
< /// Calls `projectDims(map, projectedDimensions, compressDimsFlag)`.
< /// If `compressSymbolsFlag` is true, additionally call `compressUnusedSymbols`.
---
> /// This function also compresses unused symbols away.
580,605c564
<                           const llvm::SmallBitVector &projectedDimensions,
<                           bool compressDimsFlag = true,
<                           bool compressSymbolsFlag = true);
< 
< // Return a bitvector where each bit set indicates a dimension that is not used
< // by any of the maps in the input array `maps`.
< llvm::SmallBitVector getUnusedDimsBitVector(ArrayRef<AffineMap> maps);
< 
< // Return a bitvector where each bit set indicates a symbol that is not used
< // by any of the maps in the input array `maps`.
< llvm::SmallBitVector getUnusedSymbolsBitVector(ArrayRef<AffineMap> maps);
< 
< /// Expand `map` to operate on `rank` dims while projecting out the dims in
< /// `projectedDimensions`. This amounts to composing `map` with
< /// `id(rank).dropResults(projectedDimensions)`.
< AffineMap expandDimsToRank(AffineMap map, int64_t rank,
<                            const llvm::SmallBitVector &projectedDimensions);
< 
< inline raw_ostream &operator<<(raw_ostream &os, AffineMap map) {
<   map.print(os);
<   return os;
< }
< 
< //===----------------------------------------------------------------------===//
< // Templated helper functions.
< //===----------------------------------------------------------------------===//
---
>                           const llvm::SmallBitVector &projectedDimensions);
628c587
< /// Calculates maximum dimension and symbol positions from the expressions
---
> /// Calculates maxmimum dimension and symbol positions from the expressions
643a603,611
> 
> inline raw_ostream &operator<<(raw_ostream &os, AffineMap map) {
>   map.print(os);
>   return os;
> }
> 
> // Return a bitvector where each bit set indicates a dimension that is not used
> // by any of the maps in the input array `maps`.
> llvm::SmallBitVector getUnusedDimsBitVector(ArrayRef<AffineMap> maps);
--- include/mlir/IR/AttrTypeBase.td
--- include/mlir/IR/ExtensibleDialect.h
72,82d71
<   /// Sets the verifier function for this attribute. It should emits an error
<   /// message and returns failure if a problem is detected, or returns success
<   /// if everything is ok.
<   void setVerifyFn(VerifierFn &&verify) { verifier = std::move(verify); }
< 
<   /// Sets the static hook for parsing this attribute assembly.
<   void setParseFn(ParserFn &&parse) { parser = std::move(parse); }
< 
<   /// Sets the static hook for printing this attribute assembly.
<   void setPrintFn(PrinterFn &&print) { printer = std::move(print); }
< 
227,237d215
< 
<   /// Sets the verifier function for this type. It should emits an error
<   /// message and returns failure if a problem is detected, or returns success
<   /// if everything is ok.
<   void setVerifyFn(VerifierFn &&verify) { verifier = std::move(verify); }
< 
<   /// Sets the static hook for parsing this type assembly.
<   void setParseFn(ParserFn &&parse) { parser = std::move(parse); }
< 
<   /// Sets the static hook for printing this type assembly.
<   void setPrintFn(PrinterFn &&print) { printer = std::move(print); }
--- include/mlir/IR/Diagnostics.h
--- include/mlir/IR/Visitors.h
38c38
<   WalkResult(ResultEnum result = Advance) : result(result) {}
---
>   WalkResult(ResultEnum result) : result(result) {}
65,76d64
< /// This iterator enumerates the elements in "forward" order.
< struct ForwardIterator {
<   /// Make operations iterable: return the list of regions.
<   static MutableArrayRef<Region> makeIterable(Operation &range);
< 
<   /// Regions and block are already iterable.
<   template <typename T>
<   static constexpr T &makeIterable(T &range) {
<     return range;
<   }
< };
< 
128,132c116,119
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order'. These methods are invoked for void-returning
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order'. These methods are invoked for void-returning
136d122
< template <typename Iterator>
138,170c124,125
<           WalkOrder order) {
<   // We don't use early increment for regions because they can't be erased from
<   // a callback.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     if (order == WalkOrder::PreOrder)
<       callback(&region);
<     for (auto &block : Iterator::makeIterable(region)) {
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         walk<Iterator>(&nestedOp, callback, order);
<     }
<     if (order == WalkOrder::PostOrder)
<       callback(&region);
<   }
< }
< 
< template <typename Iterator>
< void walk(Operation *op, function_ref<void(Block *)> callback,
<           WalkOrder order) {
<   for (auto &region : Iterator::makeIterable(*op)) {
<     // Early increment here in the case where the block is erased.
<     for (auto &block :
<          llvm::make_early_inc_range(Iterator::makeIterable(region))) {
<       if (order == WalkOrder::PreOrder)
<         callback(&block);
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         walk<Iterator>(&nestedOp, callback, order);
<       if (order == WalkOrder::PostOrder)
<         callback(&block);
<     }
<   }
< }
< 
< template <typename Iterator>
---
>           WalkOrder order);
> void walk(Operation *op, function_ref<void(Block *)> callback, WalkOrder order);
172,189c127
<           WalkOrder order) {
<   if (order == WalkOrder::PreOrder)
<     callback(op);
< 
<   // TODO: This walk should be iterative over the operations.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     for (auto &block : Iterator::makeIterable(region)) {
<       // Early increment here in the case where the operation is erased.
<       for (auto &nestedOp :
<            llvm::make_early_inc_range(Iterator::makeIterable(block)))
<         walk<Iterator>(&nestedOp, callback, order);
<     }
<   }
< 
<   if (order == WalkOrder::PostOrder)
<     callback(op);
< }
< 
---
>           WalkOrder order);
191,197c129,134
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order'. This method is invoked for skippable or
< /// interruptible callbacks. A callback on a block or operation is allowed to
< /// erase that block or operation if either:
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order'. This method is invoked for skippable or interruptible
> /// callbacks. A callback on a block or operation is allowed to erase that block
> /// or operation if either:
200d136
< template <typename Iterator>
202,228c138
<                 WalkOrder order) {
<   // We don't use early increment for regions because they can't be erased from
<   // a callback.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     if (order == WalkOrder::PreOrder) {
<       WalkResult result = callback(&region);
<       if (result.wasSkipped())
<         continue;
<       if (result.wasInterrupted())
<         return WalkResult::interrupt();
<     }
<     for (auto &block : Iterator::makeIterable(region)) {
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<     }
<     if (order == WalkOrder::PostOrder) {
<       if (callback(&region).wasInterrupted())
<         return WalkResult::interrupt();
<       // We don't check if this region was skipped because its walk already
<       // finished and the walk will continue with the next region.
<     }
<   }
<   return WalkResult::advance();
< }
< 
< template <typename Iterator>
---
>                 WalkOrder order);
230,256c140
<                 WalkOrder order) {
<   for (auto &region : Iterator::makeIterable(*op)) {
<     // Early increment here in the case where the block is erased.
<     for (auto &block :
<          llvm::make_early_inc_range(Iterator::makeIterable(region))) {
<       if (order == WalkOrder::PreOrder) {
<         WalkResult result = callback(&block);
<         if (result.wasSkipped())
<           continue;
<         if (result.wasInterrupted())
<           return WalkResult::interrupt();
<       }
<       for (auto &nestedOp : Iterator::makeIterable(block))
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<       if (order == WalkOrder::PostOrder) {
<         if (callback(&block).wasInterrupted())
<           return WalkResult::interrupt();
<         // We don't check if this block was skipped because its walk already
<         // finished and the walk will continue with the next block.
<       }
<     }
<   }
<   return WalkResult::advance();
< }
< 
< template <typename Iterator>
---
>                 WalkOrder order);
258,283c142
<                 WalkOrder order) {
<   if (order == WalkOrder::PreOrder) {
<     WalkResult result = callback(op);
<     // If skipped, caller will continue the walk on the next operation.
<     if (result.wasSkipped())
<       return WalkResult::advance();
<     if (result.wasInterrupted())
<       return WalkResult::interrupt();
<   }
< 
<   // TODO: This walk should be iterative over the operations.
<   for (auto &region : Iterator::makeIterable(*op)) {
<     for (auto &block : Iterator::makeIterable(region)) {
<       // Early increment here in the case where the operation is erased.
<       for (auto &nestedOp :
<            llvm::make_early_inc_range(Iterator::makeIterable(block))) {
<         if (walk<Iterator>(&nestedOp, callback, order).wasInterrupted())
<           return WalkResult::interrupt();
<       }
<     }
<   }
< 
<   if (order == WalkOrder::PostOrder)
<     return callback(op);
<   return WalkResult::advance();
< }
---
>                 WalkOrder order);
291,295c150,153
< /// the given operation. The order in which regions, blocks and operations at
< /// the same nesting level are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'Order' (post-order by default). A callback on a block or
---
> /// the given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'Order' (post-order by default). A callback on a block or
307,308c165,166
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
313c171
<   return detail::walk<Iterator>(op, function_ref<RetT(ArgT)>(callback), Order);
---
>   return detail::walk(op, function_ref<RetT(ArgT)>(callback), Order);
317,321c175,178
< /// given operation. The order in which regions, blocks and operations at
< /// the same nesting are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'order' (post-order by default). This method is selected for
---
> /// given operation. Regions, blocks and operations at the same nesting
> /// level are visited in lexicographical order. The walk order for enclosing
> /// regions, blocks and operations with respect to their nested ones is
> /// specified by 'order' (post-order by default). This method is selected for
329,330c186,187
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
341,342c198
<   return detail::walk<Iterator>(op, function_ref<RetT(Operation *)>(wrapperFn),
<                                 Order);
---
>   return detail::walk(op, function_ref<RetT(Operation *)>(wrapperFn), Order);
346,353c202,208
< /// given operation. The order in which regions, blocks and operations at
< /// the same nesting are visited (e.g., lexicographical or reverse
< /// lexicographical order) is determined by 'Iterator'. The walk order for
< /// enclosing regions, blocks and operations with respect to their nested ones
< /// is specified by 'Order' (post-order by default). This method is selected for
< /// WalkReturn returning skippable or interruptible callbacks that operate on a
< /// specific derived operation type. A callback on an operation is allowed to
< /// erase that operation if either:
---
> /// given operation. Regions, blocks and operations at the same nesting level
> /// are visited in lexicographical order. The walk order for enclosing regions,
> /// blocks and operations with respect to their nested ones is specified by
> /// 'Order' (post-order by default). This method is selected for WalkReturn
> /// returning skippable or interruptible callbacks that operate on a specific
> /// derived operation type. A callback on an operation is allowed to erase that
> /// operation if either:
366,367c221,222
<     WalkOrder Order = WalkOrder::PostOrder, typename Iterator = ForwardIterator,
<     typename FuncTy, typename ArgT = detail::first_argument<FuncTy>,
---
>     WalkOrder Order = WalkOrder::PostOrder, typename FuncTy,
>     typename ArgT = detail::first_argument<FuncTy>,
379,380c234
<   return detail::walk<Iterator>(op, function_ref<RetT(Operation *)>(wrapperFn),
<                                 Order);
---
>   return detail::walk(op, function_ref<RetT(Operation *)>(wrapperFn), Order);
--- include/mlir/IR/Value.h
430,433d429
<   using Value::Value;
< 
<   static bool classof(Value value) { return llvm::isa<Ty>(value.getType()); }
< 
436c432,444
<   void setType(Ty ty) { Value::setType(ty); }
---
>   void setType(mlir::Type ty) {
>     assert(ty.template isa<Ty>());
>     Value::setType(ty);
>   }
> 
>   TypedValue(Value val) : Value(val) {
>     assert(!val || val.getType().template isa<Ty>());
>   }
>   TypedValue &operator=(const Value &other) {
>     assert(!other || other.getType().template isa<Ty>());
>     Value::operator=(other);
>     return *this;
>   }
578,583c586,587
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     }
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
--- include/mlir/IR/PatternBase.td
--- include/mlir/IR/Dialect.h
--- include/mlir/IR/AffineExpr.h
323a324,330
> template <typename AffineExprTy>
> void bindSymbolsList(MLIRContext *ctx, SmallVectorImpl<AffineExprTy> &exprs) {
>   int idx = 0;
>   for (AffineExprTy &e : exprs)
>     e = getAffineSymbolExpr(idx++, ctx);
> }
> 
333,339d339
< template <typename AffineExprTy>
< void bindDimsList(MLIRContext *ctx, MutableArrayRef<AffineExprTy> exprs) {
<   int idx = 0;
<   for (AffineExprTy &e : exprs)
<     e = getAffineDimExpr(idx++, ctx);
< }
< 
345,351d344
< }
< 
< template <typename AffineExprTy>
< void bindSymbolsList(MLIRContext *ctx, MutableArrayRef<AffineExprTy> exprs) {
<   int idx = 0;
<   for (AffineExprTy &e : exprs)
<     e = getAffineSymbolExpr(idx++, ctx);
--- include/mlir/IR/AffineExprVisitor.h
327c327
<   // IntegerRelation::addLocalFloorDiv).
---
>   // FlatAffineConstraints::addLocalFloorDiv).
--- include/mlir/IR/Attributes.h
62a63,65
>   // Support dyn_cast'ing Attribute to itself.
>   static bool classof(Attribute) { return true; }
> 
101,142d103
<   /// Walk all of the immediately nested sub-attributes and sub-types. This
<   /// method does not recurse into sub elements.
<   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const {
<     getAbstractAttribute().walkImmediateSubElements(*this, walkAttrsFn,
<                                                     walkTypesFn);
<   }
< 
<   /// Replace the immediately nested sub-attributes and sub-types with those
<   /// provided. The order of the provided elements is derived from the order of
<   /// the elements returned by the callbacks of `walkImmediateSubElements`. The
<   /// element at index 0 would replace the very first attribute given by
<   /// `walkImmediateSubElements`. On success, the new instance with the values
<   /// replaced is returned. If replacement fails, nullptr is returned.
<   auto replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const {
<     return getAbstractAttribute().replaceImmediateSubElements(*this, replAttrs,
<                                                               replTypes);
<   }
< 
<   /// Walk this attribute and all attibutes/types nested within using the
<   /// provided walk functions. See `AttrTypeWalker` for information on the
<   /// supported walk function types.
<   template <WalkOrder Order = WalkOrder::PostOrder, typename... WalkFns>
<   auto walk(WalkFns &&...walkFns) {
<     AttrTypeWalker walker;
<     (walker.addWalk(std::forward<WalkFns>(walkFns)), ...);
<     return walker.walk<Order>(*this);
<   }
< 
<   /// Recursively replace all of the nested sub-attributes and sub-types using
<   /// the provided map functions. Returns nullptr in the case of failure. See
<   /// `AttrTypeReplacer` for information on the support replacement function
<   /// types.
<   template <typename... ReplacementFns>
<   auto replace(ReplacementFns &&...replacementFns) {
<     AttrTypeReplacer replacer;
<     (replacer.addReplacement(std::forward<ReplacementFns>(replacementFns)),
<      ...);
<     return replacer.replace(*this);
<   }
< 
243,258d203
< /// Allow walking and replacing the subelements of a NamedAttribute.
< template <>
< struct AttrTypeSubElementHandler<NamedAttribute> {
<   template <typename T>
<   static void walk(T param, AttrTypeImmediateSubElementWalker &walker) {
<     walker.walk(param.getName());
<     walker.walk(param.getValue());
<   }
<   template <typename T>
<   static T replace(T param, AttrSubElementReplacements &attrRepls,
<                    TypeSubElementReplacements &typeRepls) {
<     ArrayRef<Attribute> paramRepls = attrRepls.take_front(2);
<     return T(cast<decltype(param.getName())>(paramRepls[0]), paramRepls[1]);
<   }
< };
< 
394,399c339,340
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     }
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
--- include/mlir/IR/BuiltinTypes.td
19a20
> include "mlir/IR/SubElementInterfaces.td"
166,187d166
< // Float8E4M3B11FNUZType
< 
< def Builtin_Float8E4M3B11FNUZ : Builtin_FloatType<"Float8E4M3B11FNUZ"> {
<   let summary = "8-bit floating point with 3 bit mantissa";
<   let description = [{
<     An 8-bit floating point type with 1 sign bit, 4 bits exponent and 3 bits
<     mantissa. This is not a standard type as defined by IEEE-754, but it follows
<     similar conventions, with the exception that there are no infinity values,
<     no negative zero, and only one NaN representation. This type has the
<     following characteristics:
< 
<       * bit encoding: S1E4M3
<       * exponent bias: 11
<       * infinities: Not supported
<       * NaNs: Supported with sign bit set to 1, exponent bits and mantissa bits set to all 0s
<       * denormals when exponent is 0
< 
<     Related to: https://dl.acm.org/doi/10.5555/3454287.3454728
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
233c212,214
< def Builtin_Function : Builtin_Type<"Function"> {
---
> def Builtin_Function : Builtin_Type<"Function", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>
>   ]> {
380c361
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
715c696
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
735c716
<     low level buffer access, MLIR has a [`memref` type](#memreftype). This
---
>     low level buffer access, MLIR has a [`memref` type](#memref-type). This
819c800,802
< def Builtin_Tuple : Builtin_Type<"Tuple"> {
---
> def Builtin_Tuple : Builtin_Type<"Tuple", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>
>   ]> {
887c870
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
959c942
<     ShapedTypeInterface
---
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
1007c990,992
< def Builtin_Vector : Builtin_Type<"Vector", [ShapedTypeInterface], "Type"> {
---
> def Builtin_Vector : Builtin_Type<"Vector", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>, ShapedTypeInterface
>   ], "Type"> {
1045c1030
<     // A 2D scalable-length vector that contains a multiple of 2x8 f32 elements.
---
>     // A 2D scalable-length vector that contains a multiple of 2x8 i8 elements.
1092c1077
<     /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>     /// provided shape is `None`, the current shape of the type is used.
--- include/mlir/IR/Operation.h
77,79c77
<   /// Create a new Operation with the specific fields. This constructor
<   /// populates the provided attribute list with default attributes if
<   /// necessary.
---
>   /// Create a new Operation with the specific fields.
85,91d82
<   /// Create a new Operation with the specific fields. This constructor uses an
<   /// existing attribute dictionary to avoid uniquing a list of attributes.
<   static Operation *create(Location location, OperationName name,
<                            TypeRange resultTypes, ValueRange operands,
<                            DictionaryAttr attributes, BlockRange successors,
<                            unsigned numRegions);
< 
260,268d250
<   /// Replace uses of results of this operation with the provided `values` if
<   /// the given callback returns true.
<   template <typename ValuesT>
<   void replaceUsesWithIf(ValuesT &&values,
<                          function_ref<bool(OpOperand &)> shouldReplace) {
<     getResults().replaceUsesWithIf(std::forward<ValuesT>(values),
<                                    shouldReplace);
<   }
< 
527,529c509,511
<     NamedAttrList attrs(getAttrDictionary());
<     name.populateDefaultAttrs(attrs);
<     setAttrs(attrs.getDictionary(getContext()));
---
>       NamedAttrList attrs(getAttrDictionary());
>       name.populateDefaultAttrs(attrs);
>       setAttrs(attrs.getDictionary(getContext()));
610,613c592,594
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
635,636c616
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
641c621
<     return detail::walk<Order, Iterator>(this, std::forward<FnT>(callback));
---
>     return detail::walk<Order>(this, std::forward<FnT>(callback));
807,808d786
<     assert(resultNumber < getNumResults() &&
<            "Result number is out of range for operation");
--- include/mlir/IR/CMakeLists.txt
20,23d19
< set(LLVM_TARGET_DEFINITIONS BuiltinDialectBytecode.td)
< mlir_tablegen(BuiltinDialectBytecode.cpp.inc -gen-bytecode -bytecode-dialect="Builtin")
< add_public_tablegen_target(MLIRBuiltinDialectBytecodeIncGen)
< 
48a45,51
> 
> set(LLVM_TARGET_DEFINITIONS SubElementInterfaces.td)
> mlir_tablegen(SubElementAttrInterfaces.h.inc -gen-attr-interface-decls)
> mlir_tablegen(SubElementAttrInterfaces.cpp.inc -gen-attr-interface-defs)
> mlir_tablegen(SubElementTypeInterfaces.h.inc -gen-type-interface-decls)
> mlir_tablegen(SubElementTypeInterfaces.cpp.inc -gen-type-interface-defs)
> add_public_tablegen_target(MLIRSubElementInterfacesIncGen)
--- include/mlir/IR/ImplicitLocOpBuilder.h
--- include/mlir/IR/TensorEncoding.h
--- include/mlir/IR/DialectImplementation.h
143,148d142
< namespace detail {
< template <typename T>
< using has_push_back_t = decltype(std::declval<T>().push_back(
<     std::declval<typename T::value_type &&>()));
< } // namespace detail
< 
151,154c145,148
< struct FieldParser<ContainerT,
<                    std::enable_if_t<llvm::is_detected<detail::has_push_back_t,
<                                                       ContainerT>::value,
<                                     ContainerT>> {
---
> struct FieldParser<
>     ContainerT, std::enable_if_t<std::is_member_function_pointer<
>                                      decltype(&ContainerT::push_back)>::value,
>                                  ContainerT>> {
162c156
<       elements.push_back(std::move(*element));
---
>       elements.push_back(*element);
--- include/mlir/IR/Matchers.h
55,70d54
< /// The matcher that matches operations that have the specified op name.
< struct NameOpMatcher {
<   NameOpMatcher(StringRef name) : name(name) {}
<   bool match(Operation *op) { return op->getName().getStringRef() == name; }
< 
<   StringRef name;
< };
< 
< /// The matcher that matches operations that have the specified attribute name.
< struct AttrOpMatcher {
<   AttrOpMatcher(StringRef attrName) : attrName(attrName) {}
<   bool match(Operation *op) { return op->hasAttr(attrName); }
< 
<   StringRef attrName;
< };
< 
102,124d85
< /// The matcher that matches operations that have the specified attribute
< /// name, and binds the attribute value.
< template <typename AttrT>
< struct AttrOpBinder {
<   /// Creates a matcher instance that binds the attribute value to
<   /// bind_value if match succeeds.
<   AttrOpBinder(StringRef attrName, AttrT *bindValue)
<       : attrName(attrName), bindValue(bindValue) {}
<   /// Creates a matcher instance that doesn't bind if match succeeds.
<   AttrOpBinder(StringRef attrName) : attrName(attrName), bindValue(nullptr) {}
< 
<   bool match(Operation *op) {
<     if (auto attr = op->getAttrOfType<AttrT>(attrName)) {
<       if (bindValue)
<         *bindValue = attr;
<       return true;
<     }
<     return false;
<   }
<   StringRef attrName;
<   AttrT *bindValue;
< };
< 
291,300d251
< /// Matches a named attribute operation.
< inline detail::AttrOpMatcher m_Attr(StringRef attrName) {
<   return detail::AttrOpMatcher(attrName);
< }
< 
< /// Matches a named operation.
< inline detail::NameOpMatcher m_Op(StringRef opName) {
<   return detail::NameOpMatcher(opName);
< }
< 
306,312d256
< }
< 
< /// Matches a named attribute operation and writes the value to bind_value.
< template <typename AttrT>
< inline detail::AttrOpBinder<AttrT> m_Attr(StringRef attrName,
<                                           AttrT *bindValue) {
<   return detail::AttrOpBinder<AttrT>(attrName, bindValue);
--- include/mlir/IR/DialectInterface.h
--- include/mlir/IR/RegionGraphTraits.h
53,86d52
< struct GraphTraits<const mlir::Block *> {
<   using ChildIteratorType = mlir::Block::succ_iterator;
<   using Node = const mlir::Block;
<   using NodeRef = Node *;
< 
<   static NodeRef getEntryNode(NodeRef node) { return node; }
< 
<   static ChildIteratorType child_begin(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->succ_begin();
<   }
<   static ChildIteratorType child_end(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->succ_end();
<   }
< };
< 
< template <>
< struct GraphTraits<Inverse<const mlir::Block *>> {
<   using ChildIteratorType = mlir::Block::pred_iterator;
<   using Node = const mlir::Block;
<   using NodeRef = Node *;
< 
<   static NodeRef getEntryNode(Inverse<NodeRef> inverseGraph) {
<     return inverseGraph.Graph;
<   }
< 
<   static ChildIteratorType child_begin(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->pred_begin();
<   }
<   static ChildIteratorType child_end(NodeRef node) {
<     return const_cast<mlir::Block *>(node)->pred_end();
<   }
< };
< 
< template <>
--- include/mlir/IR/FunctionInterfaces.h
--- include/mlir/IR/Region.h
268,271c268,270
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
276,277c275
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
281c279
<       block.walk<Order, Iterator>(callback);
---
>       block.walk<Order>(callback);
286,289c284,286
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
296,297c293
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
302c298
<       if (block.walk<Order, Iterator>(callback).wasInterrupted())
---
>       if (block.walk<Order>(callback).wasInterrupted())
--- include/mlir/IR/RegionKindInterface.h
41,45d40
< /// Return "true" if the given region may have SSA dominance. This function also
< /// returns "true" in case the owner op is an unregistered op or an op that does
< /// not implement the RegionKindInterface.
< bool mayHaveSSADominance(Region &region);
< 
--- include/mlir/IR/UseDefLists.h
--- include/mlir/IR/OpBase.td
243,246d242
< // Whether a type is a RankedTensorType
< def IsRankedTensorTypePred
<         : CPred<"$_self.isa<::mlir::RankedTensorType>()">;
< 
497,498d492
< def F8E4M3B11FNUZ : Type<CPred<"$_self.isFloat8E4M3B11FNUZ()">, "f8E4M3B11FNUZ type">,
<                  BuildableType<"$_builder.getFloat8E4M3B11FNUZType()">;
561,566d554
< // Whether a shaped type has a rank greater than or equal of the specified rank.
< class HasRankGreaterOrEqualPred<int rank> : And<[
<     HasRankPred,
<     CPred<[{$_self.cast<::mlir::ShapedType>().getRank() >= }] # rank>
< ]>;
< 
728,742c716,720
< // Unranked tensor type whose element type is from the given `allowedTypes`
< // list, and which additionally satisfies an optional list of predicates.
< class UnrankedTensorOf<list<Type> allowedTypes, list<Pred> preds = [],
<                        string summary = "unranked tensor">
<   : ShapedContainerType<
<       allowedTypes, And<!listconcat([IsUnrankedTensorTypePred], preds)>,
<       summary, "::mlir::UnrankedTensorType">;
< 
< // Ranked tensor type whose element type is from the given `allowedTypes` list,
< // and which additionally satisfies an optional list of predicates.
< class RankedTensorOf<list<Type> allowedTypes, list<Pred> preds = [],
<                      string summary = "ranked tensor">
<   : ShapedContainerType<
<       allowedTypes, And<!listconcat([IsRankedTensorTypePred], preds)>,
<       summary, "::mlir::RankedTensorType">;
---
> // Unranked tensor type whose element type is from the given
> // `allowedTypes` list.
> class UnrankedTensorOf<list<Type> allowedTypes>
>   : ShapedContainerType<allowedTypes, IsUnrankedTensorTypePred,
>       "unranked.tensor", "::mlir::UnrankedTensorType">;
771,773c749,753
< class Non0RankedTensorOf<list<Type> allowedTypes>
<   : TensorOf<allowedTypes, [HasRankGreaterOrEqualPred<1>],
<       "non-0-ranked.tensor">;
---
> class RankedTensorOf<
>     list<Type> allowedTypes,
>     list<Pred> preds = [],
>     string summary = "ranked tensor">
>   : TensorOf<allowedTypes, !listconcat([HasRankPred], preds), summary>;
776,781d755
< def AnyNon0RankedTensor  : Non0RankedTensorOf<[AnyType]>;
< def AnyUnrankedTensor  : UnrankedTensorOf<[AnyType]>;
< 
< def AnyNon0RankedOrUnrankedTensor
<   : AnyTypeOf<[AnyUnrankedTensor, AnyNon0RankedTensor],
<               "non-0-ranked or unranked tensor", "::mlir::TensorType">;
785c759
<   : RankedTensorOf<allowedTypes,
---
>   : TensorOf<allowedTypes,
796,797c770
<   : RankedTensorOf<allowedTypes, [HasStaticShapePred],
<                    "statically shaped tensor">;
---
>   : TensorOf<allowedTypes, [HasStaticShapePred], "statically shaped tensor">;
804c777
< // Any unranked memref whose element type is from the given `allowedTypes` list.
---
> // Unranked Memref type
812c785
< // Any ranked memref whose element type is from the given `allowedTypes` list.
---
> // Memrefs are blocks of data with fixed type and rank.
817,821d789
< class Non0RankedMemRefOf<list<Type> allowedTypes> :
<     ConfinedType<MemRefOf<allowedTypes>, [HasRankGreaterOrEqualPred<1>],
<          "non-0-ranked." # MemRefOf<allowedTypes>.summary,
<          "::mlir::MemRefType">;
< 
823d790
< def AnyNon0RankedMemRef : Non0RankedMemRefOf<[AnyType]>;
825,834c792,793
< // Any memref (ranked or unranked) whose element type is from the given
< // `allowedTypes` list, and which additionally satisfies an optional list of
< // predicates.
< class RankedOrUnrankedMemRefOf<
<     list<Type> allowedTypes,
<     list<Pred> preds = [],
<     string summary = "ranked or unranked memref">
<   : ShapedContainerType<allowedTypes,
<       And<!listconcat([IsBaseMemRefTypePred], preds)>,
<       summary, "::mlir::BaseMemRefType">;
---
> class RankedOrUnrankedMemRefOf<list<Type> allowedTypes>:
>     AnyTypeOf<[UnrankedMemRefOf<allowedTypes>, MemRefOf<allowedTypes>]>;
836,838c795
< def AnyRankedOrUnrankedMemRef  : RankedOrUnrankedMemRefOf<[AnyType]>;
< def AnyNon0RankedOrUnrankedMemRef:
<     AnyTypeOf<[AnyUnrankedMemRef, AnyNon0RankedMemRef]>;
---
> def AnyRankedOrUnrankedMemRef: AnyTypeOf<[AnyUnrankedMemRef, AnyMemRef]>;
897c854
<                 "::llvm::all_of(" # elementTypesCall # ", [](::mlir::Type t) { "
---
>                 "::llvm::all_of(" # elementTypesCall # ", [](Type t) { "
1128,1130d1084
< def LocationAttr : Attr<CPred<"$_self.isa<::mlir::LocationAttr>()">,
<                         "location attribute">;
< 
1299,1300c1253
< class TypeAttrBase<string retType, string summary,
<                         Pred typePred = CPred<"true">> :
---
> class TypeAttrBase<string retType, string summary> :
1304,1306c1257
<             # retType # ">()">,
<       SubstLeaves<"$_self",
<                     "$_self.cast<::mlir::TypeAttr>().getValue()", typePred>]>,
---
>             # retType # ">()">]>,
1319,1320c1270
<    : TypeAttrBase<ty.cppClassName, "type attribute of " # ty.summary,
<                     ty.predicate> {
---
>    : TypeAttrBase<ty.cppClassName, "type attribute of " # ty.summary> {
1534,1536d1483
< def LocationArrayAttr : TypedArrayAttrBase<LocationAttr,
<                                            "location array attribute">;
< 
1771,1788d1717
< class IntArrayNthElemMaxValue<int index, int max> : AttrConstraint<
<     And<[
<       CPred<"$_self.cast<::mlir::ArrayAttr>().size() > " # index>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() <= " # max>
<         ]>,
<     "whose " # index # "-th element must be at most " # max>;
< 
< class IntArrayNthElemInRange<int index, int min, int max> : AttrConstraint<
<     And<[
<       CPred<"$_self.cast<::mlir::ArrayAttr>().size() > " # index>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() >= " # min>,
<       CPred<"$_self.cast<::mlir::ArrayAttr>()[" # index # "]"
<         ".cast<::mlir::IntegerAttr>().getInt() <= " # max>
<         ]>,
<     "whose " # index # "-th element must be at least " # min # " and at most " # max>;
< 
1812,1816d1740
< // A region with at most the given number of blocks.
< class MaxSizedRegion<int numBlocks> : Region<
<   CPred<"::llvm::hasNItemsOrLess($_self, " # numBlocks # ")">,
<   "region with at most " # numBlocks # " blocks">;
< 
2266,2271d2189
< 
< // OpBuilder like the above, but the emitted 'build' method is marked as
< // deprecated in C++. Use of it will emit a warning by the C++ compiler
< // with the given reason.
< class DeprecatedOpBuilder<string reason, dag p, code b = "">
<   : OpBuilder<p, b>, CppDeprecated<reason>;
--- include/mlir/IR/IntegerSet.h
20c20
< // transformation. For the latter, use FlatAffineValueConstraints.
---
> // transformation. For the latter, use FlatAffineConstraints.
--- include/mlir/IR/StorageUniquerSupport.h
16d15
< #include "mlir/IR/AttrTypeSubElements.h"
127,146d125
<     };
<   }
< 
<   /// Returns a function that walks immediate sub elements of a given instance
<   /// of the storage user.
<   static auto getWalkImmediateSubElementsFn() {
<     return [](auto instance, function_ref<void(Attribute)> walkAttrsFn,
<               function_ref<void(Type)> walkTypesFn) {
<       ::mlir::detail::walkImmediateSubElementsImpl(
<           llvm::cast<ConcreteT>(instance), walkAttrsFn, walkTypesFn);
<     };
<   }
< 
<   /// Returns a function that replaces immediate sub elements of a given
<   /// instance of the storage user.
<   static auto getReplaceImmediateSubElementsFn() {
<     return [](auto instance, ArrayRef<Attribute> replAttrs,
<               ArrayRef<Type> replTypes) {
<       return ::mlir::detail::replaceImmediateSubElementsImpl(
<           llvm::cast<ConcreteT>(instance), replAttrs, replTypes);
--- include/mlir/IR/SymbolTable.h
16d15
< #include "llvm/Support/RWMutex.h"
285,286d283
<   friend class LockedSymbolTableCollection;
< 
289,342d285
< };
< 
< //===----------------------------------------------------------------------===//
< // LockedSymbolTableCollection
< //===----------------------------------------------------------------------===//
< 
< /// This class implements a lock-based shared wrapper around a symbol table
< /// collection that allows shared access to the collection of symbol tables.
< /// This class does not protect shared access to individual symbol tables.
< /// `SymbolTableCollection` lazily instantiates `SymbolTable` instances for
< /// symbol table operations, making read operations not thread-safe. This class
< /// provides a thread-safe `lookupSymbolIn` implementation by synchronizing the
< /// lazy `SymbolTable` lookup.
< class LockedSymbolTableCollection : public SymbolTableCollection {
< public:
<   explicit LockedSymbolTableCollection(SymbolTableCollection &collection)
<       : collection(collection) {}
< 
<   /// Look up a symbol with the specified name within the specified symbol table
<   /// operation, returning null if no such name exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, StringAttr symbol);
<   /// Look up a symbol with the specified name within the specified symbol table
<   /// operation, returning null if no such name exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, FlatSymbolRefAttr symbol);
<   /// Look up a potentially nested symbol within the specified symbol table
<   /// operation, returning null if no such symbol exists.
<   Operation *lookupSymbolIn(Operation *symbolTableOp, SymbolRefAttr name);
< 
<   /// Lookup a symbol of a particular kind within the specified symbol table,
<   /// returning null if the symbol was not found.
<   template <typename T, typename NameT>
<   T lookupSymbolIn(Operation *symbolTableOp, NameT &&name) {
<     return dyn_cast_or_null<T>(
<         lookupSymbolIn(symbolTableOp, std::forward<NameT>(name)));
<   }
< 
<   /// A variant of 'lookupSymbolIn' that returns all of the symbols referenced
<   /// by a given SymbolRefAttr when resolved within the provided symbol table
<   /// operation. Returns failure if any of the nested references could not be
<   /// resolved.
<   LogicalResult lookupSymbolIn(Operation *symbolTableOp, SymbolRefAttr name,
<                                SmallVectorImpl<Operation *> &symbols);
< 
< private:
<   /// Get the symbol table for the symbol table operation, constructing if it
<   /// does not exist. This function provides thread safety over `collection`
<   /// by locking when performing the lookup and when inserting
<   /// lazily-constructed symbol tables.
<   SymbolTable &getSymbolTable(Operation *symbolTableOp);
< 
<   /// The symbol tables to manage.
<   SymbolTableCollection &collection;
<   /// The mutex protecting access to the symbol table collection.
<   llvm::sys::SmartRWMutex<true> mutex;
--- include/mlir/IR/BuiltinLocationAttributes.td
17a18
> include "mlir/IR/SubElementInterfaces.td"
30c31,33
< def CallSiteLoc : Builtin_LocationAttr<"CallSiteLoc"> {
---
> def CallSiteLoc : Builtin_LocationAttr<"CallSiteLoc", [
>     SubElementAttrInterface
>   ]> {
107c110,112
< def FusedLoc : Builtin_LocationAttr<"FusedLoc"> {
---
> def FusedLoc : Builtin_LocationAttr<"FusedLoc", [
>     SubElementAttrInterface
>   ]> {
146c151,153
< def NameLoc : Builtin_LocationAttr<"NameLoc"> {
---
> def NameLoc : Builtin_LocationAttr<"NameLoc", [
>     SubElementAttrInterface
>   ]> {
183c190,192
< def OpaqueLoc : Builtin_LocationAttr<"OpaqueLoc"> {
---
> def OpaqueLoc : Builtin_LocationAttr<"OpaqueLoc", [
>     SubElementAttrInterface
>   ]> {
--- include/mlir/IR/OpAsmInterface.td
--- include/mlir/IR/TypeUtilities.h
--- include/mlir/IR/SubElementInterfaces.td
--- include/mlir/IR/RegionKindInterface.td
--- include/mlir/IR/ValueRange.h
282,301d281
<   /// Replace uses of results of this range with the provided 'values' if the
<   /// given callback returns true. The size of `values` must match the size of
<   /// this range.
<   template <typename ValuesT>
<   std::enable_if_t<!std::is_convertible<ValuesT, Operation *>::value>
<   replaceUsesWithIf(ValuesT &&values,
<                     function_ref<bool(OpOperand &)> shouldReplace) {
<     assert(static_cast<size_t>(std::distance(values.begin(), values.end())) ==
<                size() &&
<            "expected 'values' to correspond 1-1 with the number of results");
< 
<     for (auto it : llvm::zip(*this, values))
<       std::get<0>(it).replaceUsesWithIf(std::get<1>(it), shouldReplace);
<   }
< 
<   /// Replace uses of results of this range with results of `op` if the given
<   /// callback returns true.
<   void replaceUsesWithIf(Operation *op,
<                          function_ref<bool(OpOperand &)> shouldReplace);
< 
--- include/mlir/IR/Types.h
109a110,112
>   // Support type casting Type to itself.
>   static bool classof(Type) { return true; }
> 
127d129
<   bool isFloat8E4M3B11FNUZ() const;
189c191
<   const AbstractTy &getAbstractType() const { return impl->getAbstractType(); }
---
>   const AbstractTy &getAbstractType() { return impl->getAbstractType(); }
194,234d195
<   /// Walk all of the immediately nested sub-attributes and sub-types. This
<   /// method does not recurse into sub elements.
<   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const {
<     getAbstractType().walkImmediateSubElements(*this, walkAttrsFn, walkTypesFn);
<   }
< 
<   /// Replace the immediately nested sub-attributes and sub-types with those
<   /// provided. The order of the provided elements is derived from the order of
<   /// the elements returned by the callbacks of `walkImmediateSubElements`. The
<   /// element at index 0 would replace the very first attribute given by
<   /// `walkImmediateSubElements`. On success, the new instance with the values
<   /// replaced is returned. If replacement fails, nullptr is returned.
<   auto replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const {
<     return getAbstractType().replaceImmediateSubElements(*this, replAttrs,
<                                                          replTypes);
<   }
< 
<   /// Walk this type and all attibutes/types nested within using the
<   /// provided walk functions. See `AttrTypeWalker` for information on the
<   /// supported walk function types.
<   template <WalkOrder Order = WalkOrder::PostOrder, typename... WalkFns>
<   auto walk(WalkFns &&...walkFns) {
<     AttrTypeWalker walker;
<     (walker.addWalk(std::forward<WalkFns>(walkFns)), ...);
<     return walker.walk<Order>(*this);
<   }
< 
<   /// Recursively replace all of the nested sub-attributes and sub-types using
<   /// the provided map functions. Returns nullptr in the case of failure. See
<   /// `AttrTypeReplacer` for information on the support replacement function
<   /// types.
<   template <typename... ReplacementFns>
<   auto replace(ReplacementFns &&...replacementFns) {
<     AttrTypeReplacer replacer;
<     (replacer.addReplacement(std::forward<ReplacementFns>(replacementFns)),
<      ...);
<     return replacer.replace(*this);
<   }
< 
390,395c351,352
<     if constexpr (std::is_base_of_v<To, From>) {
<       (void)ty;
<       return true;
<     } else {
<       return To::classof(ty);
<     };
---
>     return std::is_same_v<To, std::remove_const_t<From>> ||
>            std::is_base_of_v<To, From> || To::classof(ty);
--- include/mlir/IR/FunctionInterfaces.td
57,60d56
<       Returns the symbol name of the function.
<     }],
<     "StringRef", "getSymName">,
<     InterfaceMethod<[{
282c278
<       function_interface_impl::setFunctionType($_op, newType);
---
>       function_interface_impl::setFunctionType(this->getOperation(), newType);
323c319
<           $_op, argIndices, argTypes, argAttrs, argLocs,
---
>           this->getOperation(), argIndices, argTypes, argAttrs, argLocs,
343c339
<           $_op, resultIndices, resultTypes, resultAttrs,
---
>           this->getOperation(), resultIndices, resultTypes, resultAttrs,
358c354
<         $_op, argIndices, newType);
---
>         this->getOperation(), argIndices, newType);
372c368
<           $_op, resultIndices, newType);
---
>           this->getOperation(), resultIndices, newType);
421c417
<       return function_interface_impl::getArgAttrs($_op, index);
---
>       return function_interface_impl::getArgAttrs(this->getOperation(), index);
471c467
<       function_interface_impl::setAllArgAttrDicts($_op, attributes);
---
>       function_interface_impl::setAllArgAttrDicts(this->getOperation(), attributes);
475c471
<       function_interface_impl::setAllArgAttrDicts($_op, attributes);
---
>       function_interface_impl::setAllArgAttrDicts(this->getOperation(), attributes);
510c506
<       return function_interface_impl::getResultAttrs($_op, index);
---
>       return function_interface_impl::getResultAttrs(this->getOperation(), index);
561c557
<         $_op, attributes);
---
>         this->getOperation(), attributes);
566c562
<         $_op, attributes);
---
>         this->getOperation(), attributes);
596c592
<       return function_interface_impl::getArgAttrDict($_op, index);
---
>       return function_interface_impl::getArgAttrDict(this->getOperation(), index);
604c600
<       return function_interface_impl::getResultAttrDict($_op, index);
---
>       return function_interface_impl::getResultAttrDict(this->getOperation(), index);
--- include/mlir/IR/BuiltinAttributeInterfaces.h
--- include/mlir/IR/TypeRange.h
169c169
< // SubElements
---
> // SubElementInterfaces
175c175
<   static void walk(TypeRange param, AttrTypeImmediateSubElementWalker &walker) {
---
>   static void walk(TypeRange param, AttrTypeSubElementWalker &walker) {
--- include/mlir/IR/AsmState.h
--- include/mlir/IR/BuiltinDialect.h
--- include/mlir/IR/TensorEncoding.td
--- include/mlir/IR/Location.h
17a18
> #include "mlir/IR/SubElementInterfaces.h"
174c175
< // SubElements
---
> // SubElementInterfaces
180c181
<   static void walk(Location param, AttrTypeImmediateSubElementWalker &walker) {
---
>   static void walk(Location param, AttrTypeSubElementWalker &walker) {
--- include/mlir/IR/BuiltinAttributeInterfaces.td
19,37d18
< // TypedAttrInterface
< //===----------------------------------------------------------------------===//
< 
< def TypedAttrInterface : AttrInterface<"TypedAttr"> {
<   let cppNamespace = "::mlir";
< 
<   let description = [{
<     This interface is used for attributes that have a type. The type of an
<     attribute is understood to represent the type of the data contained in the
<     attribute and is often used as the type of a value with this data.
<   }];
< 
<   let methods = [InterfaceMethod<
<     "Get the attribute's type",
<     "::mlir::Type", "getType"
<   >];
< }
< 
< //===----------------------------------------------------------------------===//
41c22
< def ElementsAttrInterface : AttrInterface<"ElementsAttr", [TypedAttrInterface]> {
---
> def ElementsAttrInterface : AttrInterface<"ElementsAttr"> {
100c81
<       mlir::Type elementType = getShapedType().getElementType();
---
>       mlir::Type elementType = getType().getElementType();
176c157
<     }], "bool", "isSplat", (ins), [{}], /*defaultImplementation=*/[{
---
>     }], "bool", "isSplat", (ins), /*defaultImplementation=*/[{}], [{
182,184c163
<     }], "::mlir::ShapedType", "getShapedType", (ins), [{}], /*defaultImplementation=*/[{
<         return $_attr.getType();
<     }]>
---
>     }], "::mlir::ShapedType", "getType">
349c328
<       return getFlattenedIndex(elementsAttr.getShapedType(), index);
---
>       return getFlattenedIndex(elementsAttr.getType(), index);
381c360
<       return {getShapedType(), value_begin<T>(), value_end<T>()};
---
>       return {getType(), value_begin<T>(), value_end<T>()};
401c380
<       return {getShapedType(), llvm::map_range(getValues<Attribute>(),
---
>       return {getType(), llvm::map_range(getValues<Attribute>(),
421c400
<         return iterator_range<T>(getShapedType(), *beginIt, value_end<T>());
---
>         return iterator_range<T>(getType(), *beginIt, value_end<T>());
437c416
<         getShapedType(),
---
>         getType(),
495a475,493
> }
> 
> //===----------------------------------------------------------------------===//
> // TypedAttrInterface
> //===----------------------------------------------------------------------===//
> 
> def TypedAttrInterface : AttrInterface<"TypedAttr"> {
>   let cppNamespace = "::mlir";
> 
>   let description = [{
>     This interface is used for attributes that have a type. The type of an
>     attribute is understood to represent the type of the data contained in the
>     attribute and is often used as the type of a value with this data.
>   }];
> 
>   let methods = [InterfaceMethod<
>     "Get the attribute's type",
>     "::mlir::Type", "getType"
>   >];
--- include/mlir/IR/BuiltinAttributes.td
20a21
> include "mlir/IR/SubElementInterfaces.td"
73c74,76
< def Builtin_ArrayAttr : Builtin_Attr<"Array"> {
---
> def Builtin_ArrayAttr : Builtin_Attr<"Array", [
>     SubElementAttrInterface
>   ]> {
221c224
<     "DenseIntOrFPElements", [ElementsAttrInterface],
---
>     "DenseIntOrFPElements", [ElementsAttrInterface, TypedAttrInterface],
362c365
<     "DenseStringElements", [ElementsAttrInterface],
---
>     "DenseStringElements", [ElementsAttrInterface, TypedAttrInterface],
433c436
<     ElementsAttrInterface
---
>     ElementsAttrInterface, TypedAttrInterface
491c494,496
< def Builtin_DictionaryAttr : Builtin_Attr<"Dictionary"> {
---
> def Builtin_DictionaryAttr : Builtin_Attr<"Dictionary", [
>     SubElementAttrInterface
>   ]> {
807c812
<     "SparseElements", [ElementsAttrInterface]
---
>     "SparseElements", [ElementsAttrInterface, TypedAttrInterface]
1094c1099,1101
< def Builtin_SymbolRefAttr : Builtin_Attr<"SymbolRef"> {
---
> def Builtin_SymbolRefAttr : Builtin_Attr<"SymbolRef", [
>     SubElementAttrInterface
>   ]> {
1109a1117,1123
>     This attribute can only be held internally by
>     [array attributes](#array-attribute),
>     [dictionary attributes](#dictionary-attribute)(including the top-level
>     operation attribute dictionary) as well as attributes exposing it via
>     the `SubElementAttrInterface` interface. Symbol reference attributes
>     nested in types are currently not supported.
> 
1160c1174,1176
< def Builtin_TypeAttr : Builtin_Attr<"Type"> {
---
> def Builtin_TypeAttr : Builtin_Attr<"Type", [
>     SubElementAttrInterface
>   ]> {
--- include/mlir/IR/SymbolInterfaces.td
--- include/mlir/IR/SubElementInterfaces.h
--- include/mlir/IR/BuiltinTypes.h
13a14
> #include "mlir/IR/SubElementInterfaces.h"
52d52
<   static FloatType getFloat8E4M3B11FNUZ(MLIRContext *ctx);
94c94
<   /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>   /// provided shape is `None`, the current shape of the type is used.
130c130
<   /// provided shape is `std::nullopt`, the current shape of the type is used.
---
>   /// provided shape is `None`, the current shape of the type is used.
380,383c380,382
<   return type
<       .isa<Float8E5M2Type, Float8E4M3FNType, Float8E5M2FNUZType,
<            Float8E4M3FNUZType, Float8E4M3B11FNUZType, BFloat16Type, Float16Type,
<            Float32Type, Float64Type, Float80Type, Float128Type>();
---
>   return type.isa<Float8E5M2Type, Float8E4M3FNType, Float8E5M2FNUZType,
>                   Float8E4M3FNUZType, BFloat16Type, Float16Type, Float32Type,
>                   Float64Type, Float80Type, Float128Type>();
400,403d398
< }
< 
< inline FloatType FloatType::getFloat8E4M3B11FNUZ(MLIRContext *ctx) {
<   return Float8E4M3B11FNUZType::get(ctx);
--- include/mlir/IR/Block.h
261,264c261,263
<   /// The order in which regions, blocks and operations at the same nesting
<   /// level are visited (e.g., lexicographical or reverse lexicographical order)
<   /// is determined by 'Iterator'. The walk order for enclosing regions, blocks
<   /// and operations with respect to their nested ones is specified by 'Order'
---
>   /// Regions, blocks and operations at the same nesting level are visited in
>   /// lexicographical order. The walk order for enclosing regions, blocks and
>   /// operations with respect to their nested ones is specified by 'Order'
270,271c269
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
274c272
<     return walk<Order, Iterator>(begin(), end(), std::forward<FnT>(callback));
---
>     return walk<Order>(begin(), end(), std::forward<FnT>(callback));
279,281c277,278
<   /// depending on the callback provided. The order in which regions, blocks and
<   /// operations at the same nesting level are visited (e.g., lexicographical or
<   /// reverse lexicographical order) is determined by 'Iterator'. The walk order
---
>   /// depending on the callback provided. Regions, blocks and operations at the
>   /// same nesting level are visited in lexicographical order. The walk order
288,289c285
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
294c290
<       detail::walk<Order, Iterator>(&op, callback);
---
>       detail::walk<Order>(&op, callback);
299,301c295,296
<   /// depending on the callback provided. The order in which regions, blocks and
<   /// operations at the same nesting level are visited (e.g., lexicographical or
<   /// reverse lexicographical order) is determined by 'Iterator'. The walk order
---
>   /// depending on the callback provided. Regions, blocks and operations at the
>   /// same nesting level are visited in lexicographical order. The walk order
309,310c304
<   template <WalkOrder Order = WalkOrder::PostOrder,
<             typename Iterator = ForwardIterator, typename FnT,
---
>   template <WalkOrder Order = WalkOrder::PostOrder, typename FnT,
315c309
<       if (detail::walk<Order, Iterator>(&op, callback).wasInterrupted())
---
>       if (detail::walk<Order>(&op, callback).wasInterrupted())
--- include/mlir/IR/IRMapping.h
--- include/mlir/IR/MLIRContext.h
14d13
< #include "llvm/ADT/ArrayRef.h"
24,26c23
< namespace tracing {
< class Action;
< }
---
> class DebugActionManager;
36d32
< class IRUnit;
139,142d134
<   /// This option is **heavily discouraged**: it is convenient during testing
<   /// but it is not a good practice to use it in production code. Some system
<   /// invariants can be broken (like loading a dialect after creating
<   ///  operations) without being caught by assertions or other means.
220a213,215
>   /// Returns the manager of debug actions within the context.
>   DebugActionManager &getDebugActionManager();
> 
243,277d237
<   //===--------------------------------------------------------------------===//
<   // Action API
<   //===--------------------------------------------------------------------===//
< 
<   /// Signatures for the action handler that can be registered with the context.
<   using HandlerTy =
<       std::function<void(function_ref<void()>, const tracing::Action &)>;
< 
<   /// Register a handler for handling actions that are dispatched through this
<   /// context. A nullptr handler can be set to disable a previously set handler.
<   void registerActionHandler(HandlerTy handler);
< 
<   /// Return true if a valid ActionHandler is set.
<   bool hasActionHandler();
< 
<   /// Dispatch the provided action to the handler if any, or just execute it.
<   void executeAction(function_ref<void()> actionFn,
<                      const tracing::Action &action) {
<     if (LLVM_UNLIKELY(hasActionHandler()))
<       executeActionInternal(actionFn, action);
<     else
<       actionFn();
<   }
< 
<   /// Dispatch the provided action to the handler if any, or just execute it.
<   template <typename ActionTy, typename... Args>
<   void executeAction(function_ref<void()> actionFn, ArrayRef<IRUnit> irUnits,
<                      Args &&...args) {
<     if (LLVM_UNLIKELY(hasActionHandler()))
<       executeActionInternal<ActionTy, Args...>(actionFn, irUnits,
<                                                std::forward<Args>(args)...);
<     else
<       actionFn();
<   }
< 
281,295d240
< 
<   /// Internal helper for the dispatch method.
<   void executeActionInternal(function_ref<void()> actionFn,
<                              const tracing::Action &action);
< 
<   /// Internal helper for the dispatch method. We get here after checking that
<   /// there is a handler, for the purpose of keeping this code out-of-line. and
<   /// avoid calling the ctor for the Action unnecessarily.
<   template <typename ActionTy, typename... Args>
<   LLVM_ATTRIBUTE_NOINLINE void
<   executeActionInternal(function_ref<void()> actionFn, ArrayRef<IRUnit> irUnits,
<                         Args &&...args) {
<     executeActionInternal(actionFn,
<                           ActionTy(irUnits, std::forward<Args>(args)...));
<   }
--- include/mlir/IR/DialectResourceBlobManager.h
--- include/mlir/IR/EnumAttr.td
--- include/mlir/IR/BuiltinOps.td
--- include/mlir/IR/Verifier.h
--- include/mlir/IR/BuiltinTypeInterfaces.td
101a102,110
> 
>     /// Returns the total amount of bits occupied by a value of this type. This
>     /// does not take into account any memory layout or widening constraints,
>     /// e.g. a vector<3xi57> may report to occupy 3x57=171 bit, even though in
>     /// practice it will likely be stored as in a 4xi64 vector register. Fails
>     /// with an assertion if the size cannot be computed statically, e.g. if the
>     /// type has a dynamic shape or if its elemental type does not have a known
>     /// bit width.
>     int64_t getSizeInBits() const;
--- include/mlir/IR/AttributeSupport.h
22a23,25
> class MLIRContext;
> class Type;
> 
32,35d34
<   using WalkImmediateSubElementsFn = function_ref<void(
<       Attribute, function_ref<void(Attribute)>, function_ref<void(Type)>)>;
<   using ReplaceImmediateSubElementsFn =
<       function_ref<Attribute(Attribute, ArrayRef<Attribute>, ArrayRef<Type>)>;
46,47d44
<                              T::getWalkImmediateSubElementsFn(),
<                              T::getReplaceImmediateSubElementsFn(),
55,60c52,54
<   static AbstractAttribute
<   get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
<       HasTraitFn &&hasTrait,
<       WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<       ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<       TypeID typeID) {
---
>   static AbstractAttribute get(Dialect &dialect,
>                                detail::InterfaceMap &&interfaceMap,
>                                HasTraitFn &&hasTrait, TypeID typeID) {
62,63c56
<                              std::move(hasTrait), walkImmediateSubElementsFn,
<                              replaceImmediateSubElementsFn, typeID);
---
>                              std::move(hasTrait), typeID);
92,101d84
<   /// Walk the immediate sub-elements of this attribute.
<   void walkImmediateSubElements(Attribute attr,
<                                 function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const;
< 
<   /// Replace the immediate sub-elements of this attribute.
<   Attribute replaceImmediateSubElements(Attribute attr,
<                                         ArrayRef<Attribute> replAttrs,
<                                         ArrayRef<Type> replTypes) const;
< 
107,110c90
<                     HasTraitFn &&hasTraitFn,
<                     WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<                     ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<                     TypeID typeID)
---
>                     HasTraitFn &&hasTrait, TypeID typeID)
112,115c92
<         hasTraitFn(std::move(hasTraitFn)),
<         walkImmediateSubElementsFn(walkImmediateSubElementsFn),
<         replaceImmediateSubElementsFn(replaceImmediateSubElementsFn),
<         typeID(typeID) {}
---
>         hasTraitFn(std::move(hasTrait)), typeID(typeID) {}
136,141d112
<   /// Function to walk the immediate sub-elements of this attribute.
<   WalkImmediateSubElementsFn walkImmediateSubElementsFn;
< 
<   /// Function to replace the immediate sub-elements of this attribute.
<   ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn;
< 
296,308d266
< 
< // Internal function called by ODS generated code.
< // Default initializes the type within a FailureOr<T> if T is default
< // constructible and returns a reference to the instance.
< // Otherwise, returns a reference to the FailureOr<T>.
< template <class T>
< decltype(auto) unwrapForCustomParse(FailureOr<T> &failureOr) {
<   if constexpr (std::is_default_constructible_v<T>)
<     return failureOr.emplace();
<   else
<     return failureOr;
< }
< 
--- include/mlir/IR/BlockSupport.h
--- include/mlir/IR/TypeSupport.h
33,36d32
<   using WalkImmediateSubElementsFn = function_ref<void(
<       Type, function_ref<void(Attribute)>, function_ref<void(Type)>)>;
<   using ReplaceImmediateSubElementsFn =
<       function_ref<Type(Type, ArrayRef<Attribute>, ArrayRef<Type>)>;
47,48c43
<                         T::getWalkImmediateSubElementsFn(),
<                         T::getReplaceImmediateSubElementsFn(), T::getTypeID());
---
>                         T::getTypeID());
55,60c50,51
<   static AbstractType
<   get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
<       HasTraitFn &&hasTrait,
<       WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<       ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<       TypeID typeID) {
---
>   static AbstractType get(Dialect &dialect, detail::InterfaceMap &&interfaceMap,
>                           HasTraitFn &&hasTrait, TypeID typeID) {
62,63c53
<                         walkImmediateSubElementsFn,
<                         replaceImmediateSubElementsFn, typeID);
---
>                         typeID);
91,99d80
<   /// Walk the immediate sub-elements of the given type.
<   void walkImmediateSubElements(Type type,
<                                 function_ref<void(Attribute)> walkAttrsFn,
<                                 function_ref<void(Type)> walkTypesFn) const;
< 
<   /// Replace the immediate sub-elements of the given type.
<   Type replaceImmediateSubElements(Type type, ArrayRef<Attribute> replAttrs,
<                                    ArrayRef<Type> replTypes) const;
< 
105,108c86
<                HasTraitFn &&hasTrait,
<                WalkImmediateSubElementsFn walkImmediateSubElementsFn,
<                ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn,
<                TypeID typeID)
---
>                HasTraitFn &&hasTrait, TypeID typeID)
110,113c88
<         hasTraitFn(std::move(hasTrait)),
<         walkImmediateSubElementsFn(walkImmediateSubElementsFn),
<         replaceImmediateSubElementsFn(replaceImmediateSubElementsFn),
<         typeID(typeID) {}
---
>         hasTraitFn(std::move(hasTrait)), typeID(typeID) {}
133,138d107
< 
<   /// Function to walk the immediate sub-elements of this type.
<   WalkImmediateSubElementsFn walkImmediateSubElementsFn;
< 
<   /// Function to replace the immediate sub-elements of this type.
<   ReplaceImmediateSubElementsFn replaceImmediateSubElementsFn;
--- include/mlir/IR/OperationSupport.h
156c156
<     OperationName::ParseAssemblyFn getParseAssemblyFn() final;
---
>     virtual OperationName::ParseAssemblyFn getParseAssemblyFn() final;
325,327d324
<   /// Return the context this operation is associated with.
<   MLIRContext *getContext() { return getIdentifier().getContext(); }
< 
837,839d833
<   /// Skip printing regions.
<   OpPrintingFlags &skipRegions(bool skip = true);
< 
867,869d860
<   /// Return if regions should be skipped.
<   bool shouldSkipRegions() const;
< 
891,893d881
<   /// Always skip Regions.
<   bool skipRegionsFlag : 1;
< 
938,951c926,930
<   /// Compare two operations (including their regions) and return if they are
<   /// equivalent.
<   ///
<   /// * `checkEquivalent` is a callback to check if two values are equivalent.
<   ///   For two operations to be equivalent, their operands must be the same SSA
<   ///   value or this callback must return `success`.
<   /// * `markEquivalent` is a callback to inform the caller that the analysis
<   ///   determined that two values are equivalent.
<   ///
<   /// Note: Additional information regarding value equivalence can be injected
<   /// into the analysis via `checkEquivalent`. Typically, callers may want
<   /// values that were determined to be equivalent as per `markEquivalent` to be
<   /// reflected in `checkEquivalent`, unless `exactValueMatch` or a different
<   /// equivalence relationship is desired.
---
>   /// Compare two operations and return if they are equivalent.
>   /// `mapOperands` and `mapResults` are optional callbacks that allows the
>   /// caller to check the mapping of SSA value between the lhs and rhs
>   /// operations. It is expected to return success if the mapping is valid and
>   /// failure if it conflicts with a previous mapping.
954,955c933,934
<                  function_ref<LogicalResult(Value, Value)> checkEquivalent,
<                  function_ref<void(Value, Value)> markEquivalent = nullptr,
---
>                  function_ref<LogicalResult(Value, Value)> mapOperands,
>                  function_ref<LogicalResult(Value, Value)> mapResults,
958,974c937,938
<   /// Compare two operations and return if they are equivalent.
<   static bool isEquivalentTo(Operation *lhs, Operation *rhs, Flags flags);
< 
<   /// Compare two regions (including their subregions) and return if they are
<   /// equivalent. See also `isEquivalentTo` for details.
<   static bool isRegionEquivalentTo(
<       Region *lhs, Region *rhs,
<       function_ref<LogicalResult(Value, Value)> checkEquivalent,
<       function_ref<void(Value, Value)> markEquivalent,
<       OperationEquivalence::Flags flags);
< 
<   /// Compare two regions and return if they are equivalent.
<   static bool isRegionEquivalentTo(Region *lhs, Region *rhs,
<                                    OperationEquivalence::Flags flags);
< 
<   /// Helper that can be used with `isEquivalentTo` above to consider ops
<   /// equivalent even if their operands are not equivalent.
---
>   /// Helper that can be used with `isEquivalentTo` above to ignore operation
>   /// operands/result mapping.
978,979c942,943
<   /// Helper that can be used with `isEquivalentTo` above to consider ops
<   /// equivalent only if their operands are the exact same SSA values.
---
>   /// Helper that can be used with `isEquivalentTo` above to ignore operation
>   /// operands/result mapping.
--- include/mlir/Support
Only in include/mlir/Support: DebugAction.h
Only in include/mlir/Support: DebugCounter.h
diff ../../../../llvm-project.0/mlir/include/mlir/Support/InterfaceSupport.h include/mlir/Support/InterfaceSupport.h
94c94
<   explicit Interface(ValueT t = ValueT())
---
>   Interface(ValueT t = ValueT())
diff ../../../../llvm-project.0/mlir/include/mlir/Support/LLVM.h include/mlir/Support/LLVM.h
99d98
< using llvm::cast_if_present;
106d104
< using llvm::isa_and_present;
diff ../../../../llvm-project.0/mlir/include/mlir/Support/ThreadLocalCache.h include/mlir/Support/ThreadLocalCache.h
28,55d27
<   // Keep a separate shared_ptr protected state that can be acquired atomically
<   // instead of using shared_ptr's for each value. This avoids a problem
<   // where the instance shared_ptr is locked() successfully, and then the
<   // ThreadLocalCache gets destroyed before remove() can be called successfully.
<   struct PerInstanceState {
<     /// Remove the given value entry. This is generally called when a thread
<     /// local cache is destructing.
<     void remove(ValueT *value) {
<       // Erase the found value directly, because it is guaranteed to be in the
<       // list.
<       llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
<       auto it =
<           llvm::find_if(instances, [&](std::unique_ptr<ValueT> &instance) {
<             return instance.get() == value;
<           });
<       assert(it != instances.end() && "expected value to exist in cache");
<       instances.erase(it);
<     }
< 
<     /// Owning pointers to all of the values that have been constructed for this
<     /// object in the static cache.
<     SmallVector<std::unique_ptr<ValueT>, 1> instances;
< 
<     /// A mutex used when a new thread instance has been added to the cache for
<     /// this object.
<     llvm::sys::SmartMutex<true> instanceMutex;
<   };
< 
60,61c32,33
<   struct CacheType
<       : public llvm::SmallDenseMap<PerInstanceState *, std::weak_ptr<ValueT>> {
---
>   struct CacheType : public llvm::SmallDenseMap<ThreadLocalCache<ValueT> *,
>                                                 std::weak_ptr<ValueT>> {
91c63
<     std::weak_ptr<ValueT> &threadInstance = staticCache[perInstanceState.get()];
---
>     std::weak_ptr<ValueT> &threadInstance = staticCache[this];
96,100c68,71
<     llvm::sys::SmartScopedLock<true> threadInstanceLock(
<         perInstanceState->instanceMutex);
<     perInstanceState->instances.push_back(std::make_unique<ValueT>());
<     ValueT *instance = perInstanceState->instances.back().get();
<     threadInstance = std::shared_ptr<ValueT>(perInstanceState, instance);
---
>     llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
>     instances.push_back(std::make_shared<ValueT>());
>     std::shared_ptr<ValueT> &instance = instances.back();
>     threadInstance = instance;
122,123c93,112
<   std::shared_ptr<PerInstanceState> perInstanceState =
<       std::make_shared<PerInstanceState>();
---
>   /// Remove the given value entry. This is generally called when a thread local
>   /// cache is destructing.
>   void remove(ValueT *value) {
>     // Erase the found value directly, because it is guaranteed to be in the
>     // list.
>     llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
>     auto it = llvm::find_if(instances, [&](std::shared_ptr<ValueT> &instance) {
>       return instance.get() == value;
>     });
>     assert(it != instances.end() && "expected value to exist in cache");
>     instances.erase(it);
>   }
> 
>   /// Owning pointers to all of the values that have been constructed for this
>   /// object in the static cache.
>   SmallVector<std::shared_ptr<ValueT>, 1> instances;
> 
>   /// A mutex used when a new thread instance has been added to the cache for
>   /// this object.
>   llvm::sys::SmartMutex<true> instanceMutex;
--- include/mlir/Support/FileUtilities.h
--- include/mlir/Support/IndentedOstream.h
--- include/mlir/Support/StorageUniquer.h
--- include/mlir/Support/LLVM.h
99d98
< using llvm::cast_if_present;
106d104
< using llvm::isa_and_present;
--- include/mlir/Support/LogicalResult.h
--- include/mlir/Support/Timing.h
--- include/mlir/Support/TypeID.h
--- include/mlir/Support/MathExtras.h
--- include/mlir/Support/ToolUtilities.h
--- include/mlir/Support/ThreadLocalCache.h
28,55d27
<   // Keep a separate shared_ptr protected state that can be acquired atomically
<   // instead of using shared_ptr's for each value. This avoids a problem
<   // where the instance shared_ptr is locked() successfully, and then the
<   // ThreadLocalCache gets destroyed before remove() can be called successfully.
<   struct PerInstanceState {
<     /// Remove the given value entry. This is generally called when a thread
<     /// local cache is destructing.
<     void remove(ValueT *value) {
<       // Erase the found value directly, because it is guaranteed to be in the
<       // list.
<       llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
<       auto it =
<           llvm::find_if(instances, [&](std::unique_ptr<ValueT> &instance) {
<             return instance.get() == value;
<           });
<       assert(it != instances.end() && "expected value to exist in cache");
<       instances.erase(it);
<     }
< 
<     /// Owning pointers to all of the values that have been constructed for this
<     /// object in the static cache.
<     SmallVector<std::unique_ptr<ValueT>, 1> instances;
< 
<     /// A mutex used when a new thread instance has been added to the cache for
<     /// this object.
<     llvm::sys::SmartMutex<true> instanceMutex;
<   };
< 
60,61c32,33
<   struct CacheType
<       : public llvm::SmallDenseMap<PerInstanceState *, std::weak_ptr<ValueT>> {
---
>   struct CacheType : public llvm::SmallDenseMap<ThreadLocalCache<ValueT> *,
>                                                 std::weak_ptr<ValueT>> {
91c63
<     std::weak_ptr<ValueT> &threadInstance = staticCache[perInstanceState.get()];
---
>     std::weak_ptr<ValueT> &threadInstance = staticCache[this];
96,100c68,71
<     llvm::sys::SmartScopedLock<true> threadInstanceLock(
<         perInstanceState->instanceMutex);
<     perInstanceState->instances.push_back(std::make_unique<ValueT>());
<     ValueT *instance = perInstanceState->instances.back().get();
<     threadInstance = std::shared_ptr<ValueT>(perInstanceState, instance);
---
>     llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
>     instances.push_back(std::make_shared<ValueT>());
>     std::shared_ptr<ValueT> &instance = instances.back();
>     threadInstance = instance;
122,123c93,112
<   std::shared_ptr<PerInstanceState> perInstanceState =
<       std::make_shared<PerInstanceState>();
---
>   /// Remove the given value entry. This is generally called when a thread local
>   /// cache is destructing.
>   void remove(ValueT *value) {
>     // Erase the found value directly, because it is guaranteed to be in the
>     // list.
>     llvm::sys::SmartScopedLock<true> threadInstanceLock(instanceMutex);
>     auto it = llvm::find_if(instances, [&](std::shared_ptr<ValueT> &instance) {
>       return instance.get() == value;
>     });
>     assert(it != instances.end() && "expected value to exist in cache");
>     instances.erase(it);
>   }
> 
>   /// Owning pointers to all of the values that have been constructed for this
>   /// object in the static cache.
>   SmallVector<std::shared_ptr<ValueT>, 1> instances;
> 
>   /// A mutex used when a new thread instance has been added to the cache for
>   /// this object.
>   llvm::sys::SmartMutex<true> instanceMutex;
--- include/mlir/Support/DebugCounter.h
--- include/mlir/Support/DebugStringHelper.h
--- include/mlir/Support/InterfaceSupport.h
94c94
<   explicit Interface(ValueT t = ValueT())
---
>   Interface(ValueT t = ValueT())
--- include/mlir/Support/DebugAction.h
--- include/mlir/Parser
diff ../../../../llvm-project.0/mlir/include/mlir/Parser/Parser.h include/mlir/Parser/Parser.h
141,145c141,143
< /// context, and failure is returned.
< /// `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name. If `sourceFileLoc` is non-null, it is populated with a file location
< /// representing the start of the source file that is being parsed.
---
> /// context, and failure is returned. If `sourceFileLoc` is non-null, it is
> /// populated with a file location representing the start of the source file
> /// that is being parsed.
148d145
<                                 StringRef sourceName = "",
241,243d237
< /// `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name.
246,247c240
<                                                    const ParserConfig &config,
<                                                    StringRef sourceName = "") {
---
>                                                    const ParserConfig &config) {
250,251c243
<   if (failed(parseSourceString(sourceStr, &block, config, sourceName,
<                                &sourceFileLoc)))
---
>   if (failed(parseSourceString(sourceStr, &block, config, &sourceFileLoc)))
--- include/mlir/Parser/Parser.h
141,145c141,143
< /// context, and failure is returned.
< /// `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name. If `sourceFileLoc` is non-null, it is populated with a file location
< /// representing the start of the source file that is being parsed.
---
> /// context, and failure is returned. If `sourceFileLoc` is non-null, it is
> /// populated with a file location representing the start of the source file
> /// that is being parsed.
148d145
<                                 StringRef sourceName = "",
241,243d237
< /// `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name.
246,247c240
<                                                    const ParserConfig &config,
<                                                    StringRef sourceName = "") {
---
>                                                    const ParserConfig &config) {
250,251c243
<   if (failed(parseSourceString(sourceStr, &block, config, sourceName,
<                                &sourceFileLoc)))
---
>   if (failed(parseSourceString(sourceStr, &block, config, &sourceFileLoc)))
--- include/mlir/Analysis
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Analysis/AliasAnalysis and include/mlir/Analysis/AliasAnalysis
Only in ../../../../llvm-project.0/mlir/include/mlir/Analysis: CFGLoopInfo.h
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Analysis/DataFlow and include/mlir/Analysis/DataFlow
Only in ../../../../llvm-project.0/mlir/include/mlir/Analysis: FlatLinearValueConstraints.h
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Analysis/Presburger and include/mlir/Analysis/Presburger
diff ../../../../llvm-project.0/mlir/include/mlir/Analysis/SliceAnalysis.h include/mlir/Analysis/SliceAnalysis.h
72,73c72
<                      TransitiveFilter filter = nullptr /* pass-through*/,
<                      bool inclusive = false);
---
>                      TransitiveFilter filter = nullptr /* pass-through*/);
78,79c77
<                      TransitiveFilter filter = nullptr /* pass-through*/,
<                      bool inclusive = false);
---
>                      TransitiveFilter filter = nullptr /* pass-through*/);
116,117c114
<                       TransitiveFilter filter = nullptr /* pass-through*/,
<                       bool inclusive = false);
---
>                       TransitiveFilter filter = nullptr /* pass-through*/);
122,123c119
<                       TransitiveFilter filter = nullptr /* pass-through*/,
<                       bool inclusive = false);
---
>                       TransitiveFilter filter = nullptr /* pass-through*/);
205,206c201
<          TransitiveFilter forwardFilter = nullptr /* pass-through*/,
<          bool inclusive = false);
---
>          TransitiveFilter forwardFilter = nullptr /* pass-through*/);
--- include/mlir/Analysis/CallGraph.h
--- include/mlir/Analysis/Presburger
diff ../../../../llvm-project.0/mlir/include/mlir/Analysis/Presburger/IntegerRelation.h include/mlir/Analysis/Presburger/IntegerRelation.h
34,36d33
< /// The type of bound: equal, lower bound or upper bound.
< enum class BoundType { EQ, LB, UB };
< 
59a57,58
>     FlatAffineConstraints,
>     FlatAffineValueConstraints,
62,65d60
<     FlatLinearConstraints,
<     FlatLinearValueConstraints,
<     FlatAffineValueConstraints,
<     FlatAffineRelation
211c206
<   }
---
>   };
216c211
<   }
---
>   };
221c216
<   }
---
>   };
228c223
<   }
---
>   };
231c226
<   VarKind getVarKindAt(unsigned pos) const { return space.getVarKindAt(pos); }
---
>   VarKind getVarKindAt(unsigned pos) const { return space.getVarKindAt(pos); };
402a398,400
>   /// The type of bound: equal, lower bound or upper bound.
>   enum BoundType { EQ, LB, UB };
> 
498c496
<       *boundFloorDivisor = static_cast<int64_t>(boundFloorDivisorMPInt);
---
>       *boundFloorDivisor = int64_t(boundFloorDivisorMPInt);
853,854c851
<     return cst->getKind() >= Kind::IntegerPolyhedron &&
<            cst->getKind() <= Kind::FlatAffineRelation;
---
>     return cst->getKind() == Kind::IntegerPolyhedron;
diff ../../../../llvm-project.0/mlir/include/mlir/Analysis/Presburger/PresburgerSpace.h include/mlir/Analysis/Presburger/PresburgerSpace.h
93,94c93,94
<   /// Get the domain/range space of this space. The returned space is a set
<   /// space.
---
>   // Get the domain/range space of this space. The returned space is a set
>   // space.
97,99d96
< 
<   /// Get the space without local variables.
<   PresburgerSpace getSpaceWithoutLocals() const;
diff ../../../../llvm-project.0/mlir/include/mlir/Analysis/Presburger/PWMAFunction.h include/mlir/Analysis/Presburger/PWMAFunction.h
26,29d25
< /// Enum representing a binary comparison operator: equal, not equal, less than,
< /// less than or equal, greater than, greater than or equal.
< enum class OrderingKind { EQ, NE, LT, LE, GT, GE };
< 
72,75c68
<   /// Get the divisions used in this function.
<   const DivisionRepr &getDivs() const { return divs; }
< 
<   /// Remove the specified range of outputs.
---
>   // Remove the specified range of outputs.
99,106d91
<   /// Return the set of domain points where the output of `this` and `other`
<   /// are ordered lexicographically according to the given ordering.
<   /// For example, if the given comparison is `LT`, then the returned set
<   /// contains all points where the first output of `this` is lexicographically
<   /// less than `other`.
<   PresburgerSet getLexSet(OrderingKind comp,
<                           const MultiAffineFunction &other) const;
< 
198,200d182
< 
<   /// Return all the pieces of this piece-wise function.
<   ArrayRef<Piece> getAllPieces() const { return pieces; }
diff ../../../../llvm-project.0/mlir/include/mlir/Analysis/Presburger/Utils.h include/mlir/Analysis/Presburger/Utils.h
159,163d158
<   // Find the greatest common divisor (GCD) of the dividends and divisor for
<   // each valid division. Divide the dividends and divisor by the GCD to
<   // simplify the expression.
<   void normalizeDivs();
< 
--- include/mlir/Analysis/Presburger/PWMAFunction.h
26,29d25
< /// Enum representing a binary comparison operator: equal, not equal, less than,
< /// less than or equal, greater than, greater than or equal.
< enum class OrderingKind { EQ, NE, LT, LE, GT, GE };
< 
72,75c68
<   /// Get the divisions used in this function.
<   const DivisionRepr &getDivs() const { return divs; }
< 
<   /// Remove the specified range of outputs.
---
>   // Remove the specified range of outputs.
99,106d91
<   /// Return the set of domain points where the output of `this` and `other`
<   /// are ordered lexicographically according to the given ordering.
<   /// For example, if the given comparison is `LT`, then the returned set
<   /// contains all points where the first output of `this` is lexicographically
<   /// less than `other`.
<   PresburgerSet getLexSet(OrderingKind comp,
<                           const MultiAffineFunction &other) const;
< 
198,200d182
< 
<   /// Return all the pieces of this piece-wise function.
<   ArrayRef<Piece> getAllPieces() const { return pieces; }
--- include/mlir/Analysis/Presburger/PresburgerSpace.h
93,94c93,94
<   /// Get the domain/range space of this space. The returned space is a set
<   /// space.
---
>   // Get the domain/range space of this space. The returned space is a set
>   // space.
97,99d96
< 
<   /// Get the space without local variables.
<   PresburgerSpace getSpaceWithoutLocals() const;
--- include/mlir/Analysis/Presburger/PresburgerRelation.h
--- include/mlir/Analysis/Presburger/Simplex.h
--- include/mlir/Analysis/Presburger/IntegerRelation.h
34,36d33
< /// The type of bound: equal, lower bound or upper bound.
< enum class BoundType { EQ, LB, UB };
< 
59a57,58
>     FlatAffineConstraints,
>     FlatAffineValueConstraints,
62,65d60
<     FlatLinearConstraints,
<     FlatLinearValueConstraints,
<     FlatAffineValueConstraints,
<     FlatAffineRelation
211c206
<   }
---
>   };
216c211
<   }
---
>   };
221c216
<   }
---
>   };
228c223
<   }
---
>   };
231c226
<   VarKind getVarKindAt(unsigned pos) const { return space.getVarKindAt(pos); }
---
>   VarKind getVarKindAt(unsigned pos) const { return space.getVarKindAt(pos); };
402a398,400
>   /// The type of bound: equal, lower bound or upper bound.
>   enum BoundType { EQ, LB, UB };
> 
498c496
<       *boundFloorDivisor = static_cast<int64_t>(boundFloorDivisorMPInt);
---
>       *boundFloorDivisor = int64_t(boundFloorDivisorMPInt);
853,854c851
<     return cst->getKind() >= Kind::IntegerPolyhedron &&
<            cst->getKind() <= Kind::FlatAffineRelation;
---
>     return cst->getKind() == Kind::IntegerPolyhedron;
--- include/mlir/Analysis/Presburger/LinearTransform.h
--- include/mlir/Analysis/Presburger/Utils.h
159,163d158
<   // Find the greatest common divisor (GCD) of the dividends and divisor for
<   // each valid division. Divide the dividends and divisor by the GCD to
<   // simplify the expression.
<   void normalizeDivs();
< 
--- include/mlir/Analysis/Presburger/Matrix.h
--- include/mlir/Analysis/Presburger/MPInt.h
--- include/mlir/Analysis/Presburger/SlowMPInt.h
--- include/mlir/Analysis/Presburger/Fraction.h
--- include/mlir/Analysis/SymbolTableAnalysis.h
--- include/mlir/Analysis/SliceAnalysis.h
72,73c72
<                      TransitiveFilter filter = nullptr /* pass-through*/,
<                      bool inclusive = false);
---
>                      TransitiveFilter filter = nullptr /* pass-through*/);
78,79c77
<                      TransitiveFilter filter = nullptr /* pass-through*/,
<                      bool inclusive = false);
---
>                      TransitiveFilter filter = nullptr /* pass-through*/);
116,117c114
<                       TransitiveFilter filter = nullptr /* pass-through*/,
<                       bool inclusive = false);
---
>                       TransitiveFilter filter = nullptr /* pass-through*/);
122,123c119
<                       TransitiveFilter filter = nullptr /* pass-through*/,
<                       bool inclusive = false);
---
>                       TransitiveFilter filter = nullptr /* pass-through*/);
205,206c201
<          TransitiveFilter forwardFilter = nullptr /* pass-through*/,
<          bool inclusive = false);
---
>          TransitiveFilter forwardFilter = nullptr /* pass-through*/);
--- include/mlir/Analysis/DataLayoutAnalysis.h
--- include/mlir/Analysis/AliasAnalysis.h
--- include/mlir/Analysis/DataFlowFramework.h
--- include/mlir/Analysis/DataFlow
--- include/mlir/Analysis/DataFlow/DeadCodeAnalysis.h
--- include/mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h
--- include/mlir/Analysis/DataFlow/IntegerRangeAnalysis.h
--- include/mlir/Analysis/DataFlow/SparseAnalysis.h
--- include/mlir/Analysis/DataFlow/DenseAnalysis.h
--- include/mlir/Analysis/Liveness.h
--- include/mlir/Analysis/AliasAnalysis
--- include/mlir/Analysis/AliasAnalysis/LocalAliasAnalysis.h
--- include/mlir/Conversion
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/AffineToStandard and include/mlir/Conversion/AffineToStandard
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/AMDGPUToROCDL and include/mlir/Conversion/AMDGPUToROCDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ArithCommon and include/mlir/Conversion/ArithCommon
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ArithToLLVM and include/mlir/Conversion/ArithToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ArithToSPIRV and include/mlir/Conversion/ArithToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ArmNeon2dToIntr and include/mlir/Conversion/ArmNeon2dToIntr
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/AsyncToLLVM and include/mlir/Conversion/AsyncToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/BufferizationToMemRef and include/mlir/Conversion/BufferizationToMemRef
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ComplexToLibm and include/mlir/Conversion/ComplexToLibm
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ComplexToLLVM and include/mlir/Conversion/ComplexToLLVM
Only in ../../../../llvm-project.0/mlir/include/mlir/Conversion: ComplexToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ComplexToStandard and include/mlir/Conversion/ComplexToStandard
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ControlFlowToLLVM and include/mlir/Conversion/ControlFlowToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ControlFlowToSPIRV and include/mlir/Conversion/ControlFlowToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/FuncToLLVM and include/mlir/Conversion/FuncToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/FuncToSPIRV and include/mlir/Conversion/FuncToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUCommon and include/mlir/Conversion/GPUCommon
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToNVVM and include/mlir/Conversion/GPUToNVVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToROCDL and include/mlir/Conversion/GPUToROCDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToSPIRV and include/mlir/Conversion/GPUToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToVulkan and include/mlir/Conversion/GPUToVulkan
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/IndexToLLVM and include/mlir/Conversion/IndexToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/LinalgToLLVM and include/mlir/Conversion/LinalgToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/LinalgToStandard and include/mlir/Conversion/LinalgToStandard
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/LLVMCommon and include/mlir/Conversion/LLVMCommon
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToFuncs and include/mlir/Conversion/MathToFuncs
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToLibm and include/mlir/Conversion/MathToLibm
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToLLVM and include/mlir/Conversion/MathToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToSPIRV and include/mlir/Conversion/MathToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MemRefToLLVM and include/mlir/Conversion/MemRefToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/MemRefToSPIRV and include/mlir/Conversion/MemRefToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/NVGPUToNVVM and include/mlir/Conversion/NVGPUToNVVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/OpenACCToLLVM and include/mlir/Conversion/OpenACCToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/OpenACCToSCF and include/mlir/Conversion/OpenACCToSCF
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/OpenMPToLLVM and include/mlir/Conversion/OpenMPToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/Passes.h include/mlir/Conversion/Passes.h
21d20
< #include "mlir/Conversion/ComplexToSPIRV/ComplexToSPIRVPass.h"
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/Passes.td include/mlir/Conversion/Passes.td
146c146
< def ConvertAsyncToLLVMPass : Pass<"convert-async-to-llvm", "ModuleOp"> {
---
> def ConvertAsyncToLLVM : Pass<"convert-async-to-llvm", "ModuleOp"> {
152a153
>   let constructor = "mlir::createConvertAsyncToLLVMPass()";
159,163d159
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<            /*default=*/"true", "Generate LLVM IR using opaque pointers "
<            "instead of typed pointers">,
<   ];
205c201
< def ConvertComplexToLLVMPass : Pass<"convert-complex-to-llvm"> {
---
> def ConvertComplexToLLVM : Pass<"convert-complex-to-llvm"> {
206a203
>   let constructor = "mlir::createConvertComplexToLLVMPass()";
226,234d222
< // ComplexToSPIRV
< //===----------------------------------------------------------------------===//
< 
< def ConvertComplexToSPIRVPass : Pass<"convert-complex-to-spirv"> {
<   let summary = "Convert Complex dialect to SPIRV dialect";
<   let dependentDialects = ["spirv::SPIRVDialect"];
< }
< 
< //===----------------------------------------------------------------------===//
248c236
< def ConvertControlFlowToLLVMPass : Pass<"convert-cf-to-llvm", "ModuleOp"> {
---
> def ConvertControlFlowToLLVM : Pass<"convert-cf-to-llvm", "ModuleOp"> {
256a245
>   let constructor = "mlir::cf::createConvertControlFlowToLLVMPass()";
262,264d250
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                    /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                    "instead of typed pointers">,
288c274
< def ConvertFuncToLLVMPass : Pass<"convert-func-to-llvm", "ModuleOp"> {
---
> def ConvertFuncToLLVM : Pass<"convert-func-to-llvm", "ModuleOp"> {
311a298
>   let constructor = "mlir::createConvertFuncToLLVMPass()";
324,327c311
<            "expected on the produced module">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                        /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                        "instead of typed pointers">,
---
>            "expected on the produced module">
353,381c337,338
< 
<   let description = [{
<     Creates a pass to convert a GPU operations into a sequence of GPU runtime
<     calls.
< 
<     This pass does not generate code to call GPU runtime APIs directly but
<     instead uses a small wrapper library that exports a stable and conveniently
<     typed ABI on top of GPU runtimes such as CUDA or ROCm (HIP).
<   }];
< 
<   let options = [
<     Option<"kernelBarePtrCallConv", "use-bare-pointers-for-kernels", "bool",
<            /*default=*/"false",
<              "Use bare pointers to pass memref arguments to kernels. "
<              "The kernel must use the same setting for this option."
<            >,
<     Option<"gpuBinaryAnnotation", "gpu-binary-annotation", "std::string",
<                /*default=*/"gpu::getDefaultGpuBinaryAnnotation()",
<                "Annotation attribute string for GPU binary"
<                >,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">,
<   ];
< 
<   let dependentDialects = [
<     "LLVM::LLVMDialect",
<     "memref::MemRefDialect",
<   ];
---
>   let constructor = "mlir::createGpuToLLVMConversionPass()";
>   let dependentDialects = ["LLVM::LLVMDialect"];
384c341
< def LowerHostCodeToLLVMPass : Pass<"lower-host-to-llvm", "ModuleOp"> {
---
> def LowerHostCodeToLLVM : Pass<"lower-host-to-llvm", "ModuleOp"> {
386,403c343
< 
<   let description = [{
<     Creates a pass to emulate `gpu.launch_func` call in LLVM dialect and lower
<     the host module code to LLVM.
< 
<     This transformation creates a sequence of global variables that are later
<     linked to the variables in the kernel module, and a series of copies to/from
<     them to emulate the memory transfer from the host or to the device sides. It
<     also converts the remaining Arithmetic, Func, and MemRef dialects into LLVM
<     dialect, emitting C wrappers.
<   }];
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createLowerHostCodeToLLVMPass()";
422,427c362
<            "Bitwidth of the index type, 0 to use size of machine word">,
<     Option<"hasRedux", "has-redux", "bool", /*default=*/"false",
<            "Target gpu supports redux">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                    /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                    "instead of typed pointers">,
---
>            "Bitwidth of the index type, 0 to use size of machine word">
438,442c373
<   let dependentDialects = [
<     "ROCDL::ROCDLDialect",
<     "cf::ControlFlowDialect",
<     "memref::MemRefDialect",
<   ];
---
>   let dependentDialects = ["ROCDL::ROCDLDialect"];
461,464c392
<           )}]>,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">,
---
>           )}]>
488,492d415
<   let options = [
<     Option<"use64bitIndex", "use-64bit-index",
<            "bool", /*default=*/"false",
<            "Use 64-bit integers to convert index types">
<   ];
509c432
< def ConvertVulkanLaunchFuncToVulkanCallsPass
---
> def ConvertVulkanLaunchFuncToVulkanCalls
516,522c439
< 
<   let options = [
<      Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<             /*default=*/"true", "Generate LLVM IR using opaque pointers "
<             "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createConvertVulkanLaunchFuncToVulkanCallsPass()";
553c470
< def ConvertLinalgToLLVMPass : Pass<"convert-linalg-to-llvm", "ModuleOp"> {
---
> def ConvertLinalgToLLVM : Pass<"convert-linalg-to-llvm", "ModuleOp"> {
555a473
>   let constructor = "mlir::createConvertLinalgToLLVMPass()";
557,561d474
<   let options = [
<      Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                 /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                 "instead of typed pointers">
<   ];
596c509
< def ConvertMathToLLVMPass : Pass<"convert-math-to-llvm"> {
---
> def ConvertMathToLLVM : Pass<"convert-math-to-llvm"> {
597a511,514
>   let description = [{
>     This pass converts supported Math ops to LLVM dialect intrinsics.
>   }];
>   let constructor = "mlir::createConvertMathToLLVMPass()";
599,602d515
<   let options = [
<     Option<"approximateLog1p", "approximate-log1p", "bool", "true",
<            "Enable approximation of Log1p.">
<   ];
630d542
<     "scf::SCFDialect",
638,643c550
<            "is greater than or equal to this value">,
<     // Most backend targets support a native ctlz operation, so by default
<     // ctrlz conversion is disabled.
<     Option<"convertCtlz", "convert-ctlz", "bool", /*default=*/"false",
<            "Convert math.ctlz to a software implementation. Enable "
<            "for targets that do not natively support ctlz.">,
---
>            "is greater than or equal to this value">
651,660c558,560
< def FinalizeMemRefToLLVMConversionPass :
<     Pass<"finalize-memref-to-llvm", "ModuleOp"> {
<   let summary = "Finalize MemRef dialect to LLVM dialect conversion";
<   let description = [{
<     Finalize the conversion of the operations from the MemRef
<     dialect to the LLVM dialect.
<     This conversion will not convert some complex MemRef
<     operations. Make sure to run `expand-strided-metadata`
<     beforehand for these.
<   }];
---
> def MemRefToLLVMConversionPass : Pass<"convert-memref-to-llvm", "ModuleOp"> {
>   let summary = "Convert operations from the MemRef dialect to the LLVM "
>                 "dialect";
672,675c572
<            "classic 'malloc', 'aligned_alloc' and 'free' functions">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">
---
>            "classic 'malloc', 'aligned_alloc' and 'free' functions">
708c605
< def ConvertNVGPUToNVVMPass : Pass<"convert-nvgpu-to-nvvm"> {
---
> def ConvertNVGPUToNVVM : Pass<"convert-nvgpu-to-nvvm"> {
713c610
< 
---
>   let constructor = "mlir::createConvertNVGPUToNVVMPass()";
717,721d613
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<               /*default=*/"true", "Generate LLVM IR using opaque pointers "
<               "instead of typed pointers">
<   ];
739c631
< def ConvertOpenACCToLLVMPass : Pass<"convert-openacc-to-llvm", "ModuleOp"> {
---
> def ConvertOpenACCToLLVM : Pass<"convert-openacc-to-llvm", "ModuleOp"> {
740a633
>   let constructor = "mlir::createConvertOpenACCToLLVMPass()";
742,746d634
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<            /*default=*/"true", "Generate LLVM IR using opaque pointers "
<            "instead of typed pointers">,
<   ];
753c641
< def ConvertOpenMPToLLVMPass : Pass<"convert-openmp-to-llvm", "ModuleOp"> {
---
> def ConvertOpenMPToLLVM : Pass<"convert-openmp-to-llvm", "ModuleOp"> {
754a643
>   let constructor = "mlir::createConvertOpenMPToLLVMPass()";
809c698
< def ConvertSCFToOpenMPPass : Pass<"convert-scf-to-openmp", "ModuleOp"> {
---
> def ConvertSCFToOpenMP : Pass<"convert-scf-to-openmp", "ModuleOp"> {
812,818c701
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createConvertSCFToOpenMPPass()";
830,831c713,714
<     Converts SCF ops into SPIR-V structured control flow ops.
<     SPIR-V structured control flow ops do not support yielding values.
---
>     This pass converts SCF ops into SPIR-V structured control flow ops.
>     SPIR-V structured control flow ops does not support yielding values.
835a719
>   let constructor = "mlir::createConvertSCFToSPIRVPass()";
859c743
<   let dependentDialects = ["affine::AffineDialect", "gpu::GPUDialect"];
---
>   let dependentDialects = ["AffineDialect", "gpu::GPUDialect"];
894c778
< def ConvertSPIRVToLLVMPass : Pass<"convert-spirv-to-llvm", "ModuleOp"> {
---
> def ConvertSPIRVToLLVM : Pass<"convert-spirv-to-llvm", "ModuleOp"> {
899a784
>   let constructor = "mlir::createConvertSPIRVToLLVMPass()";
901,906d785
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
1036c915
<     "memref::MemRefDialect", "gpu::GPUDialect", "affine::AffineDialect",
---
>     "memref::MemRefDialect", "gpu::GPUDialect", "AffineDialect",
1055c934
<     "affine::AffineDialect",
---
>     "AffineDialect",
1057,1058c936
<     "scf::SCFDialect",
<     "tensor::TensorDialect"
---
>     "scf::SCFDialect"
1074c952
< def ConvertVectorToLLVMPass : Pass<"convert-vector-to-llvm", "ModuleOp"> {
---
> def ConvertVectorToLLVM : Pass<"convert-vector-to-llvm", "ModuleOp"> {
1086a965
>   let constructor = "mlir::createConvertVectorToLLVMPass()";
1112,1115c991
< 	   "dialect.">,
< 	  Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">
---
> 	   "dialect.">
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/PDLToPDLInterp and include/mlir/Conversion/PDLToPDLInterp
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ReconcileUnrealizedCasts and include/mlir/Conversion/ReconcileUnrealizedCasts
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToControlFlow and include/mlir/Conversion/SCFToControlFlow
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToGPU and include/mlir/Conversion/SCFToGPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToOpenMP and include/mlir/Conversion/SCFToOpenMP
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToSPIRV and include/mlir/Conversion/SCFToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/ShapeToStandard and include/mlir/Conversion/ShapeToStandard
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/SPIRVToLLVM and include/mlir/Conversion/SPIRVToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TensorToLinalg and include/mlir/Conversion/TensorToLinalg
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TensorToSPIRV and include/mlir/Conversion/TensorToSPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TosaToArith and include/mlir/Conversion/TosaToArith
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TosaToLinalg and include/mlir/Conversion/TosaToLinalg
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TosaToSCF and include/mlir/Conversion/TosaToSCF
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/TosaToTensor and include/mlir/Conversion/TosaToTensor
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToGPU and include/mlir/Conversion/VectorToGPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToLLVM and include/mlir/Conversion/VectorToLLVM
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToSCF and include/mlir/Conversion/VectorToSCF
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToSPIRV and include/mlir/Conversion/VectorToSPIRV
--- include/mlir/Conversion/VectorToSCF
--- include/mlir/Conversion/VectorToSCF/VectorToSCF.h
--- include/mlir/Conversion/ReconcileUnrealizedCasts
--- include/mlir/Conversion/ReconcileUnrealizedCasts/ReconcileUnrealizedCasts.h
--- include/mlir/Conversion/SCFToSPIRV
--- include/mlir/Conversion/SCFToSPIRV/SCFToSPIRVPass.h
--- include/mlir/Conversion/SCFToSPIRV/SCFToSPIRV.h
--- include/mlir/Conversion/Passes.h
21d20
< #include "mlir/Conversion/ComplexToSPIRV/ComplexToSPIRVPass.h"
--- include/mlir/Conversion/GPUCommon
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUCommon/GPUCommonPass.h include/mlir/Conversion/GPUCommon/GPUCommonPass.h
11d10
< #include "mlir/Dialect/GPU/Transforms/Utils.h"
31c30,31
< class Pass;
---
> template <typename T>
> class OperationPass;
48a49,57
> 
> /// Creates a pass to convert a GPU operations into a sequence of GPU runtime
> /// calls.
> ///
> /// This pass does not generate code to call GPU runtime APIs directly but
> /// instead uses a small wrapper library that exports a stable and conveniently
> /// typed ABI on top of GPU runtimes such as CUDA or ROCm (HIP).
> std::unique_ptr<OperationPass<ModuleOp>>
> createGpuToLLVMConversionPass(bool kernelBarePtrCallConv = false);
--- include/mlir/Conversion/GPUCommon/GPUCommonPass.h
11d10
< #include "mlir/Dialect/GPU/Transforms/Utils.h"
31c30,31
< class Pass;
---
> template <typename T>
> class OperationPass;
48a49,57
> 
> /// Creates a pass to convert a GPU operations into a sequence of GPU runtime
> /// calls.
> ///
> /// This pass does not generate code to call GPU runtime APIs directly but
> /// instead uses a small wrapper library that exports a stable and conveniently
> /// typed ABI on top of GPU runtimes such as CUDA or ROCm (HIP).
> std::unique_ptr<OperationPass<ModuleOp>>
> createGpuToLLVMConversionPass(bool kernelBarePtrCallConv = false);
--- include/mlir/Conversion/ArithToSPIRV
--- include/mlir/Conversion/ArithToSPIRV/ArithToSPIRV.h
--- include/mlir/Conversion/LinalgToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/LinalgToLLVM/LinalgToLLVM.h include/mlir/Conversion/LinalgToLLVM/LinalgToLLVM.h
16c16,18
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
19c21
< #define GEN_PASS_DECL_CONVERTLINALGTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTLINALGTOLLVM
24a27,29
> 
> /// Create a pass to convert Linalg operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertLinalgToLLVMPass();
--- include/mlir/Conversion/LinalgToLLVM/LinalgToLLVM.h
16c16,18
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
19c21
< #define GEN_PASS_DECL_CONVERTLINALGTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTLINALGTOLLVM
24a27,29
> 
> /// Create a pass to convert Linalg operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertLinalgToLLVMPass();
--- include/mlir/Conversion/Passes.td
146c146
< def ConvertAsyncToLLVMPass : Pass<"convert-async-to-llvm", "ModuleOp"> {
---
> def ConvertAsyncToLLVM : Pass<"convert-async-to-llvm", "ModuleOp"> {
152a153
>   let constructor = "mlir::createConvertAsyncToLLVMPass()";
159,163d159
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<            /*default=*/"true", "Generate LLVM IR using opaque pointers "
<            "instead of typed pointers">,
<   ];
205c201
< def ConvertComplexToLLVMPass : Pass<"convert-complex-to-llvm"> {
---
> def ConvertComplexToLLVM : Pass<"convert-complex-to-llvm"> {
206a203
>   let constructor = "mlir::createConvertComplexToLLVMPass()";
226,234d222
< // ComplexToSPIRV
< //===----------------------------------------------------------------------===//
< 
< def ConvertComplexToSPIRVPass : Pass<"convert-complex-to-spirv"> {
<   let summary = "Convert Complex dialect to SPIRV dialect";
<   let dependentDialects = ["spirv::SPIRVDialect"];
< }
< 
< //===----------------------------------------------------------------------===//
248c236
< def ConvertControlFlowToLLVMPass : Pass<"convert-cf-to-llvm", "ModuleOp"> {
---
> def ConvertControlFlowToLLVM : Pass<"convert-cf-to-llvm", "ModuleOp"> {
256a245
>   let constructor = "mlir::cf::createConvertControlFlowToLLVMPass()";
262,264d250
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                    /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                    "instead of typed pointers">,
288c274
< def ConvertFuncToLLVMPass : Pass<"convert-func-to-llvm", "ModuleOp"> {
---
> def ConvertFuncToLLVM : Pass<"convert-func-to-llvm", "ModuleOp"> {
311a298
>   let constructor = "mlir::createConvertFuncToLLVMPass()";
324,327c311
<            "expected on the produced module">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                        /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                        "instead of typed pointers">,
---
>            "expected on the produced module">
353,381c337,338
< 
<   let description = [{
<     Creates a pass to convert a GPU operations into a sequence of GPU runtime
<     calls.
< 
<     This pass does not generate code to call GPU runtime APIs directly but
<     instead uses a small wrapper library that exports a stable and conveniently
<     typed ABI on top of GPU runtimes such as CUDA or ROCm (HIP).
<   }];
< 
<   let options = [
<     Option<"kernelBarePtrCallConv", "use-bare-pointers-for-kernels", "bool",
<            /*default=*/"false",
<              "Use bare pointers to pass memref arguments to kernels. "
<              "The kernel must use the same setting for this option."
<            >,
<     Option<"gpuBinaryAnnotation", "gpu-binary-annotation", "std::string",
<                /*default=*/"gpu::getDefaultGpuBinaryAnnotation()",
<                "Annotation attribute string for GPU binary"
<                >,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">,
<   ];
< 
<   let dependentDialects = [
<     "LLVM::LLVMDialect",
<     "memref::MemRefDialect",
<   ];
---
>   let constructor = "mlir::createGpuToLLVMConversionPass()";
>   let dependentDialects = ["LLVM::LLVMDialect"];
384c341
< def LowerHostCodeToLLVMPass : Pass<"lower-host-to-llvm", "ModuleOp"> {
---
> def LowerHostCodeToLLVM : Pass<"lower-host-to-llvm", "ModuleOp"> {
386,403c343
< 
<   let description = [{
<     Creates a pass to emulate `gpu.launch_func` call in LLVM dialect and lower
<     the host module code to LLVM.
< 
<     This transformation creates a sequence of global variables that are later
<     linked to the variables in the kernel module, and a series of copies to/from
<     them to emulate the memory transfer from the host or to the device sides. It
<     also converts the remaining Arithmetic, Func, and MemRef dialects into LLVM
<     dialect, emitting C wrappers.
<   }];
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createLowerHostCodeToLLVMPass()";
422,427c362
<            "Bitwidth of the index type, 0 to use size of machine word">,
<     Option<"hasRedux", "has-redux", "bool", /*default=*/"false",
<            "Target gpu supports redux">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                    /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                    "instead of typed pointers">,
---
>            "Bitwidth of the index type, 0 to use size of machine word">
438,442c373
<   let dependentDialects = [
<     "ROCDL::ROCDLDialect",
<     "cf::ControlFlowDialect",
<     "memref::MemRefDialect",
<   ];
---
>   let dependentDialects = ["ROCDL::ROCDLDialect"];
461,464c392
<           )}]>,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">,
---
>           )}]>
488,492d415
<   let options = [
<     Option<"use64bitIndex", "use-64bit-index",
<            "bool", /*default=*/"false",
<            "Use 64-bit integers to convert index types">
<   ];
509c432
< def ConvertVulkanLaunchFuncToVulkanCallsPass
---
> def ConvertVulkanLaunchFuncToVulkanCalls
516,522c439
< 
<   let options = [
<      Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<             /*default=*/"true", "Generate LLVM IR using opaque pointers "
<             "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createConvertVulkanLaunchFuncToVulkanCallsPass()";
553c470
< def ConvertLinalgToLLVMPass : Pass<"convert-linalg-to-llvm", "ModuleOp"> {
---
> def ConvertLinalgToLLVM : Pass<"convert-linalg-to-llvm", "ModuleOp"> {
555a473
>   let constructor = "mlir::createConvertLinalgToLLVMPass()";
557,561d474
<   let options = [
<      Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                 /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                 "instead of typed pointers">
<   ];
596c509
< def ConvertMathToLLVMPass : Pass<"convert-math-to-llvm"> {
---
> def ConvertMathToLLVM : Pass<"convert-math-to-llvm"> {
597a511,514
>   let description = [{
>     This pass converts supported Math ops to LLVM dialect intrinsics.
>   }];
>   let constructor = "mlir::createConvertMathToLLVMPass()";
599,602d515
<   let options = [
<     Option<"approximateLog1p", "approximate-log1p", "bool", "true",
<            "Enable approximation of Log1p.">
<   ];
630d542
<     "scf::SCFDialect",
638,643c550
<            "is greater than or equal to this value">,
<     // Most backend targets support a native ctlz operation, so by default
<     // ctrlz conversion is disabled.
<     Option<"convertCtlz", "convert-ctlz", "bool", /*default=*/"false",
<            "Convert math.ctlz to a software implementation. Enable "
<            "for targets that do not natively support ctlz.">,
---
>            "is greater than or equal to this value">
651,660c558,560
< def FinalizeMemRefToLLVMConversionPass :
<     Pass<"finalize-memref-to-llvm", "ModuleOp"> {
<   let summary = "Finalize MemRef dialect to LLVM dialect conversion";
<   let description = [{
<     Finalize the conversion of the operations from the MemRef
<     dialect to the LLVM dialect.
<     This conversion will not convert some complex MemRef
<     operations. Make sure to run `expand-strided-metadata`
<     beforehand for these.
<   }];
---
> def MemRefToLLVMConversionPass : Pass<"convert-memref-to-llvm", "ModuleOp"> {
>   let summary = "Convert operations from the MemRef dialect to the LLVM "
>                 "dialect";
672,675c572
<            "classic 'malloc', 'aligned_alloc' and 'free' functions">,
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">
---
>            "classic 'malloc', 'aligned_alloc' and 'free' functions">
708c605
< def ConvertNVGPUToNVVMPass : Pass<"convert-nvgpu-to-nvvm"> {
---
> def ConvertNVGPUToNVVM : Pass<"convert-nvgpu-to-nvvm"> {
713c610
< 
---
>   let constructor = "mlir::createConvertNVGPUToNVVMPass()";
717,721d613
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<               /*default=*/"true", "Generate LLVM IR using opaque pointers "
<               "instead of typed pointers">
<   ];
739c631
< def ConvertOpenACCToLLVMPass : Pass<"convert-openacc-to-llvm", "ModuleOp"> {
---
> def ConvertOpenACCToLLVM : Pass<"convert-openacc-to-llvm", "ModuleOp"> {
740a633
>   let constructor = "mlir::createConvertOpenACCToLLVMPass()";
742,746d634
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<            /*default=*/"true", "Generate LLVM IR using opaque pointers "
<            "instead of typed pointers">,
<   ];
753c641
< def ConvertOpenMPToLLVMPass : Pass<"convert-openmp-to-llvm", "ModuleOp"> {
---
> def ConvertOpenMPToLLVM : Pass<"convert-openmp-to-llvm", "ModuleOp"> {
754a643
>   let constructor = "mlir::createConvertOpenMPToLLVMPass()";
809c698
< def ConvertSCFToOpenMPPass : Pass<"convert-scf-to-openmp", "ModuleOp"> {
---
> def ConvertSCFToOpenMP : Pass<"convert-scf-to-openmp", "ModuleOp"> {
812,818c701
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
< 
---
>   let constructor = "mlir::createConvertSCFToOpenMPPass()";
830,831c713,714
<     Converts SCF ops into SPIR-V structured control flow ops.
<     SPIR-V structured control flow ops do not support yielding values.
---
>     This pass converts SCF ops into SPIR-V structured control flow ops.
>     SPIR-V structured control flow ops does not support yielding values.
835a719
>   let constructor = "mlir::createConvertSCFToSPIRVPass()";
859c743
<   let dependentDialects = ["affine::AffineDialect", "gpu::GPUDialect"];
---
>   let dependentDialects = ["AffineDialect", "gpu::GPUDialect"];
894c778
< def ConvertSPIRVToLLVMPass : Pass<"convert-spirv-to-llvm", "ModuleOp"> {
---
> def ConvertSPIRVToLLVM : Pass<"convert-spirv-to-llvm", "ModuleOp"> {
899a784
>   let constructor = "mlir::createConvertSPIRVToLLVMPass()";
901,906d785
< 
<   let options = [
<     Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                  /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                  "instead of typed pointers">
<   ];
1036c915
<     "memref::MemRefDialect", "gpu::GPUDialect", "affine::AffineDialect",
---
>     "memref::MemRefDialect", "gpu::GPUDialect", "AffineDialect",
1055c934
<     "affine::AffineDialect",
---
>     "AffineDialect",
1057,1058c936
<     "scf::SCFDialect",
<     "tensor::TensorDialect"
---
>     "scf::SCFDialect"
1074c952
< def ConvertVectorToLLVMPass : Pass<"convert-vector-to-llvm", "ModuleOp"> {
---
> def ConvertVectorToLLVM : Pass<"convert-vector-to-llvm", "ModuleOp"> {
1086a965
>   let constructor = "mlir::createConvertVectorToLLVMPass()";
1112,1115c991
< 	   "dialect.">,
< 	  Option<"useOpaquePointers", "use-opaque-pointers", "bool",
<                /*default=*/"true", "Generate LLVM IR using opaque pointers "
<                "instead of typed pointers">
---
> 	   "dialect.">
--- include/mlir/Conversion/AMDGPUToROCDL
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/AMDGPUToROCDL/AMDGPUToROCDL.h include/mlir/Conversion/AMDGPUToROCDL/AMDGPUToROCDL.h
11c11
< #include "mlir/Conversion/AMDGPUToROCDL/Chipset.h"
---
> #include "mlir/Dialect/AMDGPU/Utils/Chipset.h"
Only in ../../../../llvm-project.0/mlir/include/mlir/Conversion/AMDGPUToROCDL: Chipset.h
--- include/mlir/Conversion/AMDGPUToROCDL/AMDGPUToROCDL.h
11c11
< #include "mlir/Conversion/AMDGPUToROCDL/Chipset.h"
---
> #include "mlir/Dialect/AMDGPU/Utils/Chipset.h"
--- include/mlir/Conversion/LinalgToStandard
--- include/mlir/Conversion/LinalgToStandard/LinalgToStandard.h
--- include/mlir/Conversion/SCFToOpenMP
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToOpenMP/SCFToOpenMP.h include/mlir/Conversion/SCFToOpenMP/SCFToOpenMP.h
15c15,17
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
17c19
< #define GEN_PASS_DECL_CONVERTSCFTOOPENMPPASS
---
> #define GEN_PASS_DECL_CONVERTSCFTOOPENMP
18a21,22
> 
> std::unique_ptr<OperationPass<ModuleOp>> createConvertSCFToOpenMPPass();
--- include/mlir/Conversion/SCFToOpenMP/SCFToOpenMP.h
15c15,17
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
17c19
< #define GEN_PASS_DECL_CONVERTSCFTOOPENMPPASS
---
> #define GEN_PASS_DECL_CONVERTSCFTOOPENMP
18a21,22
> 
> std::unique_ptr<OperationPass<ModuleOp>> createConvertSCFToOpenMPPass();
--- include/mlir/Conversion/AffineToStandard
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/AffineToStandard/AffineToStandard.h include/mlir/Conversion/AffineToStandard/AffineToStandard.h
14a15
> class AffineForOp;
20d20
< class RewritePatternSet;
24,26c24
< namespace affine {
< class AffineForOp;
< } // namespace affine
---
> class RewritePatternSet;
42c40
< Value lowerAffineLowerBound(affine::AffineForOp op, OpBuilder &builder);
---
> Value lowerAffineLowerBound(AffineForOp op, OpBuilder &builder);
46c44
< Value lowerAffineUpperBound(affine::AffineForOp op, OpBuilder &builder);
---
> Value lowerAffineUpperBound(AffineForOp op, OpBuilder &builder);
--- include/mlir/Conversion/AffineToStandard/AffineToStandard.h
14a15
> class AffineForOp;
20d20
< class RewritePatternSet;
24,26c24
< namespace affine {
< class AffineForOp;
< } // namespace affine
---
> class RewritePatternSet;
42c40
< Value lowerAffineLowerBound(affine::AffineForOp op, OpBuilder &builder);
---
> Value lowerAffineLowerBound(AffineForOp op, OpBuilder &builder);
46c44
< Value lowerAffineUpperBound(affine::AffineForOp op, OpBuilder &builder);
---
> Value lowerAffineUpperBound(AffineForOp op, OpBuilder &builder);
--- include/mlir/Conversion/ComplexToLibm
--- include/mlir/Conversion/ComplexToLibm/ComplexToLibm.h
--- include/mlir/Conversion/IndexToLLVM
--- include/mlir/Conversion/IndexToLLVM/IndexToLLVM.h
--- include/mlir/Conversion/MathToFuncs
--- include/mlir/Conversion/MathToFuncs/MathToFuncs.h
--- include/mlir/Conversion/TosaToLinalg
--- include/mlir/Conversion/TosaToLinalg/TosaToLinalg.h
--- include/mlir/Conversion/TosaToSCF
--- include/mlir/Conversion/TosaToSCF/TosaToSCF.h
--- include/mlir/Conversion/ArmNeon2dToIntr
--- include/mlir/Conversion/ArmNeon2dToIntr/ArmNeon2dToIntr.h
--- include/mlir/Conversion/MemRefToSPIRV
--- include/mlir/Conversion/MemRefToSPIRV/MemRefToSPIRV.h
--- include/mlir/Conversion/MemRefToSPIRV/MemRefToSPIRVPass.h
--- include/mlir/Conversion/MathToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToLLVM/MathToLLVM.h include/mlir/Conversion/MathToLLVM/MathToLLVM.h
20c20
< #define GEN_PASS_DECL_CONVERTMATHTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTMATHTOLLVM
24,25c24,26
<                                           RewritePatternSet &patterns,
<                                           bool approximateLog1p = true);
---
>                                           RewritePatternSet &patterns);
> 
> std::unique_ptr<Pass> createConvertMathToLLVMPass();
--- include/mlir/Conversion/MathToLLVM/MathToLLVM.h
20c20
< #define GEN_PASS_DECL_CONVERTMATHTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTMATHTOLLVM
24,25c24,26
<                                           RewritePatternSet &patterns,
<                                           bool approximateLog1p = true);
---
>                                           RewritePatternSet &patterns);
> 
> std::unique_ptr<Pass> createConvertMathToLLVMPass();
--- include/mlir/Conversion/PDLToPDLInterp
--- include/mlir/Conversion/PDLToPDLInterp/PDLToPDLInterp.h
--- include/mlir/Conversion/OpenACCToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/OpenACCToLLVM/ConvertOpenACCToLLVM.h include/mlir/Conversion/OpenACCToLLVM/ConvertOpenACCToLLVM.h
16c16,18
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
19c21
< #define GEN_PASS_DECL_CONVERTOPENACCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTOPENACCTOLLVM
69a72,75
> 
> /// Create a pass to convert the OpenACC dialect into the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertOpenACCToLLVMPass();
> 
--- include/mlir/Conversion/OpenACCToLLVM/ConvertOpenACCToLLVM.h
16c16,18
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
19c21
< #define GEN_PASS_DECL_CONVERTOPENACCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTOPENACCTOLLVM
69a72,75
> 
> /// Create a pass to convert the OpenACC dialect into the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertOpenACCToLLVMPass();
> 
--- include/mlir/Conversion/FuncToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/FuncToLLVM/ConvertFuncToLLVMPass.h include/mlir/Conversion/FuncToLLVM/ConvertFuncToLLVMPass.h
15a16,19
> class LowerToLLVMOptions;
> class ModuleOp;
> template <typename T>
> class OperationPass;
18c22
< #define GEN_PASS_DECL_CONVERTFUNCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTFUNCTOLLVM
19a24,28
> 
> /// Creates a pass to convert the Func dialect into the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertFuncToLLVMPass();
> std::unique_ptr<OperationPass<ModuleOp>>
> createConvertFuncToLLVMPass(const LowerToLLVMOptions &options);
--- include/mlir/Conversion/FuncToLLVM/ConvertFuncToLLVMPass.h
15a16,19
> class LowerToLLVMOptions;
> class ModuleOp;
> template <typename T>
> class OperationPass;
18c22
< #define GEN_PASS_DECL_CONVERTFUNCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTFUNCTOLLVM
19a24,28
> 
> /// Creates a pass to convert the Func dialect into the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertFuncToLLVMPass();
> std::unique_ptr<OperationPass<ModuleOp>>
> createConvertFuncToLLVMPass(const LowerToLLVMOptions &options);
--- include/mlir/Conversion/FuncToLLVM/ConvertFuncToLLVM.h
--- include/mlir/Conversion/GPUToROCDL
--- include/mlir/Conversion/GPUToROCDL/GPUToROCDLPass.h
--- include/mlir/Conversion/GPUToROCDL/Runtimes.h
--- include/mlir/Conversion/MemRefToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/MemRefToLLVM/MemRefToLLVM.h include/mlir/Conversion/MemRefToLLVM/MemRefToLLVM.h
19c19
< #define GEN_PASS_DECL_FINALIZEMEMREFTOLLVMCONVERSIONPASS
---
> #define GEN_PASS_DECL_MEMREFTOLLVMCONVERSIONPASS
24,25c24,25
< void populateFinalizeMemRefToLLVMConversionPatterns(
<     LLVMTypeConverter &converter, RewritePatternSet &patterns);
---
> void populateMemRefToLLVMConversionPatterns(LLVMTypeConverter &converter,
>                                             RewritePatternSet &patterns);
--- include/mlir/Conversion/MemRefToLLVM/AllocLikeConversion.h
--- include/mlir/Conversion/MemRefToLLVM/MemRefToLLVM.h
19c19
< #define GEN_PASS_DECL_FINALIZEMEMREFTOLLVMCONVERSIONPASS
---
> #define GEN_PASS_DECL_MEMREFTOLLVMCONVERSIONPASS
24,25c24,25
< void populateFinalizeMemRefToLLVMConversionPatterns(
<     LLVMTypeConverter &converter, RewritePatternSet &patterns);
---
> void populateMemRefToLLVMConversionPatterns(LLVMTypeConverter &converter,
>                                             RewritePatternSet &patterns);
--- include/mlir/Conversion/NVGPUToNVVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/NVGPUToNVVM/NVGPUToNVVM.h include/mlir/Conversion/NVGPUToNVVM/NVGPUToNVVM.h
19c19
< #define GEN_PASS_DECL_CONVERTNVGPUTONVVMPASS
---
> #define GEN_PASS_DECL_CONVERTNVGPUTONVVM
23a24,26
> 
> std::unique_ptr<Pass> createConvertNVGPUToNVVMPass();
> 
--- include/mlir/Conversion/NVGPUToNVVM/NVGPUToNVVM.h
19c19
< #define GEN_PASS_DECL_CONVERTNVGPUTONVVMPASS
---
> #define GEN_PASS_DECL_CONVERTNVGPUTONVVM
23a24,26
> 
> std::unique_ptr<Pass> createConvertNVGPUToNVVMPass();
> 
--- include/mlir/Conversion/CMakeLists.txt
--- include/mlir/Conversion/BufferizationToMemRef
--- include/mlir/Conversion/BufferizationToMemRef/BufferizationToMemRef.h
--- include/mlir/Conversion/LLVMCommon
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/LLVMCommon/LoweringOptions.h include/mlir/Conversion/LLVMCommon/LoweringOptions.h
36d35
<   bool useOpaquePointers = true;
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/LLVMCommon/MemRefBuilder.h include/mlir/Conversion/LLVMCommon/MemRefBuilder.h
92,99d91
<   /// Builds IR for getting the start address of the buffer represented
<   /// by this memref:
<   /// `memref.alignedPtr + memref.offset * sizeof(type.getElementType())`.
<   /// \note there is no setter for this one since it is derived from alignedPtr
<   /// and offset.
<   Value bufferPtr(OpBuilder &builder, Location loc,
<                   LLVMTypeConverter &converter, MemRefType type);
< 
122,123d113
<   bool useOpaquePointers();
< 
168c158
<   Value rank(OpBuilder &builder, Location loc) const;
---
>   Value rank(OpBuilder &builder, Location loc);
172c162
<   Value memRefDescPtr(OpBuilder &builder, Location loc) const;
---
>   Value memRefDescPtr(OpBuilder &builder, Location loc);
194,196c184
<   /// and appends the corresponding values into `sizes`. `addressSpaces`
<   /// which must have the same length as `values`, is needed to handle layouts
<   /// where sizeof(ptr addrspace(N)) != sizeof(ptr addrspace(0)).
---
>   /// and appends the corresponding values into `sizes`.
200d187
<                            ArrayRef<unsigned> addressSpaces,
210,211c197
<                             Value memRefDescPtr,
<                             LLVM::LLVMPointerType elemPtrType);
---
>                             Value memRefDescPtr, Type elemPtrPtrType);
214,215c200
<                               Value memRefDescPtr,
<                               LLVM::LLVMPointerType elemPtrType,
---
>                               Value memRefDescPtr, Type elemPtrPtrType,
221c206
<                           LLVM::LLVMPointerType elemPtrType);
---
>                           Type elemPtrPtrType);
225,226c210
<                             Value memRefDescPtr,
<                             LLVM::LLVMPointerType elemPtrType,
---
>                             Value memRefDescPtr, Type elemPtrPtrType,
229,235d212
<   /// Builds IR for getting the pointer to the offset's location.
<   /// Returns a pointer to a convertType(index), which points to the beggining
<   /// of a struct {index, index[rank], index[rank]}.
<   static Value offsetBasePtr(OpBuilder &builder, Location loc,
<                              LLVMTypeConverter &typeConverter,
<                              Value memRefDescPtr,
<                              LLVM::LLVMPointerType elemPtrType);
239c216
<                       LLVM::LLVMPointerType elemPtrType);
---
>                       Type elemPtrPtrType);
243c220
<                         LLVM::LLVMPointerType elemPtrType, Value offset);
---
>                         Type elemPtrPtrType, Value offset);
249c226
<                            LLVM::LLVMPointerType elemPtrType);
---
>                            LLVM::LLVMPointerType elemPtrPtrType);
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/LLVMCommon/StructBuilder.h include/mlir/Conversion/LLVMCommon/StructBuilder.h
44c44
<   Value extractPtr(OpBuilder &builder, Location loc, unsigned pos) const;
---
>   Value extractPtr(OpBuilder &builder, Location loc, unsigned pos);
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/LLVMCommon/TypeConverter.h include/mlir/Conversion/LLVMCommon/TypeConverter.h
27,29d26
< class LLVMPointerType;
< class LLVMFunctionType;
< class LLVMStructType;
56d52
<                                 bool useBarePtrCallConv,
59,70c55,59
<   /// Convert a non-empty list of types to be returned from a function into an
<   /// LLVM-compatible type. In particular, if more than one value is returned,
<   /// create an LLVM dialect structure type with elements that correspond to
<   /// each of the types converted with `convertCallingConventionType`.
<   Type packFunctionResults(TypeRange types,
<                            bool useBarePointerCallConv = false);
< 
<   /// Convert a non-empty list of types of values produced by an operation into
<   /// an LLVM-compatible type. In particular, if more than one value is
<   /// produced, create a literal structure with elements that correspond to each
<   /// of the LLVM-compatible types converted with `convertType`.
<   Type packOperationResults(TypeRange types);
---
>   /// Convert a non-empty list of types to be returned from a function into a
>   /// supported LLVM IR type.  In particular, if more than one value is
>   /// returned, create an LLVM IR structure type with elements that correspond
>   /// to each of the MLIR types converted with `convertType`.
>   Type packFunctionResults(TypeRange types);
77,78c66
<   Type convertCallingConventionType(Type type,
<                                     bool useBarePointerCallConv = false);
---
>   Type convertCallingConventionType(Type type);
107,108c95,96
<                                         ValueRange operands, OpBuilder &builder,
<                                         bool useBarePtrCallConv = false);
---
>                                         ValueRange operands,
>                                         OpBuilder &builder);
120,121c108
<   std::pair<LLVM::LLVMFunctionType, LLVM::LLVMStructType>
<   convertFunctionTypeCWrapper(FunctionType type);
---
>   std::pair<Type, bool> convertFunctionTypeCWrapper(FunctionType type);
135,145d121
<   /// Returns true if using opaque pointers was enabled in the lowering options.
<   bool useOpaquePointers() const { return getOptions().useOpaquePointers; }
< 
<   /// Creates an LLVM pointer type with the given element type and address
<   /// space.
<   /// This function is meant to be used in code supporting both typed and opaque
<   /// pointers, as it will create an opaque pointer with the given address space
<   /// if opaque pointers are enabled in the lowering options.
<   LLVM::LLVMPointerType getPointerType(Type elementType,
<                                        unsigned addressSpace = 0);
< 
158,162d133
< 
<   /// Return the LLVM address space corresponding to the memory space of the
<   /// memref type `type` or failure if the memory space cannot be converted to
<   /// an integer.
<   FailureOr<unsigned> getMemRefAddressSpace(BaseMemRefType type);
--- include/mlir/Conversion/LLVMCommon/MemRefBuilder.h
92,99d91
<   /// Builds IR for getting the start address of the buffer represented
<   /// by this memref:
<   /// `memref.alignedPtr + memref.offset * sizeof(type.getElementType())`.
<   /// \note there is no setter for this one since it is derived from alignedPtr
<   /// and offset.
<   Value bufferPtr(OpBuilder &builder, Location loc,
<                   LLVMTypeConverter &converter, MemRefType type);
< 
122,123d113
<   bool useOpaquePointers();
< 
168c158
<   Value rank(OpBuilder &builder, Location loc) const;
---
>   Value rank(OpBuilder &builder, Location loc);
172c162
<   Value memRefDescPtr(OpBuilder &builder, Location loc) const;
---
>   Value memRefDescPtr(OpBuilder &builder, Location loc);
194,196c184
<   /// and appends the corresponding values into `sizes`. `addressSpaces`
<   /// which must have the same length as `values`, is needed to handle layouts
<   /// where sizeof(ptr addrspace(N)) != sizeof(ptr addrspace(0)).
---
>   /// and appends the corresponding values into `sizes`.
200d187
<                            ArrayRef<unsigned> addressSpaces,
210,211c197
<                             Value memRefDescPtr,
<                             LLVM::LLVMPointerType elemPtrType);
---
>                             Value memRefDescPtr, Type elemPtrPtrType);
214,215c200
<                               Value memRefDescPtr,
<                               LLVM::LLVMPointerType elemPtrType,
---
>                               Value memRefDescPtr, Type elemPtrPtrType,
221c206
<                           LLVM::LLVMPointerType elemPtrType);
---
>                           Type elemPtrPtrType);
225,226c210
<                             Value memRefDescPtr,
<                             LLVM::LLVMPointerType elemPtrType,
---
>                             Value memRefDescPtr, Type elemPtrPtrType,
229,235d212
<   /// Builds IR for getting the pointer to the offset's location.
<   /// Returns a pointer to a convertType(index), which points to the beggining
<   /// of a struct {index, index[rank], index[rank]}.
<   static Value offsetBasePtr(OpBuilder &builder, Location loc,
<                              LLVMTypeConverter &typeConverter,
<                              Value memRefDescPtr,
<                              LLVM::LLVMPointerType elemPtrType);
239c216
<                       LLVM::LLVMPointerType elemPtrType);
---
>                       Type elemPtrPtrType);
243c220
<                         LLVM::LLVMPointerType elemPtrType, Value offset);
---
>                         Type elemPtrPtrType, Value offset);
249c226
<                            LLVM::LLVMPointerType elemPtrType);
---
>                            LLVM::LLVMPointerType elemPtrPtrType);
--- include/mlir/Conversion/LLVMCommon/TypeConverter.h
27,29d26
< class LLVMPointerType;
< class LLVMFunctionType;
< class LLVMStructType;
56d52
<                                 bool useBarePtrCallConv,
59,70c55,59
<   /// Convert a non-empty list of types to be returned from a function into an
<   /// LLVM-compatible type. In particular, if more than one value is returned,
<   /// create an LLVM dialect structure type with elements that correspond to
<   /// each of the types converted with `convertCallingConventionType`.
<   Type packFunctionResults(TypeRange types,
<                            bool useBarePointerCallConv = false);
< 
<   /// Convert a non-empty list of types of values produced by an operation into
<   /// an LLVM-compatible type. In particular, if more than one value is
<   /// produced, create a literal structure with elements that correspond to each
<   /// of the LLVM-compatible types converted with `convertType`.
<   Type packOperationResults(TypeRange types);
---
>   /// Convert a non-empty list of types to be returned from a function into a
>   /// supported LLVM IR type.  In particular, if more than one value is
>   /// returned, create an LLVM IR structure type with elements that correspond
>   /// to each of the MLIR types converted with `convertType`.
>   Type packFunctionResults(TypeRange types);
77,78c66
<   Type convertCallingConventionType(Type type,
<                                     bool useBarePointerCallConv = false);
---
>   Type convertCallingConventionType(Type type);
107,108c95,96
<                                         ValueRange operands, OpBuilder &builder,
<                                         bool useBarePtrCallConv = false);
---
>                                         ValueRange operands,
>                                         OpBuilder &builder);
120,121c108
<   std::pair<LLVM::LLVMFunctionType, LLVM::LLVMStructType>
<   convertFunctionTypeCWrapper(FunctionType type);
---
>   std::pair<Type, bool> convertFunctionTypeCWrapper(FunctionType type);
135,145d121
<   /// Returns true if using opaque pointers was enabled in the lowering options.
<   bool useOpaquePointers() const { return getOptions().useOpaquePointers; }
< 
<   /// Creates an LLVM pointer type with the given element type and address
<   /// space.
<   /// This function is meant to be used in code supporting both typed and opaque
<   /// pointers, as it will create an opaque pointer with the given address space
<   /// if opaque pointers are enabled in the lowering options.
<   LLVM::LLVMPointerType getPointerType(Type elementType,
<                                        unsigned addressSpace = 0);
< 
158,162d133
< 
<   /// Return the LLVM address space corresponding to the memory space of the
<   /// memref type `type` or failure if the memory space cannot be converted to
<   /// an integer.
<   FailureOr<unsigned> getMemRefAddressSpace(BaseMemRefType type);
--- include/mlir/Conversion/LLVMCommon/ConversionTarget.h
--- include/mlir/Conversion/LLVMCommon/StructBuilder.h
44c44
<   Value extractPtr(OpBuilder &builder, Location loc, unsigned pos) const;
---
>   Value extractPtr(OpBuilder &builder, Location loc, unsigned pos);
--- include/mlir/Conversion/LLVMCommon/Pattern.h
--- include/mlir/Conversion/LLVMCommon/VectorPattern.h
--- include/mlir/Conversion/LLVMCommon/LoweringOptions.h
36d35
<   bool useOpaquePointers = true;
--- include/mlir/Conversion/SPIRVToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/SPIRVToLLVM/SPIRVToLLVMPass.h include/mlir/Conversion/SPIRVToLLVM/SPIRVToLLVMPass.h
19c19,21
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
21,22c23,24
< #define GEN_PASS_DECL_LOWERHOSTCODETOLLVMPASS
< #define GEN_PASS_DECL_CONVERTSPIRVTOLLVMPASS
---
> #define GEN_PASS_DECL_LOWERHOSTCODETOLLVM
> #define GEN_PASS_DECL_CONVERTSPIRVTOLLVM
23a26,38
> 
> /// Creates a pass to emulate `gpu.launch_func` call in LLVM dialect and lower
> /// the host module code to LLVM.
> ///
> /// This transformation creates a sequence of global variables that are later
> /// linked to the variables in the kernel module, and a series of copies to/from
> /// them to emulate the memory transfer from the host or to the device sides. It
> /// also converts the remaining Arithmetic, Func, and MemRef dialects into LLVM
> /// dialect, emitting C wrappers.
> std::unique_ptr<OperationPass<ModuleOp>> createLowerHostCodeToLLVMPass();
> 
> /// Creates a pass to convert SPIR-V operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertSPIRVToLLVMPass();
--- include/mlir/Conversion/SPIRVToLLVM/SPIRVToLLVMPass.h
19c19,21
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
21,22c23,24
< #define GEN_PASS_DECL_LOWERHOSTCODETOLLVMPASS
< #define GEN_PASS_DECL_CONVERTSPIRVTOLLVMPASS
---
> #define GEN_PASS_DECL_LOWERHOSTCODETOLLVM
> #define GEN_PASS_DECL_CONVERTSPIRVTOLLVM
23a26,38
> 
> /// Creates a pass to emulate `gpu.launch_func` call in LLVM dialect and lower
> /// the host module code to LLVM.
> ///
> /// This transformation creates a sequence of global variables that are later
> /// linked to the variables in the kernel module, and a series of copies to/from
> /// them to emulate the memory transfer from the host or to the device sides. It
> /// also converts the remaining Arithmetic, Func, and MemRef dialects into LLVM
> /// dialect, emitting C wrappers.
> std::unique_ptr<OperationPass<ModuleOp>> createLowerHostCodeToLLVMPass();
> 
> /// Creates a pass to convert SPIR-V operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertSPIRVToLLVMPass();
--- include/mlir/Conversion/SPIRVToLLVM/SPIRVToLLVM.h
--- include/mlir/Conversion/GPUToVulkan
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToVulkan/ConvertGPUToVulkanPass.h include/mlir/Conversion/GPUToVulkan/ConvertGPUToVulkanPass.h
26d25
< class Pass;
28c27
< #define GEN_PASS_DECL_CONVERTVULKANLAUNCHFUNCTOVULKANCALLSPASS
---
> #define GEN_PASS_DECL_CONVERTVULKANLAUNCHFUNCTOVULKANCALLS
30a30,32
> 
> std::unique_ptr<OperationPass<ModuleOp>>
> createConvertVulkanLaunchFuncToVulkanCallsPass();
--- include/mlir/Conversion/GPUToVulkan/ConvertGPUToVulkanPass.h
26d25
< class Pass;
28c27
< #define GEN_PASS_DECL_CONVERTVULKANLAUNCHFUNCTOVULKANCALLSPASS
---
> #define GEN_PASS_DECL_CONVERTVULKANLAUNCHFUNCTOVULKANCALLS
30a30,32
> 
> std::unique_ptr<OperationPass<ModuleOp>>
> createConvertVulkanLaunchFuncToVulkanCallsPass();
--- include/mlir/Conversion/ShapeToStandard
--- include/mlir/Conversion/ShapeToStandard/ShapeToStandard.h
--- include/mlir/Conversion/ComplexToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/ComplexToLLVM/ComplexToLLVM.h include/mlir/Conversion/ComplexToLLVM/ComplexToLLVM.h
18c18
< #define GEN_PASS_DECL_CONVERTCOMPLEXTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTCOMPLEXTOLLVM
42a43,46
> 
> /// Create a pass to convert Complex operations to the LLVMIR dialect.
> std::unique_ptr<Pass> createConvertComplexToLLVMPass();
> 
--- include/mlir/Conversion/ComplexToLLVM/ComplexToLLVM.h
18c18
< #define GEN_PASS_DECL_CONVERTCOMPLEXTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTCOMPLEXTOLLVM
42a43,46
> 
> /// Create a pass to convert Complex operations to the LLVMIR dialect.
> std::unique_ptr<Pass> createConvertComplexToLLVMPass();
> 
--- include/mlir/Conversion/SCFToControlFlow
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h include/mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h
1c1
< //===- SCFToControlFlow.h - SCF to ControlFlow Pass entrypoint --*- C++ -*-===//
---
> //===- ConvertSCFToControlFlow.h - Pass entrypoint --------------*- C++ -*-===//
--- include/mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h
1c1
< //===- SCFToControlFlow.h - SCF to ControlFlow Pass entrypoint --*- C++ -*-===//
---
> //===- ConvertSCFToControlFlow.h - Pass entrypoint --------------*- C++ -*-===//
--- include/mlir/Conversion/MathToSPIRV
--- include/mlir/Conversion/MathToSPIRV/MathToSPIRVPass.h
--- include/mlir/Conversion/MathToSPIRV/MathToSPIRV.h
--- include/mlir/Conversion/ControlFlowToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h include/mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h
23c23
< #define GEN_PASS_DECL_CONVERTCONTROLFLOWTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTCONTROLFLOWTOLLVM
27d26
< 
39a39,41
> 
> /// Creates a pass to convert the ControlFlow dialect into the LLVMIR dialect.
> std::unique_ptr<Pass> createConvertControlFlowToLLVMPass();
--- include/mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h
23c23
< #define GEN_PASS_DECL_CONVERTCONTROLFLOWTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTCONTROLFLOWTOLLVM
27d26
< 
39a39,41
> 
> /// Creates a pass to convert the ControlFlow dialect into the LLVMIR dialect.
> std::unique_ptr<Pass> createConvertControlFlowToLLVMPass();
--- include/mlir/Conversion/MathToLibm
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/MathToLibm/MathToLibm.h include/mlir/Conversion/MathToLibm/MathToLibm.h
11c11,12
< #include "mlir/IR/PatternMatch.h"
---
> #include "mlir/Transforms/DialectConversion.h"
> #include <optional>
22c23,25
< void populateMathToLibmConversionPatterns(RewritePatternSet &patterns);
---
> void populateMathToLibmConversionPatterns(
>     RewritePatternSet &patterns, PatternBenefit benefit,
>     std::optional<PatternBenefit> log1pBenefit = std::nullopt);
--- include/mlir/Conversion/MathToLibm/MathToLibm.h
11c11,12
< #include "mlir/IR/PatternMatch.h"
---
> #include "mlir/Transforms/DialectConversion.h"
> #include <optional>
22c23,25
< void populateMathToLibmConversionPatterns(RewritePatternSet &patterns);
---
> void populateMathToLibmConversionPatterns(
>     RewritePatternSet &patterns, PatternBenefit benefit,
>     std::optional<PatternBenefit> log1pBenefit = std::nullopt);
--- include/mlir/Conversion/OpenACCToSCF
--- include/mlir/Conversion/OpenACCToSCF/ConvertOpenACCToSCF.h
--- include/mlir/Conversion/ControlFlowToSPIRV
--- include/mlir/Conversion/ControlFlowToSPIRV/ControlFlowToSPIRV.h
--- include/mlir/Conversion/ControlFlowToSPIRV/ControlFlowToSPIRVPass.h
--- include/mlir/Conversion/GPUToSPIRV
--- include/mlir/Conversion/GPUToSPIRV/GPUToSPIRV.h
--- include/mlir/Conversion/GPUToSPIRV/GPUToSPIRVPass.h
--- include/mlir/Conversion/SCFToGPU
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/SCFToGPU/SCFToGPU.h include/mlir/Conversion/SCFToGPU/SCFToGPU.h
13a14
> class AffineForOp;
21,24d21
< namespace affine {
< class AffineForOp;
< } // namespace affine
< 
43c40
< LogicalResult convertAffineLoopNestToGPULaunch(affine::AffineForOp forOp,
---
> LogicalResult convertAffineLoopNestToGPULaunch(AffineForOp forOp,
--- include/mlir/Conversion/SCFToGPU/SCFToGPU.h
13a14
> class AffineForOp;
21,24d21
< namespace affine {
< class AffineForOp;
< } // namespace affine
< 
43c40
< LogicalResult convertAffineLoopNestToGPULaunch(affine::AffineForOp forOp,
---
> LogicalResult convertAffineLoopNestToGPULaunch(AffineForOp forOp,
--- include/mlir/Conversion/SCFToGPU/SCFToGPUPass.h
--- include/mlir/Conversion/TosaToTensor
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/TosaToTensor/TosaToTensor.h include/mlir/Conversion/TosaToTensor/TosaToTensor.h
15a16
> #include "mlir/IR/PatternMatch.h"
--- include/mlir/Conversion/TosaToTensor/TosaToTensor.h
15a16
> #include "mlir/IR/PatternMatch.h"
--- include/mlir/Conversion/TensorToSPIRV
--- include/mlir/Conversion/TensorToSPIRV/TensorToSPIRVPass.h
--- include/mlir/Conversion/TensorToSPIRV/TensorToSPIRV.h
--- include/mlir/Conversion/GPUToNVVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h include/mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h
40,44d39
< /// Populate GpuSubgroupReduce pattern to NVVM. It generates a specific nvvm
< /// op that is not available on every GPU.
< void populateGpuSubgroupReduceOpLoweringPattern(LLVMTypeConverter &converter,
<                                                 RewritePatternSet &patterns);
< 
53,54c48
<     unsigned indexBitwidth = kDeriveIndexBitwidthFromDataLayout,
<     bool hasRedux = false);
---
>     unsigned indexBitwidth = kDeriveIndexBitwidthFromDataLayout);
--- include/mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h
40,44d39
< /// Populate GpuSubgroupReduce pattern to NVVM. It generates a specific nvvm
< /// op that is not available on every GPU.
< void populateGpuSubgroupReduceOpLoweringPattern(LLVMTypeConverter &converter,
<                                                 RewritePatternSet &patterns);
< 
53,54c48
<     unsigned indexBitwidth = kDeriveIndexBitwidthFromDataLayout,
<     bool hasRedux = false);
---
>     unsigned indexBitwidth = kDeriveIndexBitwidthFromDataLayout);
--- include/mlir/Conversion/VectorToSPIRV
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToSPIRV/VectorToSPIRV.h include/mlir/Conversion/VectorToSPIRV/VectorToSPIRV.h
26,34d25
< /// Appends patterns to convert vector reduction of the form:
< /// ```
< ///   vector.reduction <add>, (muli (ext %lhs), (ext %rhs)), [%acc]
< /// ```
< ///
< /// to SPIR-V integer dot product ops.
< void populateVectorReductionToSPIRVDotProductPatterns(
<     RewritePatternSet &patterns);
< 
--- include/mlir/Conversion/VectorToSPIRV/VectorToSPIRVPass.h
--- include/mlir/Conversion/VectorToSPIRV/VectorToSPIRV.h
26,34d25
< /// Appends patterns to convert vector reduction of the form:
< /// ```
< ///   vector.reduction <add>, (muli (ext %lhs), (ext %rhs)), [%acc]
< /// ```
< ///
< /// to SPIR-V integer dot product ops.
< void populateVectorReductionToSPIRVDotProductPatterns(
<     RewritePatternSet &patterns);
< 
--- include/mlir/Conversion/AsyncToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/AsyncToLLVM/AsyncToLLVM.h include/mlir/Conversion/AsyncToLLVM/AsyncToLLVM.h
17c17,19
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
22c24
< #define GEN_PASS_DECL_CONVERTASYNCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTASYNCTOLLVM
23a26,28
> 
> /// Create a pass to convert Async operations to the LLVM dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertAsyncToLLVMPass();
--- include/mlir/Conversion/AsyncToLLVM/AsyncToLLVM.h
17c17,19
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
22c24
< #define GEN_PASS_DECL_CONVERTASYNCTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTASYNCTOLLVM
23a26,28
> 
> /// Create a pass to convert Async operations to the LLVM dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertAsyncToLLVMPass();
--- include/mlir/Conversion/TosaToArith
--- include/mlir/Conversion/TosaToArith/TosaToArith.h
--- include/mlir/Conversion/TensorToLinalg
--- include/mlir/Conversion/TensorToLinalg/TensorToLinalg.h
--- include/mlir/Conversion/TensorToLinalg/TensorToLinalgPass.h
--- include/mlir/Conversion/ComplexToStandard
--- include/mlir/Conversion/ComplexToStandard/ComplexToStandard.h
--- include/mlir/Conversion/ArithToLLVM
--- include/mlir/Conversion/ArithToLLVM/ArithToLLVM.h
--- include/mlir/Conversion/OpenMPToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/OpenMPToLLVM/ConvertOpenMPToLLVM.h include/mlir/Conversion/OpenMPToLLVM/ConvertOpenMPToLLVM.h
17c17,19
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
20c22
< #define GEN_PASS_DECL_CONVERTOPENMPTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTOOPENMPTOLLVM
30a33,36
> 
> /// Create a pass to convert OpenMP operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertOpenMPToLLVMPass();
> 
--- include/mlir/Conversion/OpenMPToLLVM/ConvertOpenMPToLLVM.h
17c17,19
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
20c22
< #define GEN_PASS_DECL_CONVERTOPENMPTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTOOPENMPTOLLVM
30a33,36
> 
> /// Create a pass to convert OpenMP operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertOpenMPToLLVMPass();
> 
--- include/mlir/Conversion/VectorToGPU
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToGPU/VectorToGPU.h include/mlir/Conversion/VectorToGPU/VectorToGPU.h
15d14
< struct LogicalResult;
33c32
< LogicalResult convertVectorToMMAOps(RewriterBase &rewriter, Operation *rootOp);
---
> void convertVectorToMMAOps(Operation *rootOp);
39,40c38
< LogicalResult convertVectorToNVVMCompatibleMMASync(RewriterBase &rewriter,
<                                                    Operation *rootOp);
---
> LogicalResult convertVectorToNVVMCompatibleMMASync(Operation *rootOp);
--- include/mlir/Conversion/VectorToGPU/VectorToGPU.h
15d14
< struct LogicalResult;
33c32
< LogicalResult convertVectorToMMAOps(RewriterBase &rewriter, Operation *rootOp);
---
> void convertVectorToMMAOps(Operation *rootOp);
39,40c38
< LogicalResult convertVectorToNVVMCompatibleMMASync(RewriterBase &rewriter,
<                                                    Operation *rootOp);
---
> LogicalResult convertVectorToNVVMCompatibleMMASync(Operation *rootOp);
--- include/mlir/Conversion/VectorToLLVM
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/VectorToLLVM/ConvertVectorToLLVM.h include/mlir/Conversion/VectorToLLVM/ConvertVectorToLLVM.h
15c15,17
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
17c19
< #define GEN_PASS_DECL_CONVERTVECTORTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTVECTORTOLLVM
19a22,61
> /// Options to control Vector to LLVM lowering.
> ///
> /// This should kept in sync with VectorToLLVM options defined for the
> /// ConvertVectorToLLVM pass in include/mlir/Conversion/Passes.td
> struct LowerVectorToLLVMOptions {
>   LowerVectorToLLVMOptions() {}
> 
>   LowerVectorToLLVMOptions &enableReassociateFPReductions(bool b = true) {
>     reassociateFPReductions = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableIndexOptimizations(bool b = true) {
>     force32BitVectorIndices = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableArmNeon(bool b = true) {
>     armNeon = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableArmSVE(bool b = true) {
>     armSVE = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableAMX(bool b = true) {
>     amx = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableX86Vector(bool b = true) {
>     x86Vector = b;
>     return *this;
>   }
> 
>   bool reassociateFPReductions{false};
>   bool force32BitVectorIndices{true};
>   bool armNeon{false};
>   bool armSVE{false};
>   bool amx{false};
>   bool x86Vector{false};
> };
> 
29a72,75
> 
> /// Create a pass to convert vector operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertVectorToLLVMPass(
>     const LowerVectorToLLVMOptions &options = LowerVectorToLLVMOptions());
--- include/mlir/Conversion/VectorToLLVM/ConvertVectorToLLVM.h
15c15,17
< class Pass;
---
> class ModuleOp;
> template <typename T>
> class OperationPass;
17c19
< #define GEN_PASS_DECL_CONVERTVECTORTOLLVMPASS
---
> #define GEN_PASS_DECL_CONVERTVECTORTOLLVM
19a22,61
> /// Options to control Vector to LLVM lowering.
> ///
> /// This should kept in sync with VectorToLLVM options defined for the
> /// ConvertVectorToLLVM pass in include/mlir/Conversion/Passes.td
> struct LowerVectorToLLVMOptions {
>   LowerVectorToLLVMOptions() {}
> 
>   LowerVectorToLLVMOptions &enableReassociateFPReductions(bool b = true) {
>     reassociateFPReductions = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableIndexOptimizations(bool b = true) {
>     force32BitVectorIndices = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableArmNeon(bool b = true) {
>     armNeon = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableArmSVE(bool b = true) {
>     armSVE = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableAMX(bool b = true) {
>     amx = b;
>     return *this;
>   }
>   LowerVectorToLLVMOptions &enableX86Vector(bool b = true) {
>     x86Vector = b;
>     return *this;
>   }
> 
>   bool reassociateFPReductions{false};
>   bool force32BitVectorIndices{true};
>   bool armNeon{false};
>   bool armSVE{false};
>   bool amx{false};
>   bool x86Vector{false};
> };
> 
29a72,75
> 
> /// Create a pass to convert vector operations to the LLVMIR dialect.
> std::unique_ptr<OperationPass<ModuleOp>> createConvertVectorToLLVMPass(
>     const LowerVectorToLLVMOptions &options = LowerVectorToLLVMOptions());
--- include/mlir/Conversion/ArithCommon
diff ../../../../llvm-project.0/mlir/include/mlir/Conversion/ArithCommon/AttrToLLVMConverter.h include/mlir/Conversion/ArithCommon/AttrToLLVMConverter.h
55a56,77
> 
> // Attribute converter that populates a NamedAttrList by removing the fastmath
> // attribute from the source operation attributes. This may be useful for
> // target operations that do not require the fastmath attribute, or for targets
> // that do not yet support the LLVM fastmath attribute.
> template <typename SourceOp, typename TargetOp>
> class AttrDropFastMath {
> public:
>   AttrDropFastMath(SourceOp srcOp) {
>     // Copy the source attributes.
>     convertedAttr = NamedAttrList{srcOp->getAttrs()};
>     // Get the name of the arith fastmath attribute.
>     llvm::StringRef arithFMFAttrName = SourceOp::getFastMathAttrName();
>     // Remove the source fastmath attribute.
>     convertedAttr.erase(arithFMFAttrName);
>   }
> 
>   ArrayRef<NamedAttribute> getAttrs() const { return convertedAttr.getAttrs(); }
> 
> private:
>   NamedAttrList convertedAttr;
> };
--- include/mlir/Conversion/ArithCommon/AttrToLLVMConverter.h
55a56,77
> 
> // Attribute converter that populates a NamedAttrList by removing the fastmath
> // attribute from the source operation attributes. This may be useful for
> // target operations that do not require the fastmath attribute, or for targets
> // that do not yet support the LLVM fastmath attribute.
> template <typename SourceOp, typename TargetOp>
> class AttrDropFastMath {
> public:
>   AttrDropFastMath(SourceOp srcOp) {
>     // Copy the source attributes.
>     convertedAttr = NamedAttrList{srcOp->getAttrs()};
>     // Get the name of the arith fastmath attribute.
>     llvm::StringRef arithFMFAttrName = SourceOp::getFastMathAttrName();
>     // Remove the source fastmath attribute.
>     convertedAttr.erase(arithFMFAttrName);
>   }
> 
>   ArrayRef<NamedAttribute> getAttrs() const { return convertedAttr.getAttrs(); }
> 
> private:
>   NamedAttrList convertedAttr;
> };
--- include/mlir/Conversion/FuncToSPIRV
--- include/mlir/Conversion/FuncToSPIRV/FuncToSPIRV.h
--- include/mlir/Conversion/FuncToSPIRV/FuncToSPIRVPass.h
--- include/mlir/InitAllDialects.h
17c17
< #include "mlir/Dialect/AMDGPU/AMDGPUDialect.h"
---
> #include "mlir/Dialect/AMDGPU/IR/AMDGPUDialect.h"
20d19
< #include "mlir/Dialect/Affine/IR/ValueBoundsOpInterfaceImpl.h"
23d21
< #include "mlir/Dialect/Arith/IR/ValueBoundsOpInterfaceImpl.h"
38d35
< #include "mlir/Dialect/IRDL/IR/IRDL.h"
44,45c41
< #include "mlir/Dialect/Linalg/IR/ValueBoundsOpInterfaceImpl.h"
< #include "mlir/Dialect/Linalg/TransformOps/DialectExtension.h"
---
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h"
51d46
< #include "mlir/Dialect/MemRef/IR/ValueBoundsOpInterfaceImpl.h"
53d47
< #include "mlir/Dialect/MemRef/Transforms/BufferizableOpInterfaceImpl.h"
62d55
< #include "mlir/Dialect/SCF/IR/ValueBoundsOpInterfaceImpl.h"
73,74d65
< #include "mlir/Dialect/Tensor/IR/ValueBoundsOpInterfaceImpl.h"
< #include "mlir/Dialect/Tensor/TransformOps/TensorTransformOps.h"
90c81
<                   affine::AffineDialect,
---
>                   AffineDialect,
104d94
<                   irdl::IRDLDialect,
136d125
<   tensor::registerTransformDialectExtension(registry);
140d128
<   affine::registerValueBoundsOpInterfaceExternalModels(registry);
142d129
<   arith::registerValueBoundsOpInterfaceExternalModels(registry);
147,148d133
<   linalg::registerValueBoundsOpInterfaceExternalModels(registry);
<   memref::registerBufferizableOpInterfaceExternalModels(registry);
150d134
<   memref::registerValueBoundsOpInterfaceExternalModels(registry);
152d135
<   scf::registerValueBoundsOpInterfaceExternalModels(registry);
158d140
<   tensor::registerValueBoundsOpInterfaceExternalModels(registry);
--- include/mlir/Dialect
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine and include/mlir/Dialect/Affine
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMDGPU and include/mlir/Dialect/AMDGPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMX and include/mlir/Dialect/AMX
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith and include/mlir/Dialect/Arith
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/ArmNeon and include/mlir/Dialect/ArmNeon
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/ArmSVE and include/mlir/Dialect/ArmSVE
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Async and include/mlir/Dialect/Async
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization and include/mlir/Dialect/Bufferization
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/CMakeLists.txt include/mlir/Dialect/CMakeLists.txt
16d15
< add_subdirectory(IRDL)
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/CommonFolders.h include/mlir/Dialect/CommonFolders.h
27d26
< /// Uses `resultType` for the type of the returned attribute.
33d31
<                                        Type resultType,
36c34
<   if (!resultType || !operands[0] || !operands[1])
---
>   if (!operands[0] || !operands[1])
50c48
<     return AttrElementT::get(resultType, *calRes);
---
>     return AttrElementT::get(lhs.getType(), *calRes);
67,70c65,67
<     return DenseElementsAttr::get(cast<ShapedType>(resultType), *elementResult);
<   }
< 
<   if (operands[0].isa<ElementsAttr>() && operands[1].isa<ElementsAttr>()) {
---
>     return DenseElementsAttr::get(lhs.getType(), *elementResult);
>   } else if (operands[0].isa<ElementsAttr>() &&
>              operands[1].isa<ElementsAttr>()) {
89c86
<     return DenseElementsAttr::get(cast<ShapedType>(resultType), elementResults);
---
>     return DenseElementsAttr::get(lhs.getType(), elementResults);
94,135d90
< /// Performs constant folding `calculate` with element-wise behavior on the two
< /// attributes in `operands` and returns the result if possible.
< /// Uses the operand element type for the element type of the returned
< /// attribute.
< template <class AttrElementT,
<           class ElementValueT = typename AttrElementT::ValueType,
<           class CalculationT = function_ref<
<               std::optional<ElementValueT>(ElementValueT, ElementValueT)>>
< Attribute constFoldBinaryOpConditional(ArrayRef<Attribute> operands,
<                                        const CalculationT &calculate) {
<   assert(operands.size() == 2 && "binary op takes two operands");
<   auto getResultType = [](Attribute attr) -> Type {
<     if (auto typed = attr.dyn_cast_or_null<TypedAttr>())
<       return typed.getType();
<     return {};
<   };
< 
<   Type lhsType = getResultType(operands[0]);
<   Type rhsType = getResultType(operands[1]);
<   if (!lhsType || !rhsType)
<     return {};
<   if (lhsType != rhsType)
<     return {};
< 
<   return constFoldBinaryOpConditional<AttrElementT, ElementValueT,
<                                       CalculationT>(operands, lhsType,
<                                                     calculate);
< }
< 
< template <class AttrElementT,
<           class ElementValueT = typename AttrElementT::ValueType,
<           class CalculationT =
<               function_ref<ElementValueT(ElementValueT, ElementValueT)>>
< Attribute constFoldBinaryOp(ArrayRef<Attribute> operands, Type resultType,
<                             const CalculationT &calculate) {
<   return constFoldBinaryOpConditional<AttrElementT>(
<       operands, resultType,
<       [&](ElementValueT a, ElementValueT b) -> std::optional<ElementValueT> {
<         return calculate(a, b);
<       });
< }
< 
192c147
<     return DenseElementsAttr::get(op.getShapedType(), elementResults);
---
>     return DenseElementsAttr::get(op.getType(), elementResults);
236c191
<     return DenseElementsAttr::get(cast<ShapedType>(resType), elementResult);
---
>     return DenseElementsAttr::get(resType, elementResult);
253c208
<     return DenseElementsAttr::get(cast<ShapedType>(resType), elementResults);
---
>     return DenseElementsAttr::get(resType, elementResults);
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Complex and include/mlir/Dialect/Complex
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/ControlFlow and include/mlir/Dialect/ControlFlow
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/DLTI and include/mlir/Dialect/DLTI
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/EmitC and include/mlir/Dialect/EmitC
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func and include/mlir/Dialect/Func
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU and include/mlir/Dialect/GPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Index and include/mlir/Dialect/Index
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect: IRDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg and include/mlir/Dialect/Linalg
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR and include/mlir/Dialect/LLVMIR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Math and include/mlir/Dialect/Math
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef and include/mlir/Dialect/MemRef
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MLProgram and include/mlir/Dialect/MLProgram
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU and include/mlir/Dialect/NVGPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC and include/mlir/Dialect/OpenACC
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP and include/mlir/Dialect/OpenMP
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDL and include/mlir/Dialect/PDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDLInterp and include/mlir/Dialect/PDLInterp
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Quant and include/mlir/Dialect/Quant
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF and include/mlir/Dialect/SCF
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape and include/mlir/Dialect/Shape
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor and include/mlir/Dialect/SparseTensor
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV and include/mlir/Dialect/SPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor and include/mlir/Dialect/Tensor
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa and include/mlir/Dialect/Tosa
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform and include/mlir/Dialect/Transform
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Utils and include/mlir/Dialect/Utils
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector and include/mlir/Dialect/Vector
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/X86Vector and include/mlir/Dialect/X86Vector
--- include/mlir/Dialect/AMDGPU
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMDGPU: AMDGPUDialect.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMDGPU: AMDGPU.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMDGPU/CMakeLists.txt include/mlir/Dialect/AMDGPU/CMakeLists.txt
1,12c1,2
< add_mlir_dialect(AMDGPU amdgpu)
< add_mlir_doc(AMDGPU AMDGPU Dialects/ -gen-dialect-doc)
< 
< set(LLVM_TARGET_DEFINITIONS AMDGPU.td)
< mlir_tablegen(AMDGPUEnums.h.inc -gen-enum-decls)
< mlir_tablegen(AMDGPUEnums.cpp.inc -gen-enum-defs)
< add_public_tablegen_target(MLIRAMDGPUEnumsGen)
< 
< set(LLVM_TARGET_DEFINITIONS AMDGPU.td)
< mlir_tablegen(AMDGPUAttributes.h.inc -gen-attrdef-decls -attrdefs-dialect=amdgpu)
< mlir_tablegen(AMDGPUAttributes.cpp.inc -gen-attrdef-defs -attrdefs-dialect=amdgpu)
< add_public_tablegen_target(MLIRAMDGPUAttributesIncGen)
---
> add_subdirectory(IR)
> add_subdirectory(Transforms)
Only in include/mlir/Dialect/AMDGPU: IR
Only in include/mlir/Dialect/AMDGPU: Transforms
Only in include/mlir/Dialect/AMDGPU: Utils
--- include/mlir/Dialect/AMDGPU/Utils
--- include/mlir/Dialect/AMDGPU/Utils/Chipset.h
--- include/mlir/Dialect/AMDGPU/CMakeLists.txt
1,12c1,2
< add_mlir_dialect(AMDGPU amdgpu)
< add_mlir_doc(AMDGPU AMDGPU Dialects/ -gen-dialect-doc)
< 
< set(LLVM_TARGET_DEFINITIONS AMDGPU.td)
< mlir_tablegen(AMDGPUEnums.h.inc -gen-enum-decls)
< mlir_tablegen(AMDGPUEnums.cpp.inc -gen-enum-defs)
< add_public_tablegen_target(MLIRAMDGPUEnumsGen)
< 
< set(LLVM_TARGET_DEFINITIONS AMDGPU.td)
< mlir_tablegen(AMDGPUAttributes.h.inc -gen-attrdef-decls -attrdefs-dialect=amdgpu)
< mlir_tablegen(AMDGPUAttributes.cpp.inc -gen-attrdef-defs -attrdefs-dialect=amdgpu)
< add_public_tablegen_target(MLIRAMDGPUAttributesIncGen)
---
> add_subdirectory(IR)
> add_subdirectory(Transforms)
--- include/mlir/Dialect/AMDGPU/Transforms
--- include/mlir/Dialect/AMDGPU/Transforms/Passes.h
--- include/mlir/Dialect/AMDGPU/Transforms/Passes.td
--- include/mlir/Dialect/AMDGPU/Transforms/CMakeLists.txt
--- include/mlir/Dialect/AMDGPU/IR
--- include/mlir/Dialect/AMDGPU/IR/AMDGPUDialect.h
--- include/mlir/Dialect/AMDGPU/IR/AMDGPU.td
--- include/mlir/Dialect/AMDGPU/IR/CMakeLists.txt
--- include/mlir/Dialect/LLVMIR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/CMakeLists.txt include/mlir/Dialect/LLVMIR/CMakeLists.txt
29,31c29,31
< set(LLVM_TARGET_DEFINITIONS LLVMInterfaces.td)
< mlir_tablegen(LLVMInterfaces.h.inc -gen-op-interface-decls)
< mlir_tablegen(LLVMInterfaces.cpp.inc -gen-op-interface-defs)
---
> set(LLVM_TARGET_DEFINITIONS LLVMOpsInterfaces.td)
> mlir_tablegen(LLVMOpsInterfaces.h.inc -gen-op-interface-decls)
> mlir_tablegen(LLVMOpsInterfaces.cpp.inc -gen-op-interface-defs)
34c34
< add_public_tablegen_target(MLIRLLVMInterfacesIncGen)
---
> add_public_tablegen_target(MLIRLLVMOpsInterfacesIncGen)
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/FunctionCallUtils.h include/mlir/Dialect/LLVMIR/FunctionCallUtils.h
37,38d36
< LLVM::LLVMFuncOp lookupOrCreatePrintF16Fn(ModuleOp moduleOp);
< LLVM::LLVMFuncOp lookupOrCreatePrintBF16Fn(ModuleOp moduleOp);
41,42c39
< LLVM::LLVMFuncOp lookupOrCreatePrintStrFn(ModuleOp moduleOp,
<                                           bool opaquePointers);
---
> LLVM::LLVMFuncOp lookupOrCreatePrintStrFn(ModuleOp moduleOp);
47,53c44,49
< LLVM::LLVMFuncOp lookupOrCreateMallocFn(ModuleOp moduleOp, Type indexType,
<                                         bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateAlignedAllocFn(ModuleOp moduleOp, Type indexType,
<                                               bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateFreeFn(ModuleOp moduleOp, bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateGenericAllocFn(ModuleOp moduleOp, Type indexType,
<                                               bool opaquePointers);
---
> LLVM::LLVMFuncOp lookupOrCreateMallocFn(ModuleOp moduleOp, Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateAlignedAllocFn(ModuleOp moduleOp,
>                                               Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateFreeFn(ModuleOp moduleOp);
> LLVM::LLVMFuncOp lookupOrCreateGenericAllocFn(ModuleOp moduleOp,
>                                               Type indexType);
55,58c51,52
<                                                      Type indexType,
<                                                      bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateGenericFreeFn(ModuleOp moduleOp,
<                                              bool opaquePointers);
---
>                                                      Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateGenericFreeFn(ModuleOp moduleOp);
65c59
<                                   Type resultType = {}, bool isVarArg = false);
---
>                                   Type resultType = {});
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMAttrDefs.td include/mlir/Dialect/LLVMIR/LLVMAttrDefs.td
12d11
< include "mlir/Dialect/LLVMIR/LLVMDialect.td"
13a13,15
> include "mlir/Dialect/LLVMIR/LLVMEnums.td"
> include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
> include "mlir/IR/SubElementInterfaces.td"
32a35,43
> // FastmathFlagsAttr
> //===----------------------------------------------------------------------===//
> 
> def LLVM_FastmathFlagsAttr :
>     EnumAttr<LLVM_Dialect, FastmathFlags, "fastmath"> {
>   let assemblyFormat = "`<` $value `>`";
> }
> 
> //===----------------------------------------------------------------------===//
42c53
< // Loop Attributes
---
> // LoopOptionsAttr
45,147c56
< def LoopVectorizeAttr : LLVM_Attr<"LoopVectorize", "loop_vectorize"> {
<   let description = [{
<     This attribute defines vectorization specific loop annotations that map to
<     the "!llvm.loop.vectorize" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"BoolAttr">:$predicateEnable,
<     OptionalParameter<"BoolAttr">:$scalableEnable,
<     OptionalParameter<"IntegerAttr">:$width,
<     OptionalParameter<"LoopAnnotationAttr">:$followupVectorized,
<     OptionalParameter<"LoopAnnotationAttr">:$followupEpilogue,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopInterleaveAttr : LLVM_Attr<"LoopInterleave", "loop_interleave"> {
<   let description = [{
<     This attribute defines interleaving specific loop annotations that map to
<     the "!llvm.loop.interleave" metadata.
<   }];
< 
<   let parameters = (ins
<     "IntegerAttr":$count
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnrollAttr : LLVM_Attr<"LoopUnroll", "loop_unroll"> {
<   let description = [{
<     This attribute defines unrolling specific loop annotations that map to
<     the "!llvm.loop.unroll" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$count,
<     OptionalParameter<"BoolAttr">:$runtimeDisable,
<     OptionalParameter<"BoolAttr">:$full,
<     OptionalParameter<"LoopAnnotationAttr">:$followupUnrolled,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainder,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnrollAndJamAttr : LLVM_Attr<"LoopUnrollAndJam", "loop_unroll_and_jam"> {
<   let description = [{
<     This attribute defines "unroll and jam" specific loop annotations that map to
<     the "!llvm.loop.unroll_and_jam" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$count,
<     OptionalParameter<"LoopAnnotationAttr">:$followupOuter,
<     OptionalParameter<"LoopAnnotationAttr">:$followupInner,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainderOuter,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainderInner,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopLICMAttr : LLVM_Attr<"LoopLICM", "loop_licm"> {
<   let description = [{
<     This attribute encapsulates loop invariant code motion (licm) specific loop
<     annotations. The fields correspond to the "!llvm.licm.disable" and the
<     "!llvm.loop.licm_versioning.disable" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"BoolAttr">:$versioningDisable
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopDistributeAttr : LLVM_Attr<"LoopDistribute", "loop_distribute"> {
<   let description = [{
<     This attribute defines distribution specific loop annotations that map to
<     the "!llvm.loop.distribute" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"LoopAnnotationAttr">:$followupCoincident,
<     OptionalParameter<"LoopAnnotationAttr">:$followupSequential,
<     OptionalParameter<"LoopAnnotationAttr">:$followupFallback,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopPipelineAttr : LLVM_Attr<"LoopPipeline", "loop_pipeline"> {
---
> def LoopOptionsAttr : LLVM_Attr<"LoopOptions", "loopopts"> {
149,189c58
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.pipeline" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$initiationinterval
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopPeeledAttr : LLVM_Attr<"LoopPeeled", "loop_peeled"> {
<   let description = [{
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.peeled" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"IntegerAttr">:$count
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnswitchAttr : LLVM_Attr<"LoopUnswitch", "loop_unswitch"> {
<   let description = [{
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.unswitch" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$partialDisable
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopAnnotationAttr : LLVM_Attr<"LoopAnnotation", "loop_annotation"> {
<   let description = [{
<     This attributes encapsulates "loop metadata". It is meant to decorate
---
>     This attributes encapsulates "loop options". It is means to decorate
192,193c61,63
<     It stores annotations in attribute parameters and groups related options in
<     nested attributes to provide structured access.
---
>     It store the options as a pair <enum,int64_t> in a sorted array and expose
>     APIs to retrieve the value for each option with a stronger type (bool for
>     example).
197,209c67
<     OptionalParameter<"BoolAttr">:$disableNonforced,
<     OptionalParameter<"LoopVectorizeAttr">:$vectorize,
<     OptionalParameter<"LoopInterleaveAttr">:$interleave,
<     OptionalParameter<"LoopUnrollAttr">:$unroll,
<     OptionalParameter<"LoopUnrollAndJamAttr">:$unrollAndJam,
<     OptionalParameter<"LoopLICMAttr">:$licm,
<     OptionalParameter<"LoopDistributeAttr">:$distribute,
<     OptionalParameter<"LoopPipelineAttr">:$pipeline,
<     OptionalParameter<"LoopPeeledAttr">:$peeled,
<     OptionalParameter<"LoopUnswitchAttr">:$unswitch,
<     OptionalParameter<"BoolAttr">:$mustProgress,
<     OptionalParameter<"BoolAttr">:$isVectorized,
<     OptionalArrayRefParameter<"SymbolRefAttr">:$parallelAccesses
---
>     ArrayRefParameter<"std::pair<LoopOptionCase, int64_t>", "">:$options
212c70,84
<   let assemblyFormat = "`<` struct(params) `>`";
---
>   let extraClassDeclaration = [{
>     using OptionValuePair = std::pair<LoopOptionCase, int64_t>;
>     using OptionsArray = ArrayRef<std::pair<LoopOptionCase, int64_t>>;
>     std::optional<bool> disableUnroll();
>     std::optional<bool> disableLICM();
>     std::optional<int64_t> interleaveCount();
>   }];
> 
>   let builders = [
>     /// Build the LoopOptions Attribute from a sorted array of individual options.
>     AttrBuilder<(ins "ArrayRef<std::pair<LoopOptionCase, int64_t>>":$sortedOptions)>,
>     AttrBuilder<(ins "LoopOptionsAttrBuilder &":$optionBuilders)>
>   ];
>   let hasCustomAssemblyFormat = 1;
>   let skipDefaultBuilders = 1;
254c126
< // DINullTypeAttr
---
> // DIVoidResultTypeAttr
257,258c129,130
< def LLVM_DINullTypeAttr : LLVM_Attr<"DINullType", "di_null_type",
<                                     /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DIVoidResultTypeAttr : LLVM_Attr<"DIVoidResultType", "di_void_result_type",
>                                           /*traits=*/[], "DITypeAttr"> {
291,292c163,165
< def LLVM_DICompileUnitAttr : LLVM_Attr<"DICompileUnit", "di_compile_unit",
<                                        /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DICompileUnitAttr : LLVM_Attr<"DICompileUnit", "di_compile_unit", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
296c169
<     OptionalParameter<"StringAttr">:$producer,
---
>     "StringAttr":$producer,
307,308c180,182
< def LLVM_DICompositeTypeAttr : LLVM_Attr<"DICompositeType", "di_composite_type",
<                                          /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DICompositeTypeAttr : LLVM_Attr<"DICompositeType", "di_composite_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
311c185
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
328,329c202,204
< def LLVM_DIDerivedTypeAttr : LLVM_Attr<"DIDerivedType", "di_derived_type",
<                                        /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DIDerivedTypeAttr : LLVM_Attr<"DIDerivedType", "di_derived_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
333c208
<     OptionalParameter<"DITypeAttr">:$baseType,
---
>     "DITypeAttr":$baseType,
359,360c234,236
< def LLVM_DILexicalBlockAttr : LLVM_Attr<"DILexicalBlock", "di_lexical_block",
<                                         /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DILexicalBlockAttr : LLVM_Attr<"DILexicalBlock", "di_lexical_block", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
372c248
<       return $_get(scope.getContext(), scope, file, line, column);
---
>       return $_get(file.getContext(), scope, file, line, column);
382,383c258,260
< def LLVM_DILexicalBlockFile : LLVM_Attr<"DILexicalBlockFile", "di_lexical_block_file",
<                                         /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DILexicalBlockFile : LLVM_Attr<"DILexicalBlockFile", "di_lexical_block_file", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
393c270
<       return $_get(scope.getContext(), scope, file, discriminator);
---
>       return $_get(file.getContext(), scope, file, discriminator);
403,404c280,282
< def LLVM_DILocalVariableAttr : LLVM_Attr<"DILocalVariable", "di_local_variable",
<                                          /*traits=*/[], "DINodeAttr"> {
---
> def LLVM_DILocalVariableAttr : LLVM_Attr<"DILocalVariable", "di_local_variable", [
>     SubElementAttrInterface
>   ], "DINodeAttr"> {
407c285
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
420c298
<       MLIRContext *ctx = scope.getContext();
---
>       MLIRContext *ctx = file.getContext();
432,433c310,312
< def LLVM_DISubprogramAttr : LLVM_Attr<"DISubprogram", "di_subprogram",
<                                       /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DISubprogramAttr : LLVM_Attr<"DISubprogram", "di_subprogram", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
435c314
<     OptionalParameter<"DICompileUnitAttr">:$compileUnit,
---
>     "DICompileUnitAttr":$compileUnit,
437c316
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
463,477d341
< // DINamespaceAttr
< //===----------------------------------------------------------------------===//
< 
< def LLVM_DINamespaceAttr : LLVM_Attr<"DINamespace", "di_namespace",
<                                       /*traits=*/[], "DIScopeAttr"> {
<   let parameters = (ins
<     OptionalParameter<"StringAttr">:$name,
<     OptionalParameter<"DIScopeAttr">:$scope,
<     "bool":$exportSymbols
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< //===----------------------------------------------------------------------===//
496,497c360,362
< def LLVM_DISubroutineTypeAttr : LLVM_Attr<"DISubroutineType", "di_subroutine_type",
<                                           /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DISubroutineTypeAttr : LLVM_Attr<"DISubroutineType", "di_subroutine_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
507a373
>   let genVerifyDecl = 1;
514c380,382
< def LLVM_MemoryEffectsAttr : LLVM_Attr<"MemoryEffects", "memory_effects"> {
---
> def LLVM_MemoryEffectsAttr : LLVM_Attr<"MemoryEffects", "memory_effects", [
>     SubElementAttrInterface
>   ]> {
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMAttrs.h include/mlir/Dialect/LLVMIR/LLVMAttrs.h
24a25
> class LoopOptionsAttrBuilder;
44,53d44
< /// This class represents a LLVM attribute that describes a local debug info
< /// scope.
< class DILocalScopeAttr : public DIScopeAttr {
< public:
<   using DIScopeAttr::DIScopeAttr;
< 
<   /// Support LLVM type casting.
<   static bool classof(Attribute attr);
< };
< 
74a66,115
> 
> namespace mlir {
> namespace LLVM {
> 
> /// Builder class for LoopOptionsAttr. This helper class allows to progressively
> /// build a LoopOptionsAttr one option at a time, and pay the price of attribute
> /// creation once all the options are in place.
> class LoopOptionsAttrBuilder {
> public:
>   /// Construct a empty builder.
>   LoopOptionsAttrBuilder() = default;
> 
>   /// Construct a builder with an initial list of options from an existing
>   /// LoopOptionsAttr.
>   LoopOptionsAttrBuilder(LoopOptionsAttr attr);
> 
>   /// Set the `disable_licm` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisableLICM(std::optional<bool> value);
> 
>   /// Set the `interleave_count` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setInterleaveCount(std::optional<uint64_t> count);
> 
>   /// Set the `disable_unroll` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisableUnroll(std::optional<bool> value);
> 
>   /// Set the `disable_pipeline` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisablePipeline(std::optional<bool> value);
> 
>   /// Set the `pipeline_initiation_interval` option to the provided value.
>   /// If no value is provided the option is deleted.
>   LoopOptionsAttrBuilder &
>   setPipelineInitiationInterval(std::optional<uint64_t> count);
> 
>   /// Returns true if any option has been set.
>   bool empty() { return options.empty(); }
> 
> private:
>   template <typename T>
>   LoopOptionsAttrBuilder &setOption(LoopOptionCase tag, std::optional<T> value);
> 
>   friend class LoopOptionsAttr;
>   SmallVector<LoopOptionsAttr::OptionValuePair> options;
> };
> 
> } // namespace LLVM
> } // namespace mlir
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMDialect.h include/mlir/Dialect/LLVMIR/LLVMDialect.h
18d17
< #include "mlir/Dialect/LLVMIR/LLVMInterfaces.h"
32d30
< #include "mlir/Transforms/Mem2Reg.h"
38a37,38
> #include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.h.inc"
> 
211,212c211
<                          StringRef value, Linkage linkage,
<                          bool useOpaquePointers);
---
>                          StringRef value, Linkage linkage);
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR: LLVMDialect.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMEnums.td include/mlir/Dialect/LLVMIR/LLVMEnums.td
9,10c9,10
< #ifndef LLVMIR_ENUMS
< #define LLVMIR_ENUMS
---
> #ifndef LLVMIR_ENUMS_TD
> #define LLVMIR_ENUMS_TD
12,66c12
< include "mlir/Dialect/LLVMIR/LLVMDialect.td"
< include "mlir/IR/EnumAttr.td"
< 
< //===----------------------------------------------------------------------===//
< // Base classes for LLVM enum attributes.
< //===----------------------------------------------------------------------===//
< 
< // Case of the LLVM enum attribute backed by I64Attr with customized string
< // representation that corresponds to what is visible in the textual IR form.
< // The parameters are as follows:
< //   - `cppSym`: name of the C++ enumerant for this case in MLIR API;
< //   - `irSym`: keyword used in the custom form of MLIR operation;
< //   - `llvmSym`: name of the C++ enumerant for this case in LLVM API.
< // For example, `LLVM_EnumAttrCase<"Weak", "weak", "WeakAnyLinkage">` is usable
< // as `<MlirEnumName>::Weak` in MLIR API, `WeakAnyLinkage` in LLVM API and
< // is printed/parsed as `weak` in MLIR custom textual format.
< class LLVM_EnumAttrCase<string cppSym, string irSym, string llvmSym, int val> :
<     I64EnumAttrCase<cppSym, val, irSym> {
<   // The name of the equivalent enumerant in LLVM.
<   string llvmEnumerant = llvmSym;
< }
< 
< // LLVM enum attribute backed by I64Attr with string representation
< // corresponding to what is visible in the textual IR form.
< // The parameters are as follows:
< //   - `name`: name of the C++ enum class in MLIR API;
< //   - `llvmName`: name of the C++ enum in LLVM API;
< //   - `description`: textual description for documentation purposes;
< //   - `cases`: list of enum cases;
< //   - `unsupportedCases`: optional list of unsupported enum cases.
< // For example, `LLVM_EnumAttr<Linkage, "::llvm::GlobalValue::LinkageTypes`
< // produces `mlir::LLVM::Linkage` enum class in MLIR API that corresponds to (a
< // subset of) values in the `llvm::GlobalValue::LinkageTypes` in LLVM API.
< // All unsupported cases are excluded from the MLIR enum and trigger an error
< // during the import from LLVM IR. They are useful to handle sentinel values
< // such as `llvm::AtomicRMWInst::BinOp::BAD_BINOP` that LLVM commonly uses to
< // terminate its enums.
< class LLVM_EnumAttr<string name, string llvmName, string description,
<                     list<LLVM_EnumAttrCase> cases,
<                     list<LLVM_EnumAttrCase> unsupportedCases = []> :
<     I64EnumAttr<name, description, cases> {
<   // List of unsupported cases that have no conversion to an MLIR value.
<   list<LLVM_EnumAttrCase> unsupported = unsupportedCases;
< 
<   // The equivalent enum class name in LLVM.
<   string llvmClassName = llvmName;
< }
< 
< // LLVM_CEnumAttr is functionally identical to LLVM_EnumAttr, but to be used for
< // non-class enums.
< class LLVM_CEnumAttr<string name, string llvmNS, string description,
<       list<LLVM_EnumAttrCase> cases> :
<     I64EnumAttr<name, description, cases> {
<   string llvmClassName = llvmNS;
< }
---
> include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
90,113c36,49
< def AtomicBinOpXchg : LLVM_EnumAttrCase<"xchg", "xchg", "Xchg", 0>;
< def AtomicBinOpAdd  : LLVM_EnumAttrCase<"add", "add", "Add", 1>;
< def AtomicBinOpSub  : LLVM_EnumAttrCase<"sub", "sub", "Sub", 2>;
< def AtomicBinOpAnd  : LLVM_EnumAttrCase<"_and", "_and", "And", 3>;
< def AtomicBinOpNand : LLVM_EnumAttrCase<"nand", "nand", "Nand", 4>;
< def AtomicBinOpOr   : LLVM_EnumAttrCase<"_or", "_or", "Or", 5>;
< def AtomicBinOpXor  : LLVM_EnumAttrCase<"_xor", "_xor", "Xor", 6>;
< def AtomicBinOpMax  : LLVM_EnumAttrCase<"max", "max", "Max", 7>;
< def AtomicBinOpMin  : LLVM_EnumAttrCase<"min", "min", "Min", 8>;
< def AtomicBinOpUMax : LLVM_EnumAttrCase<"umax", "umax", "UMax", 9>;
< def AtomicBinOpUMin : LLVM_EnumAttrCase<"umin", "umin", "UMin", 10>;
< def AtomicBinOpFAdd : LLVM_EnumAttrCase<"fadd", "fadd", "FAdd", 11>;
< def AtomicBinOpFSub : LLVM_EnumAttrCase<"fsub", "fsub", "FSub", 12>;
< def AtomicBinOpFMax : LLVM_EnumAttrCase<"fmax", "fmax", "FMax", 13>;
< def AtomicBinOpFMin : LLVM_EnumAttrCase<"fmin", "fmin", "FMin", 14>;
< def AtomicBinOpUIncWrap : LLVM_EnumAttrCase<"uinc_wrap",
<                                             "uinc_wrap", "UIncWrap", 15>;
< def AtomicBinOpUDecWrap : LLVM_EnumAttrCase<"udec_wrap",
<                                             "udec_wrap", "UDecWrap", 16>;
< 
< // A sentinel value that has no MLIR counterpart.
< def AtomicBadBinOp : LLVM_EnumAttrCase<"", "", "BAD_BINOP", 0>;
< 
< def AtomicBinOp : LLVM_EnumAttr<
---
> def AtomicBinOpXchg : I64EnumAttrCase<"xchg", 0>;
> def AtomicBinOpAdd  : I64EnumAttrCase<"add", 1>;
> def AtomicBinOpSub  : I64EnumAttrCase<"sub", 2>;
> def AtomicBinOpAnd  : I64EnumAttrCase<"_and", 3>;
> def AtomicBinOpNand : I64EnumAttrCase<"nand", 4>;
> def AtomicBinOpOr   : I64EnumAttrCase<"_or", 5>;
> def AtomicBinOpXor  : I64EnumAttrCase<"_xor", 6>;
> def AtomicBinOpMax  : I64EnumAttrCase<"max", 7>;
> def AtomicBinOpMin  : I64EnumAttrCase<"min", 8>;
> def AtomicBinOpUMax : I64EnumAttrCase<"umax", 9>;
> def AtomicBinOpUMin : I64EnumAttrCase<"umin", 10>;
> def AtomicBinOpFAdd : I64EnumAttrCase<"fadd", 11>;
> def AtomicBinOpFSub : I64EnumAttrCase<"fsub", 12>;
> def AtomicBinOp : I64EnumAttr<
115d50
<     "::llvm::AtomicRMWInst::BinOp",
120,122c55
<      AtomicBinOpFSub, AtomicBinOpFMax, AtomicBinOpFMin, AtomicBinOpUIncWrap,
<      AtomicBinOpUDecWrap],
<     [AtomicBadBinOp]> {
---
>      AtomicBinOpFSub]> {
126,140c59,66
< def AtomicOrderingNotAtomic : LLVM_EnumAttrCase<"not_atomic",
<                                                 "not_atomic", "NotAtomic", 0>;
< def AtomicOrderingUnordered : LLVM_EnumAttrCase<"unordered",
<                                                 "unordered", "Unordered", 1>;
< def AtomicOrderingMonotonic : LLVM_EnumAttrCase<"monotonic",
<                                                 "monotonic", "Monotonic", 2>;
< def AtomicOrderingAcquire   : LLVM_EnumAttrCase<"acquire",
<                                                 "acquire", "Acquire", 4>;
< def AtomicOrderingRelease   : LLVM_EnumAttrCase<"release",
<                                                 "release", "Release", 5>;
< def AtomicOrderingAcquireRelease :
<       LLVM_EnumAttrCase<"acq_rel", "acq_rel", "AcquireRelease", 6>;
< def AtomicOrderingSequentiallyConsistent :
<       LLVM_EnumAttrCase<"seq_cst", "seq_cst", "SequentiallyConsistent", 7>;
< def AtomicOrdering : LLVM_EnumAttr<
---
> def AtomicOrderingNotAtomic              : I64EnumAttrCase<"not_atomic", 0>;
> def AtomicOrderingUnordered              : I64EnumAttrCase<"unordered", 1>;
> def AtomicOrderingMonotonic              : I64EnumAttrCase<"monotonic", 2>;
> def AtomicOrderingAcquire                : I64EnumAttrCase<"acquire", 4>;
> def AtomicOrderingRelease                : I64EnumAttrCase<"release", 5>;
> def AtomicOrderingAcquireRelease         : I64EnumAttrCase<"acq_rel", 6>;
> def AtomicOrderingSequentiallyConsistent : I64EnumAttrCase<"seq_cst", 7>;
> def AtomicOrdering : I64EnumAttr<
142d67
<     "::llvm::AtomicOrdering",
146,147c71
<      AtomicOrderingSequentiallyConsistent
<     ]> {
---
>      AtomicOrderingSequentiallyConsistent]> {
208,210c132,133
< def CConvHHVM : LLVM_EnumAttrCase<"DUMMY_HHVM", "hhvmcc", "DUMMY_HHVM", 81>;
< def CConvHHVM_C
<     : LLVM_EnumAttrCase<"DUMMY_HHVM_C", "hhvm_ccc", "DUMMY_HHVM_C", 82>;
---
> def CConvHHVM : LLVM_EnumAttrCase<"HHVM", "hhvmcc", "HHVM", 81>;
> def CConvHHVM_C : LLVM_EnumAttrCase<"HHVM_C", "hhvm_ccc", "HHVM_C", 82>;
459,463d381
< def LLVM_FastmathFlagsAttr :
<     EnumAttr<LLVM_Dialect, FastmathFlags, "fastmath"> {
<   let assemblyFormat = "`<` $value `>`";
< }
< 
465,508c383,404
< // FCmp and ICmp Predicates
< //===----------------------------------------------------------------------===//
< 
< // Predicates for float comparisons
< def FCmpPredicateFALSE : LLVM_EnumAttrCase<"_false", "_false", "FCMP_FALSE", 0>;
< def FCmpPredicateOEQ   : LLVM_EnumAttrCase<"oeq", "oeq", "FCMP_OEQ", 1>;
< def FCmpPredicateOGT   : LLVM_EnumAttrCase<"ogt", "ogt", "FCMP_OGT", 2>;
< def FCmpPredicateOGE   : LLVM_EnumAttrCase<"oge", "oge", "FCMP_OGE", 3>;
< def FCmpPredicateOLT   : LLVM_EnumAttrCase<"olt", "olt", "FCMP_OLT", 4>;
< def FCmpPredicateOLE   : LLVM_EnumAttrCase<"ole", "ole", "FCMP_OLE", 5>;
< def FCmpPredicateONE   : LLVM_EnumAttrCase<"one", "one", "FCMP_ONE", 6>;
< def FCmpPredicateORD   : LLVM_EnumAttrCase<"ord", "ord", "FCMP_ORD", 7>;
< def FCmpPredicateUEQ   : LLVM_EnumAttrCase<"ueq", "ueq", "FCMP_UEQ", 8>;
< def FCmpPredicateUGT   : LLVM_EnumAttrCase<"ugt", "ugt", "FCMP_UGT", 9>;
< def FCmpPredicateUGE   : LLVM_EnumAttrCase<"uge", "uge", "FCMP_UGE", 10>;
< def FCmpPredicateULT   : LLVM_EnumAttrCase<"ult", "ult", "FCMP_ULT", 11>;
< def FCmpPredicateULE   : LLVM_EnumAttrCase<"ule", "ule", "FCMP_ULE", 12>;
< def FCmpPredicateUNE   : LLVM_EnumAttrCase<"une", "une", "FCMP_UNE", 13>;
< def FCmpPredicateUNO   : LLVM_EnumAttrCase<"uno", "uno", "FCMP_UNO", 14>;
< def FCmpPredicateTRUE  : LLVM_EnumAttrCase<"_true", "_true", "FCMP_TRUE", 15>;
< 
< // A sentinel value that has no MLIR counterpart.
< def ICmpPredicateBad : LLVM_EnumAttrCase<"", "", "BAD_ICMP_PREDICATE", 0>;
< 
< // Predicates for integer comparisons.
< def ICmpPredicateEQ  : LLVM_EnumAttrCase<"eq", "eq", "ICMP_EQ", 0>;
< def ICmpPredicateNE  : LLVM_EnumAttrCase<"ne", "ne", "ICMP_NE", 1>;
< def ICmpPredicateSLT : LLVM_EnumAttrCase<"slt", "slt", "ICMP_SLT", 2>;
< def ICmpPredicateSLE : LLVM_EnumAttrCase<"sle", "sle", "ICMP_SLE", 3>;
< def ICmpPredicateSGT : LLVM_EnumAttrCase<"sgt", "sgt", "ICMP_SGT", 4>;
< def ICmpPredicateSGE : LLVM_EnumAttrCase<"sge", "sge", "ICMP_SGE", 5>;
< def ICmpPredicateULT : LLVM_EnumAttrCase<"ult", "ult", "ICMP_ULT", 6>;
< def ICmpPredicateULE : LLVM_EnumAttrCase<"ule", "ule", "ICMP_ULE", 7>;
< def ICmpPredicateUGT : LLVM_EnumAttrCase<"ugt", "ugt", "ICMP_UGT", 8>;
< def ICmpPredicateUGE : LLVM_EnumAttrCase<"uge", "uge", "ICMP_UGE", 9>;
< 
< // A sentinel value that has no MLIR counterpart.
< def FCmpPredicateBad : LLVM_EnumAttrCase<"", "", "BAD_FCMP_PREDICATE", 0>;
< 
< // LLVM's predicate enum contains the floating-point and integer comparison
< // cases, while the LLVM dialect uses two separate enums. The floating-point
< // predicate enum thus defines all integer predicates as unsupported and
< // vice versa.
< def FCmpPredicate : LLVM_EnumAttr<
---
> // FCmp Predicates
> //===----------------------------------------------------------------------===//
> 
> // Predicate for float comparisons
> def FCmpPredicateFALSE  : I64EnumAttrCase<"_false", 0>;
> def FCmpPredicateOEQ    : I64EnumAttrCase<"oeq", 1>;
> def FCmpPredicateOGT    : I64EnumAttrCase<"ogt", 2>;
> def FCmpPredicateOGE    : I64EnumAttrCase<"oge", 3>;
> def FCmpPredicateOLT    : I64EnumAttrCase<"olt", 4>;
> def FCmpPredicateOLE    : I64EnumAttrCase<"ole", 5>;
> def FCmpPredicateONE    : I64EnumAttrCase<"one", 6>;
> def FCmpPredicateORD    : I64EnumAttrCase<"ord", 7>;
> def FCmpPredicateUEQ    : I64EnumAttrCase<"ueq", 8>;
> def FCmpPredicateUGT    : I64EnumAttrCase<"ugt", 9>;
> def FCmpPredicateUGE    : I64EnumAttrCase<"uge", 10>;
> def FCmpPredicateULT    : I64EnumAttrCase<"ult", 11>;
> def FCmpPredicateULE    : I64EnumAttrCase<"ule", 12>;
> def FCmpPredicateUNE    : I64EnumAttrCase<"une", 13>;
> def FCmpPredicateUNO    : I64EnumAttrCase<"uno", 14>;
> def FCmpPredicateTRUE   : I64EnumAttrCase<"_true", 15>;
> 
> def FCmpPredicate : I64EnumAttr<
510d405
<     "::llvm::CmpInst::Predicate",
515,518c410
<      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE],
<     [ICmpPredicateEQ, ICmpPredicateNE, ICmpPredicateSLT, ICmpPredicateSLE,
<      ICmpPredicateSGT, ICmpPredicateSGE, ICmpPredicateULT, ICmpPredicateULE,
<      ICmpPredicateUGT, ICmpPredicateUGE, FCmpPredicateBad, ICmpPredicateBad
---
>      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE
523c415,430
< def ICmpPredicate : LLVM_EnumAttr<
---
> //===----------------------------------------------------------------------===//
> // ICmp Predicates
> //===----------------------------------------------------------------------===//
> 
> // Predicate for integer comparisons.
> def ICmpPredicateEQ  : I64EnumAttrCase<"eq", 0>;
> def ICmpPredicateNE  : I64EnumAttrCase<"ne", 1>;
> def ICmpPredicateSLT : I64EnumAttrCase<"slt", 2>;
> def ICmpPredicateSLE : I64EnumAttrCase<"sle", 3>;
> def ICmpPredicateSGT : I64EnumAttrCase<"sgt", 4>;
> def ICmpPredicateSGE : I64EnumAttrCase<"sge", 5>;
> def ICmpPredicateULT : I64EnumAttrCase<"ult", 6>;
> def ICmpPredicateULE : I64EnumAttrCase<"ule", 7>;
> def ICmpPredicateUGT : I64EnumAttrCase<"ugt", 8>;
> def ICmpPredicateUGE : I64EnumAttrCase<"uge", 9>;
> def ICmpPredicate : I64EnumAttr<
525,526c432
<     "::llvm::CmpInst::Predicate",
<     "lvm.icmp comparison predicate",
---
>     "llvm.icmp comparison predicate",
529,535c435
<      ICmpPredicateUGT, ICmpPredicateUGE],
<     [FCmpPredicateFALSE, FCmpPredicateOEQ, FCmpPredicateOGT, FCmpPredicateOGE,
<      FCmpPredicateOLT, FCmpPredicateOLE, FCmpPredicateONE, FCmpPredicateORD,
<      FCmpPredicateUEQ, FCmpPredicateUGT, FCmpPredicateUGE, FCmpPredicateULT,
<      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE,
<      FCmpPredicateBad, ICmpPredicateBad
<     ]> {
---
>      ICmpPredicateUGT, ICmpPredicateUGE]> {
592a493,511
> // LoopOptions
> //===----------------------------------------------------------------------===//
> 
> def LOptDisableUnroll : I32EnumAttrCase<"disable_unroll", 1>;
> def LOptDisableLICM : I32EnumAttrCase<"disable_licm", 2>;
> def LOptInterleaveCount : I32EnumAttrCase<"interleave_count", 3>;
> def LOptDisablePipeline : I32EnumAttrCase<"disable_pipeline", 4>;
> def LOptPipelineInitiationInterval : I32EnumAttrCase<"pipeline_initiation_interval", 5>;
> 
> def LoopOptionCase : I32EnumAttr<
>     "LoopOptionCase",
>     "LLVM loop option",
>     [LOptDisableUnroll, LOptDisableLICM, LOptInterleaveCount,
>      LOptDisablePipeline, LOptPipelineInitiationInterval
>     ]> {
>   let cppNamespace = "::mlir::LLVM";
> }
> 
> //===----------------------------------------------------------------------===//
609,627d527
< // Visibility
< //===----------------------------------------------------------------------===//
< 
< def VisibilityDefault
<     : LLVM_EnumAttrCase<"Default", "", "DefaultVisibility", 0>;
< def VisibilityHidden
<     : LLVM_EnumAttrCase<"Hidden", "hidden", "HiddenVisibility", 1>;
< def VisibilityProtected
<     : LLVM_EnumAttrCase<"Protected", "protected", "ProtectedVisibility", 2>;
< 
< def Visibility : LLVM_EnumAttr<
<     "Visibility",
<     "::llvm::GlobalValue::VisibilityTypes",
<     "LLVM GlobalValue Visibility",
<     [VisibilityDefault, VisibilityHidden, VisibilityProtected]> {
<   let cppNamespace = "::mlir::LLVM";
< }
< 
< //===----------------------------------------------------------------------===//
644c544
< #endif // LLVMIR_ENUMS
---
> #endif // LLVMIR_ENUMS_TD
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR: LLVMInterfaces.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR: LLVMInterfaces.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMIntrinsicOps.td include/mlir/Dialect/LLVMIR/LLVMIntrinsicOps.td
6d5
< include "mlir/Dialect/LLVMIR/LLVMEnums.td"
7a7
> include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
9d8
< include "mlir/Interfaces/Mem2RegInterfaces.td"
62,63c61
< class LLVM_TernarySameArgsIntrOpBase<string func, Type element,
<               list<Trait> traits = [], bit requiresFastmath = 0> :
---
> class LLVM_TernarySameArgsIntrOpF<string func, list<Trait> traits = []> :
66,69c64,69
<            requiresFastmath> {
<   dag commonArgs = (ins LLVM_ScalarOrVectorOf<element>:$a,
<                        LLVM_ScalarOrVectorOf<element>:$b,
<                        LLVM_ScalarOrVectorOf<element>:$c);
---
>            /*requiresFastmath=*/1> {
>   let arguments = (ins LLVM_ScalarOrVectorOf<AnyFloat>:$a,
>                        LLVM_ScalarOrVectorOf<AnyFloat>:$b,
>                        LLVM_ScalarOrVectorOf<AnyFloat>:$c,
>                        DefaultValuedAttr<LLVM_FastmathFlagsAttr,
>                                          "{}">:$fastmathFlags);
74,86d73
< class LLVM_TernarySameArgsIntrOpI<string func, list<Trait> traits = []> :
<     LLVM_TernarySameArgsIntrOpBase<func, AnySignlessInteger, traits> {
<   let arguments = commonArgs;
< }
< 
< class LLVM_TernarySameArgsIntrOpF<string func, list<Trait> traits = []> :
<     LLVM_TernarySameArgsIntrOpBase<func, LLVM_AnyFloat, traits,
<                                   /*requiresFastmath=*/1> {
<   dag fmfArg = (
<     ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
<   let arguments = !con(commonArgs, fmfArg);
< }
< 
125c112,113
<                                        [Pure], /*requiresFastmath=*/1> {
---
>     [DeclareOpInterfaceMethods<FastmathFlagsInterface>, Pure],
>     /*requiresFastmath=*/1> {
134d121
< def LLVM_ByteSwapOp : LLVM_UnaryIntrOpI<"bswap">;
138,139d124
< def LLVM_FshlOp : LLVM_TernarySameArgsIntrOpI<"fshl">;
< def LLVM_FshrOp : LLVM_TernarySameArgsIntrOpI<"fshr">;
149,209c134,152
< class LLVM_MemcpyIntrOpBase<string name> :
<     LLVM_ZeroResultIntrOp<name, [0, 1, 2], [], /*requiresAccessGroup=*/1,
<                                                /*requiresAliasAnalysis=*/1> {
<   dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
<                   Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
<                   AnySignlessInteger:$len, I1:$isVolatile);
<   // Append the alias attributes defined by LLVM_IntrOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let builders = [
<     OpBuilder<(ins "Value":$dst, "Value":$src, "Value":$len,
<                    "Value":$isVolatile), [{
<       build($_builder, $_state, dst, src, len, isVolatile,
<             /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
<             /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
<     }]
<   >];
< }
< 
< def LLVM_MemcpyOp : LLVM_MemcpyIntrOpBase<"memcpy">;
< def LLVM_MemcpyInlineOp : LLVM_MemcpyIntrOpBase<"memcpy.inline">;
< def LLVM_MemmoveOp : LLVM_MemcpyIntrOpBase<"memmove">;
< 
< def LLVM_MemsetOp : LLVM_ZeroResultIntrOp<"memset", [0, 2], [],
<                       /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1> {
<   dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
<                   I8:$val, AnySignlessInteger:$len, I1:$isVolatile);
<   // Append the alias attributes defined by LLVM_IntrOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let builders = [
<     OpBuilder<(ins "Value":$dst, "Value":$val, "Value":$len,
<                     "Value":$isVolatile), [{
<       build($_builder, $_state, dst, val, len, isVolatile,
<             /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
<             /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
<     }]
<   >];
< }
< 
< def LLVM_NoAliasScopeDeclOp
<     : LLVM_ZeroResultIntrOp<"experimental.noalias.scope.decl"> {
<   let arguments = (ins SymbolRefAttr:$scope);
<   string llvmBuilder = [{
<     // Wrap the scope argument into a list since the LLVM IR intrinsic takes
<     // a list containing exactly one scope rather than a scope itself.
<     llvm::MDNode* node = moduleTranslation.getAliasScopes(op, {$scope});
<     builder.CreateNoAliasScopeDeclaration(node);
<   }];
<   string mlirBuilder = [{
<     FailureOr<SmallVector<SymbolRefAttr>> scopeAttrs =
<       moduleImport.matchAliasScopeAttrs(llvmOperands[0]);
<     // Drop the intrinsic if the alias scope translation fails since the scope
<     // is not used by an aliasing operation, such as a load or store, that is
<     // used to convert the alias scope metadata.
<     if (failed(scopeAttrs))
<       return success();
<     if (scopeAttrs->size() != 1)
<       return failure();
<     $_op = $_builder.create<LLVM::NoAliasScopeDeclOp>(
<       $_location, (*scopeAttrs)[0]);
<   }];
<   let assemblyFormat = "$scope attr-dict";
---
> def LLVM_MemcpyOp : LLVM_ZeroResultIntrOp<"memcpy", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> def LLVM_MemcpyInlineOp : LLVM_ZeroResultIntrOp<"memcpy.inline", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> def LLVM_MemmoveOp : LLVM_ZeroResultIntrOp<"memmove", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> 
> def LLVM_MemsetOp : LLVM_ZeroResultIntrOp<"memset", [0, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        I8:$val, AnySignlessInteger:$len, I1:$isVolatile);
218,219c161
< class LLVM_LifetimeBaseOp<string opName> : LLVM_ZeroResultIntrOp<opName, [],
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> class LLVM_LifetimeBaseOp<string opName> : LLVM_ZeroResultIntrOp<opName> {
278c220
<     " attr-dict `:` functional-type(operands, results)";
---
>     " attr-dict `:` type($res)";
284c226
<   let assemblyFormat = "$token `,` $mem attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$token `,` $mem attr-dict `:` type($res)";
297c239
<   let assemblyFormat = "$handle attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$handle attr-dict `:` type($res)";
309c251
<   let assemblyFormat = "$handle `,` $unwind attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$handle `,` $unwind attr-dict `:` type($res)";
315c257
<   let assemblyFormat = "$id `,` $handle attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$id `,` $handle attr-dict `:` type($res)";
320c262
<   let assemblyFormat = "$handle attr-dict `:` qualified(type($handle))";
---
>   let assemblyFormat = "$handle attr-dict";
327,328c269
< class LLVM_DbgIntrOp<string name, string argName, list<Trait> traits = []>
<     : LLVM_IntrOp<name, [], [], traits, 0> {
---
> class LLVM_DbgIntrOp<string name, string argName> : LLVM_IntrOp<name, [], [], [], 0> {
353,357c294
< 
<     FailureOr<Value> argOperand = moduleImport.convertMetadataValue(llvmOperands[0]);
<     // Drop the intrinsic when its operand could not be converted. This can
<     // happen for use before definition cases that are allowed for debug
<     // intrinsics.
---
>     FailureOr<Value> argOperand = moduleImport.convertValue(llvmOperands[0]);
359c296
<       return success();
---
>       return failure();
369,370c306,311
< def LLVM_DbgDeclareOp : LLVM_DbgIntrOp< "dbg.declare", "addr",
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_DbgAddrOp : LLVM_DbgIntrOp<"dbg.addr", "addr"> {
>   let summary = "Describe the current address of a local debug info variable.";
>   let arguments = (ins LLVM_AnyPointer:$addr, LLVM_DILocalVariableAttr:$varInfo);
> }
> 
> def LLVM_DbgDeclareOp : LLVM_DbgIntrOp<"dbg.declare", "addr"> {
386c327
<   let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
---
>   let assemblyFormat = "$arg_list attr-dict";
392c333
<   let assemblyFormat = "$src_list `to` $dest_list attr-dict `:` type(operands)";
---
>   let assemblyFormat = "$src_list `to` $dest_list attr-dict";
398c339
<   let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
---
>   let assemblyFormat = "$arg_list attr-dict";
408c349
<     let assemblyFormat = "$type_info attr-dict `:` functional-type(operands, results)";
---
>     let assemblyFormat = "$type_info attr-dict `:` type($res)";
421c362
<   let assemblyFormat = "$ptr attr-dict `:` qualified(type($ptr))";
---
>   let assemblyFormat = "$ptr attr-dict";
429c370
< class LLVM_VecReductionBase<string mnem, Type element, bit requiresFastmath=0>
---
> class LLVM_VecReductionBase<string mnem, Type element>
431,434c372,373
<                            [Pure, SameOperandsAndResultElementType],
<                            requiresFastmath> {
<       dag commonArgs = (ins LLVM_VectorOf<element>:$in);
< }
---
>                            [Pure, SameOperandsAndResultElementType]>,
>       Arguments<(ins LLVM_VectorOf<element>)>;
437,444c376
<     : LLVM_VecReductionBase<mnem, AnyFloat, /*requiresFastmath=*/1> {
<   dag fmfArg = (
<     ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
<   let arguments = !con(commonArgs, fmfArg);
< 
<   let assemblyFormat = "`(` operands `)` custom<LLVMOpAttrs>(attr-dict) `:` "
<       "functional-type(operands, results)";
< }
---
>     : LLVM_VecReductionBase<mnem, AnyFloat>;
447,449c379
<     : LLVM_VecReductionBase<mnem, AnySignlessInteger> {
<       let arguments = commonArgs;
< }
---
>     : LLVM_VecReductionBase<mnem, AnySignlessInteger>;
898,899c828
< def LLVM_VPFMulAddOp  : LLVM_VPTernaryF<"fmuladd">;
< def LLVM_VPFmaOp      : LLVM_VPTernaryF<"fma">;
---
> def LLVM_VPFmaOp  : LLVM_VPTernaryF<"fma">;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMOpBase.td include/mlir/Dialect/LLVMIR/LLVMOpBase.td
17,18c17,18
< include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
< include "mlir/Dialect/LLVMIR/LLVMInterfaces.td"
---
> include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.td"
> include "mlir/IR/EnumAttr.td"
22a23,103
> // LLVM Dialect.
> //===----------------------------------------------------------------------===//
> 
> def LLVM_Dialect : Dialect {
>   let name = "llvm";
>   let cppNamespace = "::mlir::LLVM";
> 
>   let useDefaultAttributePrinterParser = 1;
>   let hasRegionArgAttrVerify = 1;
>   let hasRegionResultAttrVerify = 1;
>   let hasOperationAttrVerify = 1;
>   let useFoldAPI = kEmitFoldAdaptorFolder;
> 
>   let extraClassDeclaration = [{
>     /// Name of the data layout attributes.
>     static StringRef getDataLayoutAttrName() { return "llvm.data_layout"; }
>     static StringRef getAlignAttrName() { return "llvm.align"; }
>     static StringRef getNoAliasAttrName() { return "llvm.noalias"; }
>     static StringRef getReadonlyAttrName() { return "llvm.readonly"; }
>     static StringRef getNoAliasScopesAttrName() { return "noalias_scopes"; }
>     static StringRef getAliasScopesAttrName() { return "alias_scopes"; }
>     static StringRef getLoopAttrName() { return "llvm.loop"; }
>     static StringRef getParallelAccessAttrName() { return "parallel_access"; }
>     static StringRef getLoopOptionsAttrName() { return "options"; }
>     static StringRef getAccessGroupsAttrName() { return "access_groups"; }
>     static StringRef getStructAttrsAttrName() { return "llvm.struct_attrs"; }
>     static StringRef getByValAttrName() { return "llvm.byval"; }
>     static StringRef getByRefAttrName() { return "llvm.byref"; }
>     static StringRef getStructRetAttrName() { return "llvm.sret"; }
>     static StringRef getInAllocaAttrName() { return "llvm.inalloca"; }
>     static StringRef getNoUndefAttrName() { return "llvm.noundef"; }
>     static StringRef getSExtAttrName() { return "llvm.signext"; }
>     static StringRef getZExtAttrName() { return "llvm.zeroext"; }
>     static StringRef getTBAAAttrName() { return "llvm.tbaa"; }
> 
>     /// Verifies if the attribute is a well-formed value for "llvm.struct_attrs"
>     static LogicalResult verifyStructAttr(
>         Operation *op, Attribute attr, Type annotatedType);
> 
>     /// Verifies if the given string is a well-formed data layout descriptor.
>     /// Uses `reportError` to report errors.
>     static LogicalResult verifyDataLayoutString(
>         StringRef descr, llvm::function_ref<void (const Twine &)> reportError);
> 
>     /// Name of the target triple attribute.
>     static StringRef getTargetTripleAttrName() { return "llvm.target_triple"; }
> 
>     /// Name of the C wrapper emission attribute.
>     static StringRef getEmitCWrapperAttrName() {
>       return "llvm.emit_c_interface";
>     }
> 
>     /// Returns `true` if the given type is compatible with the LLVM dialect.
>     static bool isCompatibleType(Type);
> 
>     /// TODO Remove this once there is an alternative way of modeling memory
>     /// effects on FunctionOpInterface.
>     /// Name of the attribute that will cause the creation of a readnone memory
>     /// effect when lowering to the LLVMDialect.
>     static StringRef getReadnoneAttrName() {
>       return "llvm.readnone";
>     }
> 
>     Type parseType(DialectAsmParser &p) const override;
>     void printType(Type, DialectAsmPrinter &p) const override;
> 
>   private:
>     /// Register all types.
>     void registerTypes();
> 
>     /// A cache storing compatible LLVM types that have been verified. This
>     /// can save us lots of verification time if there are many occurrences
>     /// of some deeply-nested aggregate types in the program.
>     ThreadLocalCache<DenseSet<Type>> compatibleTypes;
> 
>     /// Register the attributes of this dialect.
>     void registerAttributes();
>   }];
> }
> 
> //===----------------------------------------------------------------------===//
71,74c152,156
<   And<[LLVM_PointerTo<I<width>>.predicate,
<        CPred<"$_self.cast<::mlir::LLVM::LLVMPointerType>().getAddressSpace()"
<              " == " # addressSpace>]>,
<   "LLVM pointer to " # I<width>.summary>;
---
>   LLVM_PointerTo<I<width>>.predicate,
>   "LLVM pointer to " # I<width>.summary>,
>   BuildableType<"::mlir::LLVM::LLVMPointerType::get("
>                 "::mlir::IntegerType::get($_builder.getContext(), "
>                 # width #"), "# addressSpace #")">;
182c264
< // Patterns for LLVM dialect operations.
---
> // Base classes for LLVM dialect operations.
185,224c267,320
< // Patterns with code to set flags and metadata of memory operations after their
< // translation to LLVM IR instructions. Operations may use the patterns to
< // implement their "llvmBuilder". The patterns assume the `op` and `inst`
< // variables exist and refer to the original MLIR operation and the translated
< // LLVM IR instruction, respectively.
< class LLVM_MemOpPatterns {
<   code setAlignmentCode = [{
<     if ($alignment.has_value()) {
<       auto align = *$alignment;
<       if (align != 0)
<         inst->setAlignment(llvm::Align(align));
<     }
<   }];
<   code setVolatileCode = [{
<     inst->setVolatile($volatile_);
<   }];
<   code setSyncScopeCode = [{
<     if ($syncscope.has_value()) {
<       llvm::LLVMContext &llvmContext = builder.getContext();
<       inst->setSyncScopeID(llvmContext.getOrInsertSyncScopeID(*$syncscope));
<     }
<   }];
<   code setOrderingCode = [{
<     inst->setAtomic(convertAtomicOrderingToLLVM($ordering));
<   }];
<   code setNonTemporalMetadataCode = [{
<     if ($nontemporal) {
<       llvm::MDNode *metadata = llvm::MDNode::get(
<           inst->getContext(), llvm::ConstantAsMetadata::get(
<               builder.getInt32(1)));
<       inst->setMetadata(llvm::LLVMContext::MD_nontemporal, metadata);
<     }
<   }];
<   code setAccessGroupsMetadataCode = [{
<     moduleTranslation.setAccessGroupsMetadata(op, inst);
<   }];
<   code setAliasAnalysisMetadataCode = [{
<     moduleTranslation.setAliasScopeMetadata(op, inst);
<     moduleTranslation.setTBAAMetadata(op, inst);
<   }];
---
> // Base class for LLVM operations. All operations get an "llvm." prefix in
> // their name automatically. LLVM operations have either zero or one result,
> // this class is specialized below for both cases and should not be used
> // directly.
> class LLVM_Op<string mnemonic, list<Trait> traits = []> :
>     LLVM_OpBase<LLVM_Dialect, mnemonic, traits>;
> 
> // Case of the LLVM enum attribute backed by I64Attr with customized string
> // representation that corresponds to what is visible in the textual IR form.
> // The parameters are as follows:
> //   - `cppSym`: name of the C++ enumerant for this case in MLIR API;
> //   - `irSym`: keyword used in the custom form of MLIR operation;
> //   - `llvmSym`: name of the C++ enumerant for this case in LLVM API.
> // For example, `LLVM_EnumAttrCase<"Weak", "weak", "WeakAnyLinkage">` is usable
> // as `<MlirEnumName>::Weak` in MLIR API, `WeakAnyLinkage` in LLVM API and
> // is printed/parsed as `weak` in MLIR custom textual format.
> class LLVM_EnumAttrCase<string cppSym, string irSym, string llvmSym, int val> :
>     I64EnumAttrCase<cppSym, val, irSym> {
> 
>   // The name of the equivalent enumerant in LLVM.
>   string llvmEnumerant = llvmSym;
> }
> 
> // LLVM enum attribute backed by I64Attr with string representation
> // corresponding to what is visible in the textual IR form.
> // The parameters are as follows:
> //   - `name`: name of the C++ enum class in MLIR API;
> //   - `llvmName`: name of the C++ enum in LLVM API;
> //   - `description`: textual description for documentation purposes;
> //   - `cases`: list of enum cases.
> // For example, `LLVM_EnumAttr<Linkage, "::llvm::GlobalValue::LinkageTypes`
> // produces `mlir::LLVM::Linkage` enum class in MLIR API that corresponds to (a
> // subset of) values in the `llvm::GlobalValue::LinkageTypes` in LLVM API.
> class LLVM_EnumAttr<string name, string llvmName, string description,
>                     list<LLVM_EnumAttrCase> cases> :
>     I64EnumAttr<name, description, cases> {
> 
>   // The equivalent enum class name in LLVM.
>   string llvmClassName = llvmName;
> }
> 
> // LLVM_CEnumAttr is functionally identical to LLVM_EnumAttr, but to be used for
> // non-class enums.
> class LLVM_CEnumAttr<string name, string llvmNS, string description,
>       list<LLVM_EnumAttrCase> cases> :
>     I64EnumAttr<name, description, cases> {
>   string llvmClassName = llvmNS;
> }
> 
> // For every value in the list, substitutes the value in the place of "$0" in
> // "pattern" and stores the list of strings as "lst".
> class ListIntSubst<string pattern, list<int> values> {
>   list<string> lst = !foreach(x, values,
>                               !subst("$0", !cast<string>(x), pattern));
241,270d336
< // For every value in the list, substitutes the value in the place of "$0" in
< // "pattern" and stores the list of strings as "lst".
< class ListIntSubst<string pattern, list<int> values> {
<   list<string> lst = !foreach(x, values,
<                               !subst("$0", !cast<string>(x), pattern));
< }
< 
< //===----------------------------------------------------------------------===//
< // Base classes for LLVM dialect operations.
< //===----------------------------------------------------------------------===//
< 
< // Base class for LLVM operations. All operations get an "llvm." prefix in
< // their name automatically and should either have zero or one result.
< class LLVM_Op<string mnemonic, list<Trait> traits = []> :
<     LLVM_OpBase<LLVM_Dialect, mnemonic, traits>;
< 
< // Base class for LLVM memory access operations that implement the access group
< // and alias analysis interfaces. The "aliasAttrs" list contains the arguments
< // required by the access group and alias analysis interfaces. Derived
< // operations should append the "aliasAttrs" to their argument list.
< class LLVM_MemAccessOpBase<string mnemonic, list<Trait> traits = []> :
<     LLVM_Op<mnemonic, !listconcat([
<       DeclareOpInterfaceMethods<AccessGroupOpInterface>,
<       DeclareOpInterfaceMethods<AliasAnalysisOpInterface>], traits)>,
<     LLVM_MemOpPatterns {
<   dag aliasAttrs = (ins OptionalAttr<SymbolRefArrayAttr>:$access_groups,
<                     OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
<                     OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
<                     OptionalAttr<SymbolRefArrayAttr>:$tbaa);
< }
286,291c352
< // translation starting from an LLVM IR intrinsic. The "requiresAccessGroup",
< // "requiresAliasAnalysis", and "requiresFastmath" flags specify which
< // interfaces the intrinsic implements. If the corresponding flags are set, the
< // "aliasAttrs" list contains the arguments required by the access group and
< // alias analysis interfaces. Derived intrinsics should append the "aliasAttrs"
< // to their argument list if they set one of the flags.
---
> // translation starting from an LLVM IR intrinsic.
295c356
<                       bit requiresAccessGroup = 0, bit requiresAliasAnalysis = 0,
---
>                       bit requiresAccessGroup = 0, bit requiresAliasScope = 0,
298,301d358
<         !if(!gt(requiresAccessGroup, 0),
<             [DeclareOpInterfaceMethods<AccessGroupOpInterface>], []),
<         !if(!gt(requiresAliasAnalysis, 0),
<             [DeclareOpInterfaceMethods<AliasAnalysisOpInterface>], []),
303c360,361
<             [DeclareOpInterfaceMethods<FastmathFlagsInterface>], []),
---
>             [DeclareOpInterfaceMethods<FastmathFlagsInterface>],
>             []),
305d362
<       LLVM_MemOpPatterns,
307,315d363
<   dag aliasAttrs = !con(
<         !if(!gt(requiresAccessGroup, 0),
<             (ins OptionalAttr<SymbolRefArrayAttr>:$access_groups),
<             (ins )),
<         !if(!gt(requiresAccessGroup, 0),
<             (ins OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
<                  OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
<                  OptionalAttr<SymbolRefArrayAttr>:$tbaa),
<             (ins )));
331,336c379,386
<     }] # [{
<     auto *inst = builder.CreateCall(fn, operands);
<     (void) inst;
<     }] # !if(!gt(requiresAccessGroup, 0), setAccessGroupsMetadataCode, "")
<        # !if(!gt(requiresAliasAnalysis, 0), setAliasAnalysisMetadataCode, "")
<        # !if(!gt(numResults, 0), "$res = inst;", "");
---
>     }] # [{auto *inst = builder.CreateCall(fn, operands);
>     }] # !if(!gt(requiresAccessGroup, 0),
>       "moduleTranslation.setAccessGroupsMetadata(op, inst);",
>       "(void) inst;")
>     # !if(!gt(requiresAliasScope, 0),
>       "moduleTranslation.setAliasScopeMetadata(op, inst);",
>       "(void) inst;")
>     # !if(!gt(numResults, 0), "$res = inst;", "");
357c407
<                   bit requiresAliasAnalysis = 0, bit requiresFastmath = 0>
---
>                   bit requiresAliasScope = 0, bit requiresFastmath = 0>
360c410
<                       numResults, requiresAccessGroup, requiresAliasAnalysis,
---
>                       numResults, requiresAccessGroup, requiresAliasScope,
378,382c428,429
<                             list<Trait> traits = [],
<                             bit requiresAccessGroup = 0,
<                             bit requiresAliasAnalysis = 0>
<     : LLVM_IntrOp<mnem, [], overloadedOperands, traits, /*numResults=*/0,
<                   requiresAccessGroup, requiresAliasAnalysis>;
---
>                             list<Trait> traits = []>
>     : LLVM_IntrOp<mnem, [], overloadedOperands, traits, 0>;
395c442
<                   /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
---
>                   /*requiresAccessGroup=*/0, /*requiresAliasScope=*/0,
Only in include/mlir/Dialect/LLVMIR: LLVMOpsInterfaces.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMOps.td include/mlir/Dialect/LLVMIR/LLVMOps.td
17d16
< include "mlir/Dialect/LLVMIR/LLVMEnums.td"
18a18
> include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.td"
25d24
< include "mlir/Interfaces/Mem2RegInterfaces.td"
127,128c126
<     $res = builder.CreateICmp(
<             convertICmpPredicateToLLVM($predicate), $lhs, $rhs);
---
>     $res = builder.CreateICmp(getLLVMCmpPredicate($predicate), $lhs, $rhs);
132,133c130,131
<     $res = $_builder.create<$_qualCppClassName>($_location,
<             convertICmpPredicateFromLLVM(iCmpInst->getPredicate()), $lhs, $rhs);
---
>     $res = $_builder.create<$_qualCppClassName>(
>       $_location, getICmpPredicate(iCmpInst->getPredicate()), $lhs, $rhs);
151c149
<     $res = builder.CreateFCmp(convertFCmpPredicateToLLVM($predicate), $lhs, $rhs);
---
>     $res = builder.CreateFCmp(getLLVMCmpPredicate($predicate), $lhs, $rhs);
156c154
<       $_location, convertFCmpPredicateFromLLVM(fCmpInst->getPredicate()), $lhs, $rhs);
---
>       $_location, getFCmpPredicate(fCmpInst->getPredicate()), $lhs, $rhs);
173a172,208
> // Common code definition that is used to verify and set the alignment attribute
> // of LLVM ops that accept such an attribute.
> class MemoryOpWithAlignmentBase {
>   code setAlignmentCode = [{
>     if ($alignment.has_value()) {
>       auto align = *$alignment;
>       if (align != 0)
>         inst->setAlignment(llvm::Align(align));
>     }
>   }];
> }
> 
> // Code definition that is used for nontemporal metadata creation.
> class MemoryOpWithAlignmentAndAttributes : MemoryOpWithAlignmentBase {
>   code setNonTemporalMetadataCode = [{
>     if ($nontemporal) {
>       llvm::Module *module = builder.GetInsertBlock()->getModule();
>       llvm::MDNode *metadata = llvm::MDNode::get(
>           inst->getContext(), llvm::ConstantAsMetadata::get(
>               builder.getInt32(1)));
>       inst->setMetadata(module->getMDKindID("nontemporal"), metadata);
>     }
>   }];
> 
>   code setAccessGroupsMetadataCode = [{
>     moduleTranslation.setAccessGroupsMetadata(op, inst);
>   }];
> 
>   code setAliasScopeMetadataCode = [{
>     moduleTranslation.setAliasScopeMetadata(op, inst);
>   }];
> 
>   code setTBAAMetadataCode = [{
>     moduleTranslation.setTBAAMetadata(op, inst);
>   }];
> }
> 
175,177c210
< def LLVM_AllocaOp : LLVM_Op<"alloca", 
<     [DeclareOpInterfaceMethods<PromotableAllocationOpInterface>]>,
<   LLVM_MemOpPatterns {
---
> def LLVM_AllocaOp : LLVM_Op<"alloca">, MemoryOpWithAlignmentBase {
180,181c213
<                    OptionalAttr<TypeAttr>:$elem_type,
<                    UnitAttr:$inalloca);
---
>                    OptionalAttr<TypeAttr>:$elem_type);
192d223
<     inst->setUsedWithInAlloca($inalloca);
194a226
>   // FIXME: Import attributes.
201,203c233
<       $_location, $_resultType, $arraySize,
<       alignment == 0 ? IntegerAttr() : $_builder.getI64IntegerAttr(alignment),
<       TypeAttr::get(allocatedType), allocaInst->isUsedWithInAlloca());
---
>       $_location, $_resultType, allocatedType, $arraySize, alignment);
213c243
<                      TypeAttr(), false);
---
>                      TypeAttr());
215c245
<         $_builder.getI64IntegerAttr(alignment), TypeAttr(), false);
---
>         $_builder.getI64IntegerAttr(alignment), TypeAttr());
226c256
<             elemTypeAttr, false);
---
>             elemTypeAttr);
234,235c264
< def LLVM_GEPOp : LLVM_Op<"getelementptr", [Pure,
<     DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_GEPOp : LLVM_Op<"getelementptr", [Pure]> {
318,328c347,353
< def LLVM_LoadOp : LLVM_MemAccessOpBase<"load",
<     [DeclareOpInterfaceMethods<PromotableMemOpInterface>]> {
<   dag args = (ins Arg<LLVM_PointerTo<LLVM_LoadableType>, "", [MemRead]>:$addr,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_,
<               UnitAttr:$nontemporal,
<               DefaultValuedAttr<
<                 AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
<               OptionalAttr<StrAttr>:$syncscope);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> def LLVM_LoadOp : LLVM_Op<"load">, MemoryOpWithAlignmentAndAttributes {
>   let arguments = (ins Arg<LLVM_PointerTo<LLVM_LoadableType>, "", [MemRead]>:$addr,
>                    OptionalAttr<SymbolRefArrayAttr>:$access_groups,
>                    OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
>                    OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
>                    OptionalAttr<I64Attr>:$alignment, UnitAttr:$volatile_,
>                    UnitAttr:$nontemporal);
331,359d355
<   let description = [{
<     The `load` operation is used to read from memory. A load may be marked as
<     atomic, volatile, and/or nontemporal, and takes a number of optional
<     attributes that specify aliasing information.
< 
<     An atomic load only supports a limited set of pointer, integer, and
<     floating point types, and requires an explicit alignment.
< 
<     Examples:
<     ```mlir
<     // A volatile load of a float variable.
<     %0 = llvm.load volatile %ptr : !llvm.ptr -> f32
< 
<     // A nontemporal load of a float variable.
<     %0 = llvm.load %ptr {nontemporal} : !llvm.ptr -> f32
< 
<     // An atomic load of an integer variable.
<     %0 = llvm.load %ptr atomic monotonic {alignment = 8 : i64}
<         : !llvm.ptr -> i64
<     ```
< 
<     See the following link for more details:
<     https://llvm.org/docs/LangRef.html#load-instruction
<   }];
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $addr
<     (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
<     attr-dict `:` custom<LoadType>(type($addr), type($res))
<   }];
362,365c358
<     $res = inst;
<   }] # setOrderingCode
<      # setSyncScopeCode
<      # setAlignmentCode
---
>   }] # setAlignmentCode
368c361,366
<      # setAliasAnalysisMetadataCode;
---
>      # setAliasScopeMetadataCode
>      # setTBAAMetadataCode
>      # [{
>     $res = inst;
>   }];
>   // FIXME: Import attributes.
370,376c368
<     auto *loadInst = cast<llvm::LoadInst>(inst);
<     unsigned alignment = loadInst->getAlign().value();
<     $res = $_builder.create<LLVM::LoadOp>($_location, $_resultType, $addr,
<         alignment, loadInst->isVolatile(),
<         loadInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
<         convertAtomicOrderingFromLLVM(loadInst->getOrdering()),
<         getLLVMSyncScope(loadInst));
---
>     $res = $_builder.create<LLVM::LoadOp>($_location, $_resultType, $addr);
380,381c372,379
<       CArg<"bool", "false">:$isVolatile, CArg<"bool", "false">:$isNonTemporal)>,
<     OpBuilder<(ins "Type":$type, "Value":$addr,
---
>       CArg<"bool", "false">:$isVolatile, CArg<"bool", "false">:$isNonTemporal),
>     [{
>       auto type = addr.getType().cast<LLVMPointerType>().getElementType();
>       assert(type && "must provide explicit element type to the constructor "
>                      "when the pointer type is opaque");
>       build($_builder, $_state, type, addr, alignment, isVolatile, isNonTemporal);
>     }]>,
>     OpBuilder<(ins "Type":$t, "Value":$addr,
383,386c381,382
<       CArg<"bool", "false">:$isNonTemporal,
<       CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>       CArg<"bool", "false">:$isNonTemporal)>,];
>   let hasCustomAssemblyFormat = 1;
390,401c386,393
< def LLVM_StoreOp : LLVM_MemAccessOpBase<"store",
<     [DeclareOpInterfaceMethods<PromotableMemOpInterface>]> {
<   dag args = (ins LLVM_LoadableType:$value,
<               Arg<LLVM_PointerTo<LLVM_LoadableType>,"",[MemWrite]>:$addr,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_,
<               UnitAttr:$nontemporal,
<               DefaultValuedAttr<
<                 AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
<               OptionalAttr<StrAttr>:$syncscope);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> def LLVM_StoreOp : LLVM_Op<"store">, MemoryOpWithAlignmentAndAttributes {
>   let arguments = (ins LLVM_LoadableType:$value,
>                    Arg<LLVM_PointerTo<LLVM_LoadableType>,"",[MemWrite]>:$addr,
>                    OptionalAttr<SymbolRefArrayAttr>:$access_groups,
>                    OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
>                    OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
>                    OptionalAttr<I64Attr>:$alignment, UnitAttr:$volatile_,
>                    UnitAttr:$nontemporal);
403,431d394
<   let description = [{
<     The `store` operation is used to write to memory. A store may be marked as
<     atomic, volatile, and/or nontemporal, and takes a number of optional
<     attributes that specify aliasing information.
< 
<     An atomic store only supports a limited set of pointer, integer, and
<     floating point types, and requires an explicit alignment.
< 
<     Examples:
<     ```mlir
<     // A volatile store of a float variable.
<     llvm.store volatile %val, %ptr : f32, !llvm.ptr
< 
<     // A nontemporal store of a float variable.
<     llvm.store %val, %ptr {nontemporal} : f32, !llvm.ptr
< 
<     // An atomic store of an integer variable.
<     llvm.store %val, %ptr atomic monotonic {alignment = 8 : i64}
<         : i64, !llvm.ptr
<     ```
< 
<     See the following link for more details:
<     https://llvm.org/docs/LangRef.html#store-instruction
<   }];
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $value `,` $addr
<     (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
<     attr-dict `:` custom<StoreType>(type($value), type($addr))
<   }];
434,436c397
<   }] # setOrderingCode
<      # setSyncScopeCode
<      # setAlignmentCode
---
>   }] # setAlignmentCode
439c400,402
<      # setAliasAnalysisMetadataCode;
---
>      # setAliasScopeMetadataCode
>      # setTBAAMetadataCode;
>   // FIXME: Import attributes.
441,447c404
<     auto *storeInst = cast<llvm::StoreInst>(inst);
<     unsigned alignment = storeInst->getAlign().value();
<     $_op = $_builder.create<LLVM::StoreOp>($_location, $value, $addr,
<         alignment, storeInst->isVolatile(),
<         storeInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
<         convertAtomicOrderingFromLLVM(storeInst->getOrdering()),
<         getLLVMSyncScope(storeInst));
---
>     $_op = $_builder.create<LLVM::StoreOp>($_location, $value, $addr);
452,455c409,411
<       CArg<"bool", "false">:$isNonTemporal,
<       CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>       CArg<"bool", "false">:$isNonTemporal)>
>     ];
>   let hasCustomAssemblyFormat = 1;
474,475c430,431
< def LLVM_BitcastOp : LLVM_CastOp<"bitcast", "BitCast", LLVM_AnyNonAggregate,
<     LLVM_AnyNonAggregate, [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_BitcastOp : LLVM_CastOp<"bitcast", "BitCast",
>                                  LLVM_AnyNonAggregate, LLVM_AnyNonAggregate> {
477d432
<   let hasVerifier = 1;
480,482c435,436
<     LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
<     LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
>                                        LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
>                                        LLVM_ScalarOrVectorOf<LLVM_AnyPointer>> {
547,552d500
< 
<   let extraClassDeclaration = [{
<     CallInterfaceCallable getCallableForCallee();
<     Operation::operand_range getCallOperands();
<   }];
< 
569,571d516
< // FIXME: Add a type attribute that carries the LLVM function type to support
< // indirect calls to variadic functions. The type attribute is necessary to
< // distinguish normal and variadic arguments.
586,589c531,533
<     function attribute `callee`. The trailing type list contains the optional
<     indirect callee type and the MLIR function type, which differs from the
<     LLVM function type that uses a explicit void type to model functions that do
<     not return a value.
---
>     function attribute `callee`. The trailing type of the instruction is always
>     the MLIR function type, which may be different from the indirect callee that
>     has the wrapped LLVM IR function type.
601c545
<     llvm.call %1(%0) : !llvm.ptr, (f32) -> ()
---
>     llvm.call %1(%0) : (f32) -> ()
622,626d565
<   let extraClassDeclaration = [{
<     CallInterfaceCallable getCallableForCallee();
<     Operation::operand_range getCallOperands();
<   }];
< 
793,794c732
<           [Pure, AllTypesMatch<["trueValue", "falseValue", "res"]>,
<            DeclareOpInterfaceMethods<FastmathFlagsInterface>]>,
---
>           [Pure, AllTypesMatch<["trueValue", "falseValue", "res"]>]>,
798,800c736
<                    LLVM_Type:$trueValue, LLVM_Type:$falseValue,
<                    DefaultValuedAttr<LLVM_FastmathFlagsAttr,
<                                      "{}">:$fastmathFlags);
---
>                    LLVM_Type:$trueValue, LLVM_Type:$falseValue);
805c741
<     auto op = $_builder.create<LLVM::SelectOp>(
---
>     $res = $_builder.create<LLVM::SelectOp>(
807,808d742
<     moduleImport.setFastmathFlagsAttr(inst, op);
<     $res = op;
826,829c760
<   let arguments = (ins
<     Variadic<LLVM_Type>:$destOperands,
<     OptionalAttr<LoopAnnotationAttr>:$loop_annotation
<   );
---
>   let arguments = (ins Variadic<LLVM_Type>:$destOperands);
838,840d768
<     OpBuilder<(ins "ValueRange":$operands, "Block *":$dest), [{
<       build($_builder, $_state, operands, /*loop_annotation=*/{}, dest);
<     }]>,
843d770
<   let hasVerifier = 1;
851,852c778
<                    OptionalAttr<ElementsAttr>:$branch_weights,
<                    OptionalAttr<LoopAnnotationAttr>:$loop_annotation);
---
>                    OptionalAttr<ElementsAttr>:$branch_weights);
865c791,801
<       CArg<"std::optional<std::pair<uint32_t, uint32_t>>", "{}">:$weights)>,
---
>       CArg<"std::optional<std::pair<uint32_t, uint32_t>>", "{}">:$weights),
>     [{
>         ElementsAttr weightsAttr;
>         if (weights) {
>           weightsAttr =
>               $_builder.getI32VectorAttr({static_cast<int32_t>(weights->first),
>                                        static_cast<int32_t>(weights->second)});
>         }
>         build($_builder, $_state, condition, trueOperands, falseOperands, weightsAttr,
>               trueDest, falseDest);
>   }]>,
871,876d806
<   }]>,
<   OpBuilder<(ins "Value":$condition, "ValueRange":$trueOperands, "ValueRange":$falseOperands,
<     "ElementsAttr":$branchWeights, "Block *":$trueDest, "Block *":$falseDest),
<   [{
<       build($_builder, $_state, condition, trueOperands, falseOperands, branchWeights,
<       {}, trueDest, falseDest);
878d807
<   let hasVerifier = 1;
916,917c845
<   // Consistency of llvm.resume value types is checked in LLVMFuncOp::verify().
<   let hasVerifier = false;
---
>   let hasVerifier = 1;
1064,1069c992,995
<     ```mlir
<     llvm.metadata @metadata {
<       llvm.access_group @group1
<       llvm.access_group @group2
<     }
<     ```
---
>       llvm.metadata @metadata {
>         llvm.access_group @group1
>         llvm.access_group @group2
>       }
1118,1128c1044,1061
<     ```mlir
<     module {
<       llvm.func @foo(%ptr1 : !llvm.ptr<i32>) {
<           %c0 = llvm.mlir.constant(0 : i32) : i32
<           %c4 = llvm.mlir.constant(4 : i32) : i32
<           %1 = llvm.ptrtoint %ptr1 : !llvm.ptr<i32> to i32
<           %2 = llvm.add %1, %c1 : i32
<           %ptr2 = llvm.inttoptr %2 : i32 to !llvm.ptr<i32>
<           llvm.store %c0, %ptr1 { alias_scopes = [@metadata::@scope1], llvm.noalias = [@metadata::@scope2] } : !llvm.ptr<i32>
<           llvm.store %c4, %ptr2 { alias_scopes = [@metadata::@scope2], llvm.noalias = [@metadata::@scope1] } : !llvm.ptr<i32>
<           llvm.return
---
>       module {
>         llvm.func @foo(%ptr1 : !llvm.ptr<i32>) {
>             %c0 = llvm.mlir.constant(0 : i32) : i32
>             %c4 = llvm.mlir.constant(4 : i32) : i32
>             %1 = llvm.ptrtoint %ptr1 : !llvm.ptr<i32> to i32
>             %2 = llvm.add %1, %c1 : i32
>             %ptr2 = llvm.inttoptr %2 : i32 to !llvm.ptr<i32>
>             llvm.store %c0, %ptr1 { alias_scopes = [@metadata::@scope1], llvm.noalias = [@metadata::@scope2] } : !llvm.ptr<i32>
>             llvm.store %c4, %ptr2 { alias_scopes = [@metadata::@scope2], llvm.noalias = [@metadata::@scope1] } : !llvm.ptr<i32>
>             llvm.return
>         }
> 
>         llvm.metadata @metadata {
>           llvm.alias_scope_domain @unused_domain
>           llvm.alias_scope_domain @domain { description = "Optional domain description"}
>           llvm.alias_scope @scope1 { domain = @domain }
>           llvm.alias_scope @scope2 { domain = @domain, description = "Optional scope description" }
>         }
1131,1139d1063
<       llvm.metadata @metadata {
<         llvm.alias_scope_domain @unused_domain
<         llvm.alias_scope_domain @domain { description = "Optional domain description"}
<         llvm.alias_scope @scope1 { domain = @domain }
<         llvm.alias_scope @scope2 { domain = @domain, description = "Optional scope description" }
<       }
<     }
<     ```
< 
1144d1067
<   let hasVerifier = 1;
1179,1183c1102,1104
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<     }
<     ```
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>       }
1208,1219c1129,1155
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<       llvm.tbaa_type_desc @tbaa_type_desc_1 {
<           identity = "omnipotent char",
<           members = [@tbaa_root_0],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_2 {
<           identity = "long long",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>         llvm.tbaa_type_desc @tbaa_type_desc_1 {
>             identity = "omnipotent char",
>             members = [@tbaa_root_0],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_2 {
>             identity = "long long",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_3 {
>             identity = "agg2_t",
>             members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
>             offsets = array<i64: 0, 8>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_5 {
>             identity = "int",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_6 {
>             identity = "agg1_t",
>             members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
>             offsets = array<i64: 0, 4>
>         }
1221,1237d1156
<       llvm.tbaa_type_desc @tbaa_type_desc_3 {
<           identity = "agg2_t",
<           members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
<           offsets = array<i64: 0, 8>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_5 {
<           identity = "int",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_6 {
<           identity = "agg1_t",
<           members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
<           offsets = array<i64: 0, 4>
<       }
<     }
<     ```
1269,1290c1188,1224
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<       llvm.tbaa_type_desc @tbaa_type_desc_1 {
<           identity = "omnipotent char",
<           members = [@tbaa_root_0],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_2 {
<           identity = "long long",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_3 {
<           identity = "agg2_t",
<           members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
<           offsets = array<i64: 0, 8>
<       }
<       llvm.tbaa_tag @tbaa_tag_4 {
<           access_type = @tbaa_type_desc_2,
<           base_type = @tbaa_type_desc_3,
<           offset = 8 : i64
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>         llvm.tbaa_type_desc @tbaa_type_desc_1 {
>             identity = "omnipotent char",
>             members = [@tbaa_root_0],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_2 {
>             identity = "long long",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_3 {
>             identity = "agg2_t",
>             members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
>             offsets = array<i64: 0, 8>
>         }
>         llvm.tbaa_tag @tbaa_tag_4 {
>             access_type = @tbaa_type_desc_2,
>             base_type = @tbaa_type_desc_3,
>             offset = 8 : i64
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_5 {
>             identity = "int",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_6 {
>             identity = "agg1_t",
>             members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
>             offsets = array<i64: 0, 4>
>         }
>         llvm.tbaa_tag @tbaa_tag_7 {
>             access_type = @tbaa_type_desc_5,
>             base_type = @tbaa_type_desc_6,
>             offset = 0 : i64
>         }
1292,1308d1225
<       llvm.tbaa_type_desc @tbaa_type_desc_5 {
<           identity = "int",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_6 {
<           identity = "agg1_t",
<           members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
<           offsets = array<i64: 0, 4>
<       }
<       llvm.tbaa_tag @tbaa_tag_7 {
<           access_type = @tbaa_type_desc_5,
<           base_type = @tbaa_type_desc_6,
<           offset = 0 : i64
<       }
<     }
<     ```
1329,1330c1246
<     OptionalAttr<StrAttr>:$section,
<     DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_
---
>     OptionalAttr<StrAttr>:$section
1575,1576c1491
<     OptionalAttr<LLVM_MemoryEffectsAttr>:$memory,
<     DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_
---
>     OptionalAttr<LLVM_MemoryEffectsAttr>:$memory
1606,1609d1520
<     //===------------------------------------------------------------------===//
<     // CallableOpInterface
<     //===------------------------------------------------------------------===//
< 
1623,1633d1533
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
1675c1575
<     IR dialect type.
---
>     IR dialect type wrapping an LLVM IR structure type.
1689,1710d1588
< def LLVM_PoisonOp : LLVM_Op<"mlir.poison", [Pure]>,
<                     LLVM_Builder<"$res = llvm::PoisonValue::get($_resultType);"> {
<   let summary = "Creates a poison value of LLVM dialect type.";
<   let description = [{
<     Unlike LLVM IR, MLIR does not have first-class poison values. Such values
<     must be created as SSA values using `llvm.mlir.poison`. This operation has
<     no operands or attributes. It creates a poison value of the specified LLVM
<     IR dialect type.
< 
<     Example:
< 
<     ```mlir
<     // Create a poison value for a structure with a 32-bit integer followed
<     // by a float.
<     %0 = llvm.mlir.poison : !llvm.struct<(i32, f32)>
<     ```
<   }];
<   let results = (outs LLVM_Type:$res);
<   let builders = [LLVM_OneResultOpBuilder];
<   let assemblyFormat = "attr-dict `:` type($res)";
< }
< 
1776,1786c1654,1659
< def LLVM_AtomicRMWOp : LLVM_MemAccessOpBase<"atomicrmw", [
<       TypesMatchWith<"result #0 and operand #1 have the same type",
<                      "val", "res", "$_self">]> {
<   dag args = (ins AtomicBinOp:$bin_op,
<               LLVM_PointerTo<LLVM_AtomicRMWType>:$ptr,
<               LLVM_AtomicRMWType:$val, AtomicOrdering:$ordering,
<               OptionalAttr<StrAttr>:$syncscope,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> // FIXME: Need to add alignment and syncscope attribute to MLIR atomicrmw
> // operation.
> def LLVM_AtomicRMWOp : LLVM_Op<"atomicrmw"> {
>   let arguments = (ins AtomicBinOp:$bin_op,
>                    LLVM_PointerTo<LLVM_AtomicRMWType>:$ptr,
>                    LLVM_AtomicRMWType:$val, AtomicOrdering:$ordering);
1788,1792c1661,1662
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $bin_op $ptr `,` $val
<     (`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict `:`
<     qualified(type($ptr)) `,` type($val)
<   }];
---
>   let hasCustomAssemblyFormat = 1;
>   let hasVerifier = 1;
1795,1803c1665,1668
<     auto *inst = builder.CreateAtomicRMW(
<         convertAtomicBinOpToLLVM($bin_op), $ptr, $val, llvm::MaybeAlign(),
<         convertAtomicOrderingToLLVM($ordering));
<     $res = inst;
<   }] # setVolatileCode
<      # setSyncScopeCode
<      # setAlignmentCode
<      # setAccessGroupsMetadataCode
<      # setAliasAnalysisMetadataCode;
---
>     $res = builder.CreateAtomicRMW(getLLVMAtomicBinOp($bin_op), $ptr, $val,
>                                    llvm::MaybeAlign(),
>                                    getLLVMAtomicOrdering($ordering));
>   }];
1806,1810c1671,1673
<     unsigned alignment = atomicInst->getAlign().value();
<     $res = $_builder.create<LLVM::AtomicRMWOp>($_location,
<         convertAtomicBinOpFromLLVM(atomicInst->getOperation()), $ptr, $val,
<         convertAtomicOrderingFromLLVM(atomicInst->getOrdering()),
<         getLLVMSyncScope(atomicInst), alignment, atomicInst->isVolatile());
---
>     $res = $_builder.create<LLVM::AtomicRMWOp>($_location, $_resultType,
>         getLLVMAtomicBinOp(atomicInst->getOperation()), $ptr, $val,
>         getLLVMAtomicOrdering(atomicInst->getOrdering()));
1812,1820c1675,1676
<   list<int> llvmArgIndices = [-1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1];
<   let builders = [
<     OpBuilder<(ins "LLVM::AtomicBinOp":$binOp, "Value":$ptr, "Value":$val,
<       "LLVM::AtomicOrdering":$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope,
<       CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isVolatile
<     )>
<   ];
<   let hasVerifier = 1;
---
>   // Only $ptr and $val are llvm instruction operands.
>   list<int> llvmArgIndices = [-1, 0, 1, -1];
1824,1846c1680,1700
< 
< def LLVM_AtomicCmpXchgOp : LLVM_MemAccessOpBase<"cmpxchg", [
<       TypesMatchWith<"operand #1 and operand #2 have the same type",
<                      "val", "cmp", "$_self">,
<       TypesMatchWith<"result #0 has an LLVM struct type consisting of "
<                      "the type of operand #2 and a bool", "val", "res",
<                      "getValAndBoolStructType($_self)">]> {
<   dag args = (ins LLVM_PointerTo<LLVM_AtomicCmpXchgType>:$ptr,
<               LLVM_AtomicCmpXchgType:$cmp, LLVM_AtomicCmpXchgType:$val,
<               AtomicOrdering:$success_ordering,
<               AtomicOrdering:$failure_ordering,
<               OptionalAttr<StrAttr>:$syncscope,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$weak,
<               UnitAttr:$volatile_);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let results = (outs LLVM_AnyStruct:$res);
<   let assemblyFormat = [{
<     (`weak` $weak^)? (`volatile` $volatile_^)? $ptr `,` $cmp `,` $val
<     (`syncscope` `(` $syncscope^ `)`)? $success_ordering $failure_ordering
<     attr-dict `:` qualified(type($ptr)) `,` type($val)
<   }];
---
> def LLVM_AtomicCmpXchgResultType : Type<And<[
>   LLVM_AnyStruct.predicate,
>   CPred<"$_self.cast<::mlir::LLVM::LLVMStructType>().getBody().size() == 2">,
>   SubstLeaves<"$_self",
>               "$_self.cast<::mlir::LLVM::LLVMStructType>().getBody()[0]",
>               LLVM_AtomicCmpXchgType.predicate>,
>   SubstLeaves<"$_self",
>               "$_self.cast<::mlir::LLVM::LLVMStructType>().getBody()[1]",
>               I1.predicate>]>,
>  "an LLVM struct type with any integer or pointer followed by a single-bit "
>  "integer">;
> 
> // FIXME: Need to add alignment attribute to MLIR cmpxchg operation.
> def LLVM_AtomicCmpXchgOp : LLVM_Op<"cmpxchg"> {
>   let arguments = (ins LLVM_PointerTo<LLVM_AtomicCmpXchgType>:$ptr,
>                    LLVM_AtomicCmpXchgType:$cmp, LLVM_AtomicCmpXchgType:$val,
>                    AtomicOrdering:$success_ordering,
>                    AtomicOrdering:$failure_ordering);
>   let results = (outs LLVM_AtomicCmpXchgResultType:$res);
>   let hasCustomAssemblyFormat = 1;
>   let hasVerifier = 1;
1849,1858c1703,1706
<     auto *inst = builder.CreateAtomicCmpXchg($ptr, $cmp, $val,
<         llvm::MaybeAlign(), convertAtomicOrderingToLLVM($success_ordering),
<         convertAtomicOrderingToLLVM($failure_ordering));
<     $res = inst;
<     inst->setWeak($weak);
<   }] # setVolatileCode
<      # setSyncScopeCode
<      # setAlignmentCode
<      # setAccessGroupsMetadataCode
<      # setAliasAnalysisMetadataCode;
---
>     $res = builder.CreateAtomicCmpXchg($ptr, $cmp, $val, llvm::MaybeAlign(),
>                    getLLVMAtomicOrdering($success_ordering),
>                    getLLVMAtomicOrdering($failure_ordering));
>   }];
1861d1708
<     unsigned alignment = cmpXchgInst->getAlign().value();
1863,1867c1710,1712
<       $_location, $ptr, $cmp, $val,
<       convertAtomicOrderingFromLLVM(cmpXchgInst->getSuccessOrdering()),
<       convertAtomicOrderingFromLLVM(cmpXchgInst->getFailureOrdering()),
<       getLLVMSyncScope(cmpXchgInst), alignment, cmpXchgInst->isWeak(),
<       cmpXchgInst->isVolatile());
---
>       $_location, $_resultType, $ptr, $cmp, $val,
>       getLLVMAtomicOrdering(cmpXchgInst->getSuccessOrdering()),
>       getLLVMAtomicOrdering(cmpXchgInst->getFailureOrdering()));
1869,1878d1713
<   let builders = [
<     OpBuilder<(ins "Value":$ptr, "Value":$cmp, "Value":$val,
<       "LLVM::AtomicOrdering":$successOrdering,
<       "LLVM::AtomicOrdering":$failureOrdering,
<       CArg<"StringRef", "StringRef()">:$syncscope,
<       CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isWeak,
<       CArg<"bool", "false">:$isVolatile
<     )>
<   ];
<   let hasVerifier = 1;
1881,1884c1716,1718
< def LLVM_FenceOp : LLVM_Op<"fence">, LLVM_MemOpPatterns {
<   let arguments = (ins AtomicOrdering:$ordering,
<                    OptionalAttr<StrAttr>:$syncscope);
<   let assemblyFormat = "(`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict";
---
> def LLVM_FenceOp : LLVM_Op<"fence"> {
>   let arguments = (ins AtomicOrdering:$ordering, StrAttr:$syncscope);
>   let builders = [LLVM_VoidResultTypeOpBuilder, LLVM_ZeroResultOpBuilder];
1887,1888c1721,1724
<     auto *inst = builder.CreateFence(convertAtomicOrderingToLLVM($ordering));
<   }] # setSyncScopeCode;
---
>     llvm::LLVMContext &llvmContext = builder.getContext();
>     builder.CreateFence(getLLVMAtomicOrdering($ordering),
>       llvmContext.getOrInsertSyncScopeID($syncscope));
>   }];
1893c1729
<       convertAtomicOrderingFromLLVM(fenceInst->getOrdering()),
---
>       getLLVMAtomicOrdering(fenceInst->getOrdering()),
1896,1901c1732
<   let builders = [
<     LLVM_VoidResultTypeOpBuilder,
<     LLVM_ZeroResultOpBuilder,
<     OpBuilder<(ins "LLVM::AtomicOrdering":$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>   let hasCustomAssemblyFormat = 1;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMTypes.h include/mlir/Dialect/LLVMIR/LLVMTypes.h
16a17
> #include "mlir/IR/SubElementInterfaces.h"
105a107
>                             SubElementTypeInterface::Trait,
183d184
<   using Base::verify;
200a202,206
> 
>   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
>                                 function_ref<void(Type)> walkTypesFn) const;
>   Type replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
>                                    ArrayRef<Type> replTypes) const;
216c222
< ParseResult parsePrettyLLVMType(AsmParser &p, Type &type);
---
> ParseResult parsePrettyLLVMType(AsmParser &p, FailureOr<Type> &type);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/LLVMTypes.td include/mlir/Dialect/LLVMIR/LLVMTypes.td
13a14
> include "mlir/IR/SubElementInterfaces.td"
27c28,29
<     DeclareTypeInterfaceMethods<DataLayoutTypeInterface, ["getTypeSize"]>]> {
---
>     DeclareTypeInterfaceMethods<DataLayoutTypeInterface, ["getTypeSize"]>,
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
63c65,66
< def LLVMFunctionType : LLVMType<"LLVMFunction", "func"> {
---
> def LLVMFunctionType : LLVMType<"LLVMFunction", "func", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
124c127,128
<       "areCompatible", "verifyEntries"]>]> {
---
>       "areCompatible", "verifyEntries"]>,
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
170c174,175
< def LLVMFixedVectorType : LLVMType<"LLVMFixedVector", "vec"> {
---
> def LLVMFixedVectorType : LLVMType<"LLVMFixedVector", "vec", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
200c205,206
< def LLVMScalableVectorType : LLVMType<"LLVMScalableVector", "vec"> {
---
> def LLVMScalableVectorType : LLVMType<"LLVMScalableVector", "vec", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/NVVMDialect.h include/mlir/Dialect/LLVMIR/NVVMDialect.h
40,41c40
<                                              mlir::NVVM::MMAFrag frag, int nRow,
<                                              int nCol,
---
>                                              mlir::NVVM::MMAFrag frag,
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/NVVMOps.td include/mlir/Dialect/LLVMIR/NVVMOps.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
138,176d138
< // NVVM redux op definitions
< //===----------------------------------------------------------------------===//
< 
< def ReduxKindNone : I32EnumAttrCase<"NONE", 0, "none">;
< def ReduxKindAdd  : I32EnumAttrCase<"ADD", 1, "add">;
< def ReduxKindAnd  : I32EnumAttrCase<"AND", 2, "and">;
< def ReduxKindMax  : I32EnumAttrCase<"MAX", 3, "max">;
< def ReduxKindMin  : I32EnumAttrCase<"MIN", 4, "min">;
< def ReduxKindOr   : I32EnumAttrCase<"OR", 5, "or">;
< def ReduxKindUmax : I32EnumAttrCase<"UMAX", 6, "umax">;
< def ReduxKindUmin : I32EnumAttrCase<"UMIN", 7, "umin">;
< def ReduxKindXor  : I32EnumAttrCase<"XOR", 8, "xor">; 
< 
< /// Enum attribute of the different kinds.
< def ReduxKind : I32EnumAttr<"ReduxKind", "NVVM redux kind",
<   [ReduxKindAdd, ReduxKindAnd, ReduxKindMax, ReduxKindMin, ReduxKindOr, 
<     ReduxKindUmax, ReduxKindUmin, ReduxKindXor]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = "::mlir::NVVM";
< }
< 
< def ReduxKindAttr : EnumAttr<NVVM_Dialect, ReduxKind, "redux_kind">;
< 
< def NVVM_ReduxOp :
<   NVVM_Op<"redux.sync">,
<   Results<(outs LLVM_Type:$res)>,
<   Arguments<(ins LLVM_Type:$val,
<                  ReduxKindAttr:$kind,
<                  I32:$mask_and_clamp)> {
<   string llvmBuilder = [{
<       auto intId = getReduxIntrinsicId($_resultType, $kind);
<       $res = createIntrinsicCall(builder, intId, {$val, $mask_and_clamp});
<   }];
<   let assemblyFormat = [{
<     $kind $val `,` $mask_and_clamp  attr-dict `:` type($val) `->` type($res)
<    }];   
< }
< 
< //===----------------------------------------------------------------------===//
268c230
<   let assemblyFormat = "$dst `,` $src `,` $size attr-dict `:` type(operands)";
---
>   let assemblyFormat = "$dst `,` $src `,` $size attr-dict";
388,390d349
<   list<list<WMMA_REGS>> i8_wmma_ops = MMA_OPS<
<             [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
<             ["s8","u8"], [], ["s32"], []>.ret;
393,394c352
<             fp_wmma_ops,
<             i8_wmma_ops);
---
>             fp_wmma_ops);
398c356
<             ["a", "b"], ["f16","s8","u8"]>.ret;
---
>             ["a", "b"], ["f16"]>.ret;
401c359
<             ["c", "d"], ["f16", "f32","s32"]>.ret;
---
>             ["c", "d"], ["f16", "f32"]>.ret;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/ROCDLOps.td include/mlir/Dialect/LLVMIR/ROCDLOps.td
27a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
56a58,63
> class ROCDL_IntrOp<string mnemonic, list<int> overloadedResults,
>   list<int> overloadedOperands, list<Trait> traits, int numResults> :
>   LLVM_IntrOpBase<ROCDL_Dialect,  mnemonic,
>   "amdgcn_" # !subst(".", "_", mnemonic), overloadedResults,
>   overloadedOperands, traits, numResults>;
> 
126a134,146
> def ROCDL_SetPrioOp : ROCDL_IntrOp<"s.setprio", [], [], [], 0>,
>   Arguments<(ins I16:$priority)> {
>   let results = (outs);
>   let assemblyFormat = "$priority attr-dict";
> }
> 
> def ROCDL_SchedBarrier : ROCDL_IntrOp<"sched.barrier", [], [], [], 0>,
>   Arguments<(ins I32:$mask)> {
>   let results = (outs);
>   let assemblyFormat = "$mask attr-dict";
> }
> 
> 
185a206,225
> // WMMA intrinsics
> class ROCDL_Wmma_IntrOp<string mnemonic, list<Trait> traits = []> :
>   LLVM_IntrOpBase<ROCDL_Dialect, mnemonic,
>                   "amdgcn_" # !subst(".","_", mnemonic),
>                   [0], [], traits, 1>,
>   Arguments<(ins Variadic<LLVM_Type>:$args)> {
>   let assemblyFormat =
>     "$args attr-dict `:` functional-type($args, $res)";
> }
> 
> // Available on RDNA3
> def ROCDL_wmma_f32_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.f16">;
> def ROCDL_wmma_f32_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.bf16">;
> def ROCDL_wmma_f16_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f16.16x16x16.f16">;
> def ROCDL_wmma_bf16_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.bf16.16x16x16.bf16">;
> def ROCDL_wmma_i32_16x16x16_iu8 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu8">;
> def ROCDL_wmma_i32_16x16x16_iu4 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu4">;
> 
> 
> //===---------------------------------------------------------------------===//
254a295,313
> def ROCDL_RawBufferAtomicCmpSwap :
>   ROCDL_Op<"raw.buffer.atomic.cmpswap", [AllTypesMatch<["res", "src", "cmp"]>]>,
>   Results<(outs LLVM_Type:$res)>,
>   Arguments<(ins LLVM_Type:$src,
>                  LLVM_Type:$cmp,
>                  LLVM_Type:$rsrc,
>                  I32:$offset,
>                  I32:$soffset,
>                  I32:$aux)>{
>   string llvmBuilder = [{
>       $res = createIntrinsicCall(builder,
>           llvm::Intrinsic::amdgcn_raw_buffer_atomic_cmpswap, {$src, $cmp, $rsrc,
>             $offset, $soffset, $aux}, {$_resultType});
>   }];
>   let assemblyFormat = [{
>     attr-dict `(` operands `)` `:` type($res) `,` type($rsrc)
>   }];
> }
> 
274,276d332
< //===---------------------------------------------------------------------===//
< // Buffer atomic floating point max intrinsic. GFX9 does not support fp32.
< 
293,295d348
< //===---------------------------------------------------------------------===//
< // Buffer atomic signed integer max intrinsic.
< 
312,314d364
< //===---------------------------------------------------------------------===//
< // Buffer atomic unsigned integer min intrinsic.
< 
328a379,390
> }
> 
> //===---------------------------------------------------------------------===//
> // DPP Move intrinsic
> def ROCDL_DPPMovOp : ROCDL_IntrOp<"mov.dpp", [], [0],
>     [AllTypesMatch<["res", "src"]>], 1>,
>   Arguments<(ins LLVM_Type:$src, I32:$dppCtrl, I32:$rowMask,
>       I32:$bankMask, I1:$boundCtrl)> {
>   let results = (outs LLVM_Type:$res);
>   let assemblyFormat = [{
>     attr-dict $src `with` $dppCtrl `,` $rowMask `,` $bankMask `,` $boundCtrl `:` type($src)
>   }];
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/Transforms and include/mlir/Dialect/LLVMIR/Transforms
--- include/mlir/Dialect/LLVMIR/LLVMAttrs.h
24a25
> class LoopOptionsAttrBuilder;
44,53d44
< /// This class represents a LLVM attribute that describes a local debug info
< /// scope.
< class DILocalScopeAttr : public DIScopeAttr {
< public:
<   using DIScopeAttr::DIScopeAttr;
< 
<   /// Support LLVM type casting.
<   static bool classof(Attribute attr);
< };
< 
74a66,115
> 
> namespace mlir {
> namespace LLVM {
> 
> /// Builder class for LoopOptionsAttr. This helper class allows to progressively
> /// build a LoopOptionsAttr one option at a time, and pay the price of attribute
> /// creation once all the options are in place.
> class LoopOptionsAttrBuilder {
> public:
>   /// Construct a empty builder.
>   LoopOptionsAttrBuilder() = default;
> 
>   /// Construct a builder with an initial list of options from an existing
>   /// LoopOptionsAttr.
>   LoopOptionsAttrBuilder(LoopOptionsAttr attr);
> 
>   /// Set the `disable_licm` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisableLICM(std::optional<bool> value);
> 
>   /// Set the `interleave_count` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setInterleaveCount(std::optional<uint64_t> count);
> 
>   /// Set the `disable_unroll` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisableUnroll(std::optional<bool> value);
> 
>   /// Set the `disable_pipeline` option to the provided value. If no value
>   /// is provided the option is deleted.
>   LoopOptionsAttrBuilder &setDisablePipeline(std::optional<bool> value);
> 
>   /// Set the `pipeline_initiation_interval` option to the provided value.
>   /// If no value is provided the option is deleted.
>   LoopOptionsAttrBuilder &
>   setPipelineInitiationInterval(std::optional<uint64_t> count);
> 
>   /// Returns true if any option has been set.
>   bool empty() { return options.empty(); }
> 
> private:
>   template <typename T>
>   LoopOptionsAttrBuilder &setOption(LoopOptionCase tag, std::optional<T> value);
> 
>   friend class LoopOptionsAttr;
>   SmallVector<LoopOptionsAttr::OptionValuePair> options;
> };
> 
> } // namespace LLVM
> } // namespace mlir
--- include/mlir/Dialect/LLVMIR/LLVMTypes.h
16a17
> #include "mlir/IR/SubElementInterfaces.h"
105a107
>                             SubElementTypeInterface::Trait,
183d184
<   using Base::verify;
200a202,206
> 
>   void walkImmediateSubElements(function_ref<void(Attribute)> walkAttrsFn,
>                                 function_ref<void(Type)> walkTypesFn) const;
>   Type replaceImmediateSubElements(ArrayRef<Attribute> replAttrs,
>                                    ArrayRef<Type> replTypes) const;
216c222
< ParseResult parsePrettyLLVMType(AsmParser &p, Type &type);
---
> ParseResult parsePrettyLLVMType(AsmParser &p, FailureOr<Type> &type);
--- include/mlir/Dialect/LLVMIR/LLVMAttrDefs.td
12d11
< include "mlir/Dialect/LLVMIR/LLVMDialect.td"
13a13,15
> include "mlir/Dialect/LLVMIR/LLVMEnums.td"
> include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
> include "mlir/IR/SubElementInterfaces.td"
32a35,43
> // FastmathFlagsAttr
> //===----------------------------------------------------------------------===//
> 
> def LLVM_FastmathFlagsAttr :
>     EnumAttr<LLVM_Dialect, FastmathFlags, "fastmath"> {
>   let assemblyFormat = "`<` $value `>`";
> }
> 
> //===----------------------------------------------------------------------===//
42c53
< // Loop Attributes
---
> // LoopOptionsAttr
45,147c56
< def LoopVectorizeAttr : LLVM_Attr<"LoopVectorize", "loop_vectorize"> {
<   let description = [{
<     This attribute defines vectorization specific loop annotations that map to
<     the "!llvm.loop.vectorize" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"BoolAttr">:$predicateEnable,
<     OptionalParameter<"BoolAttr">:$scalableEnable,
<     OptionalParameter<"IntegerAttr">:$width,
<     OptionalParameter<"LoopAnnotationAttr">:$followupVectorized,
<     OptionalParameter<"LoopAnnotationAttr">:$followupEpilogue,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopInterleaveAttr : LLVM_Attr<"LoopInterleave", "loop_interleave"> {
<   let description = [{
<     This attribute defines interleaving specific loop annotations that map to
<     the "!llvm.loop.interleave" metadata.
<   }];
< 
<   let parameters = (ins
<     "IntegerAttr":$count
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnrollAttr : LLVM_Attr<"LoopUnroll", "loop_unroll"> {
<   let description = [{
<     This attribute defines unrolling specific loop annotations that map to
<     the "!llvm.loop.unroll" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$count,
<     OptionalParameter<"BoolAttr">:$runtimeDisable,
<     OptionalParameter<"BoolAttr">:$full,
<     OptionalParameter<"LoopAnnotationAttr">:$followupUnrolled,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainder,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnrollAndJamAttr : LLVM_Attr<"LoopUnrollAndJam", "loop_unroll_and_jam"> {
<   let description = [{
<     This attribute defines "unroll and jam" specific loop annotations that map to
<     the "!llvm.loop.unroll_and_jam" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$count,
<     OptionalParameter<"LoopAnnotationAttr">:$followupOuter,
<     OptionalParameter<"LoopAnnotationAttr">:$followupInner,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainderOuter,
<     OptionalParameter<"LoopAnnotationAttr">:$followupRemainderInner,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopLICMAttr : LLVM_Attr<"LoopLICM", "loop_licm"> {
<   let description = [{
<     This attribute encapsulates loop invariant code motion (licm) specific loop
<     annotations. The fields correspond to the "!llvm.licm.disable" and the
<     "!llvm.loop.licm_versioning.disable" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"BoolAttr">:$versioningDisable
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopDistributeAttr : LLVM_Attr<"LoopDistribute", "loop_distribute"> {
<   let description = [{
<     This attribute defines distribution specific loop annotations that map to
<     the "!llvm.loop.distribute" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"LoopAnnotationAttr">:$followupCoincident,
<     OptionalParameter<"LoopAnnotationAttr">:$followupSequential,
<     OptionalParameter<"LoopAnnotationAttr">:$followupFallback,
<     OptionalParameter<"LoopAnnotationAttr">:$followupAll
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopPipelineAttr : LLVM_Attr<"LoopPipeline", "loop_pipeline"> {
---
> def LoopOptionsAttr : LLVM_Attr<"LoopOptions", "loopopts"> {
149,189c58
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.pipeline" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$disable,
<     OptionalParameter<"IntegerAttr">:$initiationinterval
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopPeeledAttr : LLVM_Attr<"LoopPeeled", "loop_peeled"> {
<   let description = [{
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.peeled" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"IntegerAttr">:$count
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopUnswitchAttr : LLVM_Attr<"LoopUnswitch", "loop_unswitch"> {
<   let description = [{
<     This attribute defines pipelining specific loop annotations that map to
<     the "!llvm.loop.unswitch" metadata.
<   }];
< 
<   let parameters = (ins
<     OptionalParameter<"BoolAttr">:$partialDisable
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def LoopAnnotationAttr : LLVM_Attr<"LoopAnnotation", "loop_annotation"> {
<   let description = [{
<     This attributes encapsulates "loop metadata". It is meant to decorate
---
>     This attributes encapsulates "loop options". It is means to decorate
192,193c61,63
<     It stores annotations in attribute parameters and groups related options in
<     nested attributes to provide structured access.
---
>     It store the options as a pair <enum,int64_t> in a sorted array and expose
>     APIs to retrieve the value for each option with a stronger type (bool for
>     example).
197,209c67
<     OptionalParameter<"BoolAttr">:$disableNonforced,
<     OptionalParameter<"LoopVectorizeAttr">:$vectorize,
<     OptionalParameter<"LoopInterleaveAttr">:$interleave,
<     OptionalParameter<"LoopUnrollAttr">:$unroll,
<     OptionalParameter<"LoopUnrollAndJamAttr">:$unrollAndJam,
<     OptionalParameter<"LoopLICMAttr">:$licm,
<     OptionalParameter<"LoopDistributeAttr">:$distribute,
<     OptionalParameter<"LoopPipelineAttr">:$pipeline,
<     OptionalParameter<"LoopPeeledAttr">:$peeled,
<     OptionalParameter<"LoopUnswitchAttr">:$unswitch,
<     OptionalParameter<"BoolAttr">:$mustProgress,
<     OptionalParameter<"BoolAttr">:$isVectorized,
<     OptionalArrayRefParameter<"SymbolRefAttr">:$parallelAccesses
---
>     ArrayRefParameter<"std::pair<LoopOptionCase, int64_t>", "">:$options
212c70,84
<   let assemblyFormat = "`<` struct(params) `>`";
---
>   let extraClassDeclaration = [{
>     using OptionValuePair = std::pair<LoopOptionCase, int64_t>;
>     using OptionsArray = ArrayRef<std::pair<LoopOptionCase, int64_t>>;
>     std::optional<bool> disableUnroll();
>     std::optional<bool> disableLICM();
>     std::optional<int64_t> interleaveCount();
>   }];
> 
>   let builders = [
>     /// Build the LoopOptions Attribute from a sorted array of individual options.
>     AttrBuilder<(ins "ArrayRef<std::pair<LoopOptionCase, int64_t>>":$sortedOptions)>,
>     AttrBuilder<(ins "LoopOptionsAttrBuilder &":$optionBuilders)>
>   ];
>   let hasCustomAssemblyFormat = 1;
>   let skipDefaultBuilders = 1;
254c126
< // DINullTypeAttr
---
> // DIVoidResultTypeAttr
257,258c129,130
< def LLVM_DINullTypeAttr : LLVM_Attr<"DINullType", "di_null_type",
<                                     /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DIVoidResultTypeAttr : LLVM_Attr<"DIVoidResultType", "di_void_result_type",
>                                           /*traits=*/[], "DITypeAttr"> {
291,292c163,165
< def LLVM_DICompileUnitAttr : LLVM_Attr<"DICompileUnit", "di_compile_unit",
<                                        /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DICompileUnitAttr : LLVM_Attr<"DICompileUnit", "di_compile_unit", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
296c169
<     OptionalParameter<"StringAttr">:$producer,
---
>     "StringAttr":$producer,
307,308c180,182
< def LLVM_DICompositeTypeAttr : LLVM_Attr<"DICompositeType", "di_composite_type",
<                                          /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DICompositeTypeAttr : LLVM_Attr<"DICompositeType", "di_composite_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
311c185
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
328,329c202,204
< def LLVM_DIDerivedTypeAttr : LLVM_Attr<"DIDerivedType", "di_derived_type",
<                                        /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DIDerivedTypeAttr : LLVM_Attr<"DIDerivedType", "di_derived_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
333c208
<     OptionalParameter<"DITypeAttr">:$baseType,
---
>     "DITypeAttr":$baseType,
359,360c234,236
< def LLVM_DILexicalBlockAttr : LLVM_Attr<"DILexicalBlock", "di_lexical_block",
<                                         /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DILexicalBlockAttr : LLVM_Attr<"DILexicalBlock", "di_lexical_block", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
372c248
<       return $_get(scope.getContext(), scope, file, line, column);
---
>       return $_get(file.getContext(), scope, file, line, column);
382,383c258,260
< def LLVM_DILexicalBlockFile : LLVM_Attr<"DILexicalBlockFile", "di_lexical_block_file",
<                                         /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DILexicalBlockFile : LLVM_Attr<"DILexicalBlockFile", "di_lexical_block_file", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
393c270
<       return $_get(scope.getContext(), scope, file, discriminator);
---
>       return $_get(file.getContext(), scope, file, discriminator);
403,404c280,282
< def LLVM_DILocalVariableAttr : LLVM_Attr<"DILocalVariable", "di_local_variable",
<                                          /*traits=*/[], "DINodeAttr"> {
---
> def LLVM_DILocalVariableAttr : LLVM_Attr<"DILocalVariable", "di_local_variable", [
>     SubElementAttrInterface
>   ], "DINodeAttr"> {
407c285
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
420c298
<       MLIRContext *ctx = scope.getContext();
---
>       MLIRContext *ctx = file.getContext();
432,433c310,312
< def LLVM_DISubprogramAttr : LLVM_Attr<"DISubprogram", "di_subprogram",
<                                       /*traits=*/[], "DIScopeAttr"> {
---
> def LLVM_DISubprogramAttr : LLVM_Attr<"DISubprogram", "di_subprogram", [
>     SubElementAttrInterface
>   ], "DIScopeAttr"> {
435c314
<     OptionalParameter<"DICompileUnitAttr">:$compileUnit,
---
>     "DICompileUnitAttr":$compileUnit,
437c316
<     OptionalParameter<"StringAttr">:$name,
---
>     "StringAttr":$name,
463,477d341
< // DINamespaceAttr
< //===----------------------------------------------------------------------===//
< 
< def LLVM_DINamespaceAttr : LLVM_Attr<"DINamespace", "di_namespace",
<                                       /*traits=*/[], "DIScopeAttr"> {
<   let parameters = (ins
<     OptionalParameter<"StringAttr">:$name,
<     OptionalParameter<"DIScopeAttr">:$scope,
<     "bool":$exportSymbols
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< //===----------------------------------------------------------------------===//
496,497c360,362
< def LLVM_DISubroutineTypeAttr : LLVM_Attr<"DISubroutineType", "di_subroutine_type",
<                                           /*traits=*/[], "DITypeAttr"> {
---
> def LLVM_DISubroutineTypeAttr : LLVM_Attr<"DISubroutineType", "di_subroutine_type", [
>     SubElementAttrInterface
>   ], "DITypeAttr"> {
507a373
>   let genVerifyDecl = 1;
514c380,382
< def LLVM_MemoryEffectsAttr : LLVM_Attr<"MemoryEffects", "memory_effects"> {
---
> def LLVM_MemoryEffectsAttr : LLVM_Attr<"MemoryEffects", "memory_effects", [
>     SubElementAttrInterface
>   ]> {
--- include/mlir/Dialect/LLVMIR/NVVMOps.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
138,176d138
< // NVVM redux op definitions
< //===----------------------------------------------------------------------===//
< 
< def ReduxKindNone : I32EnumAttrCase<"NONE", 0, "none">;
< def ReduxKindAdd  : I32EnumAttrCase<"ADD", 1, "add">;
< def ReduxKindAnd  : I32EnumAttrCase<"AND", 2, "and">;
< def ReduxKindMax  : I32EnumAttrCase<"MAX", 3, "max">;
< def ReduxKindMin  : I32EnumAttrCase<"MIN", 4, "min">;
< def ReduxKindOr   : I32EnumAttrCase<"OR", 5, "or">;
< def ReduxKindUmax : I32EnumAttrCase<"UMAX", 6, "umax">;
< def ReduxKindUmin : I32EnumAttrCase<"UMIN", 7, "umin">;
< def ReduxKindXor  : I32EnumAttrCase<"XOR", 8, "xor">; 
< 
< /// Enum attribute of the different kinds.
< def ReduxKind : I32EnumAttr<"ReduxKind", "NVVM redux kind",
<   [ReduxKindAdd, ReduxKindAnd, ReduxKindMax, ReduxKindMin, ReduxKindOr, 
<     ReduxKindUmax, ReduxKindUmin, ReduxKindXor]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = "::mlir::NVVM";
< }
< 
< def ReduxKindAttr : EnumAttr<NVVM_Dialect, ReduxKind, "redux_kind">;
< 
< def NVVM_ReduxOp :
<   NVVM_Op<"redux.sync">,
<   Results<(outs LLVM_Type:$res)>,
<   Arguments<(ins LLVM_Type:$val,
<                  ReduxKindAttr:$kind,
<                  I32:$mask_and_clamp)> {
<   string llvmBuilder = [{
<       auto intId = getReduxIntrinsicId($_resultType, $kind);
<       $res = createIntrinsicCall(builder, intId, {$val, $mask_and_clamp});
<   }];
<   let assemblyFormat = [{
<     $kind $val `,` $mask_and_clamp  attr-dict `:` type($val) `->` type($res)
<    }];   
< }
< 
< //===----------------------------------------------------------------------===//
268c230
<   let assemblyFormat = "$dst `,` $src `,` $size attr-dict `:` type(operands)";
---
>   let assemblyFormat = "$dst `,` $src `,` $size attr-dict";
388,390d349
<   list<list<WMMA_REGS>> i8_wmma_ops = MMA_OPS<
<             [GEOM<16, 16, 16>, GEOM<32, 8, 16>, GEOM<8, 32, 16>],
<             ["s8","u8"], [], ["s32"], []>.ret;
393,394c352
<             fp_wmma_ops,
<             i8_wmma_ops);
---
>             fp_wmma_ops);
398c356
<             ["a", "b"], ["f16","s8","u8"]>.ret;
---
>             ["a", "b"], ["f16"]>.ret;
401c359
<             ["c", "d"], ["f16", "f32","s32"]>.ret;
---
>             ["c", "d"], ["f16", "f32"]>.ret;
--- include/mlir/Dialect/LLVMIR/LLVMOpBase.td
17,18c17,18
< include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
< include "mlir/Dialect/LLVMIR/LLVMInterfaces.td"
---
> include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.td"
> include "mlir/IR/EnumAttr.td"
22a23,103
> // LLVM Dialect.
> //===----------------------------------------------------------------------===//
> 
> def LLVM_Dialect : Dialect {
>   let name = "llvm";
>   let cppNamespace = "::mlir::LLVM";
> 
>   let useDefaultAttributePrinterParser = 1;
>   let hasRegionArgAttrVerify = 1;
>   let hasRegionResultAttrVerify = 1;
>   let hasOperationAttrVerify = 1;
>   let useFoldAPI = kEmitFoldAdaptorFolder;
> 
>   let extraClassDeclaration = [{
>     /// Name of the data layout attributes.
>     static StringRef getDataLayoutAttrName() { return "llvm.data_layout"; }
>     static StringRef getAlignAttrName() { return "llvm.align"; }
>     static StringRef getNoAliasAttrName() { return "llvm.noalias"; }
>     static StringRef getReadonlyAttrName() { return "llvm.readonly"; }
>     static StringRef getNoAliasScopesAttrName() { return "noalias_scopes"; }
>     static StringRef getAliasScopesAttrName() { return "alias_scopes"; }
>     static StringRef getLoopAttrName() { return "llvm.loop"; }
>     static StringRef getParallelAccessAttrName() { return "parallel_access"; }
>     static StringRef getLoopOptionsAttrName() { return "options"; }
>     static StringRef getAccessGroupsAttrName() { return "access_groups"; }
>     static StringRef getStructAttrsAttrName() { return "llvm.struct_attrs"; }
>     static StringRef getByValAttrName() { return "llvm.byval"; }
>     static StringRef getByRefAttrName() { return "llvm.byref"; }
>     static StringRef getStructRetAttrName() { return "llvm.sret"; }
>     static StringRef getInAllocaAttrName() { return "llvm.inalloca"; }
>     static StringRef getNoUndefAttrName() { return "llvm.noundef"; }
>     static StringRef getSExtAttrName() { return "llvm.signext"; }
>     static StringRef getZExtAttrName() { return "llvm.zeroext"; }
>     static StringRef getTBAAAttrName() { return "llvm.tbaa"; }
> 
>     /// Verifies if the attribute is a well-formed value for "llvm.struct_attrs"
>     static LogicalResult verifyStructAttr(
>         Operation *op, Attribute attr, Type annotatedType);
> 
>     /// Verifies if the given string is a well-formed data layout descriptor.
>     /// Uses `reportError` to report errors.
>     static LogicalResult verifyDataLayoutString(
>         StringRef descr, llvm::function_ref<void (const Twine &)> reportError);
> 
>     /// Name of the target triple attribute.
>     static StringRef getTargetTripleAttrName() { return "llvm.target_triple"; }
> 
>     /// Name of the C wrapper emission attribute.
>     static StringRef getEmitCWrapperAttrName() {
>       return "llvm.emit_c_interface";
>     }
> 
>     /// Returns `true` if the given type is compatible with the LLVM dialect.
>     static bool isCompatibleType(Type);
> 
>     /// TODO Remove this once there is an alternative way of modeling memory
>     /// effects on FunctionOpInterface.
>     /// Name of the attribute that will cause the creation of a readnone memory
>     /// effect when lowering to the LLVMDialect.
>     static StringRef getReadnoneAttrName() {
>       return "llvm.readnone";
>     }
> 
>     Type parseType(DialectAsmParser &p) const override;
>     void printType(Type, DialectAsmPrinter &p) const override;
> 
>   private:
>     /// Register all types.
>     void registerTypes();
> 
>     /// A cache storing compatible LLVM types that have been verified. This
>     /// can save us lots of verification time if there are many occurrences
>     /// of some deeply-nested aggregate types in the program.
>     ThreadLocalCache<DenseSet<Type>> compatibleTypes;
> 
>     /// Register the attributes of this dialect.
>     void registerAttributes();
>   }];
> }
> 
> //===----------------------------------------------------------------------===//
71,74c152,156
<   And<[LLVM_PointerTo<I<width>>.predicate,
<        CPred<"$_self.cast<::mlir::LLVM::LLVMPointerType>().getAddressSpace()"
<              " == " # addressSpace>]>,
<   "LLVM pointer to " # I<width>.summary>;
---
>   LLVM_PointerTo<I<width>>.predicate,
>   "LLVM pointer to " # I<width>.summary>,
>   BuildableType<"::mlir::LLVM::LLVMPointerType::get("
>                 "::mlir::IntegerType::get($_builder.getContext(), "
>                 # width #"), "# addressSpace #")">;
182c264
< // Patterns for LLVM dialect operations.
---
> // Base classes for LLVM dialect operations.
185,224c267,320
< // Patterns with code to set flags and metadata of memory operations after their
< // translation to LLVM IR instructions. Operations may use the patterns to
< // implement their "llvmBuilder". The patterns assume the `op` and `inst`
< // variables exist and refer to the original MLIR operation and the translated
< // LLVM IR instruction, respectively.
< class LLVM_MemOpPatterns {
<   code setAlignmentCode = [{
<     if ($alignment.has_value()) {
<       auto align = *$alignment;
<       if (align != 0)
<         inst->setAlignment(llvm::Align(align));
<     }
<   }];
<   code setVolatileCode = [{
<     inst->setVolatile($volatile_);
<   }];
<   code setSyncScopeCode = [{
<     if ($syncscope.has_value()) {
<       llvm::LLVMContext &llvmContext = builder.getContext();
<       inst->setSyncScopeID(llvmContext.getOrInsertSyncScopeID(*$syncscope));
<     }
<   }];
<   code setOrderingCode = [{
<     inst->setAtomic(convertAtomicOrderingToLLVM($ordering));
<   }];
<   code setNonTemporalMetadataCode = [{
<     if ($nontemporal) {
<       llvm::MDNode *metadata = llvm::MDNode::get(
<           inst->getContext(), llvm::ConstantAsMetadata::get(
<               builder.getInt32(1)));
<       inst->setMetadata(llvm::LLVMContext::MD_nontemporal, metadata);
<     }
<   }];
<   code setAccessGroupsMetadataCode = [{
<     moduleTranslation.setAccessGroupsMetadata(op, inst);
<   }];
<   code setAliasAnalysisMetadataCode = [{
<     moduleTranslation.setAliasScopeMetadata(op, inst);
<     moduleTranslation.setTBAAMetadata(op, inst);
<   }];
---
> // Base class for LLVM operations. All operations get an "llvm." prefix in
> // their name automatically. LLVM operations have either zero or one result,
> // this class is specialized below for both cases and should not be used
> // directly.
> class LLVM_Op<string mnemonic, list<Trait> traits = []> :
>     LLVM_OpBase<LLVM_Dialect, mnemonic, traits>;
> 
> // Case of the LLVM enum attribute backed by I64Attr with customized string
> // representation that corresponds to what is visible in the textual IR form.
> // The parameters are as follows:
> //   - `cppSym`: name of the C++ enumerant for this case in MLIR API;
> //   - `irSym`: keyword used in the custom form of MLIR operation;
> //   - `llvmSym`: name of the C++ enumerant for this case in LLVM API.
> // For example, `LLVM_EnumAttrCase<"Weak", "weak", "WeakAnyLinkage">` is usable
> // as `<MlirEnumName>::Weak` in MLIR API, `WeakAnyLinkage` in LLVM API and
> // is printed/parsed as `weak` in MLIR custom textual format.
> class LLVM_EnumAttrCase<string cppSym, string irSym, string llvmSym, int val> :
>     I64EnumAttrCase<cppSym, val, irSym> {
> 
>   // The name of the equivalent enumerant in LLVM.
>   string llvmEnumerant = llvmSym;
> }
> 
> // LLVM enum attribute backed by I64Attr with string representation
> // corresponding to what is visible in the textual IR form.
> // The parameters are as follows:
> //   - `name`: name of the C++ enum class in MLIR API;
> //   - `llvmName`: name of the C++ enum in LLVM API;
> //   - `description`: textual description for documentation purposes;
> //   - `cases`: list of enum cases.
> // For example, `LLVM_EnumAttr<Linkage, "::llvm::GlobalValue::LinkageTypes`
> // produces `mlir::LLVM::Linkage` enum class in MLIR API that corresponds to (a
> // subset of) values in the `llvm::GlobalValue::LinkageTypes` in LLVM API.
> class LLVM_EnumAttr<string name, string llvmName, string description,
>                     list<LLVM_EnumAttrCase> cases> :
>     I64EnumAttr<name, description, cases> {
> 
>   // The equivalent enum class name in LLVM.
>   string llvmClassName = llvmName;
> }
> 
> // LLVM_CEnumAttr is functionally identical to LLVM_EnumAttr, but to be used for
> // non-class enums.
> class LLVM_CEnumAttr<string name, string llvmNS, string description,
>       list<LLVM_EnumAttrCase> cases> :
>     I64EnumAttr<name, description, cases> {
>   string llvmClassName = llvmNS;
> }
> 
> // For every value in the list, substitutes the value in the place of "$0" in
> // "pattern" and stores the list of strings as "lst".
> class ListIntSubst<string pattern, list<int> values> {
>   list<string> lst = !foreach(x, values,
>                               !subst("$0", !cast<string>(x), pattern));
241,270d336
< // For every value in the list, substitutes the value in the place of "$0" in
< // "pattern" and stores the list of strings as "lst".
< class ListIntSubst<string pattern, list<int> values> {
<   list<string> lst = !foreach(x, values,
<                               !subst("$0", !cast<string>(x), pattern));
< }
< 
< //===----------------------------------------------------------------------===//
< // Base classes for LLVM dialect operations.
< //===----------------------------------------------------------------------===//
< 
< // Base class for LLVM operations. All operations get an "llvm." prefix in
< // their name automatically and should either have zero or one result.
< class LLVM_Op<string mnemonic, list<Trait> traits = []> :
<     LLVM_OpBase<LLVM_Dialect, mnemonic, traits>;
< 
< // Base class for LLVM memory access operations that implement the access group
< // and alias analysis interfaces. The "aliasAttrs" list contains the arguments
< // required by the access group and alias analysis interfaces. Derived
< // operations should append the "aliasAttrs" to their argument list.
< class LLVM_MemAccessOpBase<string mnemonic, list<Trait> traits = []> :
<     LLVM_Op<mnemonic, !listconcat([
<       DeclareOpInterfaceMethods<AccessGroupOpInterface>,
<       DeclareOpInterfaceMethods<AliasAnalysisOpInterface>], traits)>,
<     LLVM_MemOpPatterns {
<   dag aliasAttrs = (ins OptionalAttr<SymbolRefArrayAttr>:$access_groups,
<                     OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
<                     OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
<                     OptionalAttr<SymbolRefArrayAttr>:$tbaa);
< }
286,291c352
< // translation starting from an LLVM IR intrinsic. The "requiresAccessGroup",
< // "requiresAliasAnalysis", and "requiresFastmath" flags specify which
< // interfaces the intrinsic implements. If the corresponding flags are set, the
< // "aliasAttrs" list contains the arguments required by the access group and
< // alias analysis interfaces. Derived intrinsics should append the "aliasAttrs"
< // to their argument list if they set one of the flags.
---
> // translation starting from an LLVM IR intrinsic.
295c356
<                       bit requiresAccessGroup = 0, bit requiresAliasAnalysis = 0,
---
>                       bit requiresAccessGroup = 0, bit requiresAliasScope = 0,
298,301d358
<         !if(!gt(requiresAccessGroup, 0),
<             [DeclareOpInterfaceMethods<AccessGroupOpInterface>], []),
<         !if(!gt(requiresAliasAnalysis, 0),
<             [DeclareOpInterfaceMethods<AliasAnalysisOpInterface>], []),
303c360,361
<             [DeclareOpInterfaceMethods<FastmathFlagsInterface>], []),
---
>             [DeclareOpInterfaceMethods<FastmathFlagsInterface>],
>             []),
305d362
<       LLVM_MemOpPatterns,
307,315d363
<   dag aliasAttrs = !con(
<         !if(!gt(requiresAccessGroup, 0),
<             (ins OptionalAttr<SymbolRefArrayAttr>:$access_groups),
<             (ins )),
<         !if(!gt(requiresAccessGroup, 0),
<             (ins OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
<                  OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
<                  OptionalAttr<SymbolRefArrayAttr>:$tbaa),
<             (ins )));
331,336c379,386
<     }] # [{
<     auto *inst = builder.CreateCall(fn, operands);
<     (void) inst;
<     }] # !if(!gt(requiresAccessGroup, 0), setAccessGroupsMetadataCode, "")
<        # !if(!gt(requiresAliasAnalysis, 0), setAliasAnalysisMetadataCode, "")
<        # !if(!gt(numResults, 0), "$res = inst;", "");
---
>     }] # [{auto *inst = builder.CreateCall(fn, operands);
>     }] # !if(!gt(requiresAccessGroup, 0),
>       "moduleTranslation.setAccessGroupsMetadata(op, inst);",
>       "(void) inst;")
>     # !if(!gt(requiresAliasScope, 0),
>       "moduleTranslation.setAliasScopeMetadata(op, inst);",
>       "(void) inst;")
>     # !if(!gt(numResults, 0), "$res = inst;", "");
357c407
<                   bit requiresAliasAnalysis = 0, bit requiresFastmath = 0>
---
>                   bit requiresAliasScope = 0, bit requiresFastmath = 0>
360c410
<                       numResults, requiresAccessGroup, requiresAliasAnalysis,
---
>                       numResults, requiresAccessGroup, requiresAliasScope,
378,382c428,429
<                             list<Trait> traits = [],
<                             bit requiresAccessGroup = 0,
<                             bit requiresAliasAnalysis = 0>
<     : LLVM_IntrOp<mnem, [], overloadedOperands, traits, /*numResults=*/0,
<                   requiresAccessGroup, requiresAliasAnalysis>;
---
>                             list<Trait> traits = []>
>     : LLVM_IntrOp<mnem, [], overloadedOperands, traits, 0>;
395c442
<                   /*requiresAccessGroup=*/0, /*requiresAliasAnalysis=*/0,
---
>                   /*requiresAccessGroup=*/0, /*requiresAliasScope=*/0,
--- include/mlir/Dialect/LLVMIR/ROCDLDialect.h
--- include/mlir/Dialect/LLVMIR/FunctionCallUtils.h
37,38d36
< LLVM::LLVMFuncOp lookupOrCreatePrintF16Fn(ModuleOp moduleOp);
< LLVM::LLVMFuncOp lookupOrCreatePrintBF16Fn(ModuleOp moduleOp);
41,42c39
< LLVM::LLVMFuncOp lookupOrCreatePrintStrFn(ModuleOp moduleOp,
<                                           bool opaquePointers);
---
> LLVM::LLVMFuncOp lookupOrCreatePrintStrFn(ModuleOp moduleOp);
47,53c44,49
< LLVM::LLVMFuncOp lookupOrCreateMallocFn(ModuleOp moduleOp, Type indexType,
<                                         bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateAlignedAllocFn(ModuleOp moduleOp, Type indexType,
<                                               bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateFreeFn(ModuleOp moduleOp, bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateGenericAllocFn(ModuleOp moduleOp, Type indexType,
<                                               bool opaquePointers);
---
> LLVM::LLVMFuncOp lookupOrCreateMallocFn(ModuleOp moduleOp, Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateAlignedAllocFn(ModuleOp moduleOp,
>                                               Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateFreeFn(ModuleOp moduleOp);
> LLVM::LLVMFuncOp lookupOrCreateGenericAllocFn(ModuleOp moduleOp,
>                                               Type indexType);
55,58c51,52
<                                                      Type indexType,
<                                                      bool opaquePointers);
< LLVM::LLVMFuncOp lookupOrCreateGenericFreeFn(ModuleOp moduleOp,
<                                              bool opaquePointers);
---
>                                                      Type indexType);
> LLVM::LLVMFuncOp lookupOrCreateGenericFreeFn(ModuleOp moduleOp);
65c59
<                                   Type resultType = {}, bool isVarArg = false);
---
>                                   Type resultType = {});
--- include/mlir/Dialect/LLVMIR/CMakeLists.txt
29,31c29,31
< set(LLVM_TARGET_DEFINITIONS LLVMInterfaces.td)
< mlir_tablegen(LLVMInterfaces.h.inc -gen-op-interface-decls)
< mlir_tablegen(LLVMInterfaces.cpp.inc -gen-op-interface-defs)
---
> set(LLVM_TARGET_DEFINITIONS LLVMOpsInterfaces.td)
> mlir_tablegen(LLVMOpsInterfaces.h.inc -gen-op-interface-decls)
> mlir_tablegen(LLVMOpsInterfaces.cpp.inc -gen-op-interface-defs)
34c34
< add_public_tablegen_target(MLIRLLVMInterfacesIncGen)
---
> add_public_tablegen_target(MLIRLLVMOpsInterfacesIncGen)
--- include/mlir/Dialect/LLVMIR/LLVMIntrinsicOps.td
6d5
< include "mlir/Dialect/LLVMIR/LLVMEnums.td"
7a7
> include "mlir/Dialect/LLVMIR/LLVMAttrDefs.td"
9d8
< include "mlir/Interfaces/Mem2RegInterfaces.td"
62,63c61
< class LLVM_TernarySameArgsIntrOpBase<string func, Type element,
<               list<Trait> traits = [], bit requiresFastmath = 0> :
---
> class LLVM_TernarySameArgsIntrOpF<string func, list<Trait> traits = []> :
66,69c64,69
<            requiresFastmath> {
<   dag commonArgs = (ins LLVM_ScalarOrVectorOf<element>:$a,
<                        LLVM_ScalarOrVectorOf<element>:$b,
<                        LLVM_ScalarOrVectorOf<element>:$c);
---
>            /*requiresFastmath=*/1> {
>   let arguments = (ins LLVM_ScalarOrVectorOf<AnyFloat>:$a,
>                        LLVM_ScalarOrVectorOf<AnyFloat>:$b,
>                        LLVM_ScalarOrVectorOf<AnyFloat>:$c,
>                        DefaultValuedAttr<LLVM_FastmathFlagsAttr,
>                                          "{}">:$fastmathFlags);
74,86d73
< class LLVM_TernarySameArgsIntrOpI<string func, list<Trait> traits = []> :
<     LLVM_TernarySameArgsIntrOpBase<func, AnySignlessInteger, traits> {
<   let arguments = commonArgs;
< }
< 
< class LLVM_TernarySameArgsIntrOpF<string func, list<Trait> traits = []> :
<     LLVM_TernarySameArgsIntrOpBase<func, LLVM_AnyFloat, traits,
<                                   /*requiresFastmath=*/1> {
<   dag fmfArg = (
<     ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
<   let arguments = !con(commonArgs, fmfArg);
< }
< 
125c112,113
<                                        [Pure], /*requiresFastmath=*/1> {
---
>     [DeclareOpInterfaceMethods<FastmathFlagsInterface>, Pure],
>     /*requiresFastmath=*/1> {
134d121
< def LLVM_ByteSwapOp : LLVM_UnaryIntrOpI<"bswap">;
138,139d124
< def LLVM_FshlOp : LLVM_TernarySameArgsIntrOpI<"fshl">;
< def LLVM_FshrOp : LLVM_TernarySameArgsIntrOpI<"fshr">;
149,209c134,152
< class LLVM_MemcpyIntrOpBase<string name> :
<     LLVM_ZeroResultIntrOp<name, [0, 1, 2], [], /*requiresAccessGroup=*/1,
<                                                /*requiresAliasAnalysis=*/1> {
<   dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
<                   Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
<                   AnySignlessInteger:$len, I1:$isVolatile);
<   // Append the alias attributes defined by LLVM_IntrOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let builders = [
<     OpBuilder<(ins "Value":$dst, "Value":$src, "Value":$len,
<                    "Value":$isVolatile), [{
<       build($_builder, $_state, dst, src, len, isVolatile,
<             /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
<             /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
<     }]
<   >];
< }
< 
< def LLVM_MemcpyOp : LLVM_MemcpyIntrOpBase<"memcpy">;
< def LLVM_MemcpyInlineOp : LLVM_MemcpyIntrOpBase<"memcpy.inline">;
< def LLVM_MemmoveOp : LLVM_MemcpyIntrOpBase<"memmove">;
< 
< def LLVM_MemsetOp : LLVM_ZeroResultIntrOp<"memset", [0, 2], [],
<                       /*requiresAccessGroup=*/1, /*requiresAliasAnalysis=*/1> {
<   dag args = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
<                   I8:$val, AnySignlessInteger:$len, I1:$isVolatile);
<   // Append the alias attributes defined by LLVM_IntrOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let builders = [
<     OpBuilder<(ins "Value":$dst, "Value":$val, "Value":$len,
<                     "Value":$isVolatile), [{
<       build($_builder, $_state, dst, val, len, isVolatile,
<             /*access_groups=*/nullptr, /*alias_scopes=*/nullptr,
<             /*noalias_scopes=*/nullptr, /*tbaa=*/nullptr);
<     }]
<   >];
< }
< 
< def LLVM_NoAliasScopeDeclOp
<     : LLVM_ZeroResultIntrOp<"experimental.noalias.scope.decl"> {
<   let arguments = (ins SymbolRefAttr:$scope);
<   string llvmBuilder = [{
<     // Wrap the scope argument into a list since the LLVM IR intrinsic takes
<     // a list containing exactly one scope rather than a scope itself.
<     llvm::MDNode* node = moduleTranslation.getAliasScopes(op, {$scope});
<     builder.CreateNoAliasScopeDeclaration(node);
<   }];
<   string mlirBuilder = [{
<     FailureOr<SmallVector<SymbolRefAttr>> scopeAttrs =
<       moduleImport.matchAliasScopeAttrs(llvmOperands[0]);
<     // Drop the intrinsic if the alias scope translation fails since the scope
<     // is not used by an aliasing operation, such as a load or store, that is
<     // used to convert the alias scope metadata.
<     if (failed(scopeAttrs))
<       return success();
<     if (scopeAttrs->size() != 1)
<       return failure();
<     $_op = $_builder.create<LLVM::NoAliasScopeDeclOp>(
<       $_location, (*scopeAttrs)[0]);
<   }];
<   let assemblyFormat = "$scope attr-dict";
---
> def LLVM_MemcpyOp : LLVM_ZeroResultIntrOp<"memcpy", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> def LLVM_MemcpyInlineOp : LLVM_ZeroResultIntrOp<"memcpy.inline", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> def LLVM_MemmoveOp : LLVM_ZeroResultIntrOp<"memmove", [0, 1, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        Arg<LLVM_AnyPointer,"",[MemRead]>:$src,
>                        AnySignlessInteger:$len, I1:$isVolatile);
> }
> 
> def LLVM_MemsetOp : LLVM_ZeroResultIntrOp<"memset", [0, 2]> {
>   let arguments = (ins Arg<LLVM_AnyPointer,"",[MemWrite]>:$dst,
>                        I8:$val, AnySignlessInteger:$len, I1:$isVolatile);
218,219c161
< class LLVM_LifetimeBaseOp<string opName> : LLVM_ZeroResultIntrOp<opName, [],
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> class LLVM_LifetimeBaseOp<string opName> : LLVM_ZeroResultIntrOp<opName> {
278c220
<     " attr-dict `:` functional-type(operands, results)";
---
>     " attr-dict `:` type($res)";
284c226
<   let assemblyFormat = "$token `,` $mem attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$token `,` $mem attr-dict `:` type($res)";
297c239
<   let assemblyFormat = "$handle attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$handle attr-dict `:` type($res)";
309c251
<   let assemblyFormat = "$handle `,` $unwind attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$handle `,` $unwind attr-dict `:` type($res)";
315c257
<   let assemblyFormat = "$id `,` $handle attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$id `,` $handle attr-dict `:` type($res)";
320c262
<   let assemblyFormat = "$handle attr-dict `:` qualified(type($handle))";
---
>   let assemblyFormat = "$handle attr-dict";
327,328c269
< class LLVM_DbgIntrOp<string name, string argName, list<Trait> traits = []>
<     : LLVM_IntrOp<name, [], [], traits, 0> {
---
> class LLVM_DbgIntrOp<string name, string argName> : LLVM_IntrOp<name, [], [], [], 0> {
353,357c294
< 
<     FailureOr<Value> argOperand = moduleImport.convertMetadataValue(llvmOperands[0]);
<     // Drop the intrinsic when its operand could not be converted. This can
<     // happen for use before definition cases that are allowed for debug
<     // intrinsics.
---
>     FailureOr<Value> argOperand = moduleImport.convertValue(llvmOperands[0]);
359c296
<       return success();
---
>       return failure();
369,370c306,311
< def LLVM_DbgDeclareOp : LLVM_DbgIntrOp< "dbg.declare", "addr",
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_DbgAddrOp : LLVM_DbgIntrOp<"dbg.addr", "addr"> {
>   let summary = "Describe the current address of a local debug info variable.";
>   let arguments = (ins LLVM_AnyPointer:$addr, LLVM_DILocalVariableAttr:$varInfo);
> }
> 
> def LLVM_DbgDeclareOp : LLVM_DbgIntrOp<"dbg.declare", "addr"> {
386c327
<   let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
---
>   let assemblyFormat = "$arg_list attr-dict";
392c333
<   let assemblyFormat = "$src_list `to` $dest_list attr-dict `:` type(operands)";
---
>   let assemblyFormat = "$src_list `to` $dest_list attr-dict";
398c339
<   let assemblyFormat = "$arg_list attr-dict `:` qualified(type($arg_list))";
---
>   let assemblyFormat = "$arg_list attr-dict";
408c349
<     let assemblyFormat = "$type_info attr-dict `:` functional-type(operands, results)";
---
>     let assemblyFormat = "$type_info attr-dict `:` type($res)";
421c362
<   let assemblyFormat = "$ptr attr-dict `:` qualified(type($ptr))";
---
>   let assemblyFormat = "$ptr attr-dict";
429c370
< class LLVM_VecReductionBase<string mnem, Type element, bit requiresFastmath=0>
---
> class LLVM_VecReductionBase<string mnem, Type element>
431,434c372,373
<                            [Pure, SameOperandsAndResultElementType],
<                            requiresFastmath> {
<       dag commonArgs = (ins LLVM_VectorOf<element>:$in);
< }
---
>                            [Pure, SameOperandsAndResultElementType]>,
>       Arguments<(ins LLVM_VectorOf<element>)>;
437,444c376
<     : LLVM_VecReductionBase<mnem, AnyFloat, /*requiresFastmath=*/1> {
<   dag fmfArg = (
<     ins DefaultValuedAttr<LLVM_FastmathFlagsAttr, "{}">:$fastmathFlags);
<   let arguments = !con(commonArgs, fmfArg);
< 
<   let assemblyFormat = "`(` operands `)` custom<LLVMOpAttrs>(attr-dict) `:` "
<       "functional-type(operands, results)";
< }
---
>     : LLVM_VecReductionBase<mnem, AnyFloat>;
447,449c379
<     : LLVM_VecReductionBase<mnem, AnySignlessInteger> {
<       let arguments = commonArgs;
< }
---
>     : LLVM_VecReductionBase<mnem, AnySignlessInteger>;
898,899c828
< def LLVM_VPFMulAddOp  : LLVM_VPTernaryF<"fmuladd">;
< def LLVM_VPFmaOp      : LLVM_VPTernaryF<"fma">;
---
> def LLVM_VPFmaOp  : LLVM_VPTernaryF<"fma">;
--- include/mlir/Dialect/LLVMIR/NVVMDialect.h
40,41c40
<                                              mlir::NVVM::MMAFrag frag, int nRow,
<                                              int nCol,
---
>                                              mlir::NVVM::MMAFrag frag,
--- include/mlir/Dialect/LLVMIR/LLVMOpsInterfaces.td
--- include/mlir/Dialect/LLVMIR/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/Transforms/Passes.h include/mlir/Dialect/LLVMIR/Transforms/Passes.h
21,22c21,22
< /// Create a pass to add DIScope to LLVMFuncOp that are missing it.
< std::unique_ptr<Pass> createDIScopeForLLVMFuncOpPass();
---
> /// Create a pass to remove BF16 types from LLVM IR.
> std::unique_ptr<Pass> createSoftwareBF16Pass();
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/LLVMIR/Transforms/Passes.td include/mlir/Dialect/LLVMIR/Transforms/Passes.td
38,39c38,39
< def DIScopeForLLVMFuncOp : Pass<"ensure-debug-info-scope-on-llvm-func", "LLVM::LLVMFuncOp"> {
<   let summary = "Materialize LLVM debug info subprogram attribute on every LLVMFuncOp";
---
> def SoftwareBF16 : Pass<"llvm-software-bf16"> {
>   let summary = "Convert BF16 to I16 in LLVM IR";
41,42c41
<     Having a debug info subprogram attribute on a function is required for
<     emitting line tables from MLIR FileLocCol locations.
---
>     This pass erases the BF16 type from LLVM IR.
44,47c43,47
<     This is not intended to be a proper replacement for frontends to emit
<     complete debug informations, however it is a convenient way to get line
<     tables for debugging purposes. This allow to step trough in a debugger
<     line-by-line or get a backtrace with line numbers.
---
>     Some LLVM targets do not support LLVM's `bfloat` type, or only support it
>     incompletely. To allow using the `bf16` type on such targets, this pass
>     replaces all of its uses by `i16` and then replaces operations on `bf16` by
>     extending the 16-bit values into `f32`, then computes the floating-point
>     operation on the extended value, and then truncates the results.
49,50c49
< 
<   let constructor = "mlir::LLVM::createDIScopeForLLVMFuncOpPass()";
---
>   let constructor = "mlir::LLVM::createSoftwareBF16Pass()";
--- include/mlir/Dialect/LLVMIR/Transforms/Passes.h
21,22c21,22
< /// Create a pass to add DIScope to LLVMFuncOp that are missing it.
< std::unique_ptr<Pass> createDIScopeForLLVMFuncOpPass();
---
> /// Create a pass to remove BF16 types from LLVM IR.
> std::unique_ptr<Pass> createSoftwareBF16Pass();
--- include/mlir/Dialect/LLVMIR/Transforms/Passes.td
38,39c38,39
< def DIScopeForLLVMFuncOp : Pass<"ensure-debug-info-scope-on-llvm-func", "LLVM::LLVMFuncOp"> {
<   let summary = "Materialize LLVM debug info subprogram attribute on every LLVMFuncOp";
---
> def SoftwareBF16 : Pass<"llvm-software-bf16"> {
>   let summary = "Convert BF16 to I16 in LLVM IR";
41,42c41
<     Having a debug info subprogram attribute on a function is required for
<     emitting line tables from MLIR FileLocCol locations.
---
>     This pass erases the BF16 type from LLVM IR.
44,47c43,47
<     This is not intended to be a proper replacement for frontends to emit
<     complete debug informations, however it is a convenient way to get line
<     tables for debugging purposes. This allow to step trough in a debugger
<     line-by-line or get a backtrace with line numbers.
---
>     Some LLVM targets do not support LLVM's `bfloat` type, or only support it
>     incompletely. To allow using the `bf16` type on such targets, this pass
>     replaces all of its uses by `i16` and then replaces operations on `bf16` by
>     extending the 16-bit values into `f32`, then computes the floating-point
>     operation on the extended value, and then truncates the results.
49,50c49
< 
<   let constructor = "mlir::LLVM::createDIScopeForLLVMFuncOpPass()";
---
>   let constructor = "mlir::LLVM::createSoftwareBF16Pass()";
--- include/mlir/Dialect/LLVMIR/Transforms/OptimizeForNVVM.h
--- include/mlir/Dialect/LLVMIR/Transforms/RequestCWrappers.h
--- include/mlir/Dialect/LLVMIR/Transforms/CMakeLists.txt
--- include/mlir/Dialect/LLVMIR/Transforms/LegalizeForExport.h
--- include/mlir/Dialect/LLVMIR/LLVMEnums.td
9,10c9,10
< #ifndef LLVMIR_ENUMS
< #define LLVMIR_ENUMS
---
> #ifndef LLVMIR_ENUMS_TD
> #define LLVMIR_ENUMS_TD
12,66c12
< include "mlir/Dialect/LLVMIR/LLVMDialect.td"
< include "mlir/IR/EnumAttr.td"
< 
< //===----------------------------------------------------------------------===//
< // Base classes for LLVM enum attributes.
< //===----------------------------------------------------------------------===//
< 
< // Case of the LLVM enum attribute backed by I64Attr with customized string
< // representation that corresponds to what is visible in the textual IR form.
< // The parameters are as follows:
< //   - `cppSym`: name of the C++ enumerant for this case in MLIR API;
< //   - `irSym`: keyword used in the custom form of MLIR operation;
< //   - `llvmSym`: name of the C++ enumerant for this case in LLVM API.
< // For example, `LLVM_EnumAttrCase<"Weak", "weak", "WeakAnyLinkage">` is usable
< // as `<MlirEnumName>::Weak` in MLIR API, `WeakAnyLinkage` in LLVM API and
< // is printed/parsed as `weak` in MLIR custom textual format.
< class LLVM_EnumAttrCase<string cppSym, string irSym, string llvmSym, int val> :
<     I64EnumAttrCase<cppSym, val, irSym> {
<   // The name of the equivalent enumerant in LLVM.
<   string llvmEnumerant = llvmSym;
< }
< 
< // LLVM enum attribute backed by I64Attr with string representation
< // corresponding to what is visible in the textual IR form.
< // The parameters are as follows:
< //   - `name`: name of the C++ enum class in MLIR API;
< //   - `llvmName`: name of the C++ enum in LLVM API;
< //   - `description`: textual description for documentation purposes;
< //   - `cases`: list of enum cases;
< //   - `unsupportedCases`: optional list of unsupported enum cases.
< // For example, `LLVM_EnumAttr<Linkage, "::llvm::GlobalValue::LinkageTypes`
< // produces `mlir::LLVM::Linkage` enum class in MLIR API that corresponds to (a
< // subset of) values in the `llvm::GlobalValue::LinkageTypes` in LLVM API.
< // All unsupported cases are excluded from the MLIR enum and trigger an error
< // during the import from LLVM IR. They are useful to handle sentinel values
< // such as `llvm::AtomicRMWInst::BinOp::BAD_BINOP` that LLVM commonly uses to
< // terminate its enums.
< class LLVM_EnumAttr<string name, string llvmName, string description,
<                     list<LLVM_EnumAttrCase> cases,
<                     list<LLVM_EnumAttrCase> unsupportedCases = []> :
<     I64EnumAttr<name, description, cases> {
<   // List of unsupported cases that have no conversion to an MLIR value.
<   list<LLVM_EnumAttrCase> unsupported = unsupportedCases;
< 
<   // The equivalent enum class name in LLVM.
<   string llvmClassName = llvmName;
< }
< 
< // LLVM_CEnumAttr is functionally identical to LLVM_EnumAttr, but to be used for
< // non-class enums.
< class LLVM_CEnumAttr<string name, string llvmNS, string description,
<       list<LLVM_EnumAttrCase> cases> :
<     I64EnumAttr<name, description, cases> {
<   string llvmClassName = llvmNS;
< }
---
> include "mlir/Dialect/LLVMIR/LLVMOpBase.td"
90,113c36,49
< def AtomicBinOpXchg : LLVM_EnumAttrCase<"xchg", "xchg", "Xchg", 0>;
< def AtomicBinOpAdd  : LLVM_EnumAttrCase<"add", "add", "Add", 1>;
< def AtomicBinOpSub  : LLVM_EnumAttrCase<"sub", "sub", "Sub", 2>;
< def AtomicBinOpAnd  : LLVM_EnumAttrCase<"_and", "_and", "And", 3>;
< def AtomicBinOpNand : LLVM_EnumAttrCase<"nand", "nand", "Nand", 4>;
< def AtomicBinOpOr   : LLVM_EnumAttrCase<"_or", "_or", "Or", 5>;
< def AtomicBinOpXor  : LLVM_EnumAttrCase<"_xor", "_xor", "Xor", 6>;
< def AtomicBinOpMax  : LLVM_EnumAttrCase<"max", "max", "Max", 7>;
< def AtomicBinOpMin  : LLVM_EnumAttrCase<"min", "min", "Min", 8>;
< def AtomicBinOpUMax : LLVM_EnumAttrCase<"umax", "umax", "UMax", 9>;
< def AtomicBinOpUMin : LLVM_EnumAttrCase<"umin", "umin", "UMin", 10>;
< def AtomicBinOpFAdd : LLVM_EnumAttrCase<"fadd", "fadd", "FAdd", 11>;
< def AtomicBinOpFSub : LLVM_EnumAttrCase<"fsub", "fsub", "FSub", 12>;
< def AtomicBinOpFMax : LLVM_EnumAttrCase<"fmax", "fmax", "FMax", 13>;
< def AtomicBinOpFMin : LLVM_EnumAttrCase<"fmin", "fmin", "FMin", 14>;
< def AtomicBinOpUIncWrap : LLVM_EnumAttrCase<"uinc_wrap",
<                                             "uinc_wrap", "UIncWrap", 15>;
< def AtomicBinOpUDecWrap : LLVM_EnumAttrCase<"udec_wrap",
<                                             "udec_wrap", "UDecWrap", 16>;
< 
< // A sentinel value that has no MLIR counterpart.
< def AtomicBadBinOp : LLVM_EnumAttrCase<"", "", "BAD_BINOP", 0>;
< 
< def AtomicBinOp : LLVM_EnumAttr<
---
> def AtomicBinOpXchg : I64EnumAttrCase<"xchg", 0>;
> def AtomicBinOpAdd  : I64EnumAttrCase<"add", 1>;
> def AtomicBinOpSub  : I64EnumAttrCase<"sub", 2>;
> def AtomicBinOpAnd  : I64EnumAttrCase<"_and", 3>;
> def AtomicBinOpNand : I64EnumAttrCase<"nand", 4>;
> def AtomicBinOpOr   : I64EnumAttrCase<"_or", 5>;
> def AtomicBinOpXor  : I64EnumAttrCase<"_xor", 6>;
> def AtomicBinOpMax  : I64EnumAttrCase<"max", 7>;
> def AtomicBinOpMin  : I64EnumAttrCase<"min", 8>;
> def AtomicBinOpUMax : I64EnumAttrCase<"umax", 9>;
> def AtomicBinOpUMin : I64EnumAttrCase<"umin", 10>;
> def AtomicBinOpFAdd : I64EnumAttrCase<"fadd", 11>;
> def AtomicBinOpFSub : I64EnumAttrCase<"fsub", 12>;
> def AtomicBinOp : I64EnumAttr<
115d50
<     "::llvm::AtomicRMWInst::BinOp",
120,122c55
<      AtomicBinOpFSub, AtomicBinOpFMax, AtomicBinOpFMin, AtomicBinOpUIncWrap,
<      AtomicBinOpUDecWrap],
<     [AtomicBadBinOp]> {
---
>      AtomicBinOpFSub]> {
126,140c59,66
< def AtomicOrderingNotAtomic : LLVM_EnumAttrCase<"not_atomic",
<                                                 "not_atomic", "NotAtomic", 0>;
< def AtomicOrderingUnordered : LLVM_EnumAttrCase<"unordered",
<                                                 "unordered", "Unordered", 1>;
< def AtomicOrderingMonotonic : LLVM_EnumAttrCase<"monotonic",
<                                                 "monotonic", "Monotonic", 2>;
< def AtomicOrderingAcquire   : LLVM_EnumAttrCase<"acquire",
<                                                 "acquire", "Acquire", 4>;
< def AtomicOrderingRelease   : LLVM_EnumAttrCase<"release",
<                                                 "release", "Release", 5>;
< def AtomicOrderingAcquireRelease :
<       LLVM_EnumAttrCase<"acq_rel", "acq_rel", "AcquireRelease", 6>;
< def AtomicOrderingSequentiallyConsistent :
<       LLVM_EnumAttrCase<"seq_cst", "seq_cst", "SequentiallyConsistent", 7>;
< def AtomicOrdering : LLVM_EnumAttr<
---
> def AtomicOrderingNotAtomic              : I64EnumAttrCase<"not_atomic", 0>;
> def AtomicOrderingUnordered              : I64EnumAttrCase<"unordered", 1>;
> def AtomicOrderingMonotonic              : I64EnumAttrCase<"monotonic", 2>;
> def AtomicOrderingAcquire                : I64EnumAttrCase<"acquire", 4>;
> def AtomicOrderingRelease                : I64EnumAttrCase<"release", 5>;
> def AtomicOrderingAcquireRelease         : I64EnumAttrCase<"acq_rel", 6>;
> def AtomicOrderingSequentiallyConsistent : I64EnumAttrCase<"seq_cst", 7>;
> def AtomicOrdering : I64EnumAttr<
142d67
<     "::llvm::AtomicOrdering",
146,147c71
<      AtomicOrderingSequentiallyConsistent
<     ]> {
---
>      AtomicOrderingSequentiallyConsistent]> {
208,210c132,133
< def CConvHHVM : LLVM_EnumAttrCase<"DUMMY_HHVM", "hhvmcc", "DUMMY_HHVM", 81>;
< def CConvHHVM_C
<     : LLVM_EnumAttrCase<"DUMMY_HHVM_C", "hhvm_ccc", "DUMMY_HHVM_C", 82>;
---
> def CConvHHVM : LLVM_EnumAttrCase<"HHVM", "hhvmcc", "HHVM", 81>;
> def CConvHHVM_C : LLVM_EnumAttrCase<"HHVM_C", "hhvm_ccc", "HHVM_C", 82>;
459,463d381
< def LLVM_FastmathFlagsAttr :
<     EnumAttr<LLVM_Dialect, FastmathFlags, "fastmath"> {
<   let assemblyFormat = "`<` $value `>`";
< }
< 
465,508c383,404
< // FCmp and ICmp Predicates
< //===----------------------------------------------------------------------===//
< 
< // Predicates for float comparisons
< def FCmpPredicateFALSE : LLVM_EnumAttrCase<"_false", "_false", "FCMP_FALSE", 0>;
< def FCmpPredicateOEQ   : LLVM_EnumAttrCase<"oeq", "oeq", "FCMP_OEQ", 1>;
< def FCmpPredicateOGT   : LLVM_EnumAttrCase<"ogt", "ogt", "FCMP_OGT", 2>;
< def FCmpPredicateOGE   : LLVM_EnumAttrCase<"oge", "oge", "FCMP_OGE", 3>;
< def FCmpPredicateOLT   : LLVM_EnumAttrCase<"olt", "olt", "FCMP_OLT", 4>;
< def FCmpPredicateOLE   : LLVM_EnumAttrCase<"ole", "ole", "FCMP_OLE", 5>;
< def FCmpPredicateONE   : LLVM_EnumAttrCase<"one", "one", "FCMP_ONE", 6>;
< def FCmpPredicateORD   : LLVM_EnumAttrCase<"ord", "ord", "FCMP_ORD", 7>;
< def FCmpPredicateUEQ   : LLVM_EnumAttrCase<"ueq", "ueq", "FCMP_UEQ", 8>;
< def FCmpPredicateUGT   : LLVM_EnumAttrCase<"ugt", "ugt", "FCMP_UGT", 9>;
< def FCmpPredicateUGE   : LLVM_EnumAttrCase<"uge", "uge", "FCMP_UGE", 10>;
< def FCmpPredicateULT   : LLVM_EnumAttrCase<"ult", "ult", "FCMP_ULT", 11>;
< def FCmpPredicateULE   : LLVM_EnumAttrCase<"ule", "ule", "FCMP_ULE", 12>;
< def FCmpPredicateUNE   : LLVM_EnumAttrCase<"une", "une", "FCMP_UNE", 13>;
< def FCmpPredicateUNO   : LLVM_EnumAttrCase<"uno", "uno", "FCMP_UNO", 14>;
< def FCmpPredicateTRUE  : LLVM_EnumAttrCase<"_true", "_true", "FCMP_TRUE", 15>;
< 
< // A sentinel value that has no MLIR counterpart.
< def ICmpPredicateBad : LLVM_EnumAttrCase<"", "", "BAD_ICMP_PREDICATE", 0>;
< 
< // Predicates for integer comparisons.
< def ICmpPredicateEQ  : LLVM_EnumAttrCase<"eq", "eq", "ICMP_EQ", 0>;
< def ICmpPredicateNE  : LLVM_EnumAttrCase<"ne", "ne", "ICMP_NE", 1>;
< def ICmpPredicateSLT : LLVM_EnumAttrCase<"slt", "slt", "ICMP_SLT", 2>;
< def ICmpPredicateSLE : LLVM_EnumAttrCase<"sle", "sle", "ICMP_SLE", 3>;
< def ICmpPredicateSGT : LLVM_EnumAttrCase<"sgt", "sgt", "ICMP_SGT", 4>;
< def ICmpPredicateSGE : LLVM_EnumAttrCase<"sge", "sge", "ICMP_SGE", 5>;
< def ICmpPredicateULT : LLVM_EnumAttrCase<"ult", "ult", "ICMP_ULT", 6>;
< def ICmpPredicateULE : LLVM_EnumAttrCase<"ule", "ule", "ICMP_ULE", 7>;
< def ICmpPredicateUGT : LLVM_EnumAttrCase<"ugt", "ugt", "ICMP_UGT", 8>;
< def ICmpPredicateUGE : LLVM_EnumAttrCase<"uge", "uge", "ICMP_UGE", 9>;
< 
< // A sentinel value that has no MLIR counterpart.
< def FCmpPredicateBad : LLVM_EnumAttrCase<"", "", "BAD_FCMP_PREDICATE", 0>;
< 
< // LLVM's predicate enum contains the floating-point and integer comparison
< // cases, while the LLVM dialect uses two separate enums. The floating-point
< // predicate enum thus defines all integer predicates as unsupported and
< // vice versa.
< def FCmpPredicate : LLVM_EnumAttr<
---
> // FCmp Predicates
> //===----------------------------------------------------------------------===//
> 
> // Predicate for float comparisons
> def FCmpPredicateFALSE  : I64EnumAttrCase<"_false", 0>;
> def FCmpPredicateOEQ    : I64EnumAttrCase<"oeq", 1>;
> def FCmpPredicateOGT    : I64EnumAttrCase<"ogt", 2>;
> def FCmpPredicateOGE    : I64EnumAttrCase<"oge", 3>;
> def FCmpPredicateOLT    : I64EnumAttrCase<"olt", 4>;
> def FCmpPredicateOLE    : I64EnumAttrCase<"ole", 5>;
> def FCmpPredicateONE    : I64EnumAttrCase<"one", 6>;
> def FCmpPredicateORD    : I64EnumAttrCase<"ord", 7>;
> def FCmpPredicateUEQ    : I64EnumAttrCase<"ueq", 8>;
> def FCmpPredicateUGT    : I64EnumAttrCase<"ugt", 9>;
> def FCmpPredicateUGE    : I64EnumAttrCase<"uge", 10>;
> def FCmpPredicateULT    : I64EnumAttrCase<"ult", 11>;
> def FCmpPredicateULE    : I64EnumAttrCase<"ule", 12>;
> def FCmpPredicateUNE    : I64EnumAttrCase<"une", 13>;
> def FCmpPredicateUNO    : I64EnumAttrCase<"uno", 14>;
> def FCmpPredicateTRUE   : I64EnumAttrCase<"_true", 15>;
> 
> def FCmpPredicate : I64EnumAttr<
510d405
<     "::llvm::CmpInst::Predicate",
515,518c410
<      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE],
<     [ICmpPredicateEQ, ICmpPredicateNE, ICmpPredicateSLT, ICmpPredicateSLE,
<      ICmpPredicateSGT, ICmpPredicateSGE, ICmpPredicateULT, ICmpPredicateULE,
<      ICmpPredicateUGT, ICmpPredicateUGE, FCmpPredicateBad, ICmpPredicateBad
---
>      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE
523c415,430
< def ICmpPredicate : LLVM_EnumAttr<
---
> //===----------------------------------------------------------------------===//
> // ICmp Predicates
> //===----------------------------------------------------------------------===//
> 
> // Predicate for integer comparisons.
> def ICmpPredicateEQ  : I64EnumAttrCase<"eq", 0>;
> def ICmpPredicateNE  : I64EnumAttrCase<"ne", 1>;
> def ICmpPredicateSLT : I64EnumAttrCase<"slt", 2>;
> def ICmpPredicateSLE : I64EnumAttrCase<"sle", 3>;
> def ICmpPredicateSGT : I64EnumAttrCase<"sgt", 4>;
> def ICmpPredicateSGE : I64EnumAttrCase<"sge", 5>;
> def ICmpPredicateULT : I64EnumAttrCase<"ult", 6>;
> def ICmpPredicateULE : I64EnumAttrCase<"ule", 7>;
> def ICmpPredicateUGT : I64EnumAttrCase<"ugt", 8>;
> def ICmpPredicateUGE : I64EnumAttrCase<"uge", 9>;
> def ICmpPredicate : I64EnumAttr<
525,526c432
<     "::llvm::CmpInst::Predicate",
<     "lvm.icmp comparison predicate",
---
>     "llvm.icmp comparison predicate",
529,535c435
<      ICmpPredicateUGT, ICmpPredicateUGE],
<     [FCmpPredicateFALSE, FCmpPredicateOEQ, FCmpPredicateOGT, FCmpPredicateOGE,
<      FCmpPredicateOLT, FCmpPredicateOLE, FCmpPredicateONE, FCmpPredicateORD,
<      FCmpPredicateUEQ, FCmpPredicateUGT, FCmpPredicateUGE, FCmpPredicateULT,
<      FCmpPredicateULE, FCmpPredicateUNE, FCmpPredicateUNO, FCmpPredicateTRUE,
<      FCmpPredicateBad, ICmpPredicateBad
<     ]> {
---
>      ICmpPredicateUGT, ICmpPredicateUGE]> {
592a493,511
> // LoopOptions
> //===----------------------------------------------------------------------===//
> 
> def LOptDisableUnroll : I32EnumAttrCase<"disable_unroll", 1>;
> def LOptDisableLICM : I32EnumAttrCase<"disable_licm", 2>;
> def LOptInterleaveCount : I32EnumAttrCase<"interleave_count", 3>;
> def LOptDisablePipeline : I32EnumAttrCase<"disable_pipeline", 4>;
> def LOptPipelineInitiationInterval : I32EnumAttrCase<"pipeline_initiation_interval", 5>;
> 
> def LoopOptionCase : I32EnumAttr<
>     "LoopOptionCase",
>     "LLVM loop option",
>     [LOptDisableUnroll, LOptDisableLICM, LOptInterleaveCount,
>      LOptDisablePipeline, LOptPipelineInitiationInterval
>     ]> {
>   let cppNamespace = "::mlir::LLVM";
> }
> 
> //===----------------------------------------------------------------------===//
609,627d527
< // Visibility
< //===----------------------------------------------------------------------===//
< 
< def VisibilityDefault
<     : LLVM_EnumAttrCase<"Default", "", "DefaultVisibility", 0>;
< def VisibilityHidden
<     : LLVM_EnumAttrCase<"Hidden", "hidden", "HiddenVisibility", 1>;
< def VisibilityProtected
<     : LLVM_EnumAttrCase<"Protected", "protected", "ProtectedVisibility", 2>;
< 
< def Visibility : LLVM_EnumAttr<
<     "Visibility",
<     "::llvm::GlobalValue::VisibilityTypes",
<     "LLVM GlobalValue Visibility",
<     [VisibilityDefault, VisibilityHidden, VisibilityProtected]> {
<   let cppNamespace = "::mlir::LLVM";
< }
< 
< //===----------------------------------------------------------------------===//
644c544
< #endif // LLVMIR_ENUMS
---
> #endif // LLVMIR_ENUMS_TD
--- include/mlir/Dialect/LLVMIR/LLVMTypes.td
13a14
> include "mlir/IR/SubElementInterfaces.td"
27c28,29
<     DeclareTypeInterfaceMethods<DataLayoutTypeInterface, ["getTypeSize"]>]> {
---
>     DeclareTypeInterfaceMethods<DataLayoutTypeInterface, ["getTypeSize"]>,
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
63c65,66
< def LLVMFunctionType : LLVMType<"LLVMFunction", "func"> {
---
> def LLVMFunctionType : LLVMType<"LLVMFunction", "func", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
124c127,128
<       "areCompatible", "verifyEntries"]>]> {
---
>       "areCompatible", "verifyEntries"]>,
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
170c174,175
< def LLVMFixedVectorType : LLVMType<"LLVMFixedVector", "vec"> {
---
> def LLVMFixedVectorType : LLVMType<"LLVMFixedVector", "vec", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
200c205,206
< def LLVMScalableVectorType : LLVMType<"LLVMScalableVector", "vec"> {
---
> def LLVMScalableVectorType : LLVMType<"LLVMScalableVector", "vec", [
>     DeclareTypeInterfaceMethods<SubElementTypeInterface>]> {
--- include/mlir/Dialect/LLVMIR/ROCDLOps.td
27a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
56a58,63
> class ROCDL_IntrOp<string mnemonic, list<int> overloadedResults,
>   list<int> overloadedOperands, list<Trait> traits, int numResults> :
>   LLVM_IntrOpBase<ROCDL_Dialect,  mnemonic,
>   "amdgcn_" # !subst(".", "_", mnemonic), overloadedResults,
>   overloadedOperands, traits, numResults>;
> 
126a134,146
> def ROCDL_SetPrioOp : ROCDL_IntrOp<"s.setprio", [], [], [], 0>,
>   Arguments<(ins I16:$priority)> {
>   let results = (outs);
>   let assemblyFormat = "$priority attr-dict";
> }
> 
> def ROCDL_SchedBarrier : ROCDL_IntrOp<"sched.barrier", [], [], [], 0>,
>   Arguments<(ins I32:$mask)> {
>   let results = (outs);
>   let assemblyFormat = "$mask attr-dict";
> }
> 
> 
185a206,225
> // WMMA intrinsics
> class ROCDL_Wmma_IntrOp<string mnemonic, list<Trait> traits = []> :
>   LLVM_IntrOpBase<ROCDL_Dialect, mnemonic,
>                   "amdgcn_" # !subst(".","_", mnemonic),
>                   [0], [], traits, 1>,
>   Arguments<(ins Variadic<LLVM_Type>:$args)> {
>   let assemblyFormat =
>     "$args attr-dict `:` functional-type($args, $res)";
> }
> 
> // Available on RDNA3
> def ROCDL_wmma_f32_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.f16">;
> def ROCDL_wmma_f32_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.f32.16x16x16.bf16">;
> def ROCDL_wmma_f16_16x16x16_f16 : ROCDL_Wmma_IntrOp<"wmma.f16.16x16x16.f16">;
> def ROCDL_wmma_bf16_16x16x16_bf16 : ROCDL_Wmma_IntrOp<"wmma.bf16.16x16x16.bf16">;
> def ROCDL_wmma_i32_16x16x16_iu8 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu8">;
> def ROCDL_wmma_i32_16x16x16_iu4 : ROCDL_Wmma_IntrOp<"wmma.i32.16x16x16.iu4">;
> 
> 
> //===---------------------------------------------------------------------===//
254a295,313
> def ROCDL_RawBufferAtomicCmpSwap :
>   ROCDL_Op<"raw.buffer.atomic.cmpswap", [AllTypesMatch<["res", "src", "cmp"]>]>,
>   Results<(outs LLVM_Type:$res)>,
>   Arguments<(ins LLVM_Type:$src,
>                  LLVM_Type:$cmp,
>                  LLVM_Type:$rsrc,
>                  I32:$offset,
>                  I32:$soffset,
>                  I32:$aux)>{
>   string llvmBuilder = [{
>       $res = createIntrinsicCall(builder,
>           llvm::Intrinsic::amdgcn_raw_buffer_atomic_cmpswap, {$src, $cmp, $rsrc,
>             $offset, $soffset, $aux}, {$_resultType});
>   }];
>   let assemblyFormat = [{
>     attr-dict `(` operands `)` `:` type($res) `,` type($rsrc)
>   }];
> }
> 
274,276d332
< //===---------------------------------------------------------------------===//
< // Buffer atomic floating point max intrinsic. GFX9 does not support fp32.
< 
293,295d348
< //===---------------------------------------------------------------------===//
< // Buffer atomic signed integer max intrinsic.
< 
312,314d364
< //===---------------------------------------------------------------------===//
< // Buffer atomic unsigned integer min intrinsic.
< 
328a379,390
> }
> 
> //===---------------------------------------------------------------------===//
> // DPP Move intrinsic
> def ROCDL_DPPMovOp : ROCDL_IntrOp<"mov.dpp", [], [0],
>     [AllTypesMatch<["res", "src"]>], 1>,
>   Arguments<(ins LLVM_Type:$src, I32:$dppCtrl, I32:$rowMask,
>       I32:$bankMask, I1:$boundCtrl)> {
>   let results = (outs LLVM_Type:$res);
>   let assemblyFormat = [{
>     attr-dict $src `with` $dppCtrl `,` $rowMask `,` $bankMask `,` $boundCtrl `:` type($src)
>   }];
--- include/mlir/Dialect/LLVMIR/LLVMOps.td
17d16
< include "mlir/Dialect/LLVMIR/LLVMEnums.td"
18a18
> include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.td"
25d24
< include "mlir/Interfaces/Mem2RegInterfaces.td"
127,128c126
<     $res = builder.CreateICmp(
<             convertICmpPredicateToLLVM($predicate), $lhs, $rhs);
---
>     $res = builder.CreateICmp(getLLVMCmpPredicate($predicate), $lhs, $rhs);
132,133c130,131
<     $res = $_builder.create<$_qualCppClassName>($_location,
<             convertICmpPredicateFromLLVM(iCmpInst->getPredicate()), $lhs, $rhs);
---
>     $res = $_builder.create<$_qualCppClassName>(
>       $_location, getICmpPredicate(iCmpInst->getPredicate()), $lhs, $rhs);
151c149
<     $res = builder.CreateFCmp(convertFCmpPredicateToLLVM($predicate), $lhs, $rhs);
---
>     $res = builder.CreateFCmp(getLLVMCmpPredicate($predicate), $lhs, $rhs);
156c154
<       $_location, convertFCmpPredicateFromLLVM(fCmpInst->getPredicate()), $lhs, $rhs);
---
>       $_location, getFCmpPredicate(fCmpInst->getPredicate()), $lhs, $rhs);
173a172,208
> // Common code definition that is used to verify and set the alignment attribute
> // of LLVM ops that accept such an attribute.
> class MemoryOpWithAlignmentBase {
>   code setAlignmentCode = [{
>     if ($alignment.has_value()) {
>       auto align = *$alignment;
>       if (align != 0)
>         inst->setAlignment(llvm::Align(align));
>     }
>   }];
> }
> 
> // Code definition that is used for nontemporal metadata creation.
> class MemoryOpWithAlignmentAndAttributes : MemoryOpWithAlignmentBase {
>   code setNonTemporalMetadataCode = [{
>     if ($nontemporal) {
>       llvm::Module *module = builder.GetInsertBlock()->getModule();
>       llvm::MDNode *metadata = llvm::MDNode::get(
>           inst->getContext(), llvm::ConstantAsMetadata::get(
>               builder.getInt32(1)));
>       inst->setMetadata(module->getMDKindID("nontemporal"), metadata);
>     }
>   }];
> 
>   code setAccessGroupsMetadataCode = [{
>     moduleTranslation.setAccessGroupsMetadata(op, inst);
>   }];
> 
>   code setAliasScopeMetadataCode = [{
>     moduleTranslation.setAliasScopeMetadata(op, inst);
>   }];
> 
>   code setTBAAMetadataCode = [{
>     moduleTranslation.setTBAAMetadata(op, inst);
>   }];
> }
> 
175,177c210
< def LLVM_AllocaOp : LLVM_Op<"alloca", 
<     [DeclareOpInterfaceMethods<PromotableAllocationOpInterface>]>,
<   LLVM_MemOpPatterns {
---
> def LLVM_AllocaOp : LLVM_Op<"alloca">, MemoryOpWithAlignmentBase {
180,181c213
<                    OptionalAttr<TypeAttr>:$elem_type,
<                    UnitAttr:$inalloca);
---
>                    OptionalAttr<TypeAttr>:$elem_type);
192d223
<     inst->setUsedWithInAlloca($inalloca);
194a226
>   // FIXME: Import attributes.
201,203c233
<       $_location, $_resultType, $arraySize,
<       alignment == 0 ? IntegerAttr() : $_builder.getI64IntegerAttr(alignment),
<       TypeAttr::get(allocatedType), allocaInst->isUsedWithInAlloca());
---
>       $_location, $_resultType, allocatedType, $arraySize, alignment);
213c243
<                      TypeAttr(), false);
---
>                      TypeAttr());
215c245
<         $_builder.getI64IntegerAttr(alignment), TypeAttr(), false);
---
>         $_builder.getI64IntegerAttr(alignment), TypeAttr());
226c256
<             elemTypeAttr, false);
---
>             elemTypeAttr);
234,235c264
< def LLVM_GEPOp : LLVM_Op<"getelementptr", [Pure,
<     DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_GEPOp : LLVM_Op<"getelementptr", [Pure]> {
318,328c347,353
< def LLVM_LoadOp : LLVM_MemAccessOpBase<"load",
<     [DeclareOpInterfaceMethods<PromotableMemOpInterface>]> {
<   dag args = (ins Arg<LLVM_PointerTo<LLVM_LoadableType>, "", [MemRead]>:$addr,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_,
<               UnitAttr:$nontemporal,
<               DefaultValuedAttr<
<                 AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
<               OptionalAttr<StrAttr>:$syncscope);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> def LLVM_LoadOp : LLVM_Op<"load">, MemoryOpWithAlignmentAndAttributes {
>   let arguments = (ins Arg<LLVM_PointerTo<LLVM_LoadableType>, "", [MemRead]>:$addr,
>                    OptionalAttr<SymbolRefArrayAttr>:$access_groups,
>                    OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
>                    OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
>                    OptionalAttr<I64Attr>:$alignment, UnitAttr:$volatile_,
>                    UnitAttr:$nontemporal);
331,359d355
<   let description = [{
<     The `load` operation is used to read from memory. A load may be marked as
<     atomic, volatile, and/or nontemporal, and takes a number of optional
<     attributes that specify aliasing information.
< 
<     An atomic load only supports a limited set of pointer, integer, and
<     floating point types, and requires an explicit alignment.
< 
<     Examples:
<     ```mlir
<     // A volatile load of a float variable.
<     %0 = llvm.load volatile %ptr : !llvm.ptr -> f32
< 
<     // A nontemporal load of a float variable.
<     %0 = llvm.load %ptr {nontemporal} : !llvm.ptr -> f32
< 
<     // An atomic load of an integer variable.
<     %0 = llvm.load %ptr atomic monotonic {alignment = 8 : i64}
<         : !llvm.ptr -> i64
<     ```
< 
<     See the following link for more details:
<     https://llvm.org/docs/LangRef.html#load-instruction
<   }];
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $addr
<     (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
<     attr-dict `:` custom<LoadType>(type($addr), type($res))
<   }];
362,365c358
<     $res = inst;
<   }] # setOrderingCode
<      # setSyncScopeCode
<      # setAlignmentCode
---
>   }] # setAlignmentCode
368c361,366
<      # setAliasAnalysisMetadataCode;
---
>      # setAliasScopeMetadataCode
>      # setTBAAMetadataCode
>      # [{
>     $res = inst;
>   }];
>   // FIXME: Import attributes.
370,376c368
<     auto *loadInst = cast<llvm::LoadInst>(inst);
<     unsigned alignment = loadInst->getAlign().value();
<     $res = $_builder.create<LLVM::LoadOp>($_location, $_resultType, $addr,
<         alignment, loadInst->isVolatile(),
<         loadInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
<         convertAtomicOrderingFromLLVM(loadInst->getOrdering()),
<         getLLVMSyncScope(loadInst));
---
>     $res = $_builder.create<LLVM::LoadOp>($_location, $_resultType, $addr);
380,381c372,379
<       CArg<"bool", "false">:$isVolatile, CArg<"bool", "false">:$isNonTemporal)>,
<     OpBuilder<(ins "Type":$type, "Value":$addr,
---
>       CArg<"bool", "false">:$isVolatile, CArg<"bool", "false">:$isNonTemporal),
>     [{
>       auto type = addr.getType().cast<LLVMPointerType>().getElementType();
>       assert(type && "must provide explicit element type to the constructor "
>                      "when the pointer type is opaque");
>       build($_builder, $_state, type, addr, alignment, isVolatile, isNonTemporal);
>     }]>,
>     OpBuilder<(ins "Type":$t, "Value":$addr,
383,386c381,382
<       CArg<"bool", "false">:$isNonTemporal,
<       CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>       CArg<"bool", "false">:$isNonTemporal)>,];
>   let hasCustomAssemblyFormat = 1;
390,401c386,393
< def LLVM_StoreOp : LLVM_MemAccessOpBase<"store",
<     [DeclareOpInterfaceMethods<PromotableMemOpInterface>]> {
<   dag args = (ins LLVM_LoadableType:$value,
<               Arg<LLVM_PointerTo<LLVM_LoadableType>,"",[MemWrite]>:$addr,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_,
<               UnitAttr:$nontemporal,
<               DefaultValuedAttr<
<                 AtomicOrdering, "AtomicOrdering::not_atomic">:$ordering,
<               OptionalAttr<StrAttr>:$syncscope);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> def LLVM_StoreOp : LLVM_Op<"store">, MemoryOpWithAlignmentAndAttributes {
>   let arguments = (ins LLVM_LoadableType:$value,
>                    Arg<LLVM_PointerTo<LLVM_LoadableType>,"",[MemWrite]>:$addr,
>                    OptionalAttr<SymbolRefArrayAttr>:$access_groups,
>                    OptionalAttr<SymbolRefArrayAttr>:$alias_scopes,
>                    OptionalAttr<SymbolRefArrayAttr>:$noalias_scopes,
>                    OptionalAttr<I64Attr>:$alignment, UnitAttr:$volatile_,
>                    UnitAttr:$nontemporal);
403,431d394
<   let description = [{
<     The `store` operation is used to write to memory. A store may be marked as
<     atomic, volatile, and/or nontemporal, and takes a number of optional
<     attributes that specify aliasing information.
< 
<     An atomic store only supports a limited set of pointer, integer, and
<     floating point types, and requires an explicit alignment.
< 
<     Examples:
<     ```mlir
<     // A volatile store of a float variable.
<     llvm.store volatile %val, %ptr : f32, !llvm.ptr
< 
<     // A nontemporal store of a float variable.
<     llvm.store %val, %ptr {nontemporal} : f32, !llvm.ptr
< 
<     // An atomic store of an integer variable.
<     llvm.store %val, %ptr atomic monotonic {alignment = 8 : i64}
<         : i64, !llvm.ptr
<     ```
< 
<     See the following link for more details:
<     https://llvm.org/docs/LangRef.html#store-instruction
<   }];
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $value `,` $addr
<     (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?
<     attr-dict `:` custom<StoreType>(type($value), type($addr))
<   }];
434,436c397
<   }] # setOrderingCode
<      # setSyncScopeCode
<      # setAlignmentCode
---
>   }] # setAlignmentCode
439c400,402
<      # setAliasAnalysisMetadataCode;
---
>      # setAliasScopeMetadataCode
>      # setTBAAMetadataCode;
>   // FIXME: Import attributes.
441,447c404
<     auto *storeInst = cast<llvm::StoreInst>(inst);
<     unsigned alignment = storeInst->getAlign().value();
<     $_op = $_builder.create<LLVM::StoreOp>($_location, $value, $addr,
<         alignment, storeInst->isVolatile(),
<         storeInst->hasMetadata(llvm::LLVMContext::MD_nontemporal),
<         convertAtomicOrderingFromLLVM(storeInst->getOrdering()),
<         getLLVMSyncScope(storeInst));
---
>     $_op = $_builder.create<LLVM::StoreOp>($_location, $value, $addr);
452,455c409,411
<       CArg<"bool", "false">:$isNonTemporal,
<       CArg<"AtomicOrdering", "AtomicOrdering::not_atomic">:$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>       CArg<"bool", "false">:$isNonTemporal)>
>     ];
>   let hasCustomAssemblyFormat = 1;
474,475c430,431
< def LLVM_BitcastOp : LLVM_CastOp<"bitcast", "BitCast", LLVM_AnyNonAggregate,
<     LLVM_AnyNonAggregate, [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
> def LLVM_BitcastOp : LLVM_CastOp<"bitcast", "BitCast",
>                                  LLVM_AnyNonAggregate, LLVM_AnyNonAggregate> {
477d432
<   let hasVerifier = 1;
480,482c435,436
<     LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
<     LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
<     [DeclareOpInterfaceMethods<PromotableOpInterface>]> {
---
>                                        LLVM_ScalarOrVectorOf<LLVM_AnyPointer>,
>                                        LLVM_ScalarOrVectorOf<LLVM_AnyPointer>> {
547,552d500
< 
<   let extraClassDeclaration = [{
<     CallInterfaceCallable getCallableForCallee();
<     Operation::operand_range getCallOperands();
<   }];
< 
569,571d516
< // FIXME: Add a type attribute that carries the LLVM function type to support
< // indirect calls to variadic functions. The type attribute is necessary to
< // distinguish normal and variadic arguments.
586,589c531,533
<     function attribute `callee`. The trailing type list contains the optional
<     indirect callee type and the MLIR function type, which differs from the
<     LLVM function type that uses a explicit void type to model functions that do
<     not return a value.
---
>     function attribute `callee`. The trailing type of the instruction is always
>     the MLIR function type, which may be different from the indirect callee that
>     has the wrapped LLVM IR function type.
601c545
<     llvm.call %1(%0) : !llvm.ptr, (f32) -> ()
---
>     llvm.call %1(%0) : (f32) -> ()
622,626d565
<   let extraClassDeclaration = [{
<     CallInterfaceCallable getCallableForCallee();
<     Operation::operand_range getCallOperands();
<   }];
< 
793,794c732
<           [Pure, AllTypesMatch<["trueValue", "falseValue", "res"]>,
<            DeclareOpInterfaceMethods<FastmathFlagsInterface>]>,
---
>           [Pure, AllTypesMatch<["trueValue", "falseValue", "res"]>]>,
798,800c736
<                    LLVM_Type:$trueValue, LLVM_Type:$falseValue,
<                    DefaultValuedAttr<LLVM_FastmathFlagsAttr,
<                                      "{}">:$fastmathFlags);
---
>                    LLVM_Type:$trueValue, LLVM_Type:$falseValue);
805c741
<     auto op = $_builder.create<LLVM::SelectOp>(
---
>     $res = $_builder.create<LLVM::SelectOp>(
807,808d742
<     moduleImport.setFastmathFlagsAttr(inst, op);
<     $res = op;
826,829c760
<   let arguments = (ins
<     Variadic<LLVM_Type>:$destOperands,
<     OptionalAttr<LoopAnnotationAttr>:$loop_annotation
<   );
---
>   let arguments = (ins Variadic<LLVM_Type>:$destOperands);
838,840d768
<     OpBuilder<(ins "ValueRange":$operands, "Block *":$dest), [{
<       build($_builder, $_state, operands, /*loop_annotation=*/{}, dest);
<     }]>,
843d770
<   let hasVerifier = 1;
851,852c778
<                    OptionalAttr<ElementsAttr>:$branch_weights,
<                    OptionalAttr<LoopAnnotationAttr>:$loop_annotation);
---
>                    OptionalAttr<ElementsAttr>:$branch_weights);
865c791,801
<       CArg<"std::optional<std::pair<uint32_t, uint32_t>>", "{}">:$weights)>,
---
>       CArg<"std::optional<std::pair<uint32_t, uint32_t>>", "{}">:$weights),
>     [{
>         ElementsAttr weightsAttr;
>         if (weights) {
>           weightsAttr =
>               $_builder.getI32VectorAttr({static_cast<int32_t>(weights->first),
>                                        static_cast<int32_t>(weights->second)});
>         }
>         build($_builder, $_state, condition, trueOperands, falseOperands, weightsAttr,
>               trueDest, falseDest);
>   }]>,
871,876d806
<   }]>,
<   OpBuilder<(ins "Value":$condition, "ValueRange":$trueOperands, "ValueRange":$falseOperands,
<     "ElementsAttr":$branchWeights, "Block *":$trueDest, "Block *":$falseDest),
<   [{
<       build($_builder, $_state, condition, trueOperands, falseOperands, branchWeights,
<       {}, trueDest, falseDest);
878d807
<   let hasVerifier = 1;
916,917c845
<   // Consistency of llvm.resume value types is checked in LLVMFuncOp::verify().
<   let hasVerifier = false;
---
>   let hasVerifier = 1;
1064,1069c992,995
<     ```mlir
<     llvm.metadata @metadata {
<       llvm.access_group @group1
<       llvm.access_group @group2
<     }
<     ```
---
>       llvm.metadata @metadata {
>         llvm.access_group @group1
>         llvm.access_group @group2
>       }
1118,1128c1044,1061
<     ```mlir
<     module {
<       llvm.func @foo(%ptr1 : !llvm.ptr<i32>) {
<           %c0 = llvm.mlir.constant(0 : i32) : i32
<           %c4 = llvm.mlir.constant(4 : i32) : i32
<           %1 = llvm.ptrtoint %ptr1 : !llvm.ptr<i32> to i32
<           %2 = llvm.add %1, %c1 : i32
<           %ptr2 = llvm.inttoptr %2 : i32 to !llvm.ptr<i32>
<           llvm.store %c0, %ptr1 { alias_scopes = [@metadata::@scope1], llvm.noalias = [@metadata::@scope2] } : !llvm.ptr<i32>
<           llvm.store %c4, %ptr2 { alias_scopes = [@metadata::@scope2], llvm.noalias = [@metadata::@scope1] } : !llvm.ptr<i32>
<           llvm.return
---
>       module {
>         llvm.func @foo(%ptr1 : !llvm.ptr<i32>) {
>             %c0 = llvm.mlir.constant(0 : i32) : i32
>             %c4 = llvm.mlir.constant(4 : i32) : i32
>             %1 = llvm.ptrtoint %ptr1 : !llvm.ptr<i32> to i32
>             %2 = llvm.add %1, %c1 : i32
>             %ptr2 = llvm.inttoptr %2 : i32 to !llvm.ptr<i32>
>             llvm.store %c0, %ptr1 { alias_scopes = [@metadata::@scope1], llvm.noalias = [@metadata::@scope2] } : !llvm.ptr<i32>
>             llvm.store %c4, %ptr2 { alias_scopes = [@metadata::@scope2], llvm.noalias = [@metadata::@scope1] } : !llvm.ptr<i32>
>             llvm.return
>         }
> 
>         llvm.metadata @metadata {
>           llvm.alias_scope_domain @unused_domain
>           llvm.alias_scope_domain @domain { description = "Optional domain description"}
>           llvm.alias_scope @scope1 { domain = @domain }
>           llvm.alias_scope @scope2 { domain = @domain, description = "Optional scope description" }
>         }
1131,1139d1063
<       llvm.metadata @metadata {
<         llvm.alias_scope_domain @unused_domain
<         llvm.alias_scope_domain @domain { description = "Optional domain description"}
<         llvm.alias_scope @scope1 { domain = @domain }
<         llvm.alias_scope @scope2 { domain = @domain, description = "Optional scope description" }
<       }
<     }
<     ```
< 
1144d1067
<   let hasVerifier = 1;
1179,1183c1102,1104
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<     }
<     ```
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>       }
1208,1219c1129,1155
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<       llvm.tbaa_type_desc @tbaa_type_desc_1 {
<           identity = "omnipotent char",
<           members = [@tbaa_root_0],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_2 {
<           identity = "long long",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>         llvm.tbaa_type_desc @tbaa_type_desc_1 {
>             identity = "omnipotent char",
>             members = [@tbaa_root_0],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_2 {
>             identity = "long long",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_3 {
>             identity = "agg2_t",
>             members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
>             offsets = array<i64: 0, 8>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_5 {
>             identity = "int",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_6 {
>             identity = "agg1_t",
>             members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
>             offsets = array<i64: 0, 4>
>         }
1221,1237d1156
<       llvm.tbaa_type_desc @tbaa_type_desc_3 {
<           identity = "agg2_t",
<           members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
<           offsets = array<i64: 0, 8>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_5 {
<           identity = "int",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_6 {
<           identity = "agg1_t",
<           members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
<           offsets = array<i64: 0, 4>
<       }
<     }
<     ```
1269,1290c1188,1224
<     ```mlir
<     llvm.metadata @tbaa {
<       llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
<       llvm.tbaa_type_desc @tbaa_type_desc_1 {
<           identity = "omnipotent char",
<           members = [@tbaa_root_0],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_2 {
<           identity = "long long",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_3 {
<           identity = "agg2_t",
<           members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
<           offsets = array<i64: 0, 8>
<       }
<       llvm.tbaa_tag @tbaa_tag_4 {
<           access_type = @tbaa_type_desc_2,
<           base_type = @tbaa_type_desc_3,
<           offset = 8 : i64
---
>       llvm.metadata @tbaa {
>         llvm.tbaa_root @tbaa_root_0 {identity = "Simple C/C++ TBAA"}
>         llvm.tbaa_type_desc @tbaa_type_desc_1 {
>             identity = "omnipotent char",
>             members = [@tbaa_root_0],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_2 {
>             identity = "long long",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_3 {
>             identity = "agg2_t",
>             members = [@tbaa_type_desc_2, @tbaa_type_desc_2],
>             offsets = array<i64: 0, 8>
>         }
>         llvm.tbaa_tag @tbaa_tag_4 {
>             access_type = @tbaa_type_desc_2,
>             base_type = @tbaa_type_desc_3,
>             offset = 8 : i64
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_5 {
>             identity = "int",
>             members = [@tbaa_type_desc_1],
>             offsets = array<i64: 0>
>         }
>         llvm.tbaa_type_desc @tbaa_type_desc_6 {
>             identity = "agg1_t",
>             members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
>             offsets = array<i64: 0, 4>
>         }
>         llvm.tbaa_tag @tbaa_tag_7 {
>             access_type = @tbaa_type_desc_5,
>             base_type = @tbaa_type_desc_6,
>             offset = 0 : i64
>         }
1292,1308d1225
<       llvm.tbaa_type_desc @tbaa_type_desc_5 {
<           identity = "int",
<           members = [@tbaa_type_desc_1],
<           offsets = array<i64: 0>
<       }
<       llvm.tbaa_type_desc @tbaa_type_desc_6 {
<           identity = "agg1_t",
<           members = [@tbaa_type_desc_5, @tbaa_type_desc_5],
<           offsets = array<i64: 0, 4>
<       }
<       llvm.tbaa_tag @tbaa_tag_7 {
<           access_type = @tbaa_type_desc_5,
<           base_type = @tbaa_type_desc_6,
<           offset = 0 : i64
<       }
<     }
<     ```
1329,1330c1246
<     OptionalAttr<StrAttr>:$section,
<     DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_
---
>     OptionalAttr<StrAttr>:$section
1575,1576c1491
<     OptionalAttr<LLVM_MemoryEffectsAttr>:$memory,
<     DefaultValuedAttr<Visibility, "mlir::LLVM::Visibility::Default">:$visibility_
---
>     OptionalAttr<LLVM_MemoryEffectsAttr>:$memory
1606,1609d1520
<     //===------------------------------------------------------------------===//
<     // CallableOpInterface
<     //===------------------------------------------------------------------===//
< 
1623,1633d1533
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
1675c1575
<     IR dialect type.
---
>     IR dialect type wrapping an LLVM IR structure type.
1689,1710d1588
< def LLVM_PoisonOp : LLVM_Op<"mlir.poison", [Pure]>,
<                     LLVM_Builder<"$res = llvm::PoisonValue::get($_resultType);"> {
<   let summary = "Creates a poison value of LLVM dialect type.";
<   let description = [{
<     Unlike LLVM IR, MLIR does not have first-class poison values. Such values
<     must be created as SSA values using `llvm.mlir.poison`. This operation has
<     no operands or attributes. It creates a poison value of the specified LLVM
<     IR dialect type.
< 
<     Example:
< 
<     ```mlir
<     // Create a poison value for a structure with a 32-bit integer followed
<     // by a float.
<     %0 = llvm.mlir.poison : !llvm.struct<(i32, f32)>
<     ```
<   }];
<   let results = (outs LLVM_Type:$res);
<   let builders = [LLVM_OneResultOpBuilder];
<   let assemblyFormat = "attr-dict `:` type($res)";
< }
< 
1776,1786c1654,1659
< def LLVM_AtomicRMWOp : LLVM_MemAccessOpBase<"atomicrmw", [
<       TypesMatchWith<"result #0 and operand #1 have the same type",
<                      "val", "res", "$_self">]> {
<   dag args = (ins AtomicBinOp:$bin_op,
<               LLVM_PointerTo<LLVM_AtomicRMWType>:$ptr,
<               LLVM_AtomicRMWType:$val, AtomicOrdering:$ordering,
<               OptionalAttr<StrAttr>:$syncscope,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$volatile_);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
---
> // FIXME: Need to add alignment and syncscope attribute to MLIR atomicrmw
> // operation.
> def LLVM_AtomicRMWOp : LLVM_Op<"atomicrmw"> {
>   let arguments = (ins AtomicBinOp:$bin_op,
>                    LLVM_PointerTo<LLVM_AtomicRMWType>:$ptr,
>                    LLVM_AtomicRMWType:$val, AtomicOrdering:$ordering);
1788,1792c1661,1662
<   let assemblyFormat = [{
<     (`volatile` $volatile_^)? $bin_op $ptr `,` $val
<     (`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict `:`
<     qualified(type($ptr)) `,` type($val)
<   }];
---
>   let hasCustomAssemblyFormat = 1;
>   let hasVerifier = 1;
1795,1803c1665,1668
<     auto *inst = builder.CreateAtomicRMW(
<         convertAtomicBinOpToLLVM($bin_op), $ptr, $val, llvm::MaybeAlign(),
<         convertAtomicOrderingToLLVM($ordering));
<     $res = inst;
<   }] # setVolatileCode
<      # setSyncScopeCode
<      # setAlignmentCode
<      # setAccessGroupsMetadataCode
<      # setAliasAnalysisMetadataCode;
---
>     $res = builder.CreateAtomicRMW(getLLVMAtomicBinOp($bin_op), $ptr, $val,
>                                    llvm::MaybeAlign(),
>                                    getLLVMAtomicOrdering($ordering));
>   }];
1806,1810c1671,1673
<     unsigned alignment = atomicInst->getAlign().value();
<     $res = $_builder.create<LLVM::AtomicRMWOp>($_location,
<         convertAtomicBinOpFromLLVM(atomicInst->getOperation()), $ptr, $val,
<         convertAtomicOrderingFromLLVM(atomicInst->getOrdering()),
<         getLLVMSyncScope(atomicInst), alignment, atomicInst->isVolatile());
---
>     $res = $_builder.create<LLVM::AtomicRMWOp>($_location, $_resultType,
>         getLLVMAtomicBinOp(atomicInst->getOperation()), $ptr, $val,
>         getLLVMAtomicOrdering(atomicInst->getOrdering()));
1812,1820c1675,1676
<   list<int> llvmArgIndices = [-1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1];
<   let builders = [
<     OpBuilder<(ins "LLVM::AtomicBinOp":$binOp, "Value":$ptr, "Value":$val,
<       "LLVM::AtomicOrdering":$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope,
<       CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isVolatile
<     )>
<   ];
<   let hasVerifier = 1;
---
>   // Only $ptr and $val are llvm instruction operands.
>   list<int> llvmArgIndices = [-1, 0, 1, -1];
1824,1846c1680,1700
< 
< def LLVM_AtomicCmpXchgOp : LLVM_MemAccessOpBase<"cmpxchg", [
<       TypesMatchWith<"operand #1 and operand #2 have the same type",
<                      "val", "cmp", "$_self">,
<       TypesMatchWith<"result #0 has an LLVM struct type consisting of "
<                      "the type of operand #2 and a bool", "val", "res",
<                      "getValAndBoolStructType($_self)">]> {
<   dag args = (ins LLVM_PointerTo<LLVM_AtomicCmpXchgType>:$ptr,
<               LLVM_AtomicCmpXchgType:$cmp, LLVM_AtomicCmpXchgType:$val,
<               AtomicOrdering:$success_ordering,
<               AtomicOrdering:$failure_ordering,
<               OptionalAttr<StrAttr>:$syncscope,
<               OptionalAttr<I64Attr>:$alignment,
<               UnitAttr:$weak,
<               UnitAttr:$volatile_);
<   // Append the aliasing related attributes define in LLVM_MemAccessOpBase.
<   let arguments = !con(args, aliasAttrs);
<   let results = (outs LLVM_AnyStruct:$res);
<   let assemblyFormat = [{
<     (`weak` $weak^)? (`volatile` $volatile_^)? $ptr `,` $cmp `,` $val
<     (`syncscope` `(` $syncscope^ `)`)? $success_ordering $failure_ordering
<     attr-dict `:` qualified(type($ptr)) `,` type($val)
<   }];
---
> def LLVM_AtomicCmpXchgResultType : Type<And<[
>   LLVM_AnyStruct.predicate,
>   CPred<"$_self.cast<::mlir::LLVM::LLVMStructType>().getBody().size() == 2">,
>   SubstLeaves<"$_self",
>               "$_self.cast<::mlir::LLVM::LLVMStructType>().getBody()[0]",
>               LLVM_AtomicCmpXchgType.predicate>,
>   SubstLeaves<"$_self",
>               "$_self.cast<::mlir::LLVM::LLVMStructType>().getBody()[1]",
>               I1.predicate>]>,
>  "an LLVM struct type with any integer or pointer followed by a single-bit "
>  "integer">;
> 
> // FIXME: Need to add alignment attribute to MLIR cmpxchg operation.
> def LLVM_AtomicCmpXchgOp : LLVM_Op<"cmpxchg"> {
>   let arguments = (ins LLVM_PointerTo<LLVM_AtomicCmpXchgType>:$ptr,
>                    LLVM_AtomicCmpXchgType:$cmp, LLVM_AtomicCmpXchgType:$val,
>                    AtomicOrdering:$success_ordering,
>                    AtomicOrdering:$failure_ordering);
>   let results = (outs LLVM_AtomicCmpXchgResultType:$res);
>   let hasCustomAssemblyFormat = 1;
>   let hasVerifier = 1;
1849,1858c1703,1706
<     auto *inst = builder.CreateAtomicCmpXchg($ptr, $cmp, $val,
<         llvm::MaybeAlign(), convertAtomicOrderingToLLVM($success_ordering),
<         convertAtomicOrderingToLLVM($failure_ordering));
<     $res = inst;
<     inst->setWeak($weak);
<   }] # setVolatileCode
<      # setSyncScopeCode
<      # setAlignmentCode
<      # setAccessGroupsMetadataCode
<      # setAliasAnalysisMetadataCode;
---
>     $res = builder.CreateAtomicCmpXchg($ptr, $cmp, $val, llvm::MaybeAlign(),
>                    getLLVMAtomicOrdering($success_ordering),
>                    getLLVMAtomicOrdering($failure_ordering));
>   }];
1861d1708
<     unsigned alignment = cmpXchgInst->getAlign().value();
1863,1867c1710,1712
<       $_location, $ptr, $cmp, $val,
<       convertAtomicOrderingFromLLVM(cmpXchgInst->getSuccessOrdering()),
<       convertAtomicOrderingFromLLVM(cmpXchgInst->getFailureOrdering()),
<       getLLVMSyncScope(cmpXchgInst), alignment, cmpXchgInst->isWeak(),
<       cmpXchgInst->isVolatile());
---
>       $_location, $_resultType, $ptr, $cmp, $val,
>       getLLVMAtomicOrdering(cmpXchgInst->getSuccessOrdering()),
>       getLLVMAtomicOrdering(cmpXchgInst->getFailureOrdering()));
1869,1878d1713
<   let builders = [
<     OpBuilder<(ins "Value":$ptr, "Value":$cmp, "Value":$val,
<       "LLVM::AtomicOrdering":$successOrdering,
<       "LLVM::AtomicOrdering":$failureOrdering,
<       CArg<"StringRef", "StringRef()">:$syncscope,
<       CArg<"unsigned", "0">:$alignment, CArg<"bool", "false">:$isWeak,
<       CArg<"bool", "false">:$isVolatile
<     )>
<   ];
<   let hasVerifier = 1;
1881,1884c1716,1718
< def LLVM_FenceOp : LLVM_Op<"fence">, LLVM_MemOpPatterns {
<   let arguments = (ins AtomicOrdering:$ordering,
<                    OptionalAttr<StrAttr>:$syncscope);
<   let assemblyFormat = "(`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict";
---
> def LLVM_FenceOp : LLVM_Op<"fence"> {
>   let arguments = (ins AtomicOrdering:$ordering, StrAttr:$syncscope);
>   let builders = [LLVM_VoidResultTypeOpBuilder, LLVM_ZeroResultOpBuilder];
1887,1888c1721,1724
<     auto *inst = builder.CreateFence(convertAtomicOrderingToLLVM($ordering));
<   }] # setSyncScopeCode;
---
>     llvm::LLVMContext &llvmContext = builder.getContext();
>     builder.CreateFence(getLLVMAtomicOrdering($ordering),
>       llvmContext.getOrInsertSyncScopeID($syncscope));
>   }];
1893c1729
<       convertAtomicOrderingFromLLVM(fenceInst->getOrdering()),
---
>       getLLVMAtomicOrdering(fenceInst->getOrdering()),
1896,1901c1732
<   let builders = [
<     LLVM_VoidResultTypeOpBuilder,
<     LLVM_ZeroResultOpBuilder,
<     OpBuilder<(ins "LLVM::AtomicOrdering":$ordering,
<       CArg<"StringRef", "StringRef()">:$syncscope)>
<   ];
---
>   let hasCustomAssemblyFormat = 1;
--- include/mlir/Dialect/LLVMIR/LLVMDialect.h
18d17
< #include "mlir/Dialect/LLVMIR/LLVMInterfaces.h"
32d30
< #include "mlir/Transforms/Mem2Reg.h"
38a37,38
> #include "mlir/Dialect/LLVMIR/LLVMOpsInterfaces.h.inc"
> 
211,212c211
<                          StringRef value, Linkage linkage,
<                          bool useOpaquePointers);
---
>                          StringRef value, Linkage linkage);
--- include/mlir/Dialect/GPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/IR and include/mlir/Dialect/GPU/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/TransformOps and include/mlir/Dialect/GPU/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/Transforms and include/mlir/Dialect/GPU/Transforms
--- include/mlir/Dialect/GPU/CMakeLists.txt
--- include/mlir/Dialect/GPU/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/Transforms/Passes.h include/mlir/Dialect/GPU/Transforms/Passes.h
16d15
< #include "Utils.h"
64a64,83
> /// A function that maps a MemorySpace enum to a target-specific integer value.
> using MemorySpaceMapping =
>     std::function<unsigned(gpu::AddressSpace gpuAddressSpace)>;
> 
> /// Populates type conversion rules for lowering memory space attributes to
> /// numeric values.
> void populateMemorySpaceAttributeTypeConversions(
>     TypeConverter &typeConverter, const MemorySpaceMapping &mapping);
> 
> /// Populates patterns to lower memory space attributes to numeric values.
> void populateMemorySpaceLoweringPatterns(TypeConverter &typeConverter,
>                                          RewritePatternSet &patterns);
> 
> /// Populates legality rules for lowering memory space attriutes to numeric
> /// values.
> void populateLowerMemorySpaceOpLegality(ConversionTarget &target);
> 
> /// Returns the default annotation name for GPU binary blobs.
> std::string getDefaultGpuBinaryAnnotation();
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/Transforms/Passes.td include/mlir/Dialect/GPU/Transforms/Passes.td
39a40,58
> def GPULowerMemorySpaceAttributesPass
>     : Pass<"gpu-lower-memory-space-attributes"> {
>   let summary = "Assign numeric values to memref memory space symbolic placeholders";
>   let description = [{
>     Updates all memref types that have a memory space attribute
>     that is a `gpu::AddressSpaceAttr`. These attributes are
>     changed to `IntegerAttr`'s using a mapping that is given in the
>     options.
>   }];
>   let options = [
>     Option<"privateAddrSpace", "private", "unsigned", "5",
>       "private address space numeric value">,
>     Option<"workgroupAddrSpace", "workgroup", "unsigned", "3",
>       "workgroup address space numeric value">,
>     Option<"globalAddrSpace", "global", "unsigned", "1",
>       "global address space numeric value">
>   ];
> }
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/Transforms/Utils.h include/mlir/Dialect/GPU/Transforms/Utils.h
18,19d17
< #include <string>
< 
28,30d25
< 
< /// Returns the default annotation name for GPU binary blobs.
< std::string getDefaultGpuBinaryAnnotation();
--- include/mlir/Dialect/GPU/Transforms/Passes.h
16d15
< #include "Utils.h"
64a64,83
> /// A function that maps a MemorySpace enum to a target-specific integer value.
> using MemorySpaceMapping =
>     std::function<unsigned(gpu::AddressSpace gpuAddressSpace)>;
> 
> /// Populates type conversion rules for lowering memory space attributes to
> /// numeric values.
> void populateMemorySpaceAttributeTypeConversions(
>     TypeConverter &typeConverter, const MemorySpaceMapping &mapping);
> 
> /// Populates patterns to lower memory space attributes to numeric values.
> void populateMemorySpaceLoweringPatterns(TypeConverter &typeConverter,
>                                          RewritePatternSet &patterns);
> 
> /// Populates legality rules for lowering memory space attriutes to numeric
> /// values.
> void populateLowerMemorySpaceOpLegality(ConversionTarget &target);
> 
> /// Returns the default annotation name for GPU binary blobs.
> std::string getDefaultGpuBinaryAnnotation();
> 
--- include/mlir/Dialect/GPU/Transforms/Passes.td
39a40,58
> def GPULowerMemorySpaceAttributesPass
>     : Pass<"gpu-lower-memory-space-attributes"> {
>   let summary = "Assign numeric values to memref memory space symbolic placeholders";
>   let description = [{
>     Updates all memref types that have a memory space attribute
>     that is a `gpu::AddressSpaceAttr`. These attributes are
>     changed to `IntegerAttr`'s using a mapping that is given in the
>     options.
>   }];
>   let options = [
>     Option<"privateAddrSpace", "private", "unsigned", "5",
>       "private address space numeric value">,
>     Option<"workgroupAddrSpace", "workgroup", "unsigned", "3",
>       "workgroup address space numeric value">,
>     Option<"globalAddrSpace", "global", "unsigned", "1",
>       "global address space numeric value">
>   ];
> }
> 
--- include/mlir/Dialect/GPU/Transforms/Utils.h
18,19d17
< #include <string>
< 
28,30d25
< 
< /// Returns the default annotation name for GPU binary blobs.
< std::string getDefaultGpuBinaryAnnotation();
--- include/mlir/Dialect/GPU/Transforms/CMakeLists.txt
--- include/mlir/Dialect/GPU/Transforms/ParallelLoopMapper.h
--- include/mlir/Dialect/GPU/Transforms/MemoryPromotion.h
--- include/mlir/Dialect/GPU/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/IR/GPUBase.td include/mlir/Dialect/GPU/IR/GPUBase.td
59a60
>   let useFoldAPI = kEmitFoldAdaptorFolder;
104c105
< def GPU_MMAMemRef : MemRefOf<[I8, I32, F16, F32, VectorOfRankAndType<[1], [I8, I32, F16, F32]>]>;
---
> def GPU_MMAMemRef : MemRefOf<[F16, F32, VectorOfRankAndType<[1], [F16, F32]>]>;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/IR/GPUDialect.h include/mlir/Dialect/GPU/IR/GPUDialect.h
25d24
< #include "mlir/Interfaces/CallInterfaces.h"
28a28,29
> #include "llvm/ADT/STLExtras.h"
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/IR/GPUOps.td include/mlir/Dialect/GPU/IR/GPUOps.td
23d22
< include "mlir/Interfaces/CallInterfaces.td"
184c183
<     CallableOpInterface, IsolatedFromAbove
---
>     IsolatedFromAbove
256c255,257
<                        OptionalAttr<DictArrayAttr>:$res_attrs);
---
>                        OptionalAttr<DictArrayAttr>:$res_attrs,
>                        OptionalAttr<DictArrayAttr>:$workgroup_attrib_attrs,
>                        OptionalAttr<DictArrayAttr>:$private_attrib_attrs);
282a284,289
>     /// Return the index of the first workgroup attribution in the block argument
>     /// list.
>     unsigned getFirstWorkgroupAttributionIndex() {
>       return getFunctionType().getNumInputs();
>     }
> 
287c294
<           std::next(getBody().args_begin(), getFunctionType().getNumInputs());
---
>           std::next(getBody().args_begin(), getFirstWorkgroupAttributionIndex());
295a303,325
>     /// Get the workgroup attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the workgroup attributions.
>     DictionaryAttr getworkgroupAttributionAttrs(unsigned index);
> 
>     /// Set the workgroup attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the workgroup attributions.
>     void setworkgroupAttributionAttrs(unsigned index, DictionaryAttr value);
> 
>     /// Get an attribute for a workgroup attribution. `index` is counted
>     /// from the start of the workgroup attributions, not the start of the block.
>     Attribute getWorkgroupAttributionAttr(unsigned index, StringAttr name);
>     Attribute getWorkgroupAttributionAttr(unsigned index, StringRef name) {
>       return getWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
>     }
> 
>     /// Set an attribute for a workgroup attribution. `index` is counted
>     /// from the start of the workgroup attributions, not the start of the block.
>     /// A null `value` removes an attributino attribute.
>     void setWorkgroupAttributionAttr(unsigned index, StringAttr name, Attribute value);
>     void setWorkgroupAttributionAttr(unsigned index, StringRef name, Attribute value) {
>       return setWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
>     }
> 
301a332,338
>     /// Returns the index of the first private buffer in the block argument list.
>     unsigned getFirstPrivateAttributionIndex() {
>       // Buffers on the private memory always come after buffers on the workgroup
>       // memory.
>       return getFunctionType().getNumInputs() + getNumWorkgroupAttributions();
>     }
> 
305,306d341
<       // Buffers on the private memory always come after buffers on the workgroup
<       // memory.
308,309c343
<           std::next(getBody().args_begin(),
<                     getFunctionType().getNumInputs() + getNumWorkgroupAttributions());
---
>           std::next(getBody().args_begin(), getFirstPrivateAttributionIndex());
316a351,373
>     /// Get the private attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the private attributions.
>     DictionaryAttr getPrivateAttributionAttrs(unsigned index);
> 
>     /// Set the private attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the private attributions.
>     void setPrivateAttributionAttrs(unsigned index, DictionaryAttr value);
> 
>     /// Get an attribute for a private attribution. `index` is counted
>     /// from the start of the private attributions, not the start of the block.
>     Attribute getPrivateAttributionAttr(unsigned index, StringAttr name);
>     Attribute getPrivateAttributionAttr(unsigned index, StringRef name) {
>       return getPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
>     }
> 
>     /// Set an attribute for a private attribution. `index` is counted
>     /// from the start of the private attributions, not the start of the block.
>     /// A null `value` removes an attribute.
>     void setPrivateAttributionAttr(unsigned index, StringAttr name, Attribute value);
>     void setPrivateAttributionAttr(unsigned index, StringRef name, Attribute value) {
>       return setPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
>     }
> 
473c530,533
<       CArg<"ValueRange", "{}">:$asyncDependencies)>
---
>       CArg<"ValueRange", "{}">:$asyncDependencies)>,
>     OpBuilder<(ins "ValueRange":$dependencies, "GPUFuncOp":$kernelFunc,
>       "KernelDim3":$gridSize, "KernelDim3":$blockSize,
>       "Value":$dynamicSharedMemorySize, "ValueRange":$kernelOperands)>
540c600
<     The body region has at least _twelve_ arguments, grouped as follows:
---
>     The body region has _twelve_ arguments, grouped as follows:
546,547d605
<     -   a variadic number of Workgroup memory attributions.
<     -   a variadic number of Private memory attributions.
556d613
<                              memory-attribution
559,560d615
<     memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?
<                            (`private` `(` ssa-id-and-type-list `)`)?
591,602d645
< 
<     // Launch with memory attributions.
<     gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)
<                threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5)
<                workgroup(%workgroup: memref<32xf32, 3>)
<                private(%private: memref<1xf32, 5>) {
<       // Block and thread identifiers, as well as block/grid sizes are
<       // immediately usable inside body region.
<       "some_op"(%bx, %tx) : (index, index) -> ()
<       // Assuming %val1 is defined outside the gpu.launch region.
<       %42 = load %workgroup[%bx] : memref<32xf32, 3>
<     }
622,624c665
<       CArg<"ValueRange", "{}">:$asyncDependencies,
<       CArg<"TypeRange", "{}">:$workgroupAttributions,
<       CArg<"TypeRange", "{}">:$privateAttributions)>
---
>       CArg<"ValueRange", "{}">:$asyncDependencies)>
655,705d695
< 
<     /// Returns the keywords used in the custom syntax for this Op.
<     static StringRef getWorkgroupKeyword() { return "workgroup"; }
<     static StringRef getPrivateKeyword() { return "private"; }
< 
<     /// Returns the number of buffers located in the workgroup memory.
<     unsigned getNumWorkgroupAttributions() {
<       auto attr = (*this)->getAttrOfType<IntegerAttr>(
<           getNumWorkgroupAttributionsAttrName());
<       return attr ? attr.getInt() : 0;
<     }
< 
<     /// Returns a list of block arguments that correspond to buffers located in
<     /// the workgroup memory
<     ArrayRef<BlockArgument> getWorkgroupAttributions() {
<       auto begin =
<           std::next(getBody().args_begin(), kNumConfigRegionAttributes);
<       auto end = std::next(begin, getNumWorkgroupAttributions());
<       return {begin, end};
<     }
< 
<     /// Adds a new block argument that corresponds to buffers located in
<     /// workgroup memory.
<     BlockArgument addWorkgroupAttribution(Type type, Location loc);
< 
<     /// Returns the number of buffers located in the private memory.
<     unsigned getNumPrivateAttributions() {
<       return getBody().getNumArguments() - kNumConfigRegionAttributes -
<           getNumWorkgroupAttributions();
<     }
< 
<     /// Returns a list of block arguments that correspond to buffers located in
<     /// the private memory.
<     ArrayRef<BlockArgument> getPrivateAttributions() {
<       // Buffers on the private memory always come after buffers on the workgroup
<       // memory.
<       auto begin =
<           std::next(getBody().args_begin(),
<                     kNumConfigRegionAttributes + getNumWorkgroupAttributions());
<       return {begin, getBody().args_end()};
<     }
< 
<     /// Adds a new block argument that corresponds to buffers located in
<     /// private memory.
<     BlockArgument addPrivateAttribution(Type type, Location loc);
< 
<     /// Returns the name of the attribute containing the number of buffers
<     /// located in the workgroup memory.
<     static StringRef getNumWorkgroupAttributionsAttrName() {
<       return "workgroup_attributions";
<     }
1003,1015d992
< def GPU_HostUnregisterOp : GPU_Op<"host_unregister">,
<     Arguments<(ins AnyUnrankedMemRef:$value)> {
<   let summary = "Unregisters a memref for access from device.";
<   let description = [{
<       This op unmaps the provided host buffer from the device address space.
< 
<       This operation may not be supported in every environment, there is not yet a
<           way to check at runtime whether this feature is supported.
<   }];
< 
<   let assemblyFormat = "$value attr-dict `:` type($value)";
< }
< 
1237,1240d1213
<     For integer types, the resulting `!gpu.mma_matrix` type needs to specify the
<     signedness of the data if the matrix type is an `A` or `B` operand for
<     `gpu.subgroup_mma_compute`.
< 
1292c1265
<   let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, I32, F16, F32]>>:$src,
---
>   let arguments = (ins Arg<MMAMatrixOf<[F16, F32]>>:$src,
1318c1291
<     transposed manner. The transpose operands are required to map to correct
---
>     transposed manner. The transpose opernads are required to map to correct
1323,1326d1295
<     For integer types, the `A` and `B` matrices carry their signedness with their
<     types. The accumulator type is expected to be signless and imply a signed integer
<     with a greater width than the other two operands.
< 
1339,1341c1308,1310
<   let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opA,
<                   Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opB,
<                   Arg<MMAMatrixOf<[I32, F16, F32]>>:$opC,
---
>   let arguments = (ins Arg<MMAMatrixOf<[F16, F32]>>:$opA,
>                   Arg<MMAMatrixOf<[F16, F32]>>:$opB,
>                   Arg<MMAMatrixOf<[F16, F32]>>:$opC,
1383c1352
<   let arguments = (ins AnyTypeOf<[SI8, UI8, I32, F16, F32]>:$value);
---
>   let arguments = (ins AnyTypeOf<[F16, F32]>:$value);
1430a1400
> 
1472a1443,1458
> }
> 
> def GPU_WarpSwizzleOp :
>     GPU_Op<"warp_swizzle">,
>     Arguments<(ins I32:$in, ConfinedAttr<I32ArrayAttr, [ArrayCount<4>]>:$selector)>,
>     Results<(outs I32:$out)> {
>   let summary = "Swizzle a value accross lanes/threads according to `selector``";
>   let description = [{
>     For a thread with ID t, the result of the operation will be the value of in
>     on thread s = (t / 4) * 4 + selector[t % 4]
>     If s is not executing, the result of this operation is undefined.
>     selector is not required to be a permutation of {0, 1, 2, 3}, but can't contain
>     any values from outside of that set
>   }];
>   let assemblyFormat = "attr-dict $in `:` type($in)";
>   let hasVerifier = 1;
--- include/mlir/Dialect/GPU/IR/ParallelLoopMapperAttr.td
--- include/mlir/Dialect/GPU/IR/CMakeLists.txt
--- include/mlir/Dialect/GPU/IR/GPUDialect.h
25d24
< #include "mlir/Interfaces/CallInterfaces.h"
28a28,29
> #include "llvm/ADT/STLExtras.h"
> 
--- include/mlir/Dialect/GPU/IR/GPUOps.td
23d22
< include "mlir/Interfaces/CallInterfaces.td"
184c183
<     CallableOpInterface, IsolatedFromAbove
---
>     IsolatedFromAbove
256c255,257
<                        OptionalAttr<DictArrayAttr>:$res_attrs);
---
>                        OptionalAttr<DictArrayAttr>:$res_attrs,
>                        OptionalAttr<DictArrayAttr>:$workgroup_attrib_attrs,
>                        OptionalAttr<DictArrayAttr>:$private_attrib_attrs);
282a284,289
>     /// Return the index of the first workgroup attribution in the block argument
>     /// list.
>     unsigned getFirstWorkgroupAttributionIndex() {
>       return getFunctionType().getNumInputs();
>     }
> 
287c294
<           std::next(getBody().args_begin(), getFunctionType().getNumInputs());
---
>           std::next(getBody().args_begin(), getFirstWorkgroupAttributionIndex());
295a303,325
>     /// Get the workgroup attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the workgroup attributions.
>     DictionaryAttr getworkgroupAttributionAttrs(unsigned index);
> 
>     /// Set the workgroup attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the workgroup attributions.
>     void setworkgroupAttributionAttrs(unsigned index, DictionaryAttr value);
> 
>     /// Get an attribute for a workgroup attribution. `index` is counted
>     /// from the start of the workgroup attributions, not the start of the block.
>     Attribute getWorkgroupAttributionAttr(unsigned index, StringAttr name);
>     Attribute getWorkgroupAttributionAttr(unsigned index, StringRef name) {
>       return getWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
>     }
> 
>     /// Set an attribute for a workgroup attribution. `index` is counted
>     /// from the start of the workgroup attributions, not the start of the block.
>     /// A null `value` removes an attributino attribute.
>     void setWorkgroupAttributionAttr(unsigned index, StringAttr name, Attribute value);
>     void setWorkgroupAttributionAttr(unsigned index, StringRef name, Attribute value) {
>       return setWorkgroupAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
>     }
> 
301a332,338
>     /// Returns the index of the first private buffer in the block argument list.
>     unsigned getFirstPrivateAttributionIndex() {
>       // Buffers on the private memory always come after buffers on the workgroup
>       // memory.
>       return getFunctionType().getNumInputs() + getNumWorkgroupAttributions();
>     }
> 
305,306d341
<       // Buffers on the private memory always come after buffers on the workgroup
<       // memory.
308,309c343
<           std::next(getBody().args_begin(),
<                     getFunctionType().getNumInputs() + getNumWorkgroupAttributions());
---
>           std::next(getBody().args_begin(), getFirstPrivateAttributionIndex());
316a351,373
>     /// Get the private attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the private attributions.
>     DictionaryAttr getPrivateAttributionAttrs(unsigned index);
> 
>     /// Set the private attribution attribute dictionary for the attribution
>     /// at index `index`, counted from the start of the private attributions.
>     void setPrivateAttributionAttrs(unsigned index, DictionaryAttr value);
> 
>     /// Get an attribute for a private attribution. `index` is counted
>     /// from the start of the private attributions, not the start of the block.
>     Attribute getPrivateAttributionAttr(unsigned index, StringAttr name);
>     Attribute getPrivateAttributionAttr(unsigned index, StringRef name) {
>       return getPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name));
>     }
> 
>     /// Set an attribute for a private attribution. `index` is counted
>     /// from the start of the private attributions, not the start of the block.
>     /// A null `value` removes an attribute.
>     void setPrivateAttributionAttr(unsigned index, StringAttr name, Attribute value);
>     void setPrivateAttributionAttr(unsigned index, StringRef name, Attribute value) {
>       return setPrivateAttributionAttr(index, StringAttr::get((*this)->getContext(), name), value);
>     }
> 
473c530,533
<       CArg<"ValueRange", "{}">:$asyncDependencies)>
---
>       CArg<"ValueRange", "{}">:$asyncDependencies)>,
>     OpBuilder<(ins "ValueRange":$dependencies, "GPUFuncOp":$kernelFunc,
>       "KernelDim3":$gridSize, "KernelDim3":$blockSize,
>       "Value":$dynamicSharedMemorySize, "ValueRange":$kernelOperands)>
540c600
<     The body region has at least _twelve_ arguments, grouped as follows:
---
>     The body region has _twelve_ arguments, grouped as follows:
546,547d605
<     -   a variadic number of Workgroup memory attributions.
<     -   a variadic number of Private memory attributions.
556d613
<                              memory-attribution
559,560d615
<     memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?
<                            (`private` `(` ssa-id-and-type-list `)`)?
591,602d645
< 
<     // Launch with memory attributions.
<     gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)
<                threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5)
<                workgroup(%workgroup: memref<32xf32, 3>)
<                private(%private: memref<1xf32, 5>) {
<       // Block and thread identifiers, as well as block/grid sizes are
<       // immediately usable inside body region.
<       "some_op"(%bx, %tx) : (index, index) -> ()
<       // Assuming %val1 is defined outside the gpu.launch region.
<       %42 = load %workgroup[%bx] : memref<32xf32, 3>
<     }
622,624c665
<       CArg<"ValueRange", "{}">:$asyncDependencies,
<       CArg<"TypeRange", "{}">:$workgroupAttributions,
<       CArg<"TypeRange", "{}">:$privateAttributions)>
---
>       CArg<"ValueRange", "{}">:$asyncDependencies)>
655,705d695
< 
<     /// Returns the keywords used in the custom syntax for this Op.
<     static StringRef getWorkgroupKeyword() { return "workgroup"; }
<     static StringRef getPrivateKeyword() { return "private"; }
< 
<     /// Returns the number of buffers located in the workgroup memory.
<     unsigned getNumWorkgroupAttributions() {
<       auto attr = (*this)->getAttrOfType<IntegerAttr>(
<           getNumWorkgroupAttributionsAttrName());
<       return attr ? attr.getInt() : 0;
<     }
< 
<     /// Returns a list of block arguments that correspond to buffers located in
<     /// the workgroup memory
<     ArrayRef<BlockArgument> getWorkgroupAttributions() {
<       auto begin =
<           std::next(getBody().args_begin(), kNumConfigRegionAttributes);
<       auto end = std::next(begin, getNumWorkgroupAttributions());
<       return {begin, end};
<     }
< 
<     /// Adds a new block argument that corresponds to buffers located in
<     /// workgroup memory.
<     BlockArgument addWorkgroupAttribution(Type type, Location loc);
< 
<     /// Returns the number of buffers located in the private memory.
<     unsigned getNumPrivateAttributions() {
<       return getBody().getNumArguments() - kNumConfigRegionAttributes -
<           getNumWorkgroupAttributions();
<     }
< 
<     /// Returns a list of block arguments that correspond to buffers located in
<     /// the private memory.
<     ArrayRef<BlockArgument> getPrivateAttributions() {
<       // Buffers on the private memory always come after buffers on the workgroup
<       // memory.
<       auto begin =
<           std::next(getBody().args_begin(),
<                     kNumConfigRegionAttributes + getNumWorkgroupAttributions());
<       return {begin, getBody().args_end()};
<     }
< 
<     /// Adds a new block argument that corresponds to buffers located in
<     /// private memory.
<     BlockArgument addPrivateAttribution(Type type, Location loc);
< 
<     /// Returns the name of the attribute containing the number of buffers
<     /// located in the workgroup memory.
<     static StringRef getNumWorkgroupAttributionsAttrName() {
<       return "workgroup_attributions";
<     }
1003,1015d992
< def GPU_HostUnregisterOp : GPU_Op<"host_unregister">,
<     Arguments<(ins AnyUnrankedMemRef:$value)> {
<   let summary = "Unregisters a memref for access from device.";
<   let description = [{
<       This op unmaps the provided host buffer from the device address space.
< 
<       This operation may not be supported in every environment, there is not yet a
<           way to check at runtime whether this feature is supported.
<   }];
< 
<   let assemblyFormat = "$value attr-dict `:` type($value)";
< }
< 
1237,1240d1213
<     For integer types, the resulting `!gpu.mma_matrix` type needs to specify the
<     signedness of the data if the matrix type is an `A` or `B` operand for
<     `gpu.subgroup_mma_compute`.
< 
1292c1265
<   let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, I32, F16, F32]>>:$src,
---
>   let arguments = (ins Arg<MMAMatrixOf<[F16, F32]>>:$src,
1318c1291
<     transposed manner. The transpose operands are required to map to correct
---
>     transposed manner. The transpose opernads are required to map to correct
1323,1326d1295
<     For integer types, the `A` and `B` matrices carry their signedness with their
<     types. The accumulator type is expected to be signless and imply a signed integer
<     with a greater width than the other two operands.
< 
1339,1341c1308,1310
<   let arguments = (ins Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opA,
<                   Arg<MMAMatrixOf<[SI8, UI8, F16, F32]>>:$opB,
<                   Arg<MMAMatrixOf<[I32, F16, F32]>>:$opC,
---
>   let arguments = (ins Arg<MMAMatrixOf<[F16, F32]>>:$opA,
>                   Arg<MMAMatrixOf<[F16, F32]>>:$opB,
>                   Arg<MMAMatrixOf<[F16, F32]>>:$opC,
1383c1352
<   let arguments = (ins AnyTypeOf<[SI8, UI8, I32, F16, F32]>:$value);
---
>   let arguments = (ins AnyTypeOf<[F16, F32]>:$value);
1430a1400
> 
1472a1443,1458
> }
> 
> def GPU_WarpSwizzleOp :
>     GPU_Op<"warp_swizzle">,
>     Arguments<(ins I32:$in, ConfinedAttr<I32ArrayAttr, [ArrayCount<4>]>:$selector)>,
>     Results<(outs I32:$out)> {
>   let summary = "Swizzle a value accross lanes/threads according to `selector``";
>   let description = [{
>     For a thread with ID t, the result of the operation will be the value of in
>     on thread s = (t / 4) * 4 + selector[t % 4]
>     If s is not executing, the result of this operation is undefined.
>     selector is not required to be a permutation of {0, 1, 2, 3}, but can't contain
>     any values from outside of that set
>   }];
>   let assemblyFormat = "attr-dict $in `:` type($in)";
>   let hasVerifier = 1;
--- include/mlir/Dialect/GPU/IR/GPUBase.td
59a60
>   let useFoldAPI = kEmitFoldAdaptorFolder;
104c105
< def GPU_MMAMemRef : MemRefOf<[I8, I32, F16, F32, VectorOfRankAndType<[1], [I8, I32, F16, F32]>]>;
---
> def GPU_MMAMemRef : MemRefOf<[F16, F32, VectorOfRankAndType<[1], [F16, F32]>]>;
--- include/mlir/Dialect/GPU/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/TransformOps/GPUDeviceMappingAttr.td include/mlir/Dialect/GPU/TransformOps/GPUDeviceMappingAttr.td
46,101d45
< def WarpsEnum : I64EnumAttr<"Warps", "threads for loop mapping", [
<     DimX, DimY, DimZ]> {
<   let cppNamespace = "::mlir::gpu";
< }
< 
< def GPUWarpMappingAttr : GPU_Attr<"GPUWarpMapping", "warp", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<WarpsEnum>:$warp
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute that allows defining thread block parallelism for GPU devices.
< 
<     Warp (aka subgroup) are grouped into a grid where grid may be
<     described by a 1-, 2-, or 3-dimensional rectangle. This attribute indicates
<     that thread block parallelism is desired. It can be consumed by lowering to
<     generate GPU code.
<   }];
< }
< 
< def LinearIdEnum : I64EnumAttr<"LinearId", "linear ids for loop mapping", [
<     DimX, DimY, DimZ]> {
<   let cppNamespace = "::mlir::gpu";
< }
< 
< def GPULinearIdMapping : GPU_Attr<"GPULinearIdMapping", "linear", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<LinearIdEnum>:$linear_id
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute to allow re-interpreting the linear mapping for threads in GPU
<     devices.
< 
<     Threads (aka work item) are grouped into a thread block where block may be
<     described by a 1-, 2-, or 3-dimensional rectangular basis.
<     The linear thread id is obtained by linearizing the 1-, 2- or 3-dimensional
<     index. For instance, if the basis is denoted as (BX, BY, BZ) and the thread
<     id is denoted by (tx, ty, tz), the linear thread id is:
<       `linear_id = tx + ty * BX + tz * BX * BY)`.
<     The linear thread id is fixed for the duration of a GPU kernel.
<     
<     This linear id mapping attribute indicates a different linearization relation
<     is applied locally to a loop nest. 
<     
<     For instance, if the new basis is denoted as (LBX, LBY, LBZ) the thread id
<     in the new basis is:
<       `(linear_id mod LBX , (linear_id / LBX) mod * LBY, linear_id / (LBX * LBY))`.
<     This reinterpretation is only fixe for the duration of a loop nest.
<     
<     It can be consumed by lowering to generate GPU code.
<   }];
< }
< 
122,140d65
< 
< 
< def GPUMemorySpaceMappingAttr : GPU_Attr<"GPUMemorySpaceMapping", "memory_space", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<GPU_AddressSpaceEnum>:$address_space
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute that allows defining memory hierarchy for GPU devices.
< 
<     GPU Memory has three memory space, global, workgroup, and private. The global memory
<     is visible to all workitems and workgroups, the workgroup memory is only available for workitems
<     within a workgroup, and private memory is only visible to a single workitem. This attribute indicates
<     that using memory hiearchy is desired. It can be consumed by lowering to
<     move data to a specific address space in GPU code.
<   }];
< }
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.h include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.h
36,47c36,60
< /// Helper type for functions that generate ids for the mapping of a
< /// scf.forall.
< struct IdBuilderResult {
<   // Ops used to replace the forall induction variables.
<   SmallVector<Value> mappingIdOps;
<   // Actual mapping sizes used to predicate the forall body when they are
<   // smaller than the available mapping sizes.
<   SmallVector<int64_t> predicateMappingSizes;
<   // Ops used to predicate the forall body when predicateMappingSizes is smaller
<   // than the available mapping sizes.
<   SmallVector<Value> predicateIdOps;
< };
---
> /// Searches `scf.foreach_thread` ops nested under `target` and maps each such
> /// op to GPU threads. Mapping is one-to-one and the induction variables of
> /// `scf.foreach_thread` are rewritten to gpu.thread_id according to the
> /// thread_dim_apping attribute. Sibling `scf.foreach_thread` are supported in
> /// which case, the union of the number of threads is computed and may result in
> /// predication. Dynamic, `scf.foreach_thread` trip counts are currently not
> /// supported. Dynamic block dim sizes are currently not supported.
> DiagnosedSilenceableFailure mapNestedForeachToThreadsImpl(
>     RewriterBase &rewriter, Operation *target,
>     const SmallVectorImpl<int64_t> &blockDim, bool syncAfterDistribute,
>     std::optional<TransformOpInterface> transformOp,
>     const ArrayRef<DeviceMappingAttrInterface> &threadMappingAttributes);
> 
> /// Maps the top level `scf.foreach_thread` op to GPU Thread Blocks. Mapping is
> /// one-to-one and the induction variables of `scf.foreach_thread` are rewritten
> /// to gpu.block_id according to the thread_dim_apping attribute. Dynamic,
> /// `scf.foreach_thread` trip counts are currently not supported. Dynamic block
> /// dim sizes are currently not supported.
> DiagnosedSilenceableFailure mapForeachToBlocksImpl(
>     RewriterBase &rewriter, scf::ForeachThreadOp foreachThreadOp,
>     function_ref<void(RewriterBase &, scf::ForeachThreadOp,
>                       SmallVectorImpl<Value> &)>
>         blockIdGenerator,
>     SmallVectorImpl<int64_t> &gridDims, TransformOpInterface transformOp,
>     const ArrayRef<DeviceMappingAttrInterface> &mappingAttributes);
49,125c62
< /// Common gpu id builder type, allows the configuration of lowering for various
< /// mapping schemes. Takes:
< ///   - A rewriter with insertion point set before the forall op to rewrite.
< ///   - The loc of the forall op to rewrite.
< ///   - A list of positive integers carrying the mapping sizes for the current
< ///     forall op to rewrite.
< using GpuIdBuilderFnType =
<     std::function<IdBuilderResult(RewriterBase &, Location, ArrayRef<int64_t>)>;
< 
< /// Helper struct for configuring the rewrite of mapped scf.forall ops to
< /// various gpu id configurations.
< struct GpuIdBuilder {
<   GpuIdBuilder(ArrayRef<OpFoldResult> blockDims, ArrayRef<int64_t> mappingSizes)
<       : blockDimsOfr(blockDims), availableMappingSizes(mappingSizes),
<         mappingAttributes(), idBuilder() {}
< 
<   /// List of OpFoldResult carrying the  multi-dimensional number of
<   /// threads available in the current kernel (i.e. the current blockDims in
<   /// CUDA parlance).
<   ArrayRef<OpFoldResult> blockDimsOfr;
< 
<   /// A list of positive integers carrying the number of available mapping
<   /// resources that can trigger predication,
<   ArrayRef<int64_t> availableMappingSizes;
< 
<   /// The mapping attributes targeted by this generator.
<   SmallVector<DeviceMappingAttrInterface> mappingAttributes;
< 
<   /// The constructor that builds the concrete IR for mapping ids.
<   GpuIdBuilderFnType idBuilder;
< };
< 
< /// Map the top level `scf.forall` op to GPU Thread Blocks.
< /// Mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to gpu.block_id according to the thread_dim_mapping attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic block dim sizes are currently not supported.
< DiagnosedSilenceableFailure
< mapForallToBlocksImpl(RewriterBase &rewriter, TransformOpInterface transformOp,
<                       scf::ForallOp forallOp,
<                       SmallVectorImpl<int64_t> &gridDims,
<                       const GpuIdBuilder &gpuIdBuilder);
< 
< /// Search `scf.forall` ops nested under `target` and map each such op to an
< /// explicit GPU implementation along `availableMappingSizes`.
< /// The mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to gpuIdBuilder.idBuilder according to the
< /// gpuIdBuilder.mappingAttributes attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic `availableMappingSizes` sizes are currently not supported.
< /// `availableMappingSizes` is expected to be of size 3.
< DiagnosedSilenceableFailure mapOneForallToThreadsImpl(
<     RewriterBase &rewriter, std::optional<TransformOpInterface> transformOp,
<     scf::ForallOp forallOp, ArrayRef<int64_t> availableMappingSizes,
<     bool syncAfterDistribute, const GpuIdBuilder &gpuIdBuilder);
< 
< /// Search `scf.forall` ops nested under `target` and map each such op to an
< /// explicit GPU implementation along blockDims and warpDims.
< /// The mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to threads and warps ids according to the mapping attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic `blockDims` or `warpDims` or `linearDims` sizes are currently not
< /// supported.
< /// `blockDims` is expected to be of size 3.
< /// `warpDims` is expected to be empty or of size 3.
< ///
< /// The insertion point of the `rewriter` is expected to be set at the
< /// beginning of the `target` body block and dominate all other blocks.
< DiagnosedSilenceableFailure mapNestedForallToThreadsImpl(
<     RewriterBase &rewriter, std::optional<TransformOpInterface> transformOp,
<     Operation *target, ArrayRef<int64_t> blockDimsOfr,
<     ArrayRef<int64_t> warpDims, bool syncAfterDistribute);
< 
< /// Find the unique top level scf::ForallOp within a given target op.
---
> /// Finds the top level scf::ForeachThreadOp of given target.
127,128c64,66
< findTopLevelForallOp(Operation *target, scf::ForallOp &topLevelForallOp,
<                      TransformOpInterface transformOp);
---
> findTopLevelForeachThreadOp(Operation *target,
>                             scf::ForeachThreadOp &topLevelForeachThreadOp,
>                             TransformOpInterface transformOp);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.td include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
18,19c19,20
< def MapNestedForallToThreads :
<   Op<Transform_Dialect, "gpu.map_nested_forall_to_threads",
---
> def MapNestedForeachToThreads :
>   Op<Transform_Dialect, "gpu.map_nested_foreach_to_threads",
25,26c26,27
<       Target the `gpu.launch op` and rewrite all `scf.forall` nested in it to 
<       distributed `gpu.thread_id` attribute.
---
>       Target the `gpu.launch op` and rewrite all `scf.foreach_thread`
>       nested in it to distributed `gpu.thread_id` attribute.
28,44c29,40
<       The operation searches for `scf.forall` ops nested under `target` and maps
<       each such op to GPU threads. 
<       
<       `scf.forall` induction variables are rewritten to `gpu.thread_id` according
<       to the `mapping` attribute.
< 
<       Different types of mappings attributes are supported:
<         - the block_dims is a list of integers that specifies the number of
<           threads in each dimension. This is a mandatory attribute that is used
<           to constrain the number of threads in each dimension. If an 
<           `scf.forall` op is mapped to fewer threads, predication occurs.
<         - the warp_dims is a list of integers that specifies the number of
<           warps in each dimension. This is an optional attribute that is used
<           to constrain the number of warps in each dimension. When present, this
<           attribute must be specified in a way that is compatible with the 
<           block_dims attribute. If an `scf.forall` op is mapped to fewer warps,
<           predicaiton occurs.
---
>       The operation searches for `scf.foreach_thread` ops nested under `target`
>       and maps each such op to GPU threads. Mapping is one-to-one and the
>       induction variables of `scf.foreach_thread` are rewritten to
>       `gpu.thread_id` according to the `mapping` attribute.
> 
>       Sibling `scf.foreach_thread` are supported in which case, the union of
>       the number of threads is computed and may result in predication.
> 
>       Multiple scf.foreach_thread are supported per `gpu.launch` in which case,
>       the max of all the threads is computed and taken for the global
>       `gpu.thread_id`. If necessary, `scf.foreach_thread` that do not use the
>       whole thread range result in predicated computations.
46c42
<       Dynamic `scf.forall` trip counts are currently not supported.
---
>       Dynamic `scf.foreach_thread` trip counts are currently not supported.
49,50c45,46
<       Only **bufferized** `scf.forall` are currently supported.
<       Only `scf.forall` distributed to **at most 3 dimensions** are
---
>       Only **bufferized** `scf.foreach_thread` are currently supported.
>       Only `scf.foreach_thread` distributed to **at most 3 dimensions** are
53,55c49
<       The `sync_after_distribute`attribute controls whether a `gpu.barrier` is
<       inserted after each scf.forall op. At this time, this is an all or nothing
<       choice. This will need to be tightened in the future.
---
>       Barriers are inserted after each scf.foreach_thread op for now.
57,58c51,52
<       The operation alters the block size of the given gpu_launch using the 
<       mandatory block_dims argument.
---
>       The operation alters the block size of the given gpu_launch using
>       blockDim argument.
64c58
<       If any scf.forall with tensors is found, the transform definitely
---
>       If any scf.foreach_thread with tensors is found, the transform definitely
67,73c61,63
<       If all the scf.forall operations with gpu.thread mapping contained
<       within the LaunchOp referred to by the `target` PDLOperation lower to GPU
<       properly, the transform succeeds. Otherwise the transform definitely
<       fails.
< 
<       scf.forall operations with mappings other than gpu.thread are
<       ignored.
---
>       If all the scf.foreach_thread operations contained within the LaunchOp
>       referred to by the `target` PDLOperation lower to GPU properly, the
>       transform succeeds. Otherwise the transform definitely fails.
84c74
<         scf.forall (%i, %j) in (7, 9) {
---
>         scf.foreach_thread (%i, %j) in (7, 9) {
87c77
<         scf.forall (%i) in (12) {
---
>         scf.foreach_thread (%i) in (12) {
93d82
< 
115,117c104,105
<                    DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$block_dims,
<                    DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$warp_dims,
<                    DefaultValuedAttr<BoolAttr, "true">:$sync_after_distribute);
---
>                    DefaultValuedAttr<I64ArrayAttr, "{}">:$blockDim,
>                    DefaultValuedAttr<BoolAttr, "true">:$syncAfterDistribute);
120,126c108
<   let assemblyFormat = [{
<     $target
<     `block_dims` `=` $block_dims
<     (`warp_dims` `=` $warp_dims^)?
<     (`sync_after_distribute` `=` $sync_after_distribute^)?
<     attr-dict
<   }];
---
>   let assemblyFormat = "$target attr-dict";
135,136c117,119
< def MapForallToBlocks :
<   Op<Transform_Dialect, "gpu.map_forall_to_blocks",
---
> 
> def MapForeachToBlocks :
>   Op<Transform_Dialect, "gpu.map_foreach_to_blocks",
142c125
<     Target the gpu_launch op and rewrite the top level `scf.forall`
---
>     Target the gpu_launch op and rewrite the top level `scf.foreach_thread`
145c128
<     `scf.forall` inside.
---
>     `scf.foreach_thread` inside.
147c130
<     The operation searches top level `scf.forall` ops under
---
>     The operation searches top level `scf.foreach_thread` ops under
149c132
<     one-to-one and the induction variables of `scf.forall` are
---
>     one-to-one and the induction variables of `scf.foreach_thread` are
152c135
<     Dynamic, `scf.forall` trip counts are currently not supported.
---
>     Dynamic, `scf.foreach_thread` trip counts are currently not supported.
155,156c138,139
<     Only **bufferized** scf.forall are currently supported.
<     Only scf.forall distributed to **at most 3 dimensions** are
---
>     Only **bufferized** scf.foreach_thread are currently supported.
>     Only scf.foreach_thread distributed to **at most 3 dimensions** are
159,160c142,143
<     The operation alters the block size of the given gpu_launch using the 
<     grid_dims argument.
---
>     The operation alters the block size of the given gpu_launch using
>     gridDim argument.
166c149
<     If any scf.forall with tensors is found, the transform definitely
---
>     If any scf.foreach_thread with tensors is found, the transform definitely
169c152
<     If all the scf.forall operations contained within the LaunchOp
---
>     If all the scf.foreach_thread operations contained within the LaunchOp
179c162
<                    DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$grid_dims,
---
>                    DefaultValuedAttr<I64ArrayAttr, "{}">:$gridDim,
183,188c166
<   let assemblyFormat = [{
<     $target
<     (`generate_gpu_launch` $generate_gpu_launch^)?
<     (`grid_dims` `=` $grid_dims^)?
<     attr-dict
<   }];
---
>   let assemblyFormat = "$target attr-dict";
--- include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
18,19c19,20
< def MapNestedForallToThreads :
<   Op<Transform_Dialect, "gpu.map_nested_forall_to_threads",
---
> def MapNestedForeachToThreads :
>   Op<Transform_Dialect, "gpu.map_nested_foreach_to_threads",
25,26c26,27
<       Target the `gpu.launch op` and rewrite all `scf.forall` nested in it to 
<       distributed `gpu.thread_id` attribute.
---
>       Target the `gpu.launch op` and rewrite all `scf.foreach_thread`
>       nested in it to distributed `gpu.thread_id` attribute.
28,44c29,40
<       The operation searches for `scf.forall` ops nested under `target` and maps
<       each such op to GPU threads. 
<       
<       `scf.forall` induction variables are rewritten to `gpu.thread_id` according
<       to the `mapping` attribute.
< 
<       Different types of mappings attributes are supported:
<         - the block_dims is a list of integers that specifies the number of
<           threads in each dimension. This is a mandatory attribute that is used
<           to constrain the number of threads in each dimension. If an 
<           `scf.forall` op is mapped to fewer threads, predication occurs.
<         - the warp_dims is a list of integers that specifies the number of
<           warps in each dimension. This is an optional attribute that is used
<           to constrain the number of warps in each dimension. When present, this
<           attribute must be specified in a way that is compatible with the 
<           block_dims attribute. If an `scf.forall` op is mapped to fewer warps,
<           predicaiton occurs.
---
>       The operation searches for `scf.foreach_thread` ops nested under `target`
>       and maps each such op to GPU threads. Mapping is one-to-one and the
>       induction variables of `scf.foreach_thread` are rewritten to
>       `gpu.thread_id` according to the `mapping` attribute.
> 
>       Sibling `scf.foreach_thread` are supported in which case, the union of
>       the number of threads is computed and may result in predication.
> 
>       Multiple scf.foreach_thread are supported per `gpu.launch` in which case,
>       the max of all the threads is computed and taken for the global
>       `gpu.thread_id`. If necessary, `scf.foreach_thread` that do not use the
>       whole thread range result in predicated computations.
46c42
<       Dynamic `scf.forall` trip counts are currently not supported.
---
>       Dynamic `scf.foreach_thread` trip counts are currently not supported.
49,50c45,46
<       Only **bufferized** `scf.forall` are currently supported.
<       Only `scf.forall` distributed to **at most 3 dimensions** are
---
>       Only **bufferized** `scf.foreach_thread` are currently supported.
>       Only `scf.foreach_thread` distributed to **at most 3 dimensions** are
53,55c49
<       The `sync_after_distribute`attribute controls whether a `gpu.barrier` is
<       inserted after each scf.forall op. At this time, this is an all or nothing
<       choice. This will need to be tightened in the future.
---
>       Barriers are inserted after each scf.foreach_thread op for now.
57,58c51,52
<       The operation alters the block size of the given gpu_launch using the 
<       mandatory block_dims argument.
---
>       The operation alters the block size of the given gpu_launch using
>       blockDim argument.
64c58
<       If any scf.forall with tensors is found, the transform definitely
---
>       If any scf.foreach_thread with tensors is found, the transform definitely
67,73c61,63
<       If all the scf.forall operations with gpu.thread mapping contained
<       within the LaunchOp referred to by the `target` PDLOperation lower to GPU
<       properly, the transform succeeds. Otherwise the transform definitely
<       fails.
< 
<       scf.forall operations with mappings other than gpu.thread are
<       ignored.
---
>       If all the scf.foreach_thread operations contained within the LaunchOp
>       referred to by the `target` PDLOperation lower to GPU properly, the
>       transform succeeds. Otherwise the transform definitely fails.
84c74
<         scf.forall (%i, %j) in (7, 9) {
---
>         scf.foreach_thread (%i, %j) in (7, 9) {
87c77
<         scf.forall (%i) in (12) {
---
>         scf.foreach_thread (%i) in (12) {
93d82
< 
115,117c104,105
<                    DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$block_dims,
<                    DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$warp_dims,
<                    DefaultValuedAttr<BoolAttr, "true">:$sync_after_distribute);
---
>                    DefaultValuedAttr<I64ArrayAttr, "{}">:$blockDim,
>                    DefaultValuedAttr<BoolAttr, "true">:$syncAfterDistribute);
120,126c108
<   let assemblyFormat = [{
<     $target
<     `block_dims` `=` $block_dims
<     (`warp_dims` `=` $warp_dims^)?
<     (`sync_after_distribute` `=` $sync_after_distribute^)?
<     attr-dict
<   }];
---
>   let assemblyFormat = "$target attr-dict";
135,136c117,119
< def MapForallToBlocks :
<   Op<Transform_Dialect, "gpu.map_forall_to_blocks",
---
> 
> def MapForeachToBlocks :
>   Op<Transform_Dialect, "gpu.map_foreach_to_blocks",
142c125
<     Target the gpu_launch op and rewrite the top level `scf.forall`
---
>     Target the gpu_launch op and rewrite the top level `scf.foreach_thread`
145c128
<     `scf.forall` inside.
---
>     `scf.foreach_thread` inside.
147c130
<     The operation searches top level `scf.forall` ops under
---
>     The operation searches top level `scf.foreach_thread` ops under
149c132
<     one-to-one and the induction variables of `scf.forall` are
---
>     one-to-one and the induction variables of `scf.foreach_thread` are
152c135
<     Dynamic, `scf.forall` trip counts are currently not supported.
---
>     Dynamic, `scf.foreach_thread` trip counts are currently not supported.
155,156c138,139
<     Only **bufferized** scf.forall are currently supported.
<     Only scf.forall distributed to **at most 3 dimensions** are
---
>     Only **bufferized** scf.foreach_thread are currently supported.
>     Only scf.foreach_thread distributed to **at most 3 dimensions** are
159,160c142,143
<     The operation alters the block size of the given gpu_launch using the 
<     grid_dims argument.
---
>     The operation alters the block size of the given gpu_launch using
>     gridDim argument.
166c149
<     If any scf.forall with tensors is found, the transform definitely
---
>     If any scf.foreach_thread with tensors is found, the transform definitely
169c152
<     If all the scf.forall operations contained within the LaunchOp
---
>     If all the scf.foreach_thread operations contained within the LaunchOp
179c162
<                    DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$grid_dims,
---
>                    DefaultValuedAttr<I64ArrayAttr, "{}">:$gridDim,
183,188c166
<   let assemblyFormat = [{
<     $target
<     (`generate_gpu_launch` $generate_gpu_launch^)?
<     (`grid_dims` `=` $grid_dims^)?
<     attr-dict
<   }];
---
>   let assemblyFormat = "$target attr-dict";
--- include/mlir/Dialect/GPU/TransformOps/GPUDeviceMappingAttr.td
46,101d45
< def WarpsEnum : I64EnumAttr<"Warps", "threads for loop mapping", [
<     DimX, DimY, DimZ]> {
<   let cppNamespace = "::mlir::gpu";
< }
< 
< def GPUWarpMappingAttr : GPU_Attr<"GPUWarpMapping", "warp", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<WarpsEnum>:$warp
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute that allows defining thread block parallelism for GPU devices.
< 
<     Warp (aka subgroup) are grouped into a grid where grid may be
<     described by a 1-, 2-, or 3-dimensional rectangle. This attribute indicates
<     that thread block parallelism is desired. It can be consumed by lowering to
<     generate GPU code.
<   }];
< }
< 
< def LinearIdEnum : I64EnumAttr<"LinearId", "linear ids for loop mapping", [
<     DimX, DimY, DimZ]> {
<   let cppNamespace = "::mlir::gpu";
< }
< 
< def GPULinearIdMapping : GPU_Attr<"GPULinearIdMapping", "linear", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<LinearIdEnum>:$linear_id
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute to allow re-interpreting the linear mapping for threads in GPU
<     devices.
< 
<     Threads (aka work item) are grouped into a thread block where block may be
<     described by a 1-, 2-, or 3-dimensional rectangular basis.
<     The linear thread id is obtained by linearizing the 1-, 2- or 3-dimensional
<     index. For instance, if the basis is denoted as (BX, BY, BZ) and the thread
<     id is denoted by (tx, ty, tz), the linear thread id is:
<       `linear_id = tx + ty * BX + tz * BX * BY)`.
<     The linear thread id is fixed for the duration of a GPU kernel.
<     
<     This linear id mapping attribute indicates a different linearization relation
<     is applied locally to a loop nest. 
<     
<     For instance, if the new basis is denoted as (LBX, LBY, LBZ) the thread id
<     in the new basis is:
<       `(linear_id mod LBX , (linear_id / LBX) mod * LBY, linear_id / (LBX * LBY))`.
<     This reinterpretation is only fixe for the duration of a loop nest.
<     
<     It can be consumed by lowering to generate GPU code.
<   }];
< }
< 
122,140d65
< 
< 
< def GPUMemorySpaceMappingAttr : GPU_Attr<"GPUMemorySpaceMapping", "memory_space", [
<   DeclareAttrInterfaceMethods<DeviceMappingAttrInterface> ] >  {
<   let parameters = (ins
<     EnumParameter<GPU_AddressSpaceEnum>:$address_space
<   );
<   let assemblyFormat = "`<` params `>`";
<   let description = [{
<     An attribute that allows defining memory hierarchy for GPU devices.
< 
<     GPU Memory has three memory space, global, workgroup, and private. The global memory
<     is visible to all workitems and workgroups, the workgroup memory is only available for workitems
<     within a workgroup, and private memory is only visible to a single workitem. This attribute indicates
<     that using memory hiearchy is desired. It can be consumed by lowering to
<     move data to a specific address space in GPU code.
<   }];
< }
< 
--- include/mlir/Dialect/GPU/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/GPU/TransformOps/GPUTransformOps.h
36,47c36,60
< /// Helper type for functions that generate ids for the mapping of a
< /// scf.forall.
< struct IdBuilderResult {
<   // Ops used to replace the forall induction variables.
<   SmallVector<Value> mappingIdOps;
<   // Actual mapping sizes used to predicate the forall body when they are
<   // smaller than the available mapping sizes.
<   SmallVector<int64_t> predicateMappingSizes;
<   // Ops used to predicate the forall body when predicateMappingSizes is smaller
<   // than the available mapping sizes.
<   SmallVector<Value> predicateIdOps;
< };
---
> /// Searches `scf.foreach_thread` ops nested under `target` and maps each such
> /// op to GPU threads. Mapping is one-to-one and the induction variables of
> /// `scf.foreach_thread` are rewritten to gpu.thread_id according to the
> /// thread_dim_apping attribute. Sibling `scf.foreach_thread` are supported in
> /// which case, the union of the number of threads is computed and may result in
> /// predication. Dynamic, `scf.foreach_thread` trip counts are currently not
> /// supported. Dynamic block dim sizes are currently not supported.
> DiagnosedSilenceableFailure mapNestedForeachToThreadsImpl(
>     RewriterBase &rewriter, Operation *target,
>     const SmallVectorImpl<int64_t> &blockDim, bool syncAfterDistribute,
>     std::optional<TransformOpInterface> transformOp,
>     const ArrayRef<DeviceMappingAttrInterface> &threadMappingAttributes);
> 
> /// Maps the top level `scf.foreach_thread` op to GPU Thread Blocks. Mapping is
> /// one-to-one and the induction variables of `scf.foreach_thread` are rewritten
> /// to gpu.block_id according to the thread_dim_apping attribute. Dynamic,
> /// `scf.foreach_thread` trip counts are currently not supported. Dynamic block
> /// dim sizes are currently not supported.
> DiagnosedSilenceableFailure mapForeachToBlocksImpl(
>     RewriterBase &rewriter, scf::ForeachThreadOp foreachThreadOp,
>     function_ref<void(RewriterBase &, scf::ForeachThreadOp,
>                       SmallVectorImpl<Value> &)>
>         blockIdGenerator,
>     SmallVectorImpl<int64_t> &gridDims, TransformOpInterface transformOp,
>     const ArrayRef<DeviceMappingAttrInterface> &mappingAttributes);
49,125c62
< /// Common gpu id builder type, allows the configuration of lowering for various
< /// mapping schemes. Takes:
< ///   - A rewriter with insertion point set before the forall op to rewrite.
< ///   - The loc of the forall op to rewrite.
< ///   - A list of positive integers carrying the mapping sizes for the current
< ///     forall op to rewrite.
< using GpuIdBuilderFnType =
<     std::function<IdBuilderResult(RewriterBase &, Location, ArrayRef<int64_t>)>;
< 
< /// Helper struct for configuring the rewrite of mapped scf.forall ops to
< /// various gpu id configurations.
< struct GpuIdBuilder {
<   GpuIdBuilder(ArrayRef<OpFoldResult> blockDims, ArrayRef<int64_t> mappingSizes)
<       : blockDimsOfr(blockDims), availableMappingSizes(mappingSizes),
<         mappingAttributes(), idBuilder() {}
< 
<   /// List of OpFoldResult carrying the  multi-dimensional number of
<   /// threads available in the current kernel (i.e. the current blockDims in
<   /// CUDA parlance).
<   ArrayRef<OpFoldResult> blockDimsOfr;
< 
<   /// A list of positive integers carrying the number of available mapping
<   /// resources that can trigger predication,
<   ArrayRef<int64_t> availableMappingSizes;
< 
<   /// The mapping attributes targeted by this generator.
<   SmallVector<DeviceMappingAttrInterface> mappingAttributes;
< 
<   /// The constructor that builds the concrete IR for mapping ids.
<   GpuIdBuilderFnType idBuilder;
< };
< 
< /// Map the top level `scf.forall` op to GPU Thread Blocks.
< /// Mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to gpu.block_id according to the thread_dim_mapping attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic block dim sizes are currently not supported.
< DiagnosedSilenceableFailure
< mapForallToBlocksImpl(RewriterBase &rewriter, TransformOpInterface transformOp,
<                       scf::ForallOp forallOp,
<                       SmallVectorImpl<int64_t> &gridDims,
<                       const GpuIdBuilder &gpuIdBuilder);
< 
< /// Search `scf.forall` ops nested under `target` and map each such op to an
< /// explicit GPU implementation along `availableMappingSizes`.
< /// The mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to gpuIdBuilder.idBuilder according to the
< /// gpuIdBuilder.mappingAttributes attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic `availableMappingSizes` sizes are currently not supported.
< /// `availableMappingSizes` is expected to be of size 3.
< DiagnosedSilenceableFailure mapOneForallToThreadsImpl(
<     RewriterBase &rewriter, std::optional<TransformOpInterface> transformOp,
<     scf::ForallOp forallOp, ArrayRef<int64_t> availableMappingSizes,
<     bool syncAfterDistribute, const GpuIdBuilder &gpuIdBuilder);
< 
< /// Search `scf.forall` ops nested under `target` and map each such op to an
< /// explicit GPU implementation along blockDims and warpDims.
< /// The mapping is one-to-one and the induction variables of `scf.forall` are
< /// rewritten to threads and warps ids according to the mapping attribute.
< ///
< /// Dynamic, `scf.forall` trip counts are currently not supported.
< /// Dynamic `blockDims` or `warpDims` or `linearDims` sizes are currently not
< /// supported.
< /// `blockDims` is expected to be of size 3.
< /// `warpDims` is expected to be empty or of size 3.
< ///
< /// The insertion point of the `rewriter` is expected to be set at the
< /// beginning of the `target` body block and dominate all other blocks.
< DiagnosedSilenceableFailure mapNestedForallToThreadsImpl(
<     RewriterBase &rewriter, std::optional<TransformOpInterface> transformOp,
<     Operation *target, ArrayRef<int64_t> blockDimsOfr,
<     ArrayRef<int64_t> warpDims, bool syncAfterDistribute);
< 
< /// Find the unique top level scf::ForallOp within a given target op.
---
> /// Finds the top level scf::ForeachThreadOp of given target.
127,128c64,66
< findTopLevelForallOp(Operation *target, scf::ForallOp &topLevelForallOp,
<                      TransformOpInterface transformOp);
---
> findTopLevelForeachThreadOp(Operation *target,
>                             scf::ForeachThreadOp &topLevelForeachThreadOp,
>                             TransformOpInterface transformOp);
--- include/mlir/Dialect/Tensor
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/CMakeLists.txt include/mlir/Dialect/Tensor/CMakeLists.txt
3d2
< add_subdirectory(TransformOps)
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/IR and include/mlir/Dialect/Tensor/IR
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor: TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Transforms and include/mlir/Dialect/Tensor/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Utils and include/mlir/Dialect/Tensor/Utils
--- include/mlir/Dialect/Tensor/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Utils/Utils.h include/mlir/Dialect/Tensor/Utils/Utils.h
29,33d28
< // Returns the tensor extent along dimension `dim` if `rankedTensor` is of
< // `RankedTensorType`. Returns `failure()` otherwise.
< FailureOr<OpFoldResult> createDimValue(OpBuilder &b, Location loc,
<                                        Value rankedTensor, int64_t dim);
< 
38,43d32
< 
< /// Returns the transposed `rankedTensorType` if `transposeVector` is non-empty.
< /// Fail if `transposeVector` is not a permutation matching the tensor rank.
< FailureOr<RankedTensorType>
< computeTransposedType(RankedTensorType rankedTensorType,
<                       ArrayRef<int64_t> transposeVector);
--- include/mlir/Dialect/Tensor/Utils/Utils.h
29,33d28
< // Returns the tensor extent along dimension `dim` if `rankedTensor` is of
< // `RankedTensorType`. Returns `failure()` otherwise.
< FailureOr<OpFoldResult> createDimValue(OpBuilder &b, Location loc,
<                                        Value rankedTensor, int64_t dim);
< 
38,43d32
< 
< /// Returns the transposed `rankedTensorType` if `transposeVector` is non-empty.
< /// Fail if `transposeVector` is not a permutation matching the tensor rank.
< FailureOr<RankedTensorType>
< computeTransposedType(RankedTensorType rankedTensorType,
<                       ArrayRef<int64_t> transposeVector);
--- include/mlir/Dialect/Tensor/CMakeLists.txt
3d2
< add_subdirectory(TransformOps)
--- include/mlir/Dialect/Tensor/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Transforms/Passes.h include/mlir/Dialect/Tensor/Transforms/Passes.h
15,19d14
< namespace tensor {
< 
< //===----------------------------------------------------------------------===//
< // Passes
< //===----------------------------------------------------------------------===//
21,22c16,17
< /// Creates an instance of the `tensor` subset folding pass.
< std::unique_ptr<Pass> createFoldTensorSubsetOpsPass();
---
> #define GEN_PASS_DECL
> #include "mlir/Dialect/Tensor/Transforms/Passes.h.inc"
24c19
< /// Creates an instance of the `tensor` dialect bufferization pass.
---
> /// Creates an instance of `tensor` dialect bufferization pass.
30a26
> namespace tensor {
34d29
< 
35a31
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Transforms/Passes.td include/mlir/Dialect/Tensor/Transforms/Passes.td
14,29d13
< def FoldTensorSubsetOps : Pass<"fold-tensor-subset-ops"> {
<   let summary = "Fold tensor subset ops into producer/consumer ops";
<   let description = [{
<     The pass folds tensor subset ops into producer/consumer ops.
< 
<     At the moment, the following foldings occur when possible:
<       - tensor.extract_slice into vector.transfer_read
<       - vector.transfer_write into tensor.insert_slice
< 
<   }];
<   let constructor = "mlir::tensor::createFoldTensorSubsetOpsPass()";
<   let dependentDialects = [
<       "affine::AffineDialect", "tensor::TensorDialect", "vector::VectorDialect"
<   ];
< }
< 
32c16
<   let constructor = "mlir::tensor::createTensorBufferizePass()";
---
>   let constructor = "mlir::createTensorBufferizePass()";
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/Transforms/Transforms.h include/mlir/Dialect/Tensor/Transforms/Transforms.h
16,18d15
< 
< struct TilingResult;
< 
21,23c18,22
< //===----------------------------------------------------------------------===//
< // Patterns
< //===----------------------------------------------------------------------===//
---
> /// Populates `patterns` with patterns to wrap a tensor.pad op with an scf.if op
> /// to separate the cases where we don't need padding (all pad sizes are
> /// actually zeros) and where we indeed need padding.
> void populateSplitPaddingPatterns(RewritePatternSet &patterns,
>                                   PatternBenefit baseBenefit = 1);
30c29
< FailureOr<TilingResult> replaceExtractSliceWithTiledProducer(
---
> FailureOr<Value> replaceExtractSliceWithTiledProducer(
33,43d31
< //===----------------------------------------------------------------------===//
< // Populate functions.
< //===----------------------------------------------------------------------===//
< 
< /// Collects a set of patterns to rewrite ops within the tensor dialect.
< void populateExpandOpsPatterns(RewritePatternSet &patterns);
< 
< /// Appends patterns for folding tensor aliasing ops into consumer load/store
< /// ops into `patterns`.
< void populateFoldTensorSubsetOpPatterns(RewritePatternSet &patterns);
< 
63,101d50
< 
< //===----------------------------------------------------------------------===//
< // Transform helpers
< //===----------------------------------------------------------------------===//
< 
< /// Build a new tensor::PadOp with low/high padding that is independent of all
< /// given independencies. If the op is already independent of all
< /// independencies, the same PadOp result is returned.
< ///
< /// Failure indicates the no suitable upper bound for low/high padding could be
< /// found.
< ///
< /// Example:
< /// scf.for %iv = %lb to %ub step %step {
< ///   %high = affine.apply affine_map<(d0)[s0] -> (s0 - d0)> (%i)[%ub]
< ///   %p = tensor.pad %t low[5] high[%high] ...
< ///   ...
< /// }
< ///
< /// The function builds IR such as:
< /// %high_new = affine.apply affine_map<()[s0, s1] -> (-s0 + s1)> ()[%lb, %ub]
< /// %p_hoistable = tensor.pad %t low[5] high[%high_new]
< /// %dim = tensor.dim %t, %c0
< /// %size = affine.apply affine_map<(d0)[s0, s1] -> (-d0 + s0 + s1 + 5)>
< ///     (%iv)[%ub, %dim]
< /// %slice = tensor.extract_slice %p_hoistable [0] [%size] [1]
< ///
< /// The slice is returned.
< FailureOr<Value> buildIndependentOp(OpBuilder &b, tensor::PadOp padOp,
<                                     ValueRange independencies);
< 
< /// Build a new tensor::EmptyOp who's dynamic sizes are independent of all
< /// given independencies. If the op is already independent of all
< /// independencies, the same EmptyOp result is returned.
< ///
< /// Failure indicates the no suitable upper bound for the dynamic sizes could be
< /// found.
< FailureOr<Value> buildIndependentOp(OpBuilder &b, tensor::EmptyOp emptyOp,
<                                     ValueRange independencies);
--- include/mlir/Dialect/Tensor/Transforms/Passes.h
15,19d14
< namespace tensor {
< 
< //===----------------------------------------------------------------------===//
< // Passes
< //===----------------------------------------------------------------------===//
21,22c16,17
< /// Creates an instance of the `tensor` subset folding pass.
< std::unique_ptr<Pass> createFoldTensorSubsetOpsPass();
---
> #define GEN_PASS_DECL
> #include "mlir/Dialect/Tensor/Transforms/Passes.h.inc"
24c19
< /// Creates an instance of the `tensor` dialect bufferization pass.
---
> /// Creates an instance of `tensor` dialect bufferization pass.
30a26
> namespace tensor {
34d29
< 
35a31
> 
--- include/mlir/Dialect/Tensor/Transforms/Passes.td
14,29d13
< def FoldTensorSubsetOps : Pass<"fold-tensor-subset-ops"> {
<   let summary = "Fold tensor subset ops into producer/consumer ops";
<   let description = [{
<     The pass folds tensor subset ops into producer/consumer ops.
< 
<     At the moment, the following foldings occur when possible:
<       - tensor.extract_slice into vector.transfer_read
<       - vector.transfer_write into tensor.insert_slice
< 
<   }];
<   let constructor = "mlir::tensor::createFoldTensorSubsetOpsPass()";
<   let dependentDialects = [
<       "affine::AffineDialect", "tensor::TensorDialect", "vector::VectorDialect"
<   ];
< }
< 
32c16
<   let constructor = "mlir::tensor::createTensorBufferizePass()";
---
>   let constructor = "mlir::createTensorBufferizePass()";
--- include/mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/Tensor/Transforms/TransformUtils.h
--- include/mlir/Dialect/Tensor/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Tensor/Transforms/Transforms.h
16,18d15
< 
< struct TilingResult;
< 
21,23c18,22
< //===----------------------------------------------------------------------===//
< // Patterns
< //===----------------------------------------------------------------------===//
---
> /// Populates `patterns` with patterns to wrap a tensor.pad op with an scf.if op
> /// to separate the cases where we don't need padding (all pad sizes are
> /// actually zeros) and where we indeed need padding.
> void populateSplitPaddingPatterns(RewritePatternSet &patterns,
>                                   PatternBenefit baseBenefit = 1);
30c29
< FailureOr<TilingResult> replaceExtractSliceWithTiledProducer(
---
> FailureOr<Value> replaceExtractSliceWithTiledProducer(
33,43d31
< //===----------------------------------------------------------------------===//
< // Populate functions.
< //===----------------------------------------------------------------------===//
< 
< /// Collects a set of patterns to rewrite ops within the tensor dialect.
< void populateExpandOpsPatterns(RewritePatternSet &patterns);
< 
< /// Appends patterns for folding tensor aliasing ops into consumer load/store
< /// ops into `patterns`.
< void populateFoldTensorSubsetOpPatterns(RewritePatternSet &patterns);
< 
63,101d50
< 
< //===----------------------------------------------------------------------===//
< // Transform helpers
< //===----------------------------------------------------------------------===//
< 
< /// Build a new tensor::PadOp with low/high padding that is independent of all
< /// given independencies. If the op is already independent of all
< /// independencies, the same PadOp result is returned.
< ///
< /// Failure indicates the no suitable upper bound for low/high padding could be
< /// found.
< ///
< /// Example:
< /// scf.for %iv = %lb to %ub step %step {
< ///   %high = affine.apply affine_map<(d0)[s0] -> (s0 - d0)> (%i)[%ub]
< ///   %p = tensor.pad %t low[5] high[%high] ...
< ///   ...
< /// }
< ///
< /// The function builds IR such as:
< /// %high_new = affine.apply affine_map<()[s0, s1] -> (-s0 + s1)> ()[%lb, %ub]
< /// %p_hoistable = tensor.pad %t low[5] high[%high_new]
< /// %dim = tensor.dim %t, %c0
< /// %size = affine.apply affine_map<(d0)[s0, s1] -> (-d0 + s0 + s1 + 5)>
< ///     (%iv)[%ub, %dim]
< /// %slice = tensor.extract_slice %p_hoistable [0] [%size] [1]
< ///
< /// The slice is returned.
< FailureOr<Value> buildIndependentOp(OpBuilder &b, tensor::PadOp padOp,
<                                     ValueRange independencies);
< 
< /// Build a new tensor::EmptyOp who's dynamic sizes are independent of all
< /// given independencies. If the op is already independent of all
< /// independencies, the same EmptyOp result is returned.
< ///
< /// Failure indicates the no suitable upper bound for the dynamic sizes could be
< /// found.
< FailureOr<Value> buildIndependentOp(OpBuilder &b, tensor::EmptyOp emptyOp,
<                                     ValueRange independencies);
--- include/mlir/Dialect/Tensor/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/IR/TensorBase.td include/mlir/Dialect/Tensor/IR/TensorBase.td
49a50
>   let useFoldAPI = kEmitFoldAdaptorFolder;
51c52
<     "affine::AffineDialect",
---
>     "AffineDialect",
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/IR/TensorOps.td include/mlir/Dialect/Tensor/IR/TensorOps.td
33,43c33
<     /// Return the type of the base tensor operand.
<     ::mlir::RankedTensorType getSourceType() { 
<       return getSource().getType().cast<RankedTensorType>();
<     }
< 
<     /// Return the type of the result tensor.
<     ::mlir::RankedTensorType getResultType() { 
<       return getResult().getType().cast<RankedTensorType>();
<     }
< 
<     /// Return the dynamic sizes for this subview operation if specified.
---
>     /// Returns the dynamic sizes for this subview operation if specified.
118c108
<     // Return the dynamic dimension of %A.
---
>     // Returns the dynamic dimension of %A.
128c118
<   let arguments = (ins AnyNon0RankedOrUnrankedTensor:$source,
---
>   let arguments = (ins AnyTensor:$source,
155a146
>   let hasVerifier = 1;
229c220
<                    "$_self.cast<TensorType>().getElementType()">]> {
---
>                    "$_self.cast<ShapedType>().getElementType()">]> {
284c275,276
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
372a365,369
>     /// Returns the type of the base tensor operand.
>     RankedTensorType getSourceType() {
>       return getSource().getType().cast<RankedTensorType>();
>     }
> 
374d370
<     // TODO: deprecate
376c372
<       return getResultType();
---
>       return getResult().getType().cast<RankedTensorType>();
391c387
<       RankedTensorType sourceTensorType,
---
>       ShapedType sourceShapedTensorType,
396c392
<       RankedTensorType sourceTensorType,
---
>       ShapedType sourceShapedTensorType,
461,462c457,458
<                    "$_self.cast<RankedTensorType>().getNumElements(), "
<                    "$_self.cast<RankedTensorType>().getElementType())">
---
>                    "$_self.cast<ShapedType>().getNumElements(), "
>                    "$_self.cast<ShapedType>().getElementType())">
486a483
>   let skipDefaultBuilders = 1;
487a485
>     OpBuilder<(ins "Type":$resultType, "ValueRange":$elements)>,
695c693
<                    "$_self.cast<TensorType>().getElementType()">]> {
---
>                    "$_self.cast<ShapedType>().getElementType()">]> {
770c768,769
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
822,823c821
<     // Build a InsertSliceOp with mixed static and dynamic entries and inferred
<     // result type.
---
>     // Build a InsertSliceOp with mixed static and dynamic entries.
828c826
<     // Build a InsertSliceOp with dynamic entries and inferred result type.
---
>     // Build a InsertSliceOp with dynamic entries.
833c831
<     // a Range vector and inferred result type.
---
>     // a Range vector.
839a838,842
>     /// Returns the type of the base tensor operand.
>     RankedTensorType getSourceType() {
>       return getSource().getType().cast<RankedTensorType>();
>     }
> 
841d843
<     // TODO: Deprecate this method.
843c845
<       return getResultType();
---
>       return getResult().getType().cast<RankedTensorType>();
848c850
<       return getResultType();
---
>       return getType();
854c856
<       unsigned rank = getResultType().getRank();
---
>       unsigned rank = getType().getRank();
858,861d859
<     /// Return the dimensions of the dest that are omitted to insert a source
<     /// when the result is rank-extended.
<     llvm::SmallBitVector getDroppedDims();
< 
974,975c972,973
<     Arguments<(ins AnyRankedTensor:$src, IndexListArrayAttr:$reassociation)>,
<     Results<(outs AnyRankedTensor:$result)> {
---
>     Arguments<(ins AnyTensor:$src, IndexListArrayAttr:$reassociation)>,
>     Results<(outs AnyTensor:$result)> {
989c987
<     }
---
>     };
1118,1124c1116
<   let extraClassDeclaration = commonExtraClassDeclaration # [{  
<     static RankedTensorType
<     inferCollapsedType(RankedTensorType type, ArrayRef<AffineMap> reassociation);
<     static RankedTensorType
<     inferCollapsedType(RankedTensorType type, 
<                        SmallVector<ReassociationIndices> reassociation);
<   }];
---
>   let extraClassDeclaration = commonExtraClassDeclaration;
1210c1202
<     AnyRankedTensor:$source,
---
>     AnyTensor:$source,
1219c1211
<   let results = (outs AnyRankedTensor:$result);
---
>   let results = (outs AnyTensor:$result);
1299,1301c1291,1293
<     OpBuilder<(ins "Type":$resultType, "Value":$source,
<       "ArrayRef<int64_t>":$staticLow, "ArrayRef<int64_t>":$staticHigh,
<       "ValueRange":$low, "ValueRange":$high, CArg<"bool", "false">:$nofold,
---
>     OpBuilder<(ins "Value":$source, "ArrayRef<int64_t>":$staticLow,
>       "ArrayRef<int64_t>":$staticHigh, "ValueRange":$low, "ValueRange":$high,
>       CArg<"bool", "false">:$nofold,
1304,1305c1296,1297
<     OpBuilder<(ins "Type":$resultType, "Value":$source, "ValueRange":$low,
<       "ValueRange":$high, CArg<"bool", "false">:$nofold,
---
>     OpBuilder<(ins "Value":$source, "ValueRange":$low, "ValueRange":$high,
>       CArg<"bool", "false">:$nofold,
1331c1323
< // TODO: Implement InParallelOpInterface.
---
> // TODO: Implement PerformConcurrentlyOpInterface.
1380c1372,1373
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
1446c1439
<     /// Return the OpResult of the enclosing ForallOp that is
---
>     /// Return the OpResult of the enclosing ForeachThreadOp that is
1449,1452d1441
< 
<     /// Return the dimensions of the dest that are omitted to insert a source
<     /// when the result is rank-extended.
<     llvm::SmallBitVector getDroppedDims();
1681,1682c1670,1671
<     size_t getSourceRank() { return getSourceType().getRank(); };
<     size_t getDestRank() { return getDestType().getRank(); };
---
>     int64_t getSourceRank() { return getSource().getType().getRank(); };
>     int64_t getDestRank() { return getDest().getType().getRank(); };
1780,1791c1769,1772
<     // Method to get the shape of the result as `SmallVector<OpFoldResult>`.
<     // This is a static method to allow getting the shape of the destination
<     // expected while creating a `pack` op.
<     static SmallVector<OpFoldResult> getResultShape(OpBuilder &builder,
<         Location loc, ArrayRef<OpFoldResult> sourceDims,
<         ArrayRef<OpFoldResult> innerTileDims, ArrayRef<int64_t> innerDimsPos,
<         ArrayRef<int64_t> outerDimsPerm = {});
< 
<     // Method to get the `RankedTensorType` of the result based on the inner
<     // tiles, position of the inner tiles (innerDimsPos)  and interchange vector
<     // of outer loops (outerDimsPerm).
<     static RankedTensorType inferPackedType(RankedTensorType sourceType,
---
>     // Method to get the `ShapedType` of the result based on the inner tiles,
>     // position of the inner tiles (innerDimsPos)  and interchange vector of
>     // outer loops (outerDimsPerm).
>     static ShapedType inferPackedType(ShapedType sourceType,
1795,1801d1775
<     // Returns true if we have enough static information to catch undefined
<     // behavior when the tile size does not divide perfectly the dimension of
<     // the input tensor.
<     static bool requirePaddingValue(ArrayRef<int64_t> inputShape,
<                                     ArrayRef<int64_t> innerDimsPos,
<                                     ArrayRef<OpFoldResult> innerTiles);
< 
1805,1826d1778
< 
<     /// Build and return a new PackOp that is a clone of the current PackOp with
<     /// (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by 
<     /// innerPermutation (resp. outerPermutation).
<     /// A new `tensor.empty` of the proper shape is built in the process.
<     /// Asserts that:
<     ///   - At least one of innerPermutation or outerPermutation is non-empty.
<     ///   - If not empty, innerPermutation is a valid permutation of size
<     ///     matching innerDimPos.
<     ///   - If not empty, outerPermutation is a valid permutation of size 
<     ///     matching outerDimsPerm.
<     PackOp createTransposedClone(OpBuilder &b,
<                                  Location loc,
<                                  ArrayRef<int64_t> innerPermutation,
<                                  ArrayRef<int64_t> outerPermutation);
< 
<     /// Check if this PackOp is like a simple pad operation.
<     /// In other words, this operation:
<     /// 1. adds useless dimensions (dimension of size 1),
<     /// 2. pads the other ones, and
<     /// 3. doesn't shuffle the dimensions
<     bool isLikePad();
1883,1909c1835
<   let extraClassDeclaration = commonExtraClassDeclaration # [{
<     static Value createDestinationTensor(OpBuilder &b, Location loc,
<         Value source, ArrayRef<OpFoldResult> innerTileSizes,
<         ArrayRef<int64_t> innerDimsPos, ArrayRef<int64_t> outerDimsPerm);
< 
<     /// Build and return a new UnPackOp that is a clone of the current UnPackOp
<     /// with (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by 
<     /// innerPermutation (resp. outerPermutation).
<     /// Asserts that:
<     ///   - At least one of innerPermutation or outerPermutation is non-empty.
<     ///   - If not empty, innerPermutation is a valid permutation of size
<     ///     matching innerDimPos.
<     ///   - If not empty, outerPermutation is a valid permutation of size 
<     ///     matching outerDimsPerm.
<     UnPackOp createTransposedClone(OpBuilder &b,
<                                    Location loc,
<                                    Value transposedSource,
<                                    ArrayRef<int64_t> innerPermutation,
<                                    ArrayRef<int64_t> outerPermutation);
< 
<     /// Check if this UnPackOp is like a simple unpad operation.
<     /// In other words, this operation:
<     /// 1. drops useless dimensions (dimension of size 1), and
<     /// 2. reduces dimensions in place (i.e., no tranpose.)
<     bool isLikeUnPad();
<   }];
< 
---
>   let extraClassDeclaration = commonExtraClassDeclaration;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/IR/TensorTilingInterfaceImpl.h include/mlir/Dialect/Tensor/IR/TensorTilingInterfaceImpl.h
19,21d18
< 
< struct TilingResult;
< 
45,48c42,45
< FailureOr<TilingResult> bubbleUpPadSlice(OpBuilder &b, tensor::PadOp padOp,
<                                          ArrayRef<OpFoldResult> offsets,
<                                          ArrayRef<OpFoldResult> sizes,
<                                          bool generateZeroSliceGuard = true);
---
> Operation *bubbleUpPadSlice(OpBuilder &b, tensor::PadOp padOp,
>                             ArrayRef<OpFoldResult> offsets,
>                             ArrayRef<OpFoldResult> sizes,
>                             bool generateZeroSliceGuard = true);
53c50
< /// * TilingInterface for `tensor.pad`, `tensor.pack`, and `tensor.unpack`.
---
> /// * TilingInterface for `tensor.pad`.
61,65d57
< 
< /// Similar to the above registeration, but it is only for `tensor.pack` and
< /// `tensor.unpack` ops.
< void registerTilingInterfaceExternalModelsForPackUnPackOps(
<     DialectRegistry &registry);
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tensor/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/Tensor/IR/TensorBase.td
49a50
>   let useFoldAPI = kEmitFoldAdaptorFolder;
51c52
<     "affine::AffineDialect",
---
>     "AffineDialect",
--- include/mlir/Dialect/Tensor/IR/TensorInferTypeOpInterfaceImpl.h
--- include/mlir/Dialect/Tensor/IR/CMakeLists.txt
--- include/mlir/Dialect/Tensor/IR/TensorOps.td
33,43c33
<     /// Return the type of the base tensor operand.
<     ::mlir::RankedTensorType getSourceType() { 
<       return getSource().getType().cast<RankedTensorType>();
<     }
< 
<     /// Return the type of the result tensor.
<     ::mlir::RankedTensorType getResultType() { 
<       return getResult().getType().cast<RankedTensorType>();
<     }
< 
<     /// Return the dynamic sizes for this subview operation if specified.
---
>     /// Returns the dynamic sizes for this subview operation if specified.
118c108
<     // Return the dynamic dimension of %A.
---
>     // Returns the dynamic dimension of %A.
128c118
<   let arguments = (ins AnyNon0RankedOrUnrankedTensor:$source,
---
>   let arguments = (ins AnyTensor:$source,
155a146
>   let hasVerifier = 1;
229c220
<                    "$_self.cast<TensorType>().getElementType()">]> {
---
>                    "$_self.cast<ShapedType>().getElementType()">]> {
284c275,276
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
372a365,369
>     /// Returns the type of the base tensor operand.
>     RankedTensorType getSourceType() {
>       return getSource().getType().cast<RankedTensorType>();
>     }
> 
374d370
<     // TODO: deprecate
376c372
<       return getResultType();
---
>       return getResult().getType().cast<RankedTensorType>();
391c387
<       RankedTensorType sourceTensorType,
---
>       ShapedType sourceShapedTensorType,
396c392
<       RankedTensorType sourceTensorType,
---
>       ShapedType sourceShapedTensorType,
461,462c457,458
<                    "$_self.cast<RankedTensorType>().getNumElements(), "
<                    "$_self.cast<RankedTensorType>().getElementType())">
---
>                    "$_self.cast<ShapedType>().getNumElements(), "
>                    "$_self.cast<ShapedType>().getElementType())">
486a483
>   let skipDefaultBuilders = 1;
487a485
>     OpBuilder<(ins "Type":$resultType, "ValueRange":$elements)>,
695c693
<                    "$_self.cast<TensorType>().getElementType()">]> {
---
>                    "$_self.cast<ShapedType>().getElementType()">]> {
770c768,769
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
822,823c821
<     // Build a InsertSliceOp with mixed static and dynamic entries and inferred
<     // result type.
---
>     // Build a InsertSliceOp with mixed static and dynamic entries.
828c826
<     // Build a InsertSliceOp with dynamic entries and inferred result type.
---
>     // Build a InsertSliceOp with dynamic entries.
833c831
<     // a Range vector and inferred result type.
---
>     // a Range vector.
839a838,842
>     /// Returns the type of the base tensor operand.
>     RankedTensorType getSourceType() {
>       return getSource().getType().cast<RankedTensorType>();
>     }
> 
841d843
<     // TODO: Deprecate this method.
843c845
<       return getResultType();
---
>       return getResult().getType().cast<RankedTensorType>();
848c850
<       return getResultType();
---
>       return getType();
854c856
<       unsigned rank = getResultType().getRank();
---
>       unsigned rank = getType().getRank();
858,861d859
<     /// Return the dimensions of the dest that are omitted to insert a source
<     /// when the result is rank-extended.
<     llvm::SmallBitVector getDroppedDims();
< 
974,975c972,973
<     Arguments<(ins AnyRankedTensor:$src, IndexListArrayAttr:$reassociation)>,
<     Results<(outs AnyRankedTensor:$result)> {
---
>     Arguments<(ins AnyTensor:$src, IndexListArrayAttr:$reassociation)>,
>     Results<(outs AnyTensor:$result)> {
989c987
<     }
---
>     };
1118,1124c1116
<   let extraClassDeclaration = commonExtraClassDeclaration # [{  
<     static RankedTensorType
<     inferCollapsedType(RankedTensorType type, ArrayRef<AffineMap> reassociation);
<     static RankedTensorType
<     inferCollapsedType(RankedTensorType type, 
<                        SmallVector<ReassociationIndices> reassociation);
<   }];
---
>   let extraClassDeclaration = commonExtraClassDeclaration;
1210c1202
<     AnyRankedTensor:$source,
---
>     AnyTensor:$source,
1219c1211
<   let results = (outs AnyRankedTensor:$result);
---
>   let results = (outs AnyTensor:$result);
1299,1301c1291,1293
<     OpBuilder<(ins "Type":$resultType, "Value":$source,
<       "ArrayRef<int64_t>":$staticLow, "ArrayRef<int64_t>":$staticHigh,
<       "ValueRange":$low, "ValueRange":$high, CArg<"bool", "false">:$nofold,
---
>     OpBuilder<(ins "Value":$source, "ArrayRef<int64_t>":$staticLow,
>       "ArrayRef<int64_t>":$staticHigh, "ValueRange":$low, "ValueRange":$high,
>       CArg<"bool", "false">:$nofold,
1304,1305c1296,1297
<     OpBuilder<(ins "Type":$resultType, "Value":$source, "ValueRange":$low,
<       "ValueRange":$high, CArg<"bool", "false">:$nofold,
---
>     OpBuilder<(ins "Value":$source, "ValueRange":$low, "ValueRange":$high,
>       CArg<"bool", "false">:$nofold,
1331c1323
< // TODO: Implement InParallelOpInterface.
---
> // TODO: Implement PerformConcurrentlyOpInterface.
1380c1372,1373
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
1446c1439
<     /// Return the OpResult of the enclosing ForallOp that is
---
>     /// Return the OpResult of the enclosing ForeachThreadOp that is
1449,1452d1441
< 
<     /// Return the dimensions of the dest that are omitted to insert a source
<     /// when the result is rank-extended.
<     llvm::SmallBitVector getDroppedDims();
1681,1682c1670,1671
<     size_t getSourceRank() { return getSourceType().getRank(); };
<     size_t getDestRank() { return getDestType().getRank(); };
---
>     int64_t getSourceRank() { return getSource().getType().getRank(); };
>     int64_t getDestRank() { return getDest().getType().getRank(); };
1780,1791c1769,1772
<     // Method to get the shape of the result as `SmallVector<OpFoldResult>`.
<     // This is a static method to allow getting the shape of the destination
<     // expected while creating a `pack` op.
<     static SmallVector<OpFoldResult> getResultShape(OpBuilder &builder,
<         Location loc, ArrayRef<OpFoldResult> sourceDims,
<         ArrayRef<OpFoldResult> innerTileDims, ArrayRef<int64_t> innerDimsPos,
<         ArrayRef<int64_t> outerDimsPerm = {});
< 
<     // Method to get the `RankedTensorType` of the result based on the inner
<     // tiles, position of the inner tiles (innerDimsPos)  and interchange vector
<     // of outer loops (outerDimsPerm).
<     static RankedTensorType inferPackedType(RankedTensorType sourceType,
---
>     // Method to get the `ShapedType` of the result based on the inner tiles,
>     // position of the inner tiles (innerDimsPos)  and interchange vector of
>     // outer loops (outerDimsPerm).
>     static ShapedType inferPackedType(ShapedType sourceType,
1795,1801d1775
<     // Returns true if we have enough static information to catch undefined
<     // behavior when the tile size does not divide perfectly the dimension of
<     // the input tensor.
<     static bool requirePaddingValue(ArrayRef<int64_t> inputShape,
<                                     ArrayRef<int64_t> innerDimsPos,
<                                     ArrayRef<OpFoldResult> innerTiles);
< 
1805,1826d1778
< 
<     /// Build and return a new PackOp that is a clone of the current PackOp with
<     /// (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by 
<     /// innerPermutation (resp. outerPermutation).
<     /// A new `tensor.empty` of the proper shape is built in the process.
<     /// Asserts that:
<     ///   - At least one of innerPermutation or outerPermutation is non-empty.
<     ///   - If not empty, innerPermutation is a valid permutation of size
<     ///     matching innerDimPos.
<     ///   - If not empty, outerPermutation is a valid permutation of size 
<     ///     matching outerDimsPerm.
<     PackOp createTransposedClone(OpBuilder &b,
<                                  Location loc,
<                                  ArrayRef<int64_t> innerPermutation,
<                                  ArrayRef<int64_t> outerPermutation);
< 
<     /// Check if this PackOp is like a simple pad operation.
<     /// In other words, this operation:
<     /// 1. adds useless dimensions (dimension of size 1),
<     /// 2. pads the other ones, and
<     /// 3. doesn't shuffle the dimensions
<     bool isLikePad();
1883,1909c1835
<   let extraClassDeclaration = commonExtraClassDeclaration # [{
<     static Value createDestinationTensor(OpBuilder &b, Location loc,
<         Value source, ArrayRef<OpFoldResult> innerTileSizes,
<         ArrayRef<int64_t> innerDimsPos, ArrayRef<int64_t> outerDimsPerm);
< 
<     /// Build and return a new UnPackOp that is a clone of the current UnPackOp
<     /// with (innerDimsPos, innerTiles) (resp. outerDimsPerm) are permuted by 
<     /// innerPermutation (resp. outerPermutation).
<     /// Asserts that:
<     ///   - At least one of innerPermutation or outerPermutation is non-empty.
<     ///   - If not empty, innerPermutation is a valid permutation of size
<     ///     matching innerDimPos.
<     ///   - If not empty, outerPermutation is a valid permutation of size 
<     ///     matching outerDimsPerm.
<     UnPackOp createTransposedClone(OpBuilder &b,
<                                    Location loc,
<                                    Value transposedSource,
<                                    ArrayRef<int64_t> innerPermutation,
<                                    ArrayRef<int64_t> outerPermutation);
< 
<     /// Check if this UnPackOp is like a simple unpad operation.
<     /// In other words, this operation:
<     /// 1. drops useless dimensions (dimension of size 1), and
<     /// 2. reduces dimensions in place (i.e., no tranpose.)
<     bool isLikeUnPad();
<   }];
< 
---
>   let extraClassDeclaration = commonExtraClassDeclaration;
--- include/mlir/Dialect/Tensor/IR/Tensor.h
--- include/mlir/Dialect/Tensor/IR/TensorTilingInterfaceImpl.h
19,21d18
< 
< struct TilingResult;
< 
45,48c42,45
< FailureOr<TilingResult> bubbleUpPadSlice(OpBuilder &b, tensor::PadOp padOp,
<                                          ArrayRef<OpFoldResult> offsets,
<                                          ArrayRef<OpFoldResult> sizes,
<                                          bool generateZeroSliceGuard = true);
---
> Operation *bubbleUpPadSlice(OpBuilder &b, tensor::PadOp padOp,
>                             ArrayRef<OpFoldResult> offsets,
>                             ArrayRef<OpFoldResult> sizes,
>                             bool generateZeroSliceGuard = true);
53c50
< /// * TilingInterface for `tensor.pad`, `tensor.pack`, and `tensor.unpack`.
---
> /// * TilingInterface for `tensor.pad`.
61,65d57
< 
< /// Similar to the above registeration, but it is only for `tensor.pack` and
< /// `tensor.unpack` ops.
< void registerTilingInterfaceExternalModelsForPackUnPackOps(
<     DialectRegistry &registry);
--- include/mlir/Dialect/Arith
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/IR and include/mlir/Dialect/Arith/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Transforms and include/mlir/Dialect/Arith/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Utils and include/mlir/Dialect/Arith/Utils
--- include/mlir/Dialect/Arith/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Utils/Utils.h include/mlir/Dialect/Arith/Utils/Utils.h
29,33c29,32
< /// Returns `success` when any of the elements in `ofrs` was produced by
< /// arith::ConstantIndexOp. In that case the constant attribute replaces the
< /// Value. Returns `failure` when no folding happened.
< LogicalResult foldDynamicIndexList(Builder &b,
<                                    SmallVectorImpl<OpFoldResult> &ofrs);
---
> /// Detects the `values` produced by a ConstantIndexOp and places the new
> /// constant in place of the corresponding sentinel value.
> void canonicalizeSubViewPart(SmallVectorImpl<OpFoldResult> &values,
>                              function_ref<bool(int64_t)> isDynamic);
46a46,53
>     // No constant operand, just return;
>     if (llvm::none_of(op.getOperands(), [](Value operand) {
>           return matchPattern(operand, matchConstantIndex());
>         }))
>       return failure();
> 
>     // At least one of offsets/sizes/strides is a new constant.
>     // Form the new list of operands and constant attributes from the existing.
50,55c57,59
< 
<     // No constant operands were folded, just return;
<     if (failed(foldDynamicIndexList(rewriter, mixedOffsets)) &&
<         failed(foldDynamicIndexList(rewriter, mixedSizes)) &&
<         failed(foldDynamicIndexList(rewriter, mixedStrides)))
<       return failure();
---
>     canonicalizeSubViewPart(mixedOffsets, ShapedType::isDynamic);
>     canonicalizeSubViewPart(mixedSizes, ShapedType::isDynamic);
>     canonicalizeSubViewPart(mixedStrides, ShapedType::isDynamic);
79,84d82
< /// Similar to the other overload, but converts multiple OpFoldResults into
< /// Values.
< SmallVector<Value>
< getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
<                                 ArrayRef<OpFoldResult> valueOrAttrVec);
< 
89a88,93
> 
> /// Similar to the other overload, but converts multiple OpFoldResults into
> /// Values.
> SmallVector<Value>
> getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
>                                 ArrayRef<OpFoldResult> valueOrAttrVec);
--- include/mlir/Dialect/Arith/Utils/Utils.h
29,33c29,32
< /// Returns `success` when any of the elements in `ofrs` was produced by
< /// arith::ConstantIndexOp. In that case the constant attribute replaces the
< /// Value. Returns `failure` when no folding happened.
< LogicalResult foldDynamicIndexList(Builder &b,
<                                    SmallVectorImpl<OpFoldResult> &ofrs);
---
> /// Detects the `values` produced by a ConstantIndexOp and places the new
> /// constant in place of the corresponding sentinel value.
> void canonicalizeSubViewPart(SmallVectorImpl<OpFoldResult> &values,
>                              function_ref<bool(int64_t)> isDynamic);
46a46,53
>     // No constant operand, just return;
>     if (llvm::none_of(op.getOperands(), [](Value operand) {
>           return matchPattern(operand, matchConstantIndex());
>         }))
>       return failure();
> 
>     // At least one of offsets/sizes/strides is a new constant.
>     // Form the new list of operands and constant attributes from the existing.
50,55c57,59
< 
<     // No constant operands were folded, just return;
<     if (failed(foldDynamicIndexList(rewriter, mixedOffsets)) &&
<         failed(foldDynamicIndexList(rewriter, mixedSizes)) &&
<         failed(foldDynamicIndexList(rewriter, mixedStrides)))
<       return failure();
---
>     canonicalizeSubViewPart(mixedOffsets, ShapedType::isDynamic);
>     canonicalizeSubViewPart(mixedSizes, ShapedType::isDynamic);
>     canonicalizeSubViewPart(mixedStrides, ShapedType::isDynamic);
79,84d82
< /// Similar to the other overload, but converts multiple OpFoldResults into
< /// Values.
< SmallVector<Value>
< getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
<                                 ArrayRef<OpFoldResult> valueOrAttrVec);
< 
89a88,93
> 
> /// Similar to the other overload, but converts multiple OpFoldResults into
> /// Values.
> SmallVector<Value>
> getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
>                                 ArrayRef<OpFoldResult> valueOrAttrVec);
--- include/mlir/Dialect/Arith/CMakeLists.txt
--- include/mlir/Dialect/Arith/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Transforms/Passes.h include/mlir/Dialect/Arith/Transforms/Passes.h
41,43d40
< /// Add patterns to expand Arith bf16 patterns to lower level bitcasts/shifts.
< void populateExpandBFloat16Patterns(RewritePatternSet &patterns);
< 
50,53d46
< /// Create a pass to legalize Arith ops with specified configuration.
< std::unique_ptr<Pass>
< createArithExpandOpsPass(const ArithExpandOpsOptions &options);
< 
64,67d56
< 
< /// Add patterns for integer bitwidth narrowing.
< void populateArithIntNarrowingPatterns(RewritePatternSet &patterns,
<                                        const ArithIntNarrowingOptions &options);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Transforms/Passes.td include/mlir/Dialect/Arith/Transforms/Passes.td
34,38d33
<   let dependentDialects = ["vector::VectorDialect"];
<   let options = [
<     Option<"includeBf16", "include-bf16", "bool", /*default=*/"false",
<            "Enable the BF16 expansion patterns">,
<   ];
85,98d79
< 
< def ArithIntNarrowing : Pass<"arith-int-narrowing"> {
<   let summary = "Reduce integer operation bitwidth";
<   let description = [{
<     Reduce bitwidths of integer types used in arith operations. This pass
<     prefers the narrowest available integer bitwidths that are guaranteed to
<     produce the same results.
<   }];
<   let dependentDialects = ["vector::VectorDialect"];
<   let options = [
<     ListOption<"bitwidthsSupported", "int-bitwidths-supported", "unsigned",
<                "Integer bitwidths supported">,
<   ];
<  }
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Transforms: Transforms.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/Transforms/WideIntEmulationConverter.h include/mlir/Dialect/Arith/Transforms/WideIntEmulationConverter.h
19c19
< /// of integers twice as wide as the maximum supported by the target. Wide
---
> /// of integers twice as wide as the maxium supported by the target. Wide
--- include/mlir/Dialect/Arith/Transforms/Passes.h
41,43d40
< /// Add patterns to expand Arith bf16 patterns to lower level bitcasts/shifts.
< void populateExpandBFloat16Patterns(RewritePatternSet &patterns);
< 
50,53d46
< /// Create a pass to legalize Arith ops with specified configuration.
< std::unique_ptr<Pass>
< createArithExpandOpsPass(const ArithExpandOpsOptions &options);
< 
64,67d56
< 
< /// Add patterns for integer bitwidth narrowing.
< void populateArithIntNarrowingPatterns(RewritePatternSet &patterns,
<                                        const ArithIntNarrowingOptions &options);
--- include/mlir/Dialect/Arith/Transforms/Passes.td
34,38d33
<   let dependentDialects = ["vector::VectorDialect"];
<   let options = [
<     Option<"includeBf16", "include-bf16", "bool", /*default=*/"false",
<            "Enable the BF16 expansion patterns">,
<   ];
85,98d79
< 
< def ArithIntNarrowing : Pass<"arith-int-narrowing"> {
<   let summary = "Reduce integer operation bitwidth";
<   let description = [{
<     Reduce bitwidths of integer types used in arith operations. This pass
<     prefers the narrowest available integer bitwidths that are guaranteed to
<     produce the same results.
<   }];
<   let dependentDialects = ["vector::VectorDialect"];
<   let options = [
<     ListOption<"bitwidthsSupported", "int-bitwidths-supported", "unsigned",
<                "Integer bitwidths supported">,
<   ];
<  }
--- include/mlir/Dialect/Arith/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/Arith/Transforms/WideIntEmulationConverter.h
19c19
< /// of integers twice as wide as the maximum supported by the target. Wide
---
> /// of integers twice as wide as the maxium supported by the target. Wide
--- include/mlir/Dialect/Arith/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Arith/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/IR/ArithBase.td include/mlir/Dialect/Arith/IR/ArithBase.td
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/IR/Arith.h include/mlir/Dialect/Arith/IR/Arith.h
124c124
< TypedAttr getIdentityValueAttr(AtomicRMWKind kind, Type resultType,
---
> Attribute getIdentityValueAttr(AtomicRMWKind kind, Type resultType,
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td include/mlir/Dialect/Arith/IR/ArithOps.td
72,73c72
<       DefaultValuedAttr<
<         Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
---
>       DefaultValuedAttr<Arith_FastMathAttr, "FastMathFlags::none">:$fastmath)>,
85,86c84
<       DefaultValuedAttr<
<         Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
---
>       DefaultValuedAttr<Arith_FastMathAttr, "FastMathFlags::none">:$fastmath)>,
107c105
<         VectorOfAnyRankOf<[AnySignlessInteger]>.predicate,
---
>         VectorOf<[AnySignlessInteger]>.predicate,
180a179,183
>   let builders = [
>     OpBuilder<(ins "Attribute":$value, "Type":$type),
>     [{ build($_builder, $_state, type, value); }]>,
>   ];
> 
185,189d187
< 
<     /// Build the constant op with `value` and `type` if possible, otherwise
<     /// returns null.
<     static ConstantOp materialize(OpBuilder &builder, Attribute value,
<                                   Type type, Location loc);
1050d1047
<   let hasFolder = 1;
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Arith/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/Arith/IR/ArithOpsInterfaces.td
--- include/mlir/Dialect/Arith/IR/ArithBase.td
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Arith/IR/CMakeLists.txt
--- include/mlir/Dialect/Arith/IR/ArithOps.td
72,73c72
<       DefaultValuedAttr<
<         Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
---
>       DefaultValuedAttr<Arith_FastMathAttr, "FastMathFlags::none">:$fastmath)>,
85,86c84
<       DefaultValuedAttr<
<         Arith_FastMathAttr, "::mlir::arith::FastMathFlags::none">:$fastmath)>,
---
>       DefaultValuedAttr<Arith_FastMathAttr, "FastMathFlags::none">:$fastmath)>,
107c105
<         VectorOfAnyRankOf<[AnySignlessInteger]>.predicate,
---
>         VectorOf<[AnySignlessInteger]>.predicate,
180a179,183
>   let builders = [
>     OpBuilder<(ins "Attribute":$value, "Type":$type),
>     [{ build($_builder, $_state, type, value); }]>,
>   ];
> 
185,189d187
< 
<     /// Build the constant op with `value` and `type` if possible, otherwise
<     /// returns null.
<     static ConstantOp materialize(OpBuilder &builder, Attribute value,
<                                   Type type, Location loc);
1050d1047
<   let hasFolder = 1;
--- include/mlir/Dialect/Arith/IR/Arith.h
124c124
< TypedAttr getIdentityValueAttr(AtomicRMWKind kind, Type resultType,
---
> Attribute getIdentityValueAttr(AtomicRMWKind kind, Type resultType,
--- include/mlir/Dialect/AMX
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/AMX/AMX.td include/mlir/Dialect/AMX/AMX.td
58a59
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/AMX/AMXDialect.h
--- include/mlir/Dialect/AMX/AMX.td
58a59
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/AMX/CMakeLists.txt
--- include/mlir/Dialect/AMX/Transforms.h
--- include/mlir/Dialect/Traits.h
--- include/mlir/Dialect/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Utils/IndexingUtils.h include/mlir/Dialect/Utils/IndexingUtils.h
26,28c26,27
< //===----------------------------------------------------------------------===//
< // Utils that operate on static integer values.
< //===----------------------------------------------------------------------===//
---
> /// Computes and returns the linearized index of 'offsets' w.r.t. 'basis'.
> int64_t linearize(ArrayRef<int64_t> offsets, ArrayRef<int64_t> basis);
30,47c29,39
< /// Given a set of sizes, return the suffix product.
< ///
< /// When applied to slicing, this is the calculation needed to derive the
< /// strides (i.e. the number of linear indices to skip along the (k-1) most
< /// minor dimensions to get the next k-slice).
< ///
< /// This is the basis to linearize an n-D offset confined to `[0 ... sizes]`.
< ///
< /// Assuming `sizes` is `[s0, .. sn]`, return the vector<int64_t>
< ///   `[s1 * ... * sn, s2 * ... * sn, ..., sn, 1]`.
< ///
< /// `sizes` elements are asserted to be non-negative.
< ///
< /// Return an empty vector if `sizes` is empty.
< SmallVector<int64_t> computeSuffixProduct(ArrayRef<int64_t> sizes);
< inline SmallVector<int64_t> computeStrides(ArrayRef<int64_t> sizes) {
<   return computeSuffixProduct(sizes);
< }
---
> /// Given the strides together with a linear index in the dimension
> /// space, returns the vector-space offsets in each dimension for a
> /// de-linearized index.
> SmallVector<int64_t> delinearize(ArrayRef<int64_t> strides,
>                                  int64_t linearIndex);
> 
> /// Given a set of sizes, compute and return the strides (i.e. the number of
> /// linear incides to skip along the (k-1) most minor dimensions to get the next
> /// k-slice). This is also the basis that one can use to linearize an n-D offset
> /// confined to `[0 .. sizes]`.
> SmallVector<int64_t> computeStrides(ArrayRef<int64_t> sizes);
49,51c41
< /// Return a vector containing llvm::zip_equal(v1, v2) multiplied elementwise.
< ///
< /// Return an empty vector if `v1` and `v2` are empty.
---
> /// Return a vector containing llvm::zip of v1 and v2 multiplied elementwise.
55,80c45,48
< /// Return the number of elements of basis (i.e. the max linear index).
< /// Return `0` if `basis` is empty.
< ///
< /// `basis` elements are asserted to be non-negative.
< ///
< /// Return `0` if `basis` is empty.
< int64_t computeMaxLinearIndex(ArrayRef<int64_t> basis);
< 
< /// Return the linearized index of 'offsets' w.r.t. 'basis'.
< ///
< /// `basis` elements are asserted to be non-negative.
< int64_t linearize(ArrayRef<int64_t> offsets, ArrayRef<int64_t> basis);
< 
< /// Given the strides together with a linear index in the dimension space,
< /// return the vector-space offsets in each dimension for a de-linearized index.
< /// `strides` elements are asserted to be non-negative.
< ///
< /// Let `li = linearIndex`, assuming `strides` are `[s0, .. sn]`, return the
< /// vector of int64_t
< ///   `[li % s0, (li / s0) % s1, ..., (li / s0 / .. / sn-1) % sn]`
< SmallVector<int64_t> delinearize(int64_t linearIndex,
<                                  ArrayRef<int64_t> strides);
< 
< /// Return the multi-dimensional integral ratio of `subShape` to the trailing
< /// dimensions of `shape`. This represents how many times `subShape` fits
< /// within `shape`. If integral division is not possible, return std::nullopt.
---
> /// Compute and return the multi-dimensional integral ratio of `subShape` to
> /// the trailing dimensions of `shape`. This represents how many times
> /// `subShape` fits within `shape`.
> /// If integral division is not possible, return std::nullopt.
82c50
< /// enforced) to only contain non-negative values.
---
> /// enforced) to only contain noonnegative values.
86,87c54
< ///   - shapeRatio({3, 8}, {2, 5, 2}) returns std::nullopt (subshape has
< ///   higher
---
> ///   - shapeRatio({3, 8}, {2, 5, 2}) returns std::nullopt (subshape has higher
96,132d62
< //===----------------------------------------------------------------------===//
< // Utils that operate on AffineExpr.
< //===----------------------------------------------------------------------===//
< 
< /// Given a set of sizes, return the suffix product.
< ///
< /// When applied to slicing, this is the calculation needed to derive the
< /// strides (i.e. the number of linear indices to skip along the (k-1) most
< /// minor dimensions to get the next k-slice).
< ///
< /// This is the basis to linearize an n-D offset confined to `[0 ... sizes]`.
< ///
< /// Assuming `sizes` is `[s0, .. sn]`, return the vector<AffineExpr>
< ///   `[s1 * ... * sn, s2 * ... * sn, ..., sn, 1]`.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// `sizes` elements are expected to bind to non-negative values.
< ///
< /// Return an empty vector if `sizes` is empty.
< SmallVector<AffineExpr> computeSuffixProduct(ArrayRef<AffineExpr> sizes);
< inline SmallVector<AffineExpr> computeStrides(ArrayRef<AffineExpr> sizes) {
<   return computeSuffixProduct(sizes);
< }
< 
< /// Return a vector containing llvm::zip_equal(v1, v2) multiplied elementwise.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// Return an empty vector if `v1` and `v2` are empty.
< SmallVector<AffineExpr> computeElementwiseMul(ArrayRef<AffineExpr> v1,
<                                               ArrayRef<AffineExpr> v2);
< 
135,179c65
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// `basis` elements are expected to bind to non-negative values.
< ///
< /// Return the `0` AffineConstantExpr if `basis` is empty.
< AffineExpr computeMaxLinearIndex(MLIRContext *ctx, ArrayRef<AffineExpr> basis);
< 
< /// Return the linearized index of 'offsets' w.r.t. 'basis'.
< ///
< /// Assuming `offsets` is `[o0, .. on]` and `basis` is `[b0, .. bn]`, return the
< /// AffineExpr `o0 * b0 + .. + on * bn`.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that result
< /// in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide by an
< /// AffineDimExpr).
< ///
< /// `basis` elements are expected to bind to non-negative values.
< AffineExpr linearize(MLIRContext *ctx, ArrayRef<AffineExpr> offsets,
<                      ArrayRef<AffineExpr> basis);
< AffineExpr linearize(MLIRContext *ctx, ArrayRef<AffineExpr> offsets,
<                      ArrayRef<int64_t> basis);
< 
< /// Given the strides together with a linear index in the dimension space,
< /// return the vector-space offsets in each dimension for a de-linearized index.
< ///
< /// Let `li = linearIndex`, assuming `strides` are `[s0, .. sn]`, return the
< /// vector of AffineExpr
< ///   `[li % s0, (li / s0) % s1, ..., (li / s0 / .. / sn-1) % sn]`
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that result
< /// in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide by an
< /// AffineDimExpr).
< ///
< /// `strides` elements are expected to bind to non-negative values.
< SmallVector<AffineExpr> delinearize(AffineExpr linearIndex,
<                                     ArrayRef<AffineExpr> strides);
< SmallVector<AffineExpr> delinearize(AffineExpr linearIndex,
<                                     ArrayRef<int64_t> strides);
< 
< //===----------------------------------------------------------------------===//
< // Permutation utils.
< //===----------------------------------------------------------------------===//
---
> int64_t computeMaxLinearIndex(ArrayRef<int64_t> basis);
183,185c69,70
< /// E.g.: for an input vector `inVec = ['a', 'b', 'c']` and a permutation
< /// vector `permutation = [2, 0, 1]`, this function leaves `inVec = ['c', 'a',
< /// 'b']`.
---
> /// E.g.: for an input vector `inVec = ['a', 'b', 'c']` and a permutation vector
> /// `permutation = [2, 0, 1]`, this function leaves `inVec = ['c', 'a', 'b']`.
201,211c86
< /// Return a permutation vector of size permSize that would result in moving
< /// positions into desiredPositions.
< ///
< /// For example, permSize == 5, positions = {2, 4}, desiredPositions = {1, 0}
< /// would result in a {4, 2, 0, 1, 3} permutation vector.
< SmallVector<int64_t>
< computePermutationVector(int64_t permSize, ArrayRef<int64_t> positions,
<                          ArrayRef<int64_t> desiredPositions);
< 
< /// Helper to return a subset of `arrayAttr` as a vector of int64_t.
< // TODO: Port everything relevant to DenseArrayAttr and drop this util.
---
> /// Helper that returns a subset of `arrayAttr` as a vector of int64_t.
213a89,96
> 
> /// Computes and returns linearized affine expression w.r.t. `basis`.
> mlir::AffineExpr getLinearAffineExpr(ArrayRef<int64_t> basis, mlir::Builder &b);
> 
> /// Given the strides in the dimension space, returns the affine expressions for
> /// vector-space offsets in each dimension for a de-linearized index.
> SmallVector<mlir::AffineExpr>
> getDelinearizedAffineExpr(ArrayRef<int64_t> strides, mlir::Builder &b);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Utils/ReshapeOpsUtils.h include/mlir/Dialect/Utils/ReshapeOpsUtils.h
478c478
<   /// `std::nullopt`, the slice should replace the collapse shape op.
---
>   /// `None`, the slice should replace the collapse shape op.
523,536d522
< struct PackingMetadata {
<   SmallVector<int64_t> insertPositions;
<   SmallVector<int64_t> outerPositions;
<   SmallVector<ReassociationIndices> reassociations;
< };
< 
< /// Given a vector of `positions` indices representing desired packing insertion
< /// points into a target vector (i.e. pack/unpack.inner_dim_pos), compute the
< /// final positions in the target shape as well as the reshape reassociations.
< // Note: This should not be called with a large positions array (or the
< // implementation needs to be updated to use an N.log N sort instead of
< // repeated N^2 counts).
< PackingMetadata computePackingMetadata(int64_t packedRank,
<                                        ArrayRef<int64_t> innerDimPos);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Utils/StaticValueUtils.h include/mlir/Dialect/Utils/StaticValueUtils.h
24,27d23
< /// Return true if `v` is an IntegerAttr with value `0` of a ConstantIndexOp
< /// with attribute with value `0`.
< bool isZeroIndex(OpFoldResult v);
< 
37,38c33,34
< /// vector, and strides vector) formed by separating out the individual
< /// elements of each range.
---
> /// vector, and strides vector) formed by separating out the individual elements
> /// of each range.
47,48c43,44
< /// `staticVec`. This is useful to extract mixed static and dynamic entries
< /// that come from an AttrSizedOperandSegments trait.
---
> /// `staticVec`. This is useful to extract mixed static and dynamic entries that
> /// come from an AttrSizedOperandSegments trait.
53,55c49,50
< /// Helper function to dispatch multiple OpFoldResults according to the
< /// behavior of `dispatchIndexOpFoldResult(OpFoldResult ofr` for a single
< /// OpFoldResult.
---
> /// Helper function to dispatch multiple OpFoldResults according to the behavior
> /// of `dispatchIndexOpFoldResult(OpFoldResult ofr` for a single OpFoldResult.
65a61
> 
68a65
> 
72,77d68
< /// Convert int64_t to integer attributes of index type and return them as
< /// OpFoldResult.
< OpFoldResult getAsIndexOpFoldResult(MLIRContext *ctx, int64_t val);
< SmallVector<OpFoldResult> getAsIndexOpFoldResult(MLIRContext *ctx,
<                                                  ArrayRef<int64_t> values);
< 
84,87c75,78
< /// Return true if ofr1 and ofr2 are the same integer constant attribute
< /// values or the same SSA value. Ignore integer bitwitdh and type mismatch
< /// that come from the fact there is no IndexAttr and that IndexType have no
< /// bitwidth.
---
> /// Return true if ofr1 and ofr2 are the same integer constant attribute values
> /// or the same SSA value.
> /// Ignore integer bitwitdh and type mismatch that come from the fact there is
> /// no IndexAttr and that IndexType have no bitwidth.
89,90d79
< bool isEqualConstantIntOrValueArray(ArrayRef<OpFoldResult> ofrs1,
<                                     ArrayRef<OpFoldResult> ofrs2);
92,105c81,86
< // To convert an OpFoldResult to a Value of index type, see:
< //   mlir/include/mlir/Dialect/Arith/Utils/Utils.h
< // TODO: find a better common landing place.
< //
< // Value getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
< //                                       OpFoldResult ofr);
< 
< // To convert an OpFoldResult to a Value of index type, see:
< //   mlir/include/mlir/Dialect/Arith/Utils/Utils.h
< // TODO: find a better common landing place.
< //
< // SmallVector<Value>
< // getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
< //                                 ArrayRef<OpFoldResult> valueOrAttrVec);
---
> /// Helper function to convert a vector of `OpFoldResult`s into a vector of
> /// `Value`s. For each `OpFoldResult` in `valueOrAttrVec` return the fold result
> /// if it casts to  a `Value` or create an index-type constant if it casts to
> /// `IntegerAttr`. No other attribute types are supported.
> SmallVector<Value> getAsValues(OpBuilder &b, Location loc,
>                                ArrayRef<OpFoldResult> valueOrAttrVec);
107,108c88,89
< /// Return a vector of OpFoldResults with the same size a staticValues, but
< /// all elements for which ShapedType::isDynamic is true, will be replaced by
---
> /// Return a vector of OpFoldResults with the same size a staticValues, but all
> /// elements for which ShapedType::isDynamic is true, will be replaced by
113,115c94,95
< /// Decompose a vector of mixed static or dynamic values into the
< /// corresponding pair of arrays. This is the inverse function of
< /// `getMixedValues`.
---
> /// Decompose a vector of mixed static or dynamic values into the corresponding
> /// pair of arrays. This is the inverse function of `getMixedValues`.
119,134d98
< 
< /// Helper to sort `values` according to matching `keys`.
< SmallVector<Value>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<Value> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< SmallVector<OpFoldResult>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<OpFoldResult> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< SmallVector<int64_t>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<int64_t> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< 
< /// Return the number of iterations for a loop with a lower bound `lb`, upper
< /// bound `ub` and step `step`.
< std::optional<int64_t> constantTripCount(OpFoldResult lb, OpFoldResult ub,
<                                          OpFoldResult step);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Utils/StructuredOpsUtils.h include/mlir/Dialect/Utils/StructuredOpsUtils.h
23d22
< #include "mlir/IR/TypeRange.h"
31a31,32
> class TypeRange;
> class ValueRange;
118,122d118
< template <typename OpT>
< OpT clone(OpBuilder &b, OpT op, TypeRange newResultTypes,
<           ValueRange newOperands) {
<   return cast<OpT>(clone(b, op.getOperation(), newResultTypes, newOperands));
< }
129,133d124
< 
< // Get the list of attributes associated with the op, ignoring
< // those with the provided name.
< SmallVector<NamedAttribute>
< getPrunedAttributeList(Operation *op, ArrayRef<StringRef> elidedAttrs);
--- include/mlir/Dialect/Utils/IndexingUtils.h
26,28c26,27
< //===----------------------------------------------------------------------===//
< // Utils that operate on static integer values.
< //===----------------------------------------------------------------------===//
---
> /// Computes and returns the linearized index of 'offsets' w.r.t. 'basis'.
> int64_t linearize(ArrayRef<int64_t> offsets, ArrayRef<int64_t> basis);
30,47c29,39
< /// Given a set of sizes, return the suffix product.
< ///
< /// When applied to slicing, this is the calculation needed to derive the
< /// strides (i.e. the number of linear indices to skip along the (k-1) most
< /// minor dimensions to get the next k-slice).
< ///
< /// This is the basis to linearize an n-D offset confined to `[0 ... sizes]`.
< ///
< /// Assuming `sizes` is `[s0, .. sn]`, return the vector<int64_t>
< ///   `[s1 * ... * sn, s2 * ... * sn, ..., sn, 1]`.
< ///
< /// `sizes` elements are asserted to be non-negative.
< ///
< /// Return an empty vector if `sizes` is empty.
< SmallVector<int64_t> computeSuffixProduct(ArrayRef<int64_t> sizes);
< inline SmallVector<int64_t> computeStrides(ArrayRef<int64_t> sizes) {
<   return computeSuffixProduct(sizes);
< }
---
> /// Given the strides together with a linear index in the dimension
> /// space, returns the vector-space offsets in each dimension for a
> /// de-linearized index.
> SmallVector<int64_t> delinearize(ArrayRef<int64_t> strides,
>                                  int64_t linearIndex);
> 
> /// Given a set of sizes, compute and return the strides (i.e. the number of
> /// linear incides to skip along the (k-1) most minor dimensions to get the next
> /// k-slice). This is also the basis that one can use to linearize an n-D offset
> /// confined to `[0 .. sizes]`.
> SmallVector<int64_t> computeStrides(ArrayRef<int64_t> sizes);
49,51c41
< /// Return a vector containing llvm::zip_equal(v1, v2) multiplied elementwise.
< ///
< /// Return an empty vector if `v1` and `v2` are empty.
---
> /// Return a vector containing llvm::zip of v1 and v2 multiplied elementwise.
55,80c45,48
< /// Return the number of elements of basis (i.e. the max linear index).
< /// Return `0` if `basis` is empty.
< ///
< /// `basis` elements are asserted to be non-negative.
< ///
< /// Return `0` if `basis` is empty.
< int64_t computeMaxLinearIndex(ArrayRef<int64_t> basis);
< 
< /// Return the linearized index of 'offsets' w.r.t. 'basis'.
< ///
< /// `basis` elements are asserted to be non-negative.
< int64_t linearize(ArrayRef<int64_t> offsets, ArrayRef<int64_t> basis);
< 
< /// Given the strides together with a linear index in the dimension space,
< /// return the vector-space offsets in each dimension for a de-linearized index.
< /// `strides` elements are asserted to be non-negative.
< ///
< /// Let `li = linearIndex`, assuming `strides` are `[s0, .. sn]`, return the
< /// vector of int64_t
< ///   `[li % s0, (li / s0) % s1, ..., (li / s0 / .. / sn-1) % sn]`
< SmallVector<int64_t> delinearize(int64_t linearIndex,
<                                  ArrayRef<int64_t> strides);
< 
< /// Return the multi-dimensional integral ratio of `subShape` to the trailing
< /// dimensions of `shape`. This represents how many times `subShape` fits
< /// within `shape`. If integral division is not possible, return std::nullopt.
---
> /// Compute and return the multi-dimensional integral ratio of `subShape` to
> /// the trailing dimensions of `shape`. This represents how many times
> /// `subShape` fits within `shape`.
> /// If integral division is not possible, return std::nullopt.
82c50
< /// enforced) to only contain non-negative values.
---
> /// enforced) to only contain noonnegative values.
86,87c54
< ///   - shapeRatio({3, 8}, {2, 5, 2}) returns std::nullopt (subshape has
< ///   higher
---
> ///   - shapeRatio({3, 8}, {2, 5, 2}) returns std::nullopt (subshape has higher
96,132d62
< //===----------------------------------------------------------------------===//
< // Utils that operate on AffineExpr.
< //===----------------------------------------------------------------------===//
< 
< /// Given a set of sizes, return the suffix product.
< ///
< /// When applied to slicing, this is the calculation needed to derive the
< /// strides (i.e. the number of linear indices to skip along the (k-1) most
< /// minor dimensions to get the next k-slice).
< ///
< /// This is the basis to linearize an n-D offset confined to `[0 ... sizes]`.
< ///
< /// Assuming `sizes` is `[s0, .. sn]`, return the vector<AffineExpr>
< ///   `[s1 * ... * sn, s2 * ... * sn, ..., sn, 1]`.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// `sizes` elements are expected to bind to non-negative values.
< ///
< /// Return an empty vector if `sizes` is empty.
< SmallVector<AffineExpr> computeSuffixProduct(ArrayRef<AffineExpr> sizes);
< inline SmallVector<AffineExpr> computeStrides(ArrayRef<AffineExpr> sizes) {
<   return computeSuffixProduct(sizes);
< }
< 
< /// Return a vector containing llvm::zip_equal(v1, v2) multiplied elementwise.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// Return an empty vector if `v1` and `v2` are empty.
< SmallVector<AffineExpr> computeElementwiseMul(ArrayRef<AffineExpr> v1,
<                                               ArrayRef<AffineExpr> v2);
< 
135,179c65
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that
< /// result in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide
< /// by an AffineDimExpr).
< ///
< /// `basis` elements are expected to bind to non-negative values.
< ///
< /// Return the `0` AffineConstantExpr if `basis` is empty.
< AffineExpr computeMaxLinearIndex(MLIRContext *ctx, ArrayRef<AffineExpr> basis);
< 
< /// Return the linearized index of 'offsets' w.r.t. 'basis'.
< ///
< /// Assuming `offsets` is `[o0, .. on]` and `basis` is `[b0, .. bn]`, return the
< /// AffineExpr `o0 * b0 + .. + on * bn`.
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that result
< /// in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide by an
< /// AffineDimExpr).
< ///
< /// `basis` elements are expected to bind to non-negative values.
< AffineExpr linearize(MLIRContext *ctx, ArrayRef<AffineExpr> offsets,
<                      ArrayRef<AffineExpr> basis);
< AffineExpr linearize(MLIRContext *ctx, ArrayRef<AffineExpr> offsets,
<                      ArrayRef<int64_t> basis);
< 
< /// Given the strides together with a linear index in the dimension space,
< /// return the vector-space offsets in each dimension for a de-linearized index.
< ///
< /// Let `li = linearIndex`, assuming `strides` are `[s0, .. sn]`, return the
< /// vector of AffineExpr
< ///   `[li % s0, (li / s0) % s1, ..., (li / s0 / .. / sn-1) % sn]`
< ///
< /// It is the caller's responsibility to pass proper AffineExpr kind that result
< /// in valid AffineExpr (i.e. cannot multiply 2 AffineDimExpr or divide by an
< /// AffineDimExpr).
< ///
< /// `strides` elements are expected to bind to non-negative values.
< SmallVector<AffineExpr> delinearize(AffineExpr linearIndex,
<                                     ArrayRef<AffineExpr> strides);
< SmallVector<AffineExpr> delinearize(AffineExpr linearIndex,
<                                     ArrayRef<int64_t> strides);
< 
< //===----------------------------------------------------------------------===//
< // Permutation utils.
< //===----------------------------------------------------------------------===//
---
> int64_t computeMaxLinearIndex(ArrayRef<int64_t> basis);
183,185c69,70
< /// E.g.: for an input vector `inVec = ['a', 'b', 'c']` and a permutation
< /// vector `permutation = [2, 0, 1]`, this function leaves `inVec = ['c', 'a',
< /// 'b']`.
---
> /// E.g.: for an input vector `inVec = ['a', 'b', 'c']` and a permutation vector
> /// `permutation = [2, 0, 1]`, this function leaves `inVec = ['c', 'a', 'b']`.
201,211c86
< /// Return a permutation vector of size permSize that would result in moving
< /// positions into desiredPositions.
< ///
< /// For example, permSize == 5, positions = {2, 4}, desiredPositions = {1, 0}
< /// would result in a {4, 2, 0, 1, 3} permutation vector.
< SmallVector<int64_t>
< computePermutationVector(int64_t permSize, ArrayRef<int64_t> positions,
<                          ArrayRef<int64_t> desiredPositions);
< 
< /// Helper to return a subset of `arrayAttr` as a vector of int64_t.
< // TODO: Port everything relevant to DenseArrayAttr and drop this util.
---
> /// Helper that returns a subset of `arrayAttr` as a vector of int64_t.
213a89,96
> 
> /// Computes and returns linearized affine expression w.r.t. `basis`.
> mlir::AffineExpr getLinearAffineExpr(ArrayRef<int64_t> basis, mlir::Builder &b);
> 
> /// Given the strides in the dimension space, returns the affine expressions for
> /// vector-space offsets in each dimension for a de-linearized index.
> SmallVector<mlir::AffineExpr>
> getDelinearizedAffineExpr(ArrayRef<int64_t> strides, mlir::Builder &b);
--- include/mlir/Dialect/Utils/StaticValueUtils.h
24,27d23
< /// Return true if `v` is an IntegerAttr with value `0` of a ConstantIndexOp
< /// with attribute with value `0`.
< bool isZeroIndex(OpFoldResult v);
< 
37,38c33,34
< /// vector, and strides vector) formed by separating out the individual
< /// elements of each range.
---
> /// vector, and strides vector) formed by separating out the individual elements
> /// of each range.
47,48c43,44
< /// `staticVec`. This is useful to extract mixed static and dynamic entries
< /// that come from an AttrSizedOperandSegments trait.
---
> /// `staticVec`. This is useful to extract mixed static and dynamic entries that
> /// come from an AttrSizedOperandSegments trait.
53,55c49,50
< /// Helper function to dispatch multiple OpFoldResults according to the
< /// behavior of `dispatchIndexOpFoldResult(OpFoldResult ofr` for a single
< /// OpFoldResult.
---
> /// Helper function to dispatch multiple OpFoldResults according to the behavior
> /// of `dispatchIndexOpFoldResult(OpFoldResult ofr` for a single OpFoldResult.
65a61
> 
68a65
> 
72,77d68
< /// Convert int64_t to integer attributes of index type and return them as
< /// OpFoldResult.
< OpFoldResult getAsIndexOpFoldResult(MLIRContext *ctx, int64_t val);
< SmallVector<OpFoldResult> getAsIndexOpFoldResult(MLIRContext *ctx,
<                                                  ArrayRef<int64_t> values);
< 
84,87c75,78
< /// Return true if ofr1 and ofr2 are the same integer constant attribute
< /// values or the same SSA value. Ignore integer bitwitdh and type mismatch
< /// that come from the fact there is no IndexAttr and that IndexType have no
< /// bitwidth.
---
> /// Return true if ofr1 and ofr2 are the same integer constant attribute values
> /// or the same SSA value.
> /// Ignore integer bitwitdh and type mismatch that come from the fact there is
> /// no IndexAttr and that IndexType have no bitwidth.
89,90d79
< bool isEqualConstantIntOrValueArray(ArrayRef<OpFoldResult> ofrs1,
<                                     ArrayRef<OpFoldResult> ofrs2);
92,105c81,86
< // To convert an OpFoldResult to a Value of index type, see:
< //   mlir/include/mlir/Dialect/Arith/Utils/Utils.h
< // TODO: find a better common landing place.
< //
< // Value getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
< //                                       OpFoldResult ofr);
< 
< // To convert an OpFoldResult to a Value of index type, see:
< //   mlir/include/mlir/Dialect/Arith/Utils/Utils.h
< // TODO: find a better common landing place.
< //
< // SmallVector<Value>
< // getValueOrCreateConstantIndexOp(OpBuilder &b, Location loc,
< //                                 ArrayRef<OpFoldResult> valueOrAttrVec);
---
> /// Helper function to convert a vector of `OpFoldResult`s into a vector of
> /// `Value`s. For each `OpFoldResult` in `valueOrAttrVec` return the fold result
> /// if it casts to  a `Value` or create an index-type constant if it casts to
> /// `IntegerAttr`. No other attribute types are supported.
> SmallVector<Value> getAsValues(OpBuilder &b, Location loc,
>                                ArrayRef<OpFoldResult> valueOrAttrVec);
107,108c88,89
< /// Return a vector of OpFoldResults with the same size a staticValues, but
< /// all elements for which ShapedType::isDynamic is true, will be replaced by
---
> /// Return a vector of OpFoldResults with the same size a staticValues, but all
> /// elements for which ShapedType::isDynamic is true, will be replaced by
113,115c94,95
< /// Decompose a vector of mixed static or dynamic values into the
< /// corresponding pair of arrays. This is the inverse function of
< /// `getMixedValues`.
---
> /// Decompose a vector of mixed static or dynamic values into the corresponding
> /// pair of arrays. This is the inverse function of `getMixedValues`.
119,134d98
< 
< /// Helper to sort `values` according to matching `keys`.
< SmallVector<Value>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<Value> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< SmallVector<OpFoldResult>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<OpFoldResult> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< SmallVector<int64_t>
< getValuesSortedByKey(ArrayRef<Attribute> keys, ArrayRef<int64_t> values,
<                      llvm::function_ref<bool(Attribute, Attribute)> compare);
< 
< /// Return the number of iterations for a loop with a lower bound `lb`, upper
< /// bound `ub` and step `step`.
< std::optional<int64_t> constantTripCount(OpFoldResult lb, OpFoldResult ub,
<                                          OpFoldResult step);
--- include/mlir/Dialect/Utils/StructuredOpsUtils.h
23d22
< #include "mlir/IR/TypeRange.h"
31a31,32
> class TypeRange;
> class ValueRange;
118,122d118
< template <typename OpT>
< OpT clone(OpBuilder &b, OpT op, TypeRange newResultTypes,
<           ValueRange newOperands) {
<   return cast<OpT>(clone(b, op.getOperation(), newResultTypes, newOperands));
< }
129,133d124
< 
< // Get the list of attributes associated with the op, ignoring
< // those with the provided name.
< SmallVector<NamedAttribute>
< getPrunedAttributeList(Operation *op, ArrayRef<StringRef> elidedAttrs);
--- include/mlir/Dialect/Utils/CMakeLists.txt
--- include/mlir/Dialect/Utils/StructuredOpsUtils.td
--- include/mlir/Dialect/Utils/ReshapeOpsUtils.h
478c478
<   /// `std::nullopt`, the slice should replace the collapse shape op.
---
>   /// `None`, the slice should replace the collapse shape op.
523,536d522
< struct PackingMetadata {
<   SmallVector<int64_t> insertPositions;
<   SmallVector<int64_t> outerPositions;
<   SmallVector<ReassociationIndices> reassociations;
< };
< 
< /// Given a vector of `positions` indices representing desired packing insertion
< /// points into a target vector (i.e. pack/unpack.inner_dim_pos), compute the
< /// final positions in the target shape as well as the reshape reassociations.
< // Note: This should not be called with a large positions array (or the
< // implementation needs to be updated to use an N.log N sort instead of
< // repeated N^2 counts).
< PackingMetadata computePackingMetadata(int64_t packedRank,
<                                        ArrayRef<int64_t> innerDimPos);
--- include/mlir/Dialect/PDLInterp
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDLInterp/IR and include/mlir/Dialect/PDLInterp/IR
--- include/mlir/Dialect/PDLInterp/CMakeLists.txt
--- include/mlir/Dialect/PDLInterp/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDLInterp/IR/PDLInterpOps.td include/mlir/Dialect/PDLInterp/IR/PDLInterpOps.td
40a41
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/PDLInterp/IR/PDLInterpOps.td
40a41
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/PDLInterp/IR/CMakeLists.txt
--- include/mlir/Dialect/PDLInterp/IR/PDLInterp.h
--- include/mlir/Dialect/ControlFlow
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/ControlFlow/IR and include/mlir/Dialect/ControlFlow/IR
--- include/mlir/Dialect/ControlFlow/CMakeLists.txt
--- include/mlir/Dialect/ControlFlow/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/ControlFlow/IR/ControlFlowOps.td include/mlir/Dialect/ControlFlow/IR/ControlFlowOps.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/ControlFlow/IR/ControlFlowOps.h
--- include/mlir/Dialect/ControlFlow/IR/CMakeLists.txt
--- include/mlir/Dialect/ControlFlow/IR/ControlFlow.h
--- include/mlir/Dialect/ControlFlow/IR/ControlFlowOps.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/ArmNeon
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/ArmNeon/ArmNeon.td include/mlir/Dialect/ArmNeon/ArmNeon.td
27a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
43c44
<                      bit requiresAliasAnalysis = 0>
---
>                      bit requiresAliasScope = 0>
52c53
<                       /*requiresAliasAnalysis=*/requiresAliasAnalysis>;
---
>                       /*requiresAliasScope=*/requiresAliasScope>;
--- include/mlir/Dialect/ArmNeon/ArmNeon.td
27a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
43c44
<                      bit requiresAliasAnalysis = 0>
---
>                      bit requiresAliasScope = 0>
52c53
<                       /*requiresAliasAnalysis=*/requiresAliasAnalysis>;
---
>                       /*requiresAliasScope=*/requiresAliasScope>;
--- include/mlir/Dialect/ArmNeon/CMakeLists.txt
--- include/mlir/Dialect/ArmNeon/ArmNeonDialect.h
--- include/mlir/Dialect/OpenACC
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC/CMakeLists.txt include/mlir/Dialect/OpenACC/CMakeLists.txt
5,9d4
< add_mlir_dialect(OpenACCOps acc)
< 
< add_mlir_doc(OpenACCOps OpenACCDialect Dialects/ -gen-dialect-doc -dialect=acc)
< add_dependencies(OpenACCDialectDocGen acc_common_td)
< 
10a6,9
> mlir_tablegen(OpenACCOpsDialect.h.inc -gen-dialect-decls -dialect=acc)
> mlir_tablegen(OpenACCOpsDialect.cpp.inc -gen-dialect-defs -dialect=acc)
> mlir_tablegen(OpenACCOps.h.inc -gen-op-decls)
> mlir_tablegen(OpenACCOps.cpp.inc -gen-op-defs)
13,16d11
< add_public_tablegen_target(MLIROpenACCEnumsIncGen)
< add_dependencies(mlir-headers MLIROpenACCEnumsIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS OpenACCOps.td)
19,26c14,16
< add_public_tablegen_target(MLIROpenACCAttributesIncGen)
< add_dependencies(mlir-headers MLIROpenACCAttributesIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS OpenACCTypeInterfaces.td)
< mlir_tablegen(OpenACCTypeInterfaces.h.inc -gen-type-interface-decls)
< mlir_tablegen(OpenACCTypeInterfaces.cpp.inc -gen-type-interface-defs)
< add_public_tablegen_target(MLIROpenACCTypeInterfacesIncGen)
< add_dependencies(mlir-headers MLIROpenACCTypeInterfacesIncGen)
---
> add_mlir_doc(OpenACCOps OpenACCDialect Dialects/ -gen-dialect-doc)
> add_public_tablegen_target(MLIROpenACCOpsIncGen)
> add_dependencies(OpenACCDialectDocGen acc_common_td)
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC: OpenACCBase.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC/OpenACC.h include/mlir/Dialect/OpenACC/OpenACC.h
16d15
< #include "mlir/IR/BuiltinTypes.h"
22,26d20
< #include "mlir/Dialect/OpenACC/OpenACCTypeInterfaces.h.inc"
< #include "mlir/Interfaces/SideEffectInterfaces.h"
< 
< #define GET_TYPEDEF_CLASSES
< #include "mlir/Dialect/OpenACC/OpenACCOpsTypes.h.inc"
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC/OpenACCOps.td include/mlir/Dialect/OpenACC/OpenACCOps.td
1c1
< //===- OpenACCOps.td - OpenACC operation definitions -------*- tablegen -*-===//
---
> //===- OpenACC.td - OpenACC operation definitions ----------*- tablegen -*-===//
16,17d15
< include "mlir/Interfaces/SideEffectInterfaces.td"
< include "mlir/IR/BuiltinTypes.td"
20,22c18,31
< include "mlir/Dialect/OpenACC/OpenACCBase.td"
< include "mlir/Dialect/OpenACC/OpenACCOpsTypes.td"
< include "mlir/Dialect/OpenACC/OpenACCTypeInterfaces.td"
---
> 
> def OpenACC_Dialect : Dialect {
>   let name = "acc";
> 
>   let summary = "An OpenACC dialect for MLIR.";
> 
>   let description = [{
>     This dialect models the construct from the OpenACC 3.1 directive language.
>   }];
> 
>   let useDefaultAttributePrinterParser = 1;
>   let useFoldAPI = kEmitFoldAdaptorFolder;
>   let cppNamespace = "::mlir::acc";
> }
61,363d69
< // Simple alias to pointer-like interface to reduce verbosity.
< def OpenACC_PointerLikeType : TypeAlias<OpenACC_PointerLikeTypeInterface,
< 	"pointer-like type">;
< 
< // Define the OpenACC data clauses. There are a few cases where a modifier
< // is used, like create(zero), copyin(readonly), and copyout(zero). Since in
< // some cases we decompose the original acc data clauses into multiple acc
< // dialect operations, we need to keep track of original clause. Thus even
< // for the clause with modifier, we create separate operation to make this
< // possible.
< def OpenACC_CopyinClause          : I64EnumAttrCase<"acc_copyin", 1>;
< def OpenACC_CopyinReadonlyClause  : I64EnumAttrCase<"acc_copyin_readonly", 2>;
< def OpenACC_CopyClause            : I64EnumAttrCase<"acc_copy", 3>;
< def OpenACC_CopyoutClause         : I64EnumAttrCase<"acc_copyout", 4>;
< def OpenACC_CopyoutZeroClause     : I64EnumAttrCase<"acc_copyout_zero", 5>;
< def OpenACC_PresentClause         : I64EnumAttrCase<"acc_present", 6>;
< def OpenACC_CreateClause          : I64EnumAttrCase<"acc_create", 7>;
< def OpenACC_CreateZeroClause      : I64EnumAttrCase<"acc_create_zero", 8>;
< def OpenACC_DeleteClause          : I64EnumAttrCase<"acc_delete", 9>;
< def OpenACC_AttachClause          : I64EnumAttrCase<"acc_attach", 10>;
< def OpenACC_DetachClause          : I64EnumAttrCase<"acc_detach", 11>;
< def OpenACC_NoCreateClause        : I64EnumAttrCase<"acc_no_create", 12>;
< def OpenACC_PrivateClause         : I64EnumAttrCase<"acc_private", 13>;
< def OpenACC_FirstPrivateClause    : I64EnumAttrCase<"acc_firstprivate", 14>;
< def OpenACC_IsDevicePtrClause     : I64EnumAttrCase<"acc_deviceptr", 15>;
< def OpenACC_GetDevicePtrClause    : I64EnumAttrCase<"acc_getdeviceptr", 16>;
< 
< def OpenACC_DataClauseEnum : I64EnumAttr<"DataClause",
<     "data clauses supported by OpenACC",
<     [OpenACC_CopyinClause, OpenACC_CopyinReadonlyClause, OpenACC_CopyClause,
<      OpenACC_CopyoutClause, OpenACC_CopyoutZeroClause, OpenACC_PresentClause,
<      OpenACC_CreateClause, OpenACC_CreateZeroClause, OpenACC_DeleteClause,
<      OpenACC_AttachClause, OpenACC_DetachClause, OpenACC_NoCreateClause,
<      OpenACC_PrivateClause, OpenACC_FirstPrivateClause,
<      OpenACC_IsDevicePtrClause, OpenACC_GetDevicePtrClause
<     ]> {
<   let cppNamespace = "::mlir::acc";
< }
< 
< // Used for data specification in data clauses (2.7.1).
< // Either (or both) extent and upperbound must be specified.
< def OpenACC_DataBoundsOp : OpenACC_Op<"bounds",
<     [AttrSizedOperandSegments, NoMemoryEffect]> {
<   let summary = "Represents normalized bounds information for acc data clause.";
< 
<   let description = [{
<     This operation is used to record bounds used in acc data clause in a
<     normalized fashion (zero-based). This works well with the `PointerLikeType`
<     requirement in data clauses - since a `lowerbound` of 0 means looking
<     at data at the zero offset from pointer.
< 
<     The operation must have an `upperbound` or `extent` (or both are allowed -
<     but not checked for consistency). When the source language's arrays are
<     not zero-based, the `startIdx` must specify the zero-position index.
< 
<     Examples below show copying a slice of 10-element array except first element.
<     Note that the examples use extent in data clause for C++ and upperbound
<     for Fortran (as per 2.7.1). To simplify examples, the constants are used
<     directly in the acc.bounds operands - this is not the syntax of operation.
< 
<     C++:
<     ```
<     int array[10];
<     #pragma acc copy(array[1:9])
<     ```
<     =>
<     ```mlir
<     acc.bounds lb(1) ub(9) extent(9) startIdx(0)
<     ```
< 
<     Fortran:
<     ```
<     integer :: array(1:10)
<     !$acc copy(array(2:10))
<     ```
<     =>
<     ```mlir
<     acc.bounds lb(1) ub(9) extent(9) startIdx(1)
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$lowerbound,
<                        Optional<IntOrIndex>:$upperbound,
<                        Optional<IntOrIndex>:$extent,
<                        Optional<IntOrIndex>:$stride,
<                        DefaultValuedAttr<BoolAttr, "false">:$strideInBytes,
<                        Optional<IntOrIndex>:$startIdx);
<   let results = (outs OpenACC_DataBoundsType:$result);
< 
<   let assemblyFormat = [{
<     oilist(
<         `lowerbound` `(` $lowerbound `:` type($lowerbound) `)`
<       | `upperbound` `(` $upperbound `:` type($upperbound) `)`
<       | `extent` `(` $extent `:` type($extent) `)`
<       | `stride` `(` $stride `:` type($stride) `)`
<       | `startIdx` `(` $startIdx `:` type($startIdx) `)`
<     ) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< // Data entry operation does not refer to OpenACC spec terminology, but to
< // terminology used in this dialect. It refers to data operations that will
< // appear before data or compute region. It will be used as the base of acc
< // dialect operations for the following OpenACC data clauses: copyin, create,
< // present, attach, deviceptr.
< //
< // The bounds are represented in rank order. Rank 0 (inner-most dimension) is
< // the first.
< class OpenACC_DataEntryOp<string mnemonic, string clause, list<Trait> traits = []> :
<     OpenACC_Op<mnemonic, !listconcat(traits,
<         [AttrSizedOperandSegments])> {
<   let arguments = (ins OpenACC_PointerLikeTypeInterface:$varPtr,
<                        Optional<OpenACC_PointerLikeTypeInterface>:$varPtrPtr,
<                        Variadic<OpenACC_DataBoundsType>:$bounds, /* rank-0 to rank-{n-1} */
<                        DefaultValuedAttr<OpenACC_DataClauseEnum,clause>:$dataClause,
<                        DefaultValuedAttr<BoolAttr, "true">:$structured,
<                        DefaultValuedAttr<BoolAttr, "false">:$implicit,
<                        OptionalAttr<StrAttr>:$name);
<   let results = (outs OpenACC_PointerLikeTypeInterface:$accPtr);
< 
<   let description = [{
<     - `varPtr`: The address of variable to copy.
<     - `varPtrPtr`: Specifies the address of varPtr - only used when the variable
<     copied is a field in a struct. This is important for OpenACC due to implicit
<     attach semantics on data clauses (2.6.4).
<     - `bounds`: Used when copying just slice of array or array's bounds are not
<     encoded in type. They are in rank order where rank 0 is inner-most dimension.
<     - `dataClause`: Keeps track of the data clause the user used. This is because
<     the acc operations are decomposed. So a 'copy' clause is decomposed to both 
<     `acc.copyin` and `acc.copyout` operations, but both have dataClause that
<     specifies `acc_copy` in this field.
<     - `structured`: Flag to note whether this is associated with structured region
<     (parallel, kernels, data) or unstructured (enter data, exit data). This is
<     important due to spec specifically calling out structured and dynamic reference
<     counters (2.6.7).
<     - `implicit`: Whether this is an implicitly generated operation, such as copies
<     done to satisfy "Variables with Implicitly Determined Data Attributes" in 2.6.2.
<     - `name`: Holds the name of variable as specified in user clause (including bounds).
<   }];
< 
<   let assemblyFormat = [{
<     `varPtr` `(` $varPtr `:` type($varPtr) `)`
<     oilist(
<         `varPtrPtr` `(` $varPtrPtr `:` type($varPtrPtr) `)`
<       | `bounds` `(` $bounds `)`
<     ) `->` type($accPtr) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.4 deviceptr clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DevicePtrOp : OpenACC_DataEntryOp<"deviceptr",
<     "mlir::acc::DataClause::acc_deviceptr"> {
<   let summary = "Specifies that the variable pointer is a device pointer.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.5 present clause
< //===----------------------------------------------------------------------===//
< def OpenACC_PresentOp : OpenACC_DataEntryOp<"present",
<     "mlir::acc::DataClause::acc_present"> {
<   let summary = "Specifies that the variable is already present on device.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.7 copyin clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CopyinOp : OpenACC_DataEntryOp<"copyin",
<     "mlir::acc::DataClause::acc_copyin"> {
<   let summary = "Represents copyin semantics for acc data clauses like acc "
<                 "copyin and acc copy.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a copyin with readonly modifier.
<     bool isCopyinReadonly();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.9 create clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CreateOp : OpenACC_DataEntryOp<"create",
<     "mlir::acc::DataClause::acc_create"> {
<   let summary = "Represents create semantics for acc data clauses like acc "
<                 "create and acc copyout.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a create with zero modifier.
<     bool isCreateZero();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.10 no_create clause
< //===----------------------------------------------------------------------===//
< def OpenACC_NoCreateOp : OpenACC_DataEntryOp<"nocreate",
<     "mlir::acc::DataClause::acc_no_create"> {
<   let summary = "Represents acc no_create semantics.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.12 attach clause
< //===----------------------------------------------------------------------===//
< def OpenACC_AttachOp : OpenACC_DataEntryOp<"attach",
<     "mlir::acc::DataClause::acc_attach"> {
<   let summary = "Represents acc attach semantics which updates a pointer in "
<                 "device memory with the corresponding device address of the "
<                 "pointee.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 3.2.23 acc_deviceptr
< //===----------------------------------------------------------------------===//
< // This is needed to get device address without the additional semantics in
< // acc present.
< // It is also useful for providing the device address for unstructured construct
< // exit_data since unlike structured constructs, there is no matching data entry
< // operation.
< def OpenACC_GetDevicePtrOp : OpenACC_DataEntryOp<"getdeviceptr",
<     "mlir::acc::DataClause::acc_getdeviceptr"> {
<   let summary = "Gets device address from host address if it exists on device.";
< }
< 
< // Data exit operation does not refer to OpenACC spec terminology, but to
< // terminology used in this dialect. It refers to data operations that will appear
< // after data or compute region. It will be used as the base of acc dialect
< // operations for the following OpenACC data clauses: copyout, detach, delete.
< class OpenACC_DataExitOp<string mnemonic, string clause, list<Trait> traits = []> :
<     OpenACC_Op<mnemonic, !listconcat(traits,
<         [AttrSizedOperandSegments])> {
<   let arguments = (ins Optional<OpenACC_PointerLikeTypeInterface>:$varPtr,
<                        OpenACC_PointerLikeTypeInterface:$accPtr,
<                        Variadic<OpenACC_DataBoundsType>:$bounds,
<                        DefaultValuedAttr<OpenACC_DataClauseEnum,clause>:$dataClause,
<                        DefaultValuedAttr<BoolAttr, "true">:$structured,
<                        DefaultValuedAttr<BoolAttr, "false">:$implicit,
<                        OptionalAttr<StrAttr>:$name);
< 
<   let description = [{
<     - `varPtr`: The address of variable to copy back to. This only applies to
<     `acc.copyout`
<     - `accPtr`: The acc address of variable. This is the link from the data-entry
<     operation used.
<     - `bounds`: Used when copying just slice of array or array's bounds are not
<     encoded in type. They are in rank order where rank 0 is inner-most dimension.
<     - `dataClause`: Keeps track of the data clause the user used. This is because
<     the acc operations are decomposed. So a 'copy' clause is decomposed to both 
<     `acc.copyin` and `acc.copyout` operations, but both have dataClause that
<     specifies `acc_copy` in this field.
<     - `structured`: Flag to note whether this is associated with structured region
<     (parallel, kernels, data) or unstructured (enter data, exit data). This is
<     important due to spec specifically calling out structured and dynamic reference
<     counters (2.6.7).
<     - `implicit`: Whether this is an implicitly generated operation, such as copies
<     done to satisfy "Variables with Implicitly Determined Data Attributes" in 2.6.2.
<     - `name`: Holds the name of variable as specified in user clause (including bounds).
<   }];
< 
<   let assemblyFormat = [{
<     `accPtr` `(` $accPtr `:` type($accPtr) `)`
<     oilist(
<         `bounds` `(` $bounds `)`
<       | `to` `varPtr` `(` $varPtr `:` type($varPtr) `)`
<     ) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.8 copyout clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CopyoutOp : OpenACC_DataExitOp<"copyout",
<     "mlir::acc::DataClause::acc_copyout"> {
<   let summary = "Represents acc copyout semantics - reverse of copyin.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a copyout with zero modifier.
<     bool isCopyoutZero();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.11 delete clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DeleteOp : OpenACC_DataExitOp<"delete",
<     "mlir::acc::DataClause::acc_delete"> {
<   let summary = "Represents acc delete semantics - reverse of create.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.13 detach clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DetachOp : OpenACC_DataExitOp<"detach",
<     "mlir::acc::DataClause::acc_detach"> {
<   let summary = "Represents acc detach semantics - reverse of attach.";
< }
< 
369c75
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>     [AttrSizedOperandSegments]> {
410d115
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
416,421c121,144
<     /// The number of data operands.
<     unsigned getNumDataOperands();
< 
<     /// The i-th data operand passed.
<     Value getDataOperand(unsigned i);
<   }];
---
>     static StringRef getAsyncKeyword() { return "async"; }
>     static StringRef getAsyncAttrName() { return "asyncAttr"; }
>     static StringRef getWaitKeyword() { return "wait"; }
>     static StringRef getWaitAttrName() { return "waitAttr"; }
>     static StringRef getNumGangsKeyword() { return "num_gangs"; }
>     static StringRef getNumWorkersKeyword() { return "num_workers"; }
>     static StringRef getVectorLengthKeyword() { return "vector_length"; }
>     static StringRef getIfKeyword() { return "if"; }
>     static StringRef getSelfKeyword() { return "self"; }
>     static StringRef getSelfAttrName() { return "selfAttr"; }
>     static StringRef getReductionKeyword() { return "reduction"; }
>     static StringRef getCopyKeyword() { return "copy"; }
>     static StringRef getCopyinKeyword() { return "copyin"; }
>     static StringRef getCopyinReadonlyKeyword() { return "copyin_readonly"; }
>     static StringRef getCopyoutKeyword() { return "copyout"; }
>     static StringRef getCopyoutZeroKeyword() { return "copyout_zero"; }
>     static StringRef getCreateKeyword() { return "create"; }
>     static StringRef getCreateZeroKeyword() { return "create_zero"; }
>     static StringRef getNoCreateKeyword() { return "no_create"; }
>     static StringRef getPresentKeyword() { return "present"; }
>     static StringRef getDevicePtrKeyword() { return "deviceptr"; }
>     static StringRef getAttachKeyword() { return "attach"; }
>     static StringRef getPrivateKeyword() { return "private"; }
>     static StringRef getFirstPrivateKeyword() { return "firstprivate"; }
423,589d145
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `firstprivate` `(` $gangFirstPrivateOperands `:`
<             type($gangFirstPrivateOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `num_gangs` `(` $numGangs `:` type($numGangs) `)`
<       | `num_workers` `(` $numWorkers `:` type($numWorkers) `)`
<       | `private` `(` $gangPrivateOperands `:` type($gangPrivateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `vector_length` `(` $vectorLength `:` type($vectorLength) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region attr-dict-with-keyword
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.5.2 serial Construct
< //===----------------------------------------------------------------------===//
< 
< def OpenACC_SerialOp : OpenACC_Op<"serial",
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
<   let summary = "serial construct";
<   let description = [{
<     The "acc.serial" operation represents a serial construct block. It has
<     one region to be executed in serial on the current device.
< 
<     Example:
< 
<     ```mlir
<     acc.serial private(%c : memref<10xf32>) {
<       // serial region
<     }
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$async,
<                        UnitAttr:$asyncAttr,
<                        Variadic<IntOrIndex>:$waitOperands,
<                        UnitAttr:$waitAttr,
<                        Optional<I1>:$ifCond,
<                        Optional<I1>:$selfCond,
<                        UnitAttr:$selfAttr,
<                        OptionalAttr<OpenACC_ReductionOpAttr>:$reductionOp,
<                        Variadic<AnyType>:$reductionOperands,
<                        Variadic<AnyType>:$copyOperands,
<                        Variadic<AnyType>:$copyinOperands,
<                        Variadic<AnyType>:$copyinReadonlyOperands,
<                        Variadic<AnyType>:$copyoutOperands,
<                        Variadic<AnyType>:$copyoutZeroOperands,
<                        Variadic<AnyType>:$createOperands,
<                        Variadic<AnyType>:$createZeroOperands,
<                        Variadic<AnyType>:$noCreateOperands,
<                        Variadic<AnyType>:$presentOperands,
<                        Variadic<AnyType>:$devicePtrOperands,
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<AnyType>:$gangPrivateOperands,
<                        Variadic<AnyType>:$gangFirstPrivateOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
<                        OptionalAttr<DefaultValueAttr>:$defaultAttr);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let extraClassDeclaration = [{
<     /// The number of data operands.
<     unsigned getNumDataOperands();
< 
<     /// The i-th data operand passed.
<     Value getDataOperand(unsigned i);
<   }];
< 
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `firstprivate` `(` $gangFirstPrivateOperands `:`
<             type($gangFirstPrivateOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `private` `(` $gangPrivateOperands `:` type($gangPrivateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region attr-dict-with-keyword
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.5.1 kernels Construct
< //===----------------------------------------------------------------------===//
< 
< def OpenACC_KernelsOp : OpenACC_Op<"kernels",
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
<   let summary = "kernels construct";
<   let description = [{
<     The "acc.kernels" operation represents a kernels construct block. It has
<     one region to be compiled into a sequence of kernels for execution on the
<     current device.
< 
<     Example:
< 
<     ```mlir
<     acc.kernels num_gangs(%c10) num_workers(%c10)
<         private(%c : memref<10xf32>) {
<       // kernels region
<     }
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$async,
<                        UnitAttr:$asyncAttr,
<                        Variadic<IntOrIndex>:$waitOperands,
<                        UnitAttr:$waitAttr,
<                        Optional<IntOrIndex>:$numGangs,
<                        Optional<IntOrIndex>:$numWorkers,
<                        Optional<IntOrIndex>:$vectorLength,
<                        Optional<I1>:$ifCond,
<                        Optional<I1>:$selfCond,
<                        UnitAttr:$selfAttr,
<                        Variadic<AnyType>:$copyOperands,
<                        Variadic<AnyType>:$copyinOperands,
<                        Variadic<AnyType>:$copyinReadonlyOperands,
<                        Variadic<AnyType>:$copyoutOperands,
<                        Variadic<AnyType>:$copyoutZeroOperands,
<                        Variadic<AnyType>:$createOperands,
<                        Variadic<AnyType>:$createZeroOperands,
<                        Variadic<AnyType>:$noCreateOperands,
<                        Variadic<AnyType>:$presentOperands,
<                        Variadic<AnyType>:$devicePtrOperands,
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
<                        OptionalAttr<DefaultValueAttr>:$defaultAttr);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let extraClassDeclaration = [{
596,622c152
< 
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:` type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `num_gangs` `(` $numGangs `:` type($numGangs) `)`
<       | `num_workers` `(` $numWorkers `:` type($numWorkers) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `vector_length` `(` $vectorLength `:` type($vectorLength) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<     )
<     $region attr-dict-with-keyword
<   }];
---
>   let hasCustomAssemblyFormat = 1;
630c160
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>     [AttrSizedOperandSegments]> {
663d192
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
677,694c206,220
<     oilist(
<         `if` `(` $ifCond `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<             type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<             type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<             type($createZeroOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `deviceptr` `(` $deviceptrOperands `:` type($deviceptrOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `copy` `(` $copyOperands^ `:` type($copyOperands) `)` )?
>     ( `copyin` `(` $copyinOperands^ `:` type($copyinOperands) `)` )?
>     ( `copyin_readonly` `(` $copyinReadonlyOperands^ `:`
>         type($copyinReadonlyOperands) `)` )?
>     ( `copyout` `(` $copyoutOperands^ `:` type($copyoutOperands) `)` )?
>     ( `copyout_zero` `(` $copyoutZeroOperands^ `:`
>         type($copyoutZeroOperands) `)` )?
>     ( `create` `(` $createOperands^ `:` type($createOperands) `)` )?
>     ( `create_zero` `(` $createZeroOperands^ `:`
>         type($createZeroOperands) `)` )?
>     ( `no_create` `(` $noCreateOperands^ `:` type($noCreateOperands) `)` )?
>     ( `present` `(` $presentOperands^ `:` type($presentOperands) `)` )?
>     ( `deviceptr` `(` $deviceptrOperands^ `:` type($deviceptrOperands) `)` )?
>     ( `attach` `(` $attachOperands^ `:` type($attachOperands) `)` )?
739,740c265
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands);
---
>                        Variadic<AnyType>:$attachOperands);
751,762c276,284
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `copyin` `(` $copyinOperands^ `:` type($copyinOperands) `)` )?
>     ( `create` `(` $createOperands^ `:` type($createOperands) `)` )?
>     ( `create_zero` `(` $createZeroOperands^ `:`
>         type($createZeroOperands) `)` )?
>     ( `attach` `(` $attachOperands^ `:` type($attachOperands) `)` )?
796d317
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
808,817c329,335
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `delete` `(` $deleteOperands `:` type($deleteOperands) `)`
<       | `detach` `(` $detachOperands `:` type($detachOperands) `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `copyout` `(` $copyoutOperands^ `:` type($copyoutOperands) `)` )?
>     ( `delete` `(` $deleteOperands^ `:` type($deleteOperands) `)` )?
>     ( `detach` `(` $detachOperands^ `:` type($detachOperands) `)` )?
830c348,349
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>       [AttrSizedOperandSegments,
>        SingleBlockImplicitTerminator<"acc::YieldOp">]> {
851a371
> 
860,862d379
<                        UnitAttr:$hasGang,
<                        UnitAttr:$hasWorker,
<                        UnitAttr:$hasVector,
866c383,384
<                        Variadic<AnyType>:$reductionOperands);
---
>                        Variadic<AnyType>:$reductionOperands,
>                        DefaultValuedAttr<I64Attr, "0">:$exec_mapping);
872a391,393
>     static StringRef getCollapseAttrStrName() { return "collapse"; }
>     static StringRef getSeqAttrStrName() { return "seq"; }
>     static StringRef getIndependentAttrStrName() { return "independent"; }
873a395,396
>     static StringRef getExecutionMappingAttrStrName() { return "exec_mapping"; }
>     static StringRef getGangKeyword() { return "gang"; }
875a399,403
>     static StringRef getVectorKeyword() { return "vector"; }
>     static StringRef getWorkerKeyword() { return "worker"; }
>     static StringRef getTileKeyword() { return "tile"; }
>     static StringRef getPrivateKeyword() { return "private"; }
>     static StringRef getReductionKeyword() { return "reduction"; }
877d404
< 
879,892d405
<   let assemblyFormat = [{
<     oilist(
<         `gang` `` custom<GangClause>($gangNum, type($gangNum), $gangStatic, type($gangStatic), $hasGang)
<       | `worker` `` custom<WorkerClause>($workerNum, type($workerNum), $hasWorker)
<       | `vector` `` custom<VectorClause>($vectorLength, type($vectorLength), $hasVector)
<       | `private` `(` $privateOperands `:` type($privateOperands) `)`
<       | `tile` `(` $tileOperands `:` type($tileOperands) `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region
<     ( `(` type($results)^ `)` )?
<     attr-dict-with-keyword
<   }];
< 
898c411
<     ParentOneOf<["ParallelOp, LoopOp, SerialOp"]>]> {
---
>     ParentOneOf<["ParallelOp, LoopOp"]>]> {
938,942c451,453
<     oilist(
<         `device_type` `(` $deviceTypeOperands `:` type($deviceTypeOperands) `)`
<       | `device_num` `(` $deviceNumOperand `:` type($deviceNumOperand) `)`
<       | `if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `device_type` `(` $deviceTypeOperands^ `:` type($deviceTypeOperands) `)` )?
>     ( `device_num` `(` $deviceNumOperand^ `:` type($deviceNumOperand) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
971,974c482,484
<     oilist(`device_type` `(` $deviceTypeOperands `:` type($deviceTypeOperands) `)`
<     |`device_num` `(` $deviceNumOperand `:` type($deviceNumOperand) `)`
<     |`if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `device_type` `(` $deviceTypeOperands^ `:` type($deviceTypeOperands) `)` )?
>     ( `device_num` `(` $deviceNumOperand^ `:` type($deviceNumOperand) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
1019,1028c529,536
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `device_type` `(` $deviceTypeOperands `:`
<           type($deviceTypeOperands) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `host` `(` $hostOperands `:` type($hostOperands) `)`
<       | `device` `(` $deviceOperands `:` type($deviceOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `device_type` `(` $deviceTypeOperands^ `:`
>         type($deviceTypeOperands) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `host` `(` $hostOperands^ `:` type($hostOperands) `)` )?
>     ( `device` `(` $deviceOperands^ `:` type($deviceOperands) `)` )?
1063,1066c571,573
<     oilist(`async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       |`wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       |`if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC: OpenACCOpsTypes.td
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenACC: OpenACCTypeInterfaces.td
--- include/mlir/Dialect/OpenACC/OpenACCOps.td
1c1
< //===- OpenACCOps.td - OpenACC operation definitions -------*- tablegen -*-===//
---
> //===- OpenACC.td - OpenACC operation definitions ----------*- tablegen -*-===//
16,17d15
< include "mlir/Interfaces/SideEffectInterfaces.td"
< include "mlir/IR/BuiltinTypes.td"
20,22c18,31
< include "mlir/Dialect/OpenACC/OpenACCBase.td"
< include "mlir/Dialect/OpenACC/OpenACCOpsTypes.td"
< include "mlir/Dialect/OpenACC/OpenACCTypeInterfaces.td"
---
> 
> def OpenACC_Dialect : Dialect {
>   let name = "acc";
> 
>   let summary = "An OpenACC dialect for MLIR.";
> 
>   let description = [{
>     This dialect models the construct from the OpenACC 3.1 directive language.
>   }];
> 
>   let useDefaultAttributePrinterParser = 1;
>   let useFoldAPI = kEmitFoldAdaptorFolder;
>   let cppNamespace = "::mlir::acc";
> }
61,363d69
< // Simple alias to pointer-like interface to reduce verbosity.
< def OpenACC_PointerLikeType : TypeAlias<OpenACC_PointerLikeTypeInterface,
< 	"pointer-like type">;
< 
< // Define the OpenACC data clauses. There are a few cases where a modifier
< // is used, like create(zero), copyin(readonly), and copyout(zero). Since in
< // some cases we decompose the original acc data clauses into multiple acc
< // dialect operations, we need to keep track of original clause. Thus even
< // for the clause with modifier, we create separate operation to make this
< // possible.
< def OpenACC_CopyinClause          : I64EnumAttrCase<"acc_copyin", 1>;
< def OpenACC_CopyinReadonlyClause  : I64EnumAttrCase<"acc_copyin_readonly", 2>;
< def OpenACC_CopyClause            : I64EnumAttrCase<"acc_copy", 3>;
< def OpenACC_CopyoutClause         : I64EnumAttrCase<"acc_copyout", 4>;
< def OpenACC_CopyoutZeroClause     : I64EnumAttrCase<"acc_copyout_zero", 5>;
< def OpenACC_PresentClause         : I64EnumAttrCase<"acc_present", 6>;
< def OpenACC_CreateClause          : I64EnumAttrCase<"acc_create", 7>;
< def OpenACC_CreateZeroClause      : I64EnumAttrCase<"acc_create_zero", 8>;
< def OpenACC_DeleteClause          : I64EnumAttrCase<"acc_delete", 9>;
< def OpenACC_AttachClause          : I64EnumAttrCase<"acc_attach", 10>;
< def OpenACC_DetachClause          : I64EnumAttrCase<"acc_detach", 11>;
< def OpenACC_NoCreateClause        : I64EnumAttrCase<"acc_no_create", 12>;
< def OpenACC_PrivateClause         : I64EnumAttrCase<"acc_private", 13>;
< def OpenACC_FirstPrivateClause    : I64EnumAttrCase<"acc_firstprivate", 14>;
< def OpenACC_IsDevicePtrClause     : I64EnumAttrCase<"acc_deviceptr", 15>;
< def OpenACC_GetDevicePtrClause    : I64EnumAttrCase<"acc_getdeviceptr", 16>;
< 
< def OpenACC_DataClauseEnum : I64EnumAttr<"DataClause",
<     "data clauses supported by OpenACC",
<     [OpenACC_CopyinClause, OpenACC_CopyinReadonlyClause, OpenACC_CopyClause,
<      OpenACC_CopyoutClause, OpenACC_CopyoutZeroClause, OpenACC_PresentClause,
<      OpenACC_CreateClause, OpenACC_CreateZeroClause, OpenACC_DeleteClause,
<      OpenACC_AttachClause, OpenACC_DetachClause, OpenACC_NoCreateClause,
<      OpenACC_PrivateClause, OpenACC_FirstPrivateClause,
<      OpenACC_IsDevicePtrClause, OpenACC_GetDevicePtrClause
<     ]> {
<   let cppNamespace = "::mlir::acc";
< }
< 
< // Used for data specification in data clauses (2.7.1).
< // Either (or both) extent and upperbound must be specified.
< def OpenACC_DataBoundsOp : OpenACC_Op<"bounds",
<     [AttrSizedOperandSegments, NoMemoryEffect]> {
<   let summary = "Represents normalized bounds information for acc data clause.";
< 
<   let description = [{
<     This operation is used to record bounds used in acc data clause in a
<     normalized fashion (zero-based). This works well with the `PointerLikeType`
<     requirement in data clauses - since a `lowerbound` of 0 means looking
<     at data at the zero offset from pointer.
< 
<     The operation must have an `upperbound` or `extent` (or both are allowed -
<     but not checked for consistency). When the source language's arrays are
<     not zero-based, the `startIdx` must specify the zero-position index.
< 
<     Examples below show copying a slice of 10-element array except first element.
<     Note that the examples use extent in data clause for C++ and upperbound
<     for Fortran (as per 2.7.1). To simplify examples, the constants are used
<     directly in the acc.bounds operands - this is not the syntax of operation.
< 
<     C++:
<     ```
<     int array[10];
<     #pragma acc copy(array[1:9])
<     ```
<     =>
<     ```mlir
<     acc.bounds lb(1) ub(9) extent(9) startIdx(0)
<     ```
< 
<     Fortran:
<     ```
<     integer :: array(1:10)
<     !$acc copy(array(2:10))
<     ```
<     =>
<     ```mlir
<     acc.bounds lb(1) ub(9) extent(9) startIdx(1)
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$lowerbound,
<                        Optional<IntOrIndex>:$upperbound,
<                        Optional<IntOrIndex>:$extent,
<                        Optional<IntOrIndex>:$stride,
<                        DefaultValuedAttr<BoolAttr, "false">:$strideInBytes,
<                        Optional<IntOrIndex>:$startIdx);
<   let results = (outs OpenACC_DataBoundsType:$result);
< 
<   let assemblyFormat = [{
<     oilist(
<         `lowerbound` `(` $lowerbound `:` type($lowerbound) `)`
<       | `upperbound` `(` $upperbound `:` type($upperbound) `)`
<       | `extent` `(` $extent `:` type($extent) `)`
<       | `stride` `(` $stride `:` type($stride) `)`
<       | `startIdx` `(` $startIdx `:` type($startIdx) `)`
<     ) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< // Data entry operation does not refer to OpenACC spec terminology, but to
< // terminology used in this dialect. It refers to data operations that will
< // appear before data or compute region. It will be used as the base of acc
< // dialect operations for the following OpenACC data clauses: copyin, create,
< // present, attach, deviceptr.
< //
< // The bounds are represented in rank order. Rank 0 (inner-most dimension) is
< // the first.
< class OpenACC_DataEntryOp<string mnemonic, string clause, list<Trait> traits = []> :
<     OpenACC_Op<mnemonic, !listconcat(traits,
<         [AttrSizedOperandSegments])> {
<   let arguments = (ins OpenACC_PointerLikeTypeInterface:$varPtr,
<                        Optional<OpenACC_PointerLikeTypeInterface>:$varPtrPtr,
<                        Variadic<OpenACC_DataBoundsType>:$bounds, /* rank-0 to rank-{n-1} */
<                        DefaultValuedAttr<OpenACC_DataClauseEnum,clause>:$dataClause,
<                        DefaultValuedAttr<BoolAttr, "true">:$structured,
<                        DefaultValuedAttr<BoolAttr, "false">:$implicit,
<                        OptionalAttr<StrAttr>:$name);
<   let results = (outs OpenACC_PointerLikeTypeInterface:$accPtr);
< 
<   let description = [{
<     - `varPtr`: The address of variable to copy.
<     - `varPtrPtr`: Specifies the address of varPtr - only used when the variable
<     copied is a field in a struct. This is important for OpenACC due to implicit
<     attach semantics on data clauses (2.6.4).
<     - `bounds`: Used when copying just slice of array or array's bounds are not
<     encoded in type. They are in rank order where rank 0 is inner-most dimension.
<     - `dataClause`: Keeps track of the data clause the user used. This is because
<     the acc operations are decomposed. So a 'copy' clause is decomposed to both 
<     `acc.copyin` and `acc.copyout` operations, but both have dataClause that
<     specifies `acc_copy` in this field.
<     - `structured`: Flag to note whether this is associated with structured region
<     (parallel, kernels, data) or unstructured (enter data, exit data). This is
<     important due to spec specifically calling out structured and dynamic reference
<     counters (2.6.7).
<     - `implicit`: Whether this is an implicitly generated operation, such as copies
<     done to satisfy "Variables with Implicitly Determined Data Attributes" in 2.6.2.
<     - `name`: Holds the name of variable as specified in user clause (including bounds).
<   }];
< 
<   let assemblyFormat = [{
<     `varPtr` `(` $varPtr `:` type($varPtr) `)`
<     oilist(
<         `varPtrPtr` `(` $varPtrPtr `:` type($varPtrPtr) `)`
<       | `bounds` `(` $bounds `)`
<     ) `->` type($accPtr) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.4 deviceptr clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DevicePtrOp : OpenACC_DataEntryOp<"deviceptr",
<     "mlir::acc::DataClause::acc_deviceptr"> {
<   let summary = "Specifies that the variable pointer is a device pointer.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.5 present clause
< //===----------------------------------------------------------------------===//
< def OpenACC_PresentOp : OpenACC_DataEntryOp<"present",
<     "mlir::acc::DataClause::acc_present"> {
<   let summary = "Specifies that the variable is already present on device.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.7 copyin clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CopyinOp : OpenACC_DataEntryOp<"copyin",
<     "mlir::acc::DataClause::acc_copyin"> {
<   let summary = "Represents copyin semantics for acc data clauses like acc "
<                 "copyin and acc copy.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a copyin with readonly modifier.
<     bool isCopyinReadonly();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.9 create clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CreateOp : OpenACC_DataEntryOp<"create",
<     "mlir::acc::DataClause::acc_create"> {
<   let summary = "Represents create semantics for acc data clauses like acc "
<                 "create and acc copyout.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a create with zero modifier.
<     bool isCreateZero();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.10 no_create clause
< //===----------------------------------------------------------------------===//
< def OpenACC_NoCreateOp : OpenACC_DataEntryOp<"nocreate",
<     "mlir::acc::DataClause::acc_no_create"> {
<   let summary = "Represents acc no_create semantics.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.12 attach clause
< //===----------------------------------------------------------------------===//
< def OpenACC_AttachOp : OpenACC_DataEntryOp<"attach",
<     "mlir::acc::DataClause::acc_attach"> {
<   let summary = "Represents acc attach semantics which updates a pointer in "
<                 "device memory with the corresponding device address of the "
<                 "pointee.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 3.2.23 acc_deviceptr
< //===----------------------------------------------------------------------===//
< // This is needed to get device address without the additional semantics in
< // acc present.
< // It is also useful for providing the device address for unstructured construct
< // exit_data since unlike structured constructs, there is no matching data entry
< // operation.
< def OpenACC_GetDevicePtrOp : OpenACC_DataEntryOp<"getdeviceptr",
<     "mlir::acc::DataClause::acc_getdeviceptr"> {
<   let summary = "Gets device address from host address if it exists on device.";
< }
< 
< // Data exit operation does not refer to OpenACC spec terminology, but to
< // terminology used in this dialect. It refers to data operations that will appear
< // after data or compute region. It will be used as the base of acc dialect
< // operations for the following OpenACC data clauses: copyout, detach, delete.
< class OpenACC_DataExitOp<string mnemonic, string clause, list<Trait> traits = []> :
<     OpenACC_Op<mnemonic, !listconcat(traits,
<         [AttrSizedOperandSegments])> {
<   let arguments = (ins Optional<OpenACC_PointerLikeTypeInterface>:$varPtr,
<                        OpenACC_PointerLikeTypeInterface:$accPtr,
<                        Variadic<OpenACC_DataBoundsType>:$bounds,
<                        DefaultValuedAttr<OpenACC_DataClauseEnum,clause>:$dataClause,
<                        DefaultValuedAttr<BoolAttr, "true">:$structured,
<                        DefaultValuedAttr<BoolAttr, "false">:$implicit,
<                        OptionalAttr<StrAttr>:$name);
< 
<   let description = [{
<     - `varPtr`: The address of variable to copy back to. This only applies to
<     `acc.copyout`
<     - `accPtr`: The acc address of variable. This is the link from the data-entry
<     operation used.
<     - `bounds`: Used when copying just slice of array or array's bounds are not
<     encoded in type. They are in rank order where rank 0 is inner-most dimension.
<     - `dataClause`: Keeps track of the data clause the user used. This is because
<     the acc operations are decomposed. So a 'copy' clause is decomposed to both 
<     `acc.copyin` and `acc.copyout` operations, but both have dataClause that
<     specifies `acc_copy` in this field.
<     - `structured`: Flag to note whether this is associated with structured region
<     (parallel, kernels, data) or unstructured (enter data, exit data). This is
<     important due to spec specifically calling out structured and dynamic reference
<     counters (2.6.7).
<     - `implicit`: Whether this is an implicitly generated operation, such as copies
<     done to satisfy "Variables with Implicitly Determined Data Attributes" in 2.6.2.
<     - `name`: Holds the name of variable as specified in user clause (including bounds).
<   }];
< 
<   let assemblyFormat = [{
<     `accPtr` `(` $accPtr `:` type($accPtr) `)`
<     oilist(
<         `bounds` `(` $bounds `)`
<       | `to` `varPtr` `(` $varPtr `:` type($varPtr) `)`
<     ) attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.8 copyout clause
< //===----------------------------------------------------------------------===//
< def OpenACC_CopyoutOp : OpenACC_DataExitOp<"copyout",
<     "mlir::acc::DataClause::acc_copyout"> {
<   let summary = "Represents acc copyout semantics - reverse of copyin.";
< 
<   let extraClassDeclaration = [{
<     /// Check if this is a copyout with zero modifier.
<     bool isCopyoutZero();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.11 delete clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DeleteOp : OpenACC_DataExitOp<"delete",
<     "mlir::acc::DataClause::acc_delete"> {
<   let summary = "Represents acc delete semantics - reverse of create.";
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.7.13 detach clause
< //===----------------------------------------------------------------------===//
< def OpenACC_DetachOp : OpenACC_DataExitOp<"detach",
<     "mlir::acc::DataClause::acc_detach"> {
<   let summary = "Represents acc detach semantics - reverse of attach.";
< }
< 
369c75
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>     [AttrSizedOperandSegments]> {
410d115
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
416,421c121,144
<     /// The number of data operands.
<     unsigned getNumDataOperands();
< 
<     /// The i-th data operand passed.
<     Value getDataOperand(unsigned i);
<   }];
---
>     static StringRef getAsyncKeyword() { return "async"; }
>     static StringRef getAsyncAttrName() { return "asyncAttr"; }
>     static StringRef getWaitKeyword() { return "wait"; }
>     static StringRef getWaitAttrName() { return "waitAttr"; }
>     static StringRef getNumGangsKeyword() { return "num_gangs"; }
>     static StringRef getNumWorkersKeyword() { return "num_workers"; }
>     static StringRef getVectorLengthKeyword() { return "vector_length"; }
>     static StringRef getIfKeyword() { return "if"; }
>     static StringRef getSelfKeyword() { return "self"; }
>     static StringRef getSelfAttrName() { return "selfAttr"; }
>     static StringRef getReductionKeyword() { return "reduction"; }
>     static StringRef getCopyKeyword() { return "copy"; }
>     static StringRef getCopyinKeyword() { return "copyin"; }
>     static StringRef getCopyinReadonlyKeyword() { return "copyin_readonly"; }
>     static StringRef getCopyoutKeyword() { return "copyout"; }
>     static StringRef getCopyoutZeroKeyword() { return "copyout_zero"; }
>     static StringRef getCreateKeyword() { return "create"; }
>     static StringRef getCreateZeroKeyword() { return "create_zero"; }
>     static StringRef getNoCreateKeyword() { return "no_create"; }
>     static StringRef getPresentKeyword() { return "present"; }
>     static StringRef getDevicePtrKeyword() { return "deviceptr"; }
>     static StringRef getAttachKeyword() { return "attach"; }
>     static StringRef getPrivateKeyword() { return "private"; }
>     static StringRef getFirstPrivateKeyword() { return "firstprivate"; }
423,589d145
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `firstprivate` `(` $gangFirstPrivateOperands `:`
<             type($gangFirstPrivateOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `num_gangs` `(` $numGangs `:` type($numGangs) `)`
<       | `num_workers` `(` $numWorkers `:` type($numWorkers) `)`
<       | `private` `(` $gangPrivateOperands `:` type($gangPrivateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `vector_length` `(` $vectorLength `:` type($vectorLength) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region attr-dict-with-keyword
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.5.2 serial Construct
< //===----------------------------------------------------------------------===//
< 
< def OpenACC_SerialOp : OpenACC_Op<"serial",
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
<   let summary = "serial construct";
<   let description = [{
<     The "acc.serial" operation represents a serial construct block. It has
<     one region to be executed in serial on the current device.
< 
<     Example:
< 
<     ```mlir
<     acc.serial private(%c : memref<10xf32>) {
<       // serial region
<     }
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$async,
<                        UnitAttr:$asyncAttr,
<                        Variadic<IntOrIndex>:$waitOperands,
<                        UnitAttr:$waitAttr,
<                        Optional<I1>:$ifCond,
<                        Optional<I1>:$selfCond,
<                        UnitAttr:$selfAttr,
<                        OptionalAttr<OpenACC_ReductionOpAttr>:$reductionOp,
<                        Variadic<AnyType>:$reductionOperands,
<                        Variadic<AnyType>:$copyOperands,
<                        Variadic<AnyType>:$copyinOperands,
<                        Variadic<AnyType>:$copyinReadonlyOperands,
<                        Variadic<AnyType>:$copyoutOperands,
<                        Variadic<AnyType>:$copyoutZeroOperands,
<                        Variadic<AnyType>:$createOperands,
<                        Variadic<AnyType>:$createZeroOperands,
<                        Variadic<AnyType>:$noCreateOperands,
<                        Variadic<AnyType>:$presentOperands,
<                        Variadic<AnyType>:$devicePtrOperands,
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<AnyType>:$gangPrivateOperands,
<                        Variadic<AnyType>:$gangFirstPrivateOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
<                        OptionalAttr<DefaultValueAttr>:$defaultAttr);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let extraClassDeclaration = [{
<     /// The number of data operands.
<     unsigned getNumDataOperands();
< 
<     /// The i-th data operand passed.
<     Value getDataOperand(unsigned i);
<   }];
< 
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `firstprivate` `(` $gangFirstPrivateOperands `:`
<             type($gangFirstPrivateOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `private` `(` $gangPrivateOperands `:` type($gangPrivateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region attr-dict-with-keyword
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // 2.5.1 kernels Construct
< //===----------------------------------------------------------------------===//
< 
< def OpenACC_KernelsOp : OpenACC_Op<"kernels",
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
<   let summary = "kernels construct";
<   let description = [{
<     The "acc.kernels" operation represents a kernels construct block. It has
<     one region to be compiled into a sequence of kernels for execution on the
<     current device.
< 
<     Example:
< 
<     ```mlir
<     acc.kernels num_gangs(%c10) num_workers(%c10)
<         private(%c : memref<10xf32>) {
<       // kernels region
<     }
<     ```
<   }];
< 
<   let arguments = (ins Optional<IntOrIndex>:$async,
<                        UnitAttr:$asyncAttr,
<                        Variadic<IntOrIndex>:$waitOperands,
<                        UnitAttr:$waitAttr,
<                        Optional<IntOrIndex>:$numGangs,
<                        Optional<IntOrIndex>:$numWorkers,
<                        Optional<IntOrIndex>:$vectorLength,
<                        Optional<I1>:$ifCond,
<                        Optional<I1>:$selfCond,
<                        UnitAttr:$selfAttr,
<                        Variadic<AnyType>:$copyOperands,
<                        Variadic<AnyType>:$copyinOperands,
<                        Variadic<AnyType>:$copyinReadonlyOperands,
<                        Variadic<AnyType>:$copyoutOperands,
<                        Variadic<AnyType>:$copyoutZeroOperands,
<                        Variadic<AnyType>:$createOperands,
<                        Variadic<AnyType>:$createZeroOperands,
<                        Variadic<AnyType>:$noCreateOperands,
<                        Variadic<AnyType>:$presentOperands,
<                        Variadic<AnyType>:$devicePtrOperands,
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
<                        OptionalAttr<DefaultValueAttr>:$defaultAttr);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let extraClassDeclaration = [{
596,622c152
< 
<   let assemblyFormat = [{
<     oilist(
<         `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `async` `(` $async `:` type($async) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<           type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<           type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:` type($createZeroOperands) `)`
<       | `deviceptr` `(` $devicePtrOperands `:` type($devicePtrOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `num_gangs` `(` $numGangs `:` type($numGangs) `)`
<       | `num_workers` `(` $numWorkers `:` type($numWorkers) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `vector_length` `(` $vectorLength `:` type($vectorLength) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `self` `(` $selfCond `)`
<       | `if` `(` $ifCond `)`
<     )
<     $region attr-dict-with-keyword
<   }];
---
>   let hasCustomAssemblyFormat = 1;
630c160
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>     [AttrSizedOperandSegments]> {
663d192
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
677,694c206,220
<     oilist(
<         `if` `(` $ifCond `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<       | `copy` `(` $copyOperands `:` type($copyOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `copyin_readonly` `(` $copyinReadonlyOperands `:`
<             type($copyinReadonlyOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `copyout_zero` `(` $copyoutZeroOperands `:`
<             type($copyoutZeroOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<             type($createZeroOperands) `)`
<       | `no_create` `(` $noCreateOperands `:` type($noCreateOperands) `)`
<       | `present` `(` $presentOperands `:` type($presentOperands) `)`
<       | `deviceptr` `(` $deviceptrOperands `:` type($deviceptrOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `copy` `(` $copyOperands^ `:` type($copyOperands) `)` )?
>     ( `copyin` `(` $copyinOperands^ `:` type($copyinOperands) `)` )?
>     ( `copyin_readonly` `(` $copyinReadonlyOperands^ `:`
>         type($copyinReadonlyOperands) `)` )?
>     ( `copyout` `(` $copyoutOperands^ `:` type($copyoutOperands) `)` )?
>     ( `copyout_zero` `(` $copyoutZeroOperands^ `:`
>         type($copyoutZeroOperands) `)` )?
>     ( `create` `(` $createOperands^ `:` type($createOperands) `)` )?
>     ( `create_zero` `(` $createZeroOperands^ `:`
>         type($createZeroOperands) `)` )?
>     ( `no_create` `(` $noCreateOperands^ `:` type($noCreateOperands) `)` )?
>     ( `present` `(` $presentOperands^ `:` type($presentOperands) `)` )?
>     ( `deviceptr` `(` $deviceptrOperands^ `:` type($deviceptrOperands) `)` )?
>     ( `attach` `(` $attachOperands^ `:` type($attachOperands) `)` )?
739,740c265
<                        Variadic<AnyType>:$attachOperands,
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands);
---
>                        Variadic<AnyType>:$attachOperands);
751,762c276,284
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `copyin` `(` $copyinOperands `:` type($copyinOperands) `)`
<       | `create` `(` $createOperands `:` type($createOperands) `)`
<       | `create_zero` `(` $createZeroOperands `:`
<           type($createZeroOperands) `)`
<       | `attach` `(` $attachOperands `:` type($attachOperands) `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `copyin` `(` $copyinOperands^ `:` type($copyinOperands) `)` )?
>     ( `create` `(` $createOperands^ `:` type($createOperands) `)` )?
>     ( `create_zero` `(` $createZeroOperands^ `:`
>         type($createZeroOperands) `)` )?
>     ( `attach` `(` $attachOperands^ `:` type($attachOperands) `)` )?
796d317
<                        Variadic<OpenACC_PointerLikeTypeInterface>:$dataClauseOperands,
808,817c329,335
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `copyout` `(` $copyoutOperands `:` type($copyoutOperands) `)`
<       | `delete` `(` $deleteOperands `:` type($deleteOperands) `)`
<       | `detach` `(` $detachOperands `:` type($detachOperands) `)`
<       | `dataOperands` `(` $dataClauseOperands `:` type($dataClauseOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `copyout` `(` $copyoutOperands^ `:` type($copyoutOperands) `)` )?
>     ( `delete` `(` $deleteOperands^ `:` type($deleteOperands) `)` )?
>     ( `detach` `(` $detachOperands^ `:` type($detachOperands) `)` )?
830c348,349
<     [AttrSizedOperandSegments, RecursiveMemoryEffects]> {
---
>       [AttrSizedOperandSegments,
>        SingleBlockImplicitTerminator<"acc::YieldOp">]> {
851a371
> 
860,862d379
<                        UnitAttr:$hasGang,
<                        UnitAttr:$hasWorker,
<                        UnitAttr:$hasVector,
866c383,384
<                        Variadic<AnyType>:$reductionOperands);
---
>                        Variadic<AnyType>:$reductionOperands,
>                        DefaultValuedAttr<I64Attr, "0">:$exec_mapping);
872a391,393
>     static StringRef getCollapseAttrStrName() { return "collapse"; }
>     static StringRef getSeqAttrStrName() { return "seq"; }
>     static StringRef getIndependentAttrStrName() { return "independent"; }
873a395,396
>     static StringRef getExecutionMappingAttrStrName() { return "exec_mapping"; }
>     static StringRef getGangKeyword() { return "gang"; }
875a399,403
>     static StringRef getVectorKeyword() { return "vector"; }
>     static StringRef getWorkerKeyword() { return "worker"; }
>     static StringRef getTileKeyword() { return "tile"; }
>     static StringRef getPrivateKeyword() { return "private"; }
>     static StringRef getReductionKeyword() { return "reduction"; }
877d404
< 
879,892d405
<   let assemblyFormat = [{
<     oilist(
<         `gang` `` custom<GangClause>($gangNum, type($gangNum), $gangStatic, type($gangStatic), $hasGang)
<       | `worker` `` custom<WorkerClause>($workerNum, type($workerNum), $hasWorker)
<       | `vector` `` custom<VectorClause>($vectorLength, type($vectorLength), $hasVector)
<       | `private` `(` $privateOperands `:` type($privateOperands) `)`
<       | `tile` `(` $tileOperands `:` type($tileOperands) `)`
<       | `reduction` `(` $reductionOperands `:` type($reductionOperands) `)`
<     )
<     $region
<     ( `(` type($results)^ `)` )?
<     attr-dict-with-keyword
<   }];
< 
898c411
<     ParentOneOf<["ParallelOp, LoopOp, SerialOp"]>]> {
---
>     ParentOneOf<["ParallelOp, LoopOp"]>]> {
938,942c451,453
<     oilist(
<         `device_type` `(` $deviceTypeOperands `:` type($deviceTypeOperands) `)`
<       | `device_num` `(` $deviceNumOperand `:` type($deviceNumOperand) `)`
<       | `if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `device_type` `(` $deviceTypeOperands^ `:` type($deviceTypeOperands) `)` )?
>     ( `device_num` `(` $deviceNumOperand^ `:` type($deviceNumOperand) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
971,974c482,484
<     oilist(`device_type` `(` $deviceTypeOperands `:` type($deviceTypeOperands) `)`
<     |`device_num` `(` $deviceNumOperand `:` type($deviceNumOperand) `)`
<     |`if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `device_type` `(` $deviceTypeOperands^ `:` type($deviceTypeOperands) `)` )?
>     ( `device_num` `(` $deviceNumOperand^ `:` type($deviceNumOperand) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
1019,1028c529,536
<     oilist(
<         `if` `(` $ifCond `)`
<       | `async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       | `wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       | `device_type` `(` $deviceTypeOperands `:`
<           type($deviceTypeOperands) `)`
<       | `wait` `(` $waitOperands `:` type($waitOperands) `)`
<       | `host` `(` $hostOperands `:` type($hostOperands) `)`
<       | `device` `(` $deviceOperands `:` type($deviceOperands) `)`
<     )
---
>     ( `if` `(` $ifCond^ `)` )?
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `device_type` `(` $deviceTypeOperands^ `:`
>         type($deviceTypeOperands) `)` )?
>     ( `wait` `(` $waitOperands^ `:` type($waitOperands) `)` )?
>     ( `host` `(` $hostOperands^ `:` type($hostOperands) `)` )?
>     ( `device` `(` $deviceOperands^ `:` type($deviceOperands) `)` )?
1063,1066c571,573
<     oilist(`async` `(` $asyncOperand `:` type($asyncOperand) `)`
<       |`wait_devnum` `(` $waitDevnum `:` type($waitDevnum) `)`
<       |`if` `(` $ifCond `)`
<     ) attr-dict-with-keyword
---
>     ( `async` `(` $asyncOperand^ `:` type($asyncOperand) `)` )?
>     ( `wait_devnum` `(` $waitDevnum^ `:` type($waitDevnum) `)` )?
>     ( `if` `(` $ifCond^ `)` )? attr-dict-with-keyword
--- include/mlir/Dialect/OpenACC/CMakeLists.txt
5,9d4
< add_mlir_dialect(OpenACCOps acc)
< 
< add_mlir_doc(OpenACCOps OpenACCDialect Dialects/ -gen-dialect-doc -dialect=acc)
< add_dependencies(OpenACCDialectDocGen acc_common_td)
< 
10a6,9
> mlir_tablegen(OpenACCOpsDialect.h.inc -gen-dialect-decls -dialect=acc)
> mlir_tablegen(OpenACCOpsDialect.cpp.inc -gen-dialect-defs -dialect=acc)
> mlir_tablegen(OpenACCOps.h.inc -gen-op-decls)
> mlir_tablegen(OpenACCOps.cpp.inc -gen-op-defs)
13,16d11
< add_public_tablegen_target(MLIROpenACCEnumsIncGen)
< add_dependencies(mlir-headers MLIROpenACCEnumsIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS OpenACCOps.td)
19,26c14,16
< add_public_tablegen_target(MLIROpenACCAttributesIncGen)
< add_dependencies(mlir-headers MLIROpenACCAttributesIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS OpenACCTypeInterfaces.td)
< mlir_tablegen(OpenACCTypeInterfaces.h.inc -gen-type-interface-decls)
< mlir_tablegen(OpenACCTypeInterfaces.cpp.inc -gen-type-interface-defs)
< add_public_tablegen_target(MLIROpenACCTypeInterfacesIncGen)
< add_dependencies(mlir-headers MLIROpenACCTypeInterfacesIncGen)
---
> add_mlir_doc(OpenACCOps OpenACCDialect Dialects/ -gen-dialect-doc)
> add_public_tablegen_target(MLIROpenACCOpsIncGen)
> add_dependencies(OpenACCDialectDocGen acc_common_td)
--- include/mlir/Dialect/OpenACC/OpenACC.h
16d15
< #include "mlir/IR/BuiltinTypes.h"
22,26d20
< #include "mlir/Dialect/OpenACC/OpenACCTypeInterfaces.h.inc"
< #include "mlir/Interfaces/SideEffectInterfaces.h"
< 
< #define GET_TYPEDEF_CLASSES
< #include "mlir/Dialect/OpenACC/OpenACCOpsTypes.h.inc"
--- include/mlir/Dialect/EmitC
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/EmitC/IR and include/mlir/Dialect/EmitC/IR
--- include/mlir/Dialect/EmitC/CMakeLists.txt
--- include/mlir/Dialect/EmitC/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/EmitC/IR/EmitCBase.td include/mlir/Dialect/EmitC/IR/EmitCBase.td
33a34
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/EmitC/IR/EmitC.td include/mlir/Dialect/EmitC/IR/EmitC.td
47a48
>   // TODO: Disallow to apply the operator & (address of) to emitc.constant operations.
144a146
>   // TODO: Disallow empty constants.
--- include/mlir/Dialect/EmitC/IR/CMakeLists.txt
--- include/mlir/Dialect/EmitC/IR/EmitC.h
--- include/mlir/Dialect/EmitC/IR/EmitCAttributes.td
--- include/mlir/Dialect/EmitC/IR/EmitC.td
47a48
>   // TODO: Disallow to apply the operator & (address of) to emitc.constant operations.
144a146
>   // TODO: Disallow empty constants.
--- include/mlir/Dialect/EmitC/IR/EmitCTypes.td
--- include/mlir/Dialect/EmitC/IR/EmitCBase.td
33a34
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Transform
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR and include/mlir/Dialect/Transform/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Transforms and include/mlir/Dialect/Transform/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Utils and include/mlir/Dialect/Transform/Utils
--- include/mlir/Dialect/Transform/Utils
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Utils: RaggedArray.h
--- include/mlir/Dialect/Transform/Utils/DiagnosedSilenceableFailure.h
--- include/mlir/Dialect/Transform/Utils/Utils.h
--- include/mlir/Dialect/Transform/CMakeLists.txt
--- include/mlir/Dialect/Transform/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Transforms/Passes.h include/mlir/Dialect/Transform/Transforms/Passes.h
1c1
< //===- Passes.h - Transform dialect pass entry points -----------*- C++ -*-===//
---
> //===- CheckUses.h - Expensive transform value validity checks --*- C++ -*-===//
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Transforms/Passes.td include/mlir/Dialect/Transform/Transforms/Passes.td
35,44d34
< def InferEffectsPass : Pass<"transform-infer-effects"> {
<   let summary = "infer transform side effects for symbols";
<   let description = [{
<     This pass analyzes the definitions of transform dialect callable symbol
<     operations, such as `transform.named_sequence`, and annotates the symbol
<     arguments with attributes indicating the side effects that the nested
<     operations have on them.
<   }];
< }
< 
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/Transforms: TransformInterpreterPassBase.h
--- include/mlir/Dialect/Transform/Transforms/Passes.h
1c1
< //===- Passes.h - Transform dialect pass entry points -----------*- C++ -*-===//
---
> //===- CheckUses.h - Expensive transform value validity checks --*- C++ -*-===//
--- include/mlir/Dialect/Transform/Transforms/Passes.td
35,44d34
< def InferEffectsPass : Pass<"transform-infer-effects"> {
<   let summary = "infer transform side effects for symbols";
<   let description = [{
<     This pass analyzes the definitions of transform dialect callable symbol
<     operations, such as `transform.named_sequence`, and annotates the symbol
<     arguments with attributes indicating the side effects that the nested
<     operations have on them.
<   }];
< }
< 
--- include/mlir/Dialect/Transform/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Transform/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/CMakeLists.txt include/mlir/Dialect/Transform/IR/CMakeLists.txt
31,34d30
< add_mlir_interface(MatchInterfaces)
< add_dependencies(MLIRMatchInterfacesIncGen MLIRTransformInterfacesIncGen)
< add_mlir_doc(TransformInterfaces MatchOpInterfaces Dialects/ -gen-op-interface-docs)
< 
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR: MatchInterfaces.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR: MatchInterfaces.td
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR: TransformAttrs.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformAttrs.td include/mlir/Dialect/Transform/IR/TransformAttrs.td
23,35d22
< def MatchCmpIPredicateAttr : I32EnumAttr<
<     "MatchCmpIPredicate", "",
<     [
<       I32EnumAttrCase<"eq", 0>,
<       I32EnumAttrCase<"ne", 1>,
<       I32EnumAttrCase<"lt", 2>,
<       I32EnumAttrCase<"le", 3>,
<       I32EnumAttrCase<"gt", 4>,
<       I32EnumAttrCase<"ge", 5>,
<     ]> {
<   let cppNamespace = "::mlir::transform";
< }
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformDialect.h include/mlir/Dialect/Transform/IR/TransformDialect.h
245a246,247
> #include "mlir/Dialect/Transform/IR/TransformDialectEnums.h.inc"
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformDialect.td include/mlir/Dialect/Transform/IR/TransformDialect.td
26c26
<   let hasOperationAttrVerify = 1;
---
>   let useFoldAPI = kEmitFoldAdaptorFolder;
29,45d28
<       /// Name of the attribute attachable to the symbol table operation
<       /// containing named sequences. This is used to trigger verification.
<       constexpr const static llvm::StringLiteral
<           kWithNamedSequenceAttrName = "transform.with_named_sequence";
< 
<       /// Names of the attribute attachable to an operation so it can be
<       /// identified as root by the default interpreter pass.
<       constexpr const static llvm::StringLiteral
<           kTargetTagAttrName = "transform.target_tag";
< 
<       /// Names of the attributes indicating whether an argument of an external
<       /// transform dialect symbol is consumed or only read.
<       constexpr const static llvm::StringLiteral
<           kArgConsumedAttrName = "transform.consumed";
<       constexpr const static llvm::StringLiteral
<           kArgReadOnlyAttrName = "transform.readonly";
< 
124c107
< // Base class for ops that belong to the transform dialect. Ops defined in
---
> // Base class for ops that belong to the tranfsorm dialect. Ops defined in
Only in include/mlir/Dialect/Transform/IR: TransformEffects.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformInterfaces.h include/mlir/Dialect/Transform/IR/TransformInterfaces.h
14d13
< #include "mlir/Dialect/Transform/Utils/RaggedArray.h"
15a15
> 
20d19
< 
24d22
< class TransformResults;
30,32c28
<   TransformOptions() = default;
<   TransformOptions(const TransformOptions &) = default;
<   TransformOptions &operator=(const TransformOptions &) = default;
---
>   TransformOptions() {}
49,51d44
< using Param = Attribute;
< using MappedValue = llvm::PointerUnion<Operation *, Param, Value>;
< 
60d52
<                 const RaggedArray<MappedValue> &extraMapping = {},
96c88
<   using Param = transform::Param;
---
>   using Param = Attribute;
112,115d103
<   /// Mapping between a Value in the transform IR and the corrsponding list of
<   /// values in the payload IR. Also works for reverse mappings.
<   using ValueMapping = DenseMap<Value, SmallVector<Value>>;
< 
122,123d109
<     ValueMapping values;
<     ValueMapping reverseValues;
126,128c112,114
<   friend LogicalResult applyTransforms(Operation *, TransformOpInterface,
<                                        const RaggedArray<MappedValue> &,
<                                        const TransformOptions &);
---
>   friend LogicalResult applyTransforms(Operation *payloadRoot,
>                                        TransformOpInterface transform,
>                                        const TransformOptions &options);
135,142d120
<   /// Returns the number of extra mappings for the top-level operation.
<   size_t getNumTopLevelMappings() const { return topLevelMappedValues.size(); }
< 
<   /// Returns the position-th extra mapping for the top-level operation.
<   ArrayRef<MappedValue> getTopLevelMapping(size_t position) const {
<     return topLevelMappedValues[position];
<   }
< 
151,154d128
<   /// Returns the list of payload IR values that the given transform IR value
<   /// corresponds to.
<   ArrayRef<Value> getPayloadValues(Value handleValue) const;
< 
160,165d133
<   /// Populates `handles` with all handles pointing to the given payload IR
<   /// value. Returns success if such handles exist, failure otherwise.
<   LogicalResult
<   getHandlesForPayloadValue(Value payloadValue,
<                             SmallVectorImpl<Value> &handles) const;
< 
185,186d152
<   LogicalResult mapBlockArgument(BlockArgument argument,
<                                  ArrayRef<MappedValue> values);
197,202d162
<   /// Creates a new region scope for the given isolated-from-above region.
<   /// Unlike the non-isolated counterpart, there is no nesting expectation.
<   // Implementation note: this method is inline but implemented outside of the
<   // class body to comply with visibility and full-declaration requirements
<   inline RegionScope make_isolated_region_scope(Region &region);
< 
212,213c172
<     /// transform IR region, and restores the mapping that existed before
<     /// entering this scope.
---
>     /// transform IR region.
216,217d174
<       if (storedMappings.has_value())
<         state.mappings.swap(*storedMappings);
224,226d180
<     /// Tag structure for differentiating the constructor for isolated regions.
<     struct Isolated {};
< 
228c182
<     /// transform IR region and payload IR objects.
---
>     /// transform IR region and payload IR operations.
235,236c189
<       assert(((state.regionStack.size() == 1 && !state.regionStack.back()) ||
<               state.regionStack.back()->isProperAncestor(&region)) &&
---
>       assert(state.regionStack.back()->isProperAncestor(&region) &&
242,254d194
<     /// Creates a new scope for mappings between values defined in the given
<     /// isolated-from-above transform IR region and payload IR objects.
<     RegionScope(TransformState &state, Region &region, Isolated)
<         : state(state), region(&region) {
<       // Store the previous mapping stack locally.
<       storedMappings = llvm::SmallDenseMap<Region *, Mappings>();
<       storedMappings->swap(state.mappings);
<       state.mappings.try_emplace(this->region);
< #if LLVM_ENABLE_ABI_BREAKING_CHECKS
<       state.regionStack.push_back(this->region);
< #endif // LLVM_ENABLE_ABI_BREAKING_CHECKS
<     }
< 
261,266d200
<     /// Local copy of the mappings that existed before entering the current
<     /// region. Used only when the current region is isolated so we don't
<     /// accidentally look up the values defined outside the isolated region.
<     std::optional<llvm::SmallDenseMap<Region *, Mappings>> storedMappings =
<         std::nullopt;
< 
268d201
<     friend RegionScope TransformState::make_isolated_region_scope(Region &);
318,320d250
<     ///
<     /// Note: This function does not update value handles. None of the original
<     /// op's results are allowed to be mapped to any value handle.
323,327d252
<     /// Replaces the given payload value with another value. If the replacement
<     /// value is null, removes the association of the payload value with its
<     /// handle. Returns failure if the value is not associated with any handle.
<     LogicalResult replacePayloadValue(Value value, Value replacement);
< 
380d304
<                  const RaggedArray<MappedValue> &extraMappings = {},
383c307
<   /// Returns the mappings frame for the region in which the value is defined.
---
>   /// Returns the mappings frame for the reigon in which the value is defined.
405,408c329,331
<   /// Updates the state to include the associations between op results and the
<   /// provided result of applying a transform op.
<   LogicalResult updateStateFromResults(const TransformResults &results,
<                                        ResultRange opResults);
---
>   /// Removes the mapping between the given payload IR operation and the given
>   /// transform IR value.
>   void dropReverseMapping(Mappings &mappings, Operation *op, Value value);
429,441d351
<   /// Similarly, operation handles may be invalidate and should not be used
<   /// after a transform that consumed a value handle pointing to a payload value
<   /// defined by the operation as either block argument or op result. For
<   /// example, in the following sequence, the last transform operation rewrites
<   /// the callee to not return a specified result:
<   ///
<   ///   %0 = transform.find_call "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///   transform.drop_call_result_from_signature %1[0]
<   ///
<   /// which requires the call operations to be recreated. Therefore, the handle
<   /// %0 becomes associated with a dangling pointer and should not be used.
<   ///
447,487d356
<   /// Sets the payload IR values association with the given transform IR value
<   /// (handle). A payload value may be associated with multiple handles as long
<   /// as at most one of them is consumed by further transformations. For
<   /// example, a hypothetical "get results of calls to function with the given
<   /// name" transform may be performed twice in a row producing handles pointing
<   /// to the same values:
<   ///
<   ///   %0 = transform.find_results_of_calling "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///
<   /// which is valid by itself. However, calling a hypothetical "erase value
<   /// producer" transform on both handles:
<   ///
<   ///   transform.erase_value_produce %0
<   ///   transform.erase_value_produce %1
<   ///
<   /// is invalid provided the transformation "consumes" the handle as expressed
<   /// by side effects (which themselves reflect the semantics of the transform
<   /// erasing the producer and making the handle dangling). Practically, a
<   /// transformation consuming a handle means the associated payload value may
<   /// no longer exist.
<   ///
<   /// Similarly, value handles are invalidated and should not be used after a
<   /// transform that consumed an operation handle pointing to the payload IR
<   /// operation defining the values associated the value handle, as either block
<   /// arguments or op results, or any ancestor operation. For example,
<   ///
<   ///   %0 = transform.find_call "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///   transform.rewrite_and_rename %0 { new_name = "func" }
<   ///
<   /// makes %1 unusable after the last transformation if it consumes %0. When an
<   /// operation handle is consumed, it usually indicates that the operation was
<   /// destroyed or heavily modified, meaning that the values it defines may no
<   /// longer exist.
<   ///
<   /// Returns failure if the payload values do not satisfy the conditions
<   /// associated with the type of the handle value. The value is expected to
<   /// have a type implementing TransformValueHandleTypeInterface.
<   LogicalResult setPayloadValues(Value handle, ValueRange payloadValues);
< 
494,497c363,364
<   /// Forgets the payload IR ops associated with the given transform IR value,
<   /// as well as any association between value handles and the results of said
<   /// payload IR op.
<   void forgetMapping(Value opHandle, ValueRange origOpFlatResults);
---
>   /// Forgets the payload IR ops associated with the given transform IR value.
>   void removePayloadOps(Value value);
499,503c366,370
<   void forgetValueMapping(Value valueHandle,
<                           ArrayRef<Operation *> payloadOperations);
< 
<   /// Replaces the given payload op with another op. If the replacement op is
<   /// null, removes the association of the payload op with its handle.
---
>   /// Updates the payload IR ops associated with the given transform IR value.
>   /// The callback function is called once per associated operation and is
>   /// expected to return the modified operation or nullptr. In the latter case,
>   /// the corresponding operation is no longer associated with the transform IR
>   /// value.
505,512c372,376
<   /// Note: This function does not update value handles. None of the original
<   /// op's results are allowed to be mapped to any value handle.
<   LogicalResult replacePayloadOp(Operation *op, Operation *replacement);
< 
<   /// Replaces the given payload value with another value. If the replacement
<   /// value is null, removes the association of the payload value with its
<   /// handle.
<   LogicalResult replacePayloadValue(Value value, Value replacement);
---
>   /// Returns failure if the payload does not satisfy the conditions associated
>   /// with the type of the handle value.
>   LogicalResult
>   updatePayloadOps(Value value,
>                    function_ref<Operation *(Operation *)> callback);
518,533c382,385
<   /// errors if they are used. If `throughValue` is passed, record the fact that
<   /// an op handle was invalidated because a value handle associated with
<   /// results of the payload op or its block arguments was invalidated.
<   void recordOpHandleInvalidation(OpOperand &consumingHandle,
<                                   ArrayRef<Operation *> potentialAncestors,
<                                   Value throughValue = nullptr);
<   void recordOpHandleInvalidationOne(OpOperand &handle,
<                                      ArrayRef<Operation *> potentialAncestors,
<                                      Operation *payloadOp, Value otherHandle,
<                                      Value throughValue = nullptr);
< 
<   void recordValueHandleInvalidationByOpHandleOne(
<       OpOperand &opHandle, ArrayRef<Operation *> potentialAncestors,
<       Value payloadValue, Value valueHandle);
< 
<   void recordValueHandleInvalidation(OpOperand &valueHandle);
---
>   /// errors if they are used.
>   void recordHandleInvalidation(OpOperand &handle);
>   void recordHandleInvalidationOne(OpOperand &handle, Operation *payloadOp,
>                                    Value otherHandle);
554,558d405
<   /// Extra mapped values (payload operations, values or parameters) to be
<   /// associated with additional entry block arguments of the top-level
<   /// transform operation.
<   RaggedArray<MappedValue> topLevelMappedValues;
< 
573,584d419
< 
<   /// This cache stores operation names for operations that are tracked in the
<   /// transform dialect state. It is used to detect missing memory side effects
<   /// and op tracking.
<   ///
<   /// All tracked ops are added to this cache before a transform op is applied.
<   /// After the application of the transform op, the names of all tracked ops
<   /// are compared with the names in the cache. If there is a mismatch (or a
<   /// crash), op tracking is missing somewhere. This is typically a missing
<   /// "consumesHandle" side effect or a pattern that removes an op without
<   /// notifying a TrackingListener.
<   DenseMap<Operation *, OperationName> cachedNames;
596,597c431,432
<   /// by the transformation exactly once in case of transformation succeeding.
<   /// The value must have a type implementing TransformHandleTypeInterface.
---
>   /// by the transformation exactly once. The value must have a type
>   /// implementing TransformHandleTypeInterface.
602,603c437,438
<   /// the transformation exactly once in case of transformation succeeding. The
<   /// value must have a type implementing TransformParamTypeInterface.
---
>   /// the transformation exactly once. The value must have a type implementing
>   /// TransformParamTypeInterface.
606,623d440
<   /// Indicates that the result of the transform IR op at the given position
<   /// corresponds to the given range of payload IR values. Each result must be
<   /// set by the transformation exactly once in case of transformation
<   /// succeeding. The value must have a type implementing
<   /// TransformValueHandleTypeInterface.
<   void setValues(OpResult handle, ValueRange values);
< 
<   /// Indicates that the result of the transform IR op at the given position
<   /// corresponds to the given range of mapped values. All mapped values are
<   /// expected to be compatible with the type of the result, e.g., if the result
<   /// is an operation handle, all mapped values are expected to be payload
<   /// operations.
<   void setMappedValues(OpResult handle, ArrayRef<MappedValue> values);
< 
<   /// Sets the currently unset results to empty lists of the kind expected by
<   /// the corresponding results of the given `transform` op.
<   void setRemainingToEmpty(TransformOpInterface transform);
< 
640,644d456
<   /// Gets the list of payload IR values associated with the result identified
<   /// by its number in the list of operation results. The result must have been
<   /// set to be associated with payload IR values.
<   ArrayRef<Value> getValues(unsigned resultNumber) const;
< 
646,647c458,459
<   /// operation results is associated with a list of parameters, `false`
<   /// otherwise.
---
>   /// operation results is associated with a list of parameters, `false` if it
>   /// is associated with the list of payload IR operations.
651,655d462
<   /// operation results is associated with a list of payload IR value, `false`
<   /// otherwise.
<   bool isValue(unsigned resultNumber) const;
< 
<   /// Returns `true` if the result identified by its number in the list of
659,667c466,484
<   /// Pointers to payload IR ops that are associated with results of a transform
<   /// IR op.
<   RaggedArray<Operation *> operations;
< 
<   /// Parameters that are associated with results of the transform IR op.
<   RaggedArray<Param> params;
< 
<   /// Payload IR values that are associated with results of a transform IR op.
<   RaggedArray<Value> values;
---
>   /// Storage for pointers to payload IR ops that are associated with results of
>   /// a transform IR op. `segments` contains as many entries as the transform IR
>   /// op has results, even if some of them are not associated with payload IR
>   /// operations. Each entry is a reference to a contiguous segment in the
>   /// `operations` list that contains the pointers to operations. This allows
>   /// for operations to be stored contiguously without nested vectors and for
>   /// different segments to be set in any order.
>   SmallVector<ArrayRef<Operation *>, 2> segments;
>   SmallVector<Operation *> operations;
> 
>   /// Storage for parameters that are associated with results of the transform
>   /// IR op. `paramSegments` contains as many entries as the transform IR op has
>   /// results, even if some of them are not associated with parameters. Each
>   /// entry is a reference to a contiguous segment in the `params` list that
>   /// contains the actual parameters. This allows for parameters to be stored
>   /// contiguously without nested vectors and for different segments to be set
>   /// in any order.
>   SmallVector<ArrayRef<TransformState::Param>, 2> paramSegments;
>   SmallVector<TransformState::Param> params;
670,672d486
< /// Creates a RAII object the lifetime of which corresponds to the new mapping
< /// for transform IR values defined in the given region. Values defined in
< /// surrounding regions remain accessible.
677,684d490
< /// Creates a RAII object the lifetime of which corresponds to the new mapping
< /// for transform IR values defined in the given isolated-from-above region.
< /// Values defined in surrounding regions cannot be accessed.
< TransformState::RegionScope
< TransformState::make_isolated_region_scope(Region &region) {
<   return RegionScope(*this, region, RegionScope::Isolated());
< }
< 
698,708d503
< 
< /// Populates `mappings` with mapped values associated with the given transform
< /// IR values in the given `state`.
< void prepareValueMappings(
<     SmallVectorImpl<SmallVector<transform::MappedValue>> &mappings,
<     ValueRange values, const transform::TransformState &state);
< 
< /// Populates `results` with payload associations that match exactly those of
< /// the operands to `block`'s terminator.
< void forwardTerminatorOperands(Block *block, transform::TransformState &state,
<                                transform::TransformResults &results);
756,757d550
< class ApplyToEachResultList;
< 
763c556
< ///       ApplyToEachResultList &results, TransformState &state)
---
> ///       SmallVector<Operation*> &results, state)
770c563
< /// zero, one or multiple operations depending on the number of results expected
---
> /// zero, one or multiple operations depending on the number of resultd expected
805,828d597
< /// Trait implementing the applyToOne function required by TransformEachOpTrait
< /// by greedily applying a set of patterns to each target payload operation.
< /// This requires the transform operation to implement TransformEachOpTrait and
< /// to provide the following method:
< ///   - void populatePatterns(RewritePatternSet &)
< /// that populates the given object with the patterns to apply. This is an
< /// instance method that can depend on the transform operation attributes.
< ///
< /// The payload operation is expected to have the IsolatedFromAboveTrait, which
< /// is a requirement of the pattern rewriter. If it does not, or if pattern
< /// application fails, the transform fails definitively as the rewriter will
< /// have likely left the payload IR in some intermediate state that precludes
< /// further transformation.
< template <typename OpTy>
< class TransformWithPatternsOpTrait
<     : public OpTrait::TraitBase<OpTy, TransformWithPatternsOpTrait> {
< public:
<   DiagnosedSilenceableFailure applyToOne(Operation *target,
<                                          ApplyToEachResultList &results,
<                                          TransformState &state);
< 
<   static LogicalResult verifyTrait(Operation *op);
< };
< 
832c601
< /// effect. A Read effect from this resource means accessing the mapping. A Free
---
> /// effet. A Read effect from this resource means accessing the mapping. A Free
839c608
< /// modified, potentially erased, by the previous transformations.
---
> /// modified, potentially erased, by the previous tranfsormations.
878,882d646
< /// Populates `consumedArguments` with positions of `block` arguments that are
< /// consumed by the operations in the `block`.
< void getConsumedBlockArguments(
<     Block &block, llvm::SmallDenseSet<unsigned> &consumedArguments);
< 
981c745
< using ApplyToEachResult = MappedValue;
---
> using ApplyToEachResult = llvm::PointerUnion<Operation *, Attribute>;
1005,1007d768
<       } else if constexpr (std::is_convertible_v<decltype(*std::begin(range)),
<                                                  Value>) {
<         results.push_back(element.template get<Value>());
1015,1017d775
<   // Using ApplyToEachResult that can be implicitly constructed from a Value but
<   // not from a concrete Op that is implicitly convertible to a Value to avoid
<   // ambiguity.
1020d777
<   void push_back(ApplyToEachResult r) { results.push_back(r); }
1120,1127d876
< /// Applies patterns configured by `populatePatterns` greedily to the contents
< /// of `target`. Reports (definite) errors at the location of `transformOp`.
< /// Sets up `results` to point to `target` after pattern application on success.
< DiagnosedSilenceableFailure transformWithPatternsApply(
<     Operation *transformOp, Operation *target, ApplyToEachResultList &results,
<     TransformState &state,
<     function_ref<void(RewritePatternSet &)> populatePatterns);
< 
1149,1150d897
<       else if (r.getType().isa<TransformValueHandleTypeInterface>())
<         transformResults.setValues(r, ValueRange());
1188,1209d934
<   return success();
< }
< 
< template <typename OpTy>
< mlir::DiagnosedSilenceableFailure
< mlir::transform::TransformWithPatternsOpTrait<OpTy>::applyToOne(
<     Operation *target, ApplyToEachResultList &results, TransformState &state) {
<   return detail::transformWithPatternsApply(
<       this->getOperation(), target, results, state,
<       [this](RewritePatternSet &patterns) {
<         cast<OpTy>(this->getOperation()).populatePatterns(patterns);
<       });
< }
< 
< template <typename OpTy>
< mlir::LogicalResult
< mlir::transform::TransformWithPatternsOpTrait<OpTy>::verifyTrait(
<     Operation *op) {
<   if (!op->hasTrait<mlir::transform::TransformEachOpTrait>()) {
<     return op->emitOpError()
<            << "TransformWithPatternsOpTrait requires TransformEachOpTrait";
<   }
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformInterfaces.td include/mlir/Dialect/Transform/IR/TransformInterfaces.td
140,143c140,143
<     Types that can be used for the Transform dialect operation handle values.
<     Such types define the properties of Payload IR operations associated with
<     the handle. A user of such a handle can assume that these properties have
<     been verified for any Payload IR operation associated with it.
---
>     Types that can be used for the Transform dialect handle values. Such types
>     define the properties of Payload IR operations associated with the handle.
>     A user of such a handle can assume that these properties have been verified
>     for any Payload IR operation associated with it.
158,168d157
< def TransformValueHandleTypeInterface
<     : TransformTypeInterfaceBase<"TransformValueHandleTypeInterface",
<                                  "::mlir::Value"> {
<   let description = [{
<     Types that can be used for the Transform dialect handle values pointing to
<     Payload IR values. Such types define the properties of Payload IR values
<     associated with the handle. Users of such a handle can assume that these
<     properties have been verified for any Payload IR value associated with it.
<   }];
< }
< 
171,172c160
<              TransformHandleTypeInterface.predicate,
<              TransformValueHandleTypeInterface.predicate]>,
---
>              TransformHandleTypeInterface.predicate]>,
181,184d168
<   let cppNamespace = "::mlir::transform";
< }
< 
< def TransformWithPatternsOpTrait : NativeOpTrait<"TransformWithPatternsOpTrait"> {
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformOps.h include/mlir/Dialect/Transform/IR/TransformOps.h
13,14d12
< #include "mlir/Dialect/Transform/IR/MatchInterfaces.h"
< #include "mlir/Dialect/Transform/IR/TransformAttrs.h"
17d14
< #include "mlir/IR/FunctionInterfaces.h"
20d16
< #include "mlir/IR/PatternMatch.h"
22d17
< #include "mlir/Interfaces/CallInterfaces.h"
34,88d28
< using SequenceBodyBuilderArgsFn =
<     ::llvm::function_ref<void(::mlir::OpBuilder &, ::mlir::Location,
<                               ::mlir::BlockArgument, ::mlir::ValueRange)>;
< 
< /// A listener that updates a TransformState based on IR modifications. This
< /// listener can be used during a greedy pattern rewrite to keep the transform
< /// state up-to-date.
< class TrackingListener : public RewriterBase::Listener,
<                          public TransformState::Extension {
< public:
<   /// Create a new TrackingListener for usage in the specified transform op.
<   explicit TrackingListener(TransformState &state, TransformOpInterface op)
<       : TransformState::Extension(state), transformOp(op) {}
< 
< protected:
<   /// Return a replacement payload op for the given op, which is going to be
<   /// replaced with the given values. By default, if all values are defined by
<   /// the same newly-created op, which also has the same type as the given op,
<   /// that defining op is used as a replacement.
<   virtual Operation *findReplacementOp(Operation *op,
<                                        ValueRange newValues) const;
< 
<   /// Notify the listener that the pattern failed to match the given operation,
<   /// and provide a callback to populate a diagnostic with the reason why the
<   /// failure occurred.
<   LogicalResult
<   notifyMatchFailure(Location loc,
<                      function_ref<void(Diagnostic &)> reasonCallback) override;
< 
<   /// This function is called when a tracked payload op is dropped because no
<   /// replacement op was found. Derived classes can implement this function for
<   /// custom error handling.
<   virtual void notifyPayloadReplacementNotFound(Operation *op,
<                                                 ValueRange values) {}
< 
<   /// Return "true" if the given op is a new op.
<   bool isNewOp(Operation *op) const;
< 
<   /// Return the single op that defines all given values (if any).
<   static Operation *getCommonDefiningOp(ValueRange values);
< 
< private:
<   void notifyOperationInserted(Operation *op) override;
< 
<   void notifyOperationRemoved(Operation *op) override;
< 
<   void notifyOperationReplaced(Operation *op, ValueRange newValues) override;
< 
<   /// Ops that were newly created during the transform.
<   DenseMap<OperationName, DenseSet<Operation *>> newOps;
< 
<   /// The transform op in which this TrackingListener is used.
<   TransformOpInterface transformOp;
< };
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformOps.td include/mlir/Dialect/Transform/IR/TransformOps.td
12d11
< include "mlir/Interfaces/CallInterfaces.td"
16,17d14
< include "mlir/Interfaces/SideEffectInterfaces.td"
< include "mlir/IR/FunctionInterfaces.td"
20d16
< include "mlir/Dialect/Transform/IR/MatchInterfaces.td"
22a19
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
30c27
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
---
>      FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
120,182d116
< def ForeachMatchOp : TransformDialectOp<"foreach_match", [
<     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<     DeclareOpInterfaceMethods<SymbolUserOpInterface>,
<     DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Applies named sequences when a named matcher succeeds";
<   let description = [{
<     Given a pair of co-indexed lists of transform dialect symbols (such as
<     `transform.named_sequence`), walks the payload IR associated with the root
<     handle and interprets the symbols as matcher/action pairs by applying the
<     body of the corresponding symbol definition. The symbol from the first list
<     is the matcher part: if it results in a silenceable error, the error is
<     silenced and the next matcher is attempted. Definite failures from any
<     matcher stop the application immediately and are propagated unconditionally.
<     If none of the matchers succeeds, the next payload operation in walk order
<     (post-order at the moment of writing, double check `Operation::walk`) is
<     matched. If a matcher succeeds, the co-indexed action symbol is applied and
<     the following matchers are not applied to the same payload operation. If the
<     action succeeds, the next payload operation in walk order is matched. If it
<     fails, both silenceable and definite errors are propagated as the result of
<     this op.
< 
<     The matcher symbol must take one operand of a type that implements the same
<     transform dialect interface as the `root` operand (a check is performed at
<     application time to see if the associated payload satisfies the constraints
<     of the actual type). It must not consume the operand as multiple matchers
<     may be applied. The matcher may produce any number of results. The action
<     symbol paired with the matcher must take the same number of arguments as the
<     matcher has results, and these arguments must implement the same transform
<     dialect interfaces, but not necessarily have the exact same type (again, a
<     check is performed at application time to see if the associated payload
<     satisfies the constraints of actual types on both sides). The action symbol
<     may not have results. The actions are expected to only modify payload
<     operations nested in the `root` payload operations associated with the
<     operand of this transform operation.
< 
<     This operation consumes the operand and produces a new handle associated
<     with the same payload. This is necessary to trigger invalidation of handles
<     to any of the payload operations nested in the payload operations associated
<     with the operand, as those are likely to be modified by actions. Note that
<     the root payload operation associated with the operand are not matched.
< 
<     The operation succeeds if none of the matchers produced a definite failure
<     during application and if all of the applied actions produced success. Note
<     that it also succeeds if all the matchers failed on all payload operations,
<     i.e. failure to apply is not an error. The operation produces a silenceable
<     failure if any applied action produced a silenceable failure. In this case,
<     the resulting handle is associated with an empty payload. The operation
<     produces a definite failure if any of the applied matchers or actions
<     produced a definite failure.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$root,
<                        SymbolRefArrayAttr:$matchers,
<                        SymbolRefArrayAttr:$actions);
<   let results = (outs TransformHandleTypeInterface:$updated);
< 
<   let assemblyFormat =
<       "`in` $root custom<ForeachMatchSymbols>($matchers, $actions) "
<       "attr-dict `:` functional-type($root, $updated)";
< 
<   let hasVerifier = 1;
< }
< 
279,295d212
< def GetDefiningOp : TransformDialectOp<"get_defining_op",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
<   let summary = "Get handle to the defining op of a value";
<   let description = [{
<     The handle defined by this Transform op corresponds to the defining op of
<     the targeted value.
< 
<     This transform fails silently if the targeted value is a block argument.
<   }];
< 
<   let arguments = (ins TransformValueHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$result);
<   let assemblyFormat = "$target attr-dict `:` "
<                        "functional-type(operands, results)";
< }
< 
311c228
<   let results = (outs TransformHandleTypeInterface:$producer);
---
>   let results = (outs TransformHandleTypeInterface:$parent);
316,420d232
< def GetResultOp : TransformDialectOp<"get_result",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
<   let summary = "Get handle to the a result of the targeted op";
<   let description = [{
<     The handle defined by this Transform op corresponds to the OpResult with
<     `result_number` that is defined by the given `target` operation.
<     
<     This transform fails silently if the targeted operation does not have enough
<     results. It reads the target handle and produces the result handle.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                        I64Attr:$result_number);
<   let results = (outs TransformValueHandleTypeInterface:$result);
<   let assemblyFormat = "$target `[` $result_number `]` attr-dict `:` "
<                        "functional-type(operands, results)";
< }
< 
< def IncludeOp : TransformDialectOp<"include",
<     [CallOpInterface,
<      MatchOpInterface,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<SymbolUserOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Includes a named transform sequence";
<   let description = [{
<     The application of this transform operation is equivalent to applying the
<     operations contained in the named transform sequence with operands being
<     remapped to block arguments. The behavior of the operation when a
<     transformation in the included named sequence produces a silenceable error
<     is controlled by the `failure_propagation_mode` attribute. When set to
<     `propagate`, the failure of any nested transformation in the sequence
<     implies immediate failure of the entire sequence with a silenceable error,
<     and no further transformation is attempted. When set to `suppress`,
<     silenceable errors in nested operations are ignored and further
<     transformations are applied. Beware that even silenceable errors may leave
<     the payload IR in a state unsuitable for further transformations. It is the
<     responsibility of the user to ensure the following transformations are
<     robust enough when errors are suppressed. Definite errors are propagated
<     immediately regardless of the mode. The objects associated with the results
<     of this operation are the same as those associated with the operands of the
<     `transform.yield` in the referenced named sequence.
<   }];
< 
<   let arguments = (ins SymbolRefAttr:$target,
<                        FailurePropagationMode:$failure_propagation_mode,
<                        Variadic<Transform_AnyHandleOrParamType>:$operands);
<   let results = (outs Variadic<Transform_AnyHandleOrParamType>:$results);
<   
<   let assemblyFormat =
<       "$target `failures` `(` $failure_propagation_mode `)`"
<       "`(` $operands `)` attr-dict `:` functional-type($operands, $results)";
< 
<   let extraClassDeclaration = [{
<     ::mlir::CallInterfaceCallable getCallableForCallee() {
<       return getTarget();
<     }
< 
<     ::mlir::Operation::operand_range getArgOperands() {
<       return getOperands();
<     }
<   }];
< }
< 
< def MatchOperationNameOp : TransformDialectOp<"match.operation_name",
<     [SingleOpMatcher,
<      MatchOpInterface,
<      MemoryEffectsOpInterface]> {
<   let summary = "Matches a single operation of one of the given kinds";
<   let description = [{
<     Succeeds if the operation associated with the operand handle has one of the
<     given operation names. Produces a silenceable failure otherwise.
< 
<     If more than one payload operation is associated with the operand handle,
<     produces a definite failure.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$operand_handle,
<                        StrArrayAttr:$op_names);
<   let assemblyFormat =
<       "$operand_handle $op_names attr-dict `:` type($operand_handle)";
<   let extraClassDeclaration = SingleOpMatcher.extraDeclaration;
< }
< 
< def MatchParamCmpIOp : Op<Transform_Dialect, "match.param.cmpi", [
<     DeclareOpInterfaceMethods<TransformOpInterface>,
<     MatchOpInterface,
<     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<     SameTypeOperands]> {
<   let summary =
<     "Matches if two parameter lists are associated with the same value";
<   let description = [{
<     Succeeds if all of the co-indexed values associated with the given
<     parameters relate as specified by the predicate (greater than, less than,
<     equal to, or their combinations). Comparison treats all values as signed.
<     Produces a silenceable failure otherwise.
<   }];
<   let arguments = (ins TransformParamTypeInterface:$param,
<                        TransformParamTypeInterface:$reference,
<                        MatchCmpIPredicateAttr:$predicate);
<   let assemblyFormat =
<       "$predicate $param `,` $reference attr-dict `:` type($param)";
< }
< 
444,511d255
< def NamedSequenceOp : TransformDialectOp<"named_sequence",
<     [CallableOpInterface,
<      FunctionOpInterface,
<      IsolatedFromAbove,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Named transform sequence that can be included elsewhere";
<   let description = [{
<     Defines a named (callable, function-like) sequence of other Transform
<     dialect operations that can be included using `transform.include` as part of
<     another Transform dialect construct. This sequence is not processed
<     immediately but rather dispatched to when the inclusion is processed. The
<     arguments and results can be used to communicate a subset of mapping into
<     the named sequence. The sequence must consist of a single block and end with
<     a `transform.yield` terminator. The operands of the terminator become the
<     results of the `transform.include`.
< 
<     When dispatched to, the operations in the named sequence are executed one by
<     one, similarly to the regular unnamed sequence. The failure propagation mode
<     is specified on the `transform.include`. Different inclusions may use
<     different failure propagation modes. This transform operation always
<     succeeds by itself, but the inclusion may fail if any of the operations
<     fail.
< 
<     Named sequences can only appear at the top-level of the Transform dialect
<     nesting structure. That is, they cannot be nested in other Transform dialect
<     operations. Furthermore, one of the ancestors must have the `SymbolTable`
<     trait and have the `transform.with_named_sequence` attribute attached.
< 
<     Named sequences may include other named sequences via `transform.include`,
<     but recursion is *not* allowed.
<   }];
<   
<   let arguments = (ins
<     SymbolNameAttr:$sym_name,
<     TypeAttrBase<"::mlir::FunctionType",
<                  "function type attribute">:$function_type,
<     OptionalAttr<StrAttr>:$sym_visibility,
<     OptionalAttr<DictArrayAttr>:$arg_attrs,
<     OptionalAttr<DictArrayAttr>:$res_attrs);
<   let regions = (region MaxSizedRegion<1>:$body);
< 
<   let hasCustomAssemblyFormat = 1;
<   let hasVerifier = 1;
< 
<   let extraClassDeclaration = [{
<     ::llvm::ArrayRef<::mlir::Type> getArgumentTypes() {
<       return getFunctionType().getInputs();
<     }
<     ::llvm::ArrayRef<::mlir::Type> getResultTypes() {
<       return getFunctionType().getResults();
<     }
< 
<     ::mlir::Region *getCallableRegion() {
<       return &getBody();
<     }
<     ::llvm::ArrayRef<::mlir::Type> getCallableResults() {
<       return getFunctionType().getResults();
<     }
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
<   }];
< }
< 
544,561d287
< def ParamConstantOp : Op<Transform_Dialect, "param.constant", [
<     MatchOpInterface,
<     DeclareOpInterfaceMethods<TransformOpInterface>,
<     MemoryEffectsOpInterface,
<     ParamProducerTransformOpTrait]> {
<   let summary = "Produces a new transform dialect parameter value associated "
<                 "with the given attribute";
<   let description = [{
<     Produces a new transform dialect parameter associated with the singleton
<     list containing the given attribute. The operation itself always succeeds,
<     but the general association check may fail if the parameter type does not
<     accept the given kind of attribute as valid.
<   }];
<   let arguments = (ins AnyAttr:$value);
<   let results = (outs TransformParamTypeInterface:$param);
<   let assemblyFormat = "$value attr-dict `->` type($param)";
< }
< 
616a343
> 
660,661c387
<      SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">,
<      AttrSizedOperandSegments]> {
---
>      SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">]> {
666,672c392,397
<     corresponds to a group of operations or values in the payload IR, or to a
<     group of parameters, depending on the type of the value. The behavior of the
<     operation when a nested transformation produces a silenceable error is
<     controlled by the `failure_propagation_mode` attribute. When set to
<     `propagate`, the failure of any nested transformation in the sequence
<     implies immediate failure of the entire sequence with a silenceable error,
<     and no further transformation is attempted. When set to `suppress`,
---
>     corresponds to an operation or a group of operations in the payload IR.
>     The behavior of the operation when a nested transformation produces a
>     silenceable error is controlled by the `failure_propagation_mode` attribute.
>     When set to `propagate`, the failure of any nested transformation in the
>     sequence implies immediate failure of the entire sequence with a silenceable
>     error, and no further transformation is attempted. When set to `suppress`,
675,680c400,406
<     the payload IR in a state unsuitable for further transformations. It is the
<     responsibility of the caller to ensure the following transformations are
<     robust enough when errors are suppressed. Definite errors reported by nested
<     transformations abort the sequence regardless of the propagation mode. The
<     set of modes may be extended in the future, e.g., to collect silenceable
<     errors and report them after attempting all transformations in the sequence.
---
>     the payload IR in a state unsuitable for further transformations. It is
>     the responsibility of the caller to ensure the following transformations
>     are robust enough when errors are suppressed. Definite errors reported by
>     nested transformations abort the sequence regardless of the propagation
>     mode. The set of modes may be extended in the future, e.g., to collect
>     silenceable errors and report them after attempting all transformations in
>     the sequence.
694,695c420
<                        Optional<TransformHandleTypeInterface>:$root,
<                        Variadic<Transform_AnyHandleOrParamType>:$extra_bindings);
---
>                        Optional<TransformHandleTypeInterface>:$root);
700,701c425
<     "custom<SequenceOpOperands>($root, type($root), $extra_bindings, type($extra_bindings))"
<     " (`->` type($results)^)? `failures` `(` "
---
>     "($root^ `:` type($root))? (`->` type($results)^)? `failures` `(` "
711c435
<     // Build a sequence with a root and additional arguments.
---
>     // Build a sequence without a root but a certain bbArg type.
715,729c439
<         "::mlir::Value":$root, "::mlir::ValueRange":$extraBindings,
<         "SequenceBodyBuilderArgsFn":$bodyBuilder)>,
< 
<     // Build a top-level sequence (no root).
<     OpBuilder<(ins
<         "::mlir::TypeRange":$resultTypes,
<         "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
<         "::mlir::Type":$bbArgType, "SequenceBodyBuilderFn":$bodyBuilder)>,
< 
<     // Build a top-level sequence (no root) with extra arguments.
<     OpBuilder<(ins
<         "::mlir::TypeRange":$resultTypes,
<         "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
<         "::mlir::Type":$bbArgType, "::mlir::TypeRange":$extraBindingTypes,
<         "SequenceBodyBuilderArgsFn":$bodyBuilder)>
---
>         "::mlir::Type":$bbArgType, "SequenceBodyBuilderFn":$bodyBuilder)>
742,743c452
<      OpAsmOpInterface, PossibleTopLevelTransformOpTrait,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
---
>      OpAsmOpInterface, PossibleTopLevelTransformOpTrait, RecursiveMemoryEffects,
782,783c491,492
<     Arg<Optional<TransformHandleTypeInterface>, "Root operation of the Payload IR"
<         >:$root);
---
>     Arg<Optional<TransformHandleTypeInterface>, "Root operation of the Payload IR",
>         [TransformMappingRead]>:$root);
795,796c504
< def YieldOp : TransformDialectOp<"yield",
<     [Terminator, DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
> def YieldOp : TransformDialectOp<"yield", [Terminator]> {
805,807c513,514
<     Arg<Variadic<Transform_AnyHandleOrParamType>,
<         "Transform values yielded back to the parent"
<         >:$operands);
---
>     Arg<Variadic<TransformHandleTypeInterface>, "Operation handles yielded back to the parent",
>         [TransformMappingRead]>:$operands);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Transform/IR/TransformTypes.td include/mlir/Dialect/Transform/IR/TransformTypes.td
55,63d54
< def Transform_AnyValue : TypeDef<Transform_Dialect, "AnyValue",
<     [DeclareTypeInterfaceMethods<TransformValueHandleTypeInterface>]> {
<   let description = [{
<     Transform IR value that can be associated with a list of Payload IR values.
<   }];
<   let mnemonic = "any_value";
<   let assemblyFormat = "";
< }
< 
70,74d60
< 
< def TransformAnyHandle : Type<
<     Or<[TransformHandleTypeInterface.predicate,
<         TransformValueHandleTypeInterface.predicate]>,
<     "transform operation or value handle">;
Only in include/mlir/Dialect/Transform/IR: TransformUtils.h
--- include/mlir/Dialect/Transform/IR/TransformOps.h
13,14d12
< #include "mlir/Dialect/Transform/IR/MatchInterfaces.h"
< #include "mlir/Dialect/Transform/IR/TransformAttrs.h"
17d14
< #include "mlir/IR/FunctionInterfaces.h"
20d16
< #include "mlir/IR/PatternMatch.h"
22d17
< #include "mlir/Interfaces/CallInterfaces.h"
34,88d28
< using SequenceBodyBuilderArgsFn =
<     ::llvm::function_ref<void(::mlir::OpBuilder &, ::mlir::Location,
<                               ::mlir::BlockArgument, ::mlir::ValueRange)>;
< 
< /// A listener that updates a TransformState based on IR modifications. This
< /// listener can be used during a greedy pattern rewrite to keep the transform
< /// state up-to-date.
< class TrackingListener : public RewriterBase::Listener,
<                          public TransformState::Extension {
< public:
<   /// Create a new TrackingListener for usage in the specified transform op.
<   explicit TrackingListener(TransformState &state, TransformOpInterface op)
<       : TransformState::Extension(state), transformOp(op) {}
< 
< protected:
<   /// Return a replacement payload op for the given op, which is going to be
<   /// replaced with the given values. By default, if all values are defined by
<   /// the same newly-created op, which also has the same type as the given op,
<   /// that defining op is used as a replacement.
<   virtual Operation *findReplacementOp(Operation *op,
<                                        ValueRange newValues) const;
< 
<   /// Notify the listener that the pattern failed to match the given operation,
<   /// and provide a callback to populate a diagnostic with the reason why the
<   /// failure occurred.
<   LogicalResult
<   notifyMatchFailure(Location loc,
<                      function_ref<void(Diagnostic &)> reasonCallback) override;
< 
<   /// This function is called when a tracked payload op is dropped because no
<   /// replacement op was found. Derived classes can implement this function for
<   /// custom error handling.
<   virtual void notifyPayloadReplacementNotFound(Operation *op,
<                                                 ValueRange values) {}
< 
<   /// Return "true" if the given op is a new op.
<   bool isNewOp(Operation *op) const;
< 
<   /// Return the single op that defines all given values (if any).
<   static Operation *getCommonDefiningOp(ValueRange values);
< 
< private:
<   void notifyOperationInserted(Operation *op) override;
< 
<   void notifyOperationRemoved(Operation *op) override;
< 
<   void notifyOperationReplaced(Operation *op, ValueRange newValues) override;
< 
<   /// Ops that were newly created during the transform.
<   DenseMap<OperationName, DenseSet<Operation *>> newOps;
< 
<   /// The transform op in which this TrackingListener is used.
<   TransformOpInterface transformOp;
< };
< 
--- include/mlir/Dialect/Transform/IR/TransformTypes.h
--- include/mlir/Dialect/Transform/IR/TransformDialect.td
26c26
<   let hasOperationAttrVerify = 1;
---
>   let useFoldAPI = kEmitFoldAdaptorFolder;
29,45d28
<       /// Name of the attribute attachable to the symbol table operation
<       /// containing named sequences. This is used to trigger verification.
<       constexpr const static llvm::StringLiteral
<           kWithNamedSequenceAttrName = "transform.with_named_sequence";
< 
<       /// Names of the attribute attachable to an operation so it can be
<       /// identified as root by the default interpreter pass.
<       constexpr const static llvm::StringLiteral
<           kTargetTagAttrName = "transform.target_tag";
< 
<       /// Names of the attributes indicating whether an argument of an external
<       /// transform dialect symbol is consumed or only read.
<       constexpr const static llvm::StringLiteral
<           kArgConsumedAttrName = "transform.consumed";
<       constexpr const static llvm::StringLiteral
<           kArgReadOnlyAttrName = "transform.readonly";
< 
124c107
< // Base class for ops that belong to the transform dialect. Ops defined in
---
> // Base class for ops that belong to the tranfsorm dialect. Ops defined in
--- include/mlir/Dialect/Transform/IR/TransformAttrs.td
23,35d22
< def MatchCmpIPredicateAttr : I32EnumAttr<
<     "MatchCmpIPredicate", "",
<     [
<       I32EnumAttrCase<"eq", 0>,
<       I32EnumAttrCase<"ne", 1>,
<       I32EnumAttrCase<"lt", 2>,
<       I32EnumAttrCase<"le", 3>,
<       I32EnumAttrCase<"gt", 4>,
<       I32EnumAttrCase<"ge", 5>,
<     ]> {
<   let cppNamespace = "::mlir::transform";
< }
< 
--- include/mlir/Dialect/Transform/IR/TransformInterfaces.h
14d13
< #include "mlir/Dialect/Transform/Utils/RaggedArray.h"
15a15
> 
20d19
< 
24d22
< class TransformResults;
30,32c28
<   TransformOptions() = default;
<   TransformOptions(const TransformOptions &) = default;
<   TransformOptions &operator=(const TransformOptions &) = default;
---
>   TransformOptions() {}
49,51d44
< using Param = Attribute;
< using MappedValue = llvm::PointerUnion<Operation *, Param, Value>;
< 
60d52
<                 const RaggedArray<MappedValue> &extraMapping = {},
96c88
<   using Param = transform::Param;
---
>   using Param = Attribute;
112,115d103
<   /// Mapping between a Value in the transform IR and the corrsponding list of
<   /// values in the payload IR. Also works for reverse mappings.
<   using ValueMapping = DenseMap<Value, SmallVector<Value>>;
< 
122,123d109
<     ValueMapping values;
<     ValueMapping reverseValues;
126,128c112,114
<   friend LogicalResult applyTransforms(Operation *, TransformOpInterface,
<                                        const RaggedArray<MappedValue> &,
<                                        const TransformOptions &);
---
>   friend LogicalResult applyTransforms(Operation *payloadRoot,
>                                        TransformOpInterface transform,
>                                        const TransformOptions &options);
135,142d120
<   /// Returns the number of extra mappings for the top-level operation.
<   size_t getNumTopLevelMappings() const { return topLevelMappedValues.size(); }
< 
<   /// Returns the position-th extra mapping for the top-level operation.
<   ArrayRef<MappedValue> getTopLevelMapping(size_t position) const {
<     return topLevelMappedValues[position];
<   }
< 
151,154d128
<   /// Returns the list of payload IR values that the given transform IR value
<   /// corresponds to.
<   ArrayRef<Value> getPayloadValues(Value handleValue) const;
< 
160,165d133
<   /// Populates `handles` with all handles pointing to the given payload IR
<   /// value. Returns success if such handles exist, failure otherwise.
<   LogicalResult
<   getHandlesForPayloadValue(Value payloadValue,
<                             SmallVectorImpl<Value> &handles) const;
< 
185,186d152
<   LogicalResult mapBlockArgument(BlockArgument argument,
<                                  ArrayRef<MappedValue> values);
197,202d162
<   /// Creates a new region scope for the given isolated-from-above region.
<   /// Unlike the non-isolated counterpart, there is no nesting expectation.
<   // Implementation note: this method is inline but implemented outside of the
<   // class body to comply with visibility and full-declaration requirements
<   inline RegionScope make_isolated_region_scope(Region &region);
< 
212,213c172
<     /// transform IR region, and restores the mapping that existed before
<     /// entering this scope.
---
>     /// transform IR region.
216,217d174
<       if (storedMappings.has_value())
<         state.mappings.swap(*storedMappings);
224,226d180
<     /// Tag structure for differentiating the constructor for isolated regions.
<     struct Isolated {};
< 
228c182
<     /// transform IR region and payload IR objects.
---
>     /// transform IR region and payload IR operations.
235,236c189
<       assert(((state.regionStack.size() == 1 && !state.regionStack.back()) ||
<               state.regionStack.back()->isProperAncestor(&region)) &&
---
>       assert(state.regionStack.back()->isProperAncestor(&region) &&
242,254d194
<     /// Creates a new scope for mappings between values defined in the given
<     /// isolated-from-above transform IR region and payload IR objects.
<     RegionScope(TransformState &state, Region &region, Isolated)
<         : state(state), region(&region) {
<       // Store the previous mapping stack locally.
<       storedMappings = llvm::SmallDenseMap<Region *, Mappings>();
<       storedMappings->swap(state.mappings);
<       state.mappings.try_emplace(this->region);
< #if LLVM_ENABLE_ABI_BREAKING_CHECKS
<       state.regionStack.push_back(this->region);
< #endif // LLVM_ENABLE_ABI_BREAKING_CHECKS
<     }
< 
261,266d200
<     /// Local copy of the mappings that existed before entering the current
<     /// region. Used only when the current region is isolated so we don't
<     /// accidentally look up the values defined outside the isolated region.
<     std::optional<llvm::SmallDenseMap<Region *, Mappings>> storedMappings =
<         std::nullopt;
< 
268d201
<     friend RegionScope TransformState::make_isolated_region_scope(Region &);
318,320d250
<     ///
<     /// Note: This function does not update value handles. None of the original
<     /// op's results are allowed to be mapped to any value handle.
323,327d252
<     /// Replaces the given payload value with another value. If the replacement
<     /// value is null, removes the association of the payload value with its
<     /// handle. Returns failure if the value is not associated with any handle.
<     LogicalResult replacePayloadValue(Value value, Value replacement);
< 
380d304
<                  const RaggedArray<MappedValue> &extraMappings = {},
383c307
<   /// Returns the mappings frame for the region in which the value is defined.
---
>   /// Returns the mappings frame for the reigon in which the value is defined.
405,408c329,331
<   /// Updates the state to include the associations between op results and the
<   /// provided result of applying a transform op.
<   LogicalResult updateStateFromResults(const TransformResults &results,
<                                        ResultRange opResults);
---
>   /// Removes the mapping between the given payload IR operation and the given
>   /// transform IR value.
>   void dropReverseMapping(Mappings &mappings, Operation *op, Value value);
429,441d351
<   /// Similarly, operation handles may be invalidate and should not be used
<   /// after a transform that consumed a value handle pointing to a payload value
<   /// defined by the operation as either block argument or op result. For
<   /// example, in the following sequence, the last transform operation rewrites
<   /// the callee to not return a specified result:
<   ///
<   ///   %0 = transform.find_call "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///   transform.drop_call_result_from_signature %1[0]
<   ///
<   /// which requires the call operations to be recreated. Therefore, the handle
<   /// %0 becomes associated with a dangling pointer and should not be used.
<   ///
447,487d356
<   /// Sets the payload IR values association with the given transform IR value
<   /// (handle). A payload value may be associated with multiple handles as long
<   /// as at most one of them is consumed by further transformations. For
<   /// example, a hypothetical "get results of calls to function with the given
<   /// name" transform may be performed twice in a row producing handles pointing
<   /// to the same values:
<   ///
<   ///   %0 = transform.find_results_of_calling "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///
<   /// which is valid by itself. However, calling a hypothetical "erase value
<   /// producer" transform on both handles:
<   ///
<   ///   transform.erase_value_produce %0
<   ///   transform.erase_value_produce %1
<   ///
<   /// is invalid provided the transformation "consumes" the handle as expressed
<   /// by side effects (which themselves reflect the semantics of the transform
<   /// erasing the producer and making the handle dangling). Practically, a
<   /// transformation consuming a handle means the associated payload value may
<   /// no longer exist.
<   ///
<   /// Similarly, value handles are invalidated and should not be used after a
<   /// transform that consumed an operation handle pointing to the payload IR
<   /// operation defining the values associated the value handle, as either block
<   /// arguments or op results, or any ancestor operation. For example,
<   ///
<   ///   %0 = transform.find_call "myfunc"
<   ///   %1 = transform.find_results_of_calling "myfunc"
<   ///   transform.rewrite_and_rename %0 { new_name = "func" }
<   ///
<   /// makes %1 unusable after the last transformation if it consumes %0. When an
<   /// operation handle is consumed, it usually indicates that the operation was
<   /// destroyed or heavily modified, meaning that the values it defines may no
<   /// longer exist.
<   ///
<   /// Returns failure if the payload values do not satisfy the conditions
<   /// associated with the type of the handle value. The value is expected to
<   /// have a type implementing TransformValueHandleTypeInterface.
<   LogicalResult setPayloadValues(Value handle, ValueRange payloadValues);
< 
494,497c363,364
<   /// Forgets the payload IR ops associated with the given transform IR value,
<   /// as well as any association between value handles and the results of said
<   /// payload IR op.
<   void forgetMapping(Value opHandle, ValueRange origOpFlatResults);
---
>   /// Forgets the payload IR ops associated with the given transform IR value.
>   void removePayloadOps(Value value);
499,503c366,370
<   void forgetValueMapping(Value valueHandle,
<                           ArrayRef<Operation *> payloadOperations);
< 
<   /// Replaces the given payload op with another op. If the replacement op is
<   /// null, removes the association of the payload op with its handle.
---
>   /// Updates the payload IR ops associated with the given transform IR value.
>   /// The callback function is called once per associated operation and is
>   /// expected to return the modified operation or nullptr. In the latter case,
>   /// the corresponding operation is no longer associated with the transform IR
>   /// value.
505,512c372,376
<   /// Note: This function does not update value handles. None of the original
<   /// op's results are allowed to be mapped to any value handle.
<   LogicalResult replacePayloadOp(Operation *op, Operation *replacement);
< 
<   /// Replaces the given payload value with another value. If the replacement
<   /// value is null, removes the association of the payload value with its
<   /// handle.
<   LogicalResult replacePayloadValue(Value value, Value replacement);
---
>   /// Returns failure if the payload does not satisfy the conditions associated
>   /// with the type of the handle value.
>   LogicalResult
>   updatePayloadOps(Value value,
>                    function_ref<Operation *(Operation *)> callback);
518,533c382,385
<   /// errors if they are used. If `throughValue` is passed, record the fact that
<   /// an op handle was invalidated because a value handle associated with
<   /// results of the payload op or its block arguments was invalidated.
<   void recordOpHandleInvalidation(OpOperand &consumingHandle,
<                                   ArrayRef<Operation *> potentialAncestors,
<                                   Value throughValue = nullptr);
<   void recordOpHandleInvalidationOne(OpOperand &handle,
<                                      ArrayRef<Operation *> potentialAncestors,
<                                      Operation *payloadOp, Value otherHandle,
<                                      Value throughValue = nullptr);
< 
<   void recordValueHandleInvalidationByOpHandleOne(
<       OpOperand &opHandle, ArrayRef<Operation *> potentialAncestors,
<       Value payloadValue, Value valueHandle);
< 
<   void recordValueHandleInvalidation(OpOperand &valueHandle);
---
>   /// errors if they are used.
>   void recordHandleInvalidation(OpOperand &handle);
>   void recordHandleInvalidationOne(OpOperand &handle, Operation *payloadOp,
>                                    Value otherHandle);
554,558d405
<   /// Extra mapped values (payload operations, values or parameters) to be
<   /// associated with additional entry block arguments of the top-level
<   /// transform operation.
<   RaggedArray<MappedValue> topLevelMappedValues;
< 
573,584d419
< 
<   /// This cache stores operation names for operations that are tracked in the
<   /// transform dialect state. It is used to detect missing memory side effects
<   /// and op tracking.
<   ///
<   /// All tracked ops are added to this cache before a transform op is applied.
<   /// After the application of the transform op, the names of all tracked ops
<   /// are compared with the names in the cache. If there is a mismatch (or a
<   /// crash), op tracking is missing somewhere. This is typically a missing
<   /// "consumesHandle" side effect or a pattern that removes an op without
<   /// notifying a TrackingListener.
<   DenseMap<Operation *, OperationName> cachedNames;
596,597c431,432
<   /// by the transformation exactly once in case of transformation succeeding.
<   /// The value must have a type implementing TransformHandleTypeInterface.
---
>   /// by the transformation exactly once. The value must have a type
>   /// implementing TransformHandleTypeInterface.
602,603c437,438
<   /// the transformation exactly once in case of transformation succeeding. The
<   /// value must have a type implementing TransformParamTypeInterface.
---
>   /// the transformation exactly once. The value must have a type implementing
>   /// TransformParamTypeInterface.
606,623d440
<   /// Indicates that the result of the transform IR op at the given position
<   /// corresponds to the given range of payload IR values. Each result must be
<   /// set by the transformation exactly once in case of transformation
<   /// succeeding. The value must have a type implementing
<   /// TransformValueHandleTypeInterface.
<   void setValues(OpResult handle, ValueRange values);
< 
<   /// Indicates that the result of the transform IR op at the given position
<   /// corresponds to the given range of mapped values. All mapped values are
<   /// expected to be compatible with the type of the result, e.g., if the result
<   /// is an operation handle, all mapped values are expected to be payload
<   /// operations.
<   void setMappedValues(OpResult handle, ArrayRef<MappedValue> values);
< 
<   /// Sets the currently unset results to empty lists of the kind expected by
<   /// the corresponding results of the given `transform` op.
<   void setRemainingToEmpty(TransformOpInterface transform);
< 
640,644d456
<   /// Gets the list of payload IR values associated with the result identified
<   /// by its number in the list of operation results. The result must have been
<   /// set to be associated with payload IR values.
<   ArrayRef<Value> getValues(unsigned resultNumber) const;
< 
646,647c458,459
<   /// operation results is associated with a list of parameters, `false`
<   /// otherwise.
---
>   /// operation results is associated with a list of parameters, `false` if it
>   /// is associated with the list of payload IR operations.
651,655d462
<   /// operation results is associated with a list of payload IR value, `false`
<   /// otherwise.
<   bool isValue(unsigned resultNumber) const;
< 
<   /// Returns `true` if the result identified by its number in the list of
659,667c466,484
<   /// Pointers to payload IR ops that are associated with results of a transform
<   /// IR op.
<   RaggedArray<Operation *> operations;
< 
<   /// Parameters that are associated with results of the transform IR op.
<   RaggedArray<Param> params;
< 
<   /// Payload IR values that are associated with results of a transform IR op.
<   RaggedArray<Value> values;
---
>   /// Storage for pointers to payload IR ops that are associated with results of
>   /// a transform IR op. `segments` contains as many entries as the transform IR
>   /// op has results, even if some of them are not associated with payload IR
>   /// operations. Each entry is a reference to a contiguous segment in the
>   /// `operations` list that contains the pointers to operations. This allows
>   /// for operations to be stored contiguously without nested vectors and for
>   /// different segments to be set in any order.
>   SmallVector<ArrayRef<Operation *>, 2> segments;
>   SmallVector<Operation *> operations;
> 
>   /// Storage for parameters that are associated with results of the transform
>   /// IR op. `paramSegments` contains as many entries as the transform IR op has
>   /// results, even if some of them are not associated with parameters. Each
>   /// entry is a reference to a contiguous segment in the `params` list that
>   /// contains the actual parameters. This allows for parameters to be stored
>   /// contiguously without nested vectors and for different segments to be set
>   /// in any order.
>   SmallVector<ArrayRef<TransformState::Param>, 2> paramSegments;
>   SmallVector<TransformState::Param> params;
670,672d486
< /// Creates a RAII object the lifetime of which corresponds to the new mapping
< /// for transform IR values defined in the given region. Values defined in
< /// surrounding regions remain accessible.
677,684d490
< /// Creates a RAII object the lifetime of which corresponds to the new mapping
< /// for transform IR values defined in the given isolated-from-above region.
< /// Values defined in surrounding regions cannot be accessed.
< TransformState::RegionScope
< TransformState::make_isolated_region_scope(Region &region) {
<   return RegionScope(*this, region, RegionScope::Isolated());
< }
< 
698,708d503
< 
< /// Populates `mappings` with mapped values associated with the given transform
< /// IR values in the given `state`.
< void prepareValueMappings(
<     SmallVectorImpl<SmallVector<transform::MappedValue>> &mappings,
<     ValueRange values, const transform::TransformState &state);
< 
< /// Populates `results` with payload associations that match exactly those of
< /// the operands to `block`'s terminator.
< void forwardTerminatorOperands(Block *block, transform::TransformState &state,
<                                transform::TransformResults &results);
756,757d550
< class ApplyToEachResultList;
< 
763c556
< ///       ApplyToEachResultList &results, TransformState &state)
---
> ///       SmallVector<Operation*> &results, state)
770c563
< /// zero, one or multiple operations depending on the number of results expected
---
> /// zero, one or multiple operations depending on the number of resultd expected
805,828d597
< /// Trait implementing the applyToOne function required by TransformEachOpTrait
< /// by greedily applying a set of patterns to each target payload operation.
< /// This requires the transform operation to implement TransformEachOpTrait and
< /// to provide the following method:
< ///   - void populatePatterns(RewritePatternSet &)
< /// that populates the given object with the patterns to apply. This is an
< /// instance method that can depend on the transform operation attributes.
< ///
< /// The payload operation is expected to have the IsolatedFromAboveTrait, which
< /// is a requirement of the pattern rewriter. If it does not, or if pattern
< /// application fails, the transform fails definitively as the rewriter will
< /// have likely left the payload IR in some intermediate state that precludes
< /// further transformation.
< template <typename OpTy>
< class TransformWithPatternsOpTrait
<     : public OpTrait::TraitBase<OpTy, TransformWithPatternsOpTrait> {
< public:
<   DiagnosedSilenceableFailure applyToOne(Operation *target,
<                                          ApplyToEachResultList &results,
<                                          TransformState &state);
< 
<   static LogicalResult verifyTrait(Operation *op);
< };
< 
832c601
< /// effect. A Read effect from this resource means accessing the mapping. A Free
---
> /// effet. A Read effect from this resource means accessing the mapping. A Free
839c608
< /// modified, potentially erased, by the previous transformations.
---
> /// modified, potentially erased, by the previous tranfsormations.
878,882d646
< /// Populates `consumedArguments` with positions of `block` arguments that are
< /// consumed by the operations in the `block`.
< void getConsumedBlockArguments(
<     Block &block, llvm::SmallDenseSet<unsigned> &consumedArguments);
< 
981c745
< using ApplyToEachResult = MappedValue;
---
> using ApplyToEachResult = llvm::PointerUnion<Operation *, Attribute>;
1005,1007d768
<       } else if constexpr (std::is_convertible_v<decltype(*std::begin(range)),
<                                                  Value>) {
<         results.push_back(element.template get<Value>());
1015,1017d775
<   // Using ApplyToEachResult that can be implicitly constructed from a Value but
<   // not from a concrete Op that is implicitly convertible to a Value to avoid
<   // ambiguity.
1020d777
<   void push_back(ApplyToEachResult r) { results.push_back(r); }
1120,1127d876
< /// Applies patterns configured by `populatePatterns` greedily to the contents
< /// of `target`. Reports (definite) errors at the location of `transformOp`.
< /// Sets up `results` to point to `target` after pattern application on success.
< DiagnosedSilenceableFailure transformWithPatternsApply(
<     Operation *transformOp, Operation *target, ApplyToEachResultList &results,
<     TransformState &state,
<     function_ref<void(RewritePatternSet &)> populatePatterns);
< 
1149,1150d897
<       else if (r.getType().isa<TransformValueHandleTypeInterface>())
<         transformResults.setValues(r, ValueRange());
1188,1209d934
<   return success();
< }
< 
< template <typename OpTy>
< mlir::DiagnosedSilenceableFailure
< mlir::transform::TransformWithPatternsOpTrait<OpTy>::applyToOne(
<     Operation *target, ApplyToEachResultList &results, TransformState &state) {
<   return detail::transformWithPatternsApply(
<       this->getOperation(), target, results, state,
<       [this](RewritePatternSet &patterns) {
<         cast<OpTy>(this->getOperation()).populatePatterns(patterns);
<       });
< }
< 
< template <typename OpTy>
< mlir::LogicalResult
< mlir::transform::TransformWithPatternsOpTrait<OpTy>::verifyTrait(
<     Operation *op) {
<   if (!op->hasTrait<mlir::transform::TransformEachOpTrait>()) {
<     return op->emitOpError()
<            << "TransformWithPatternsOpTrait requires TransformEachOpTrait";
<   }
--- include/mlir/Dialect/Transform/IR/TransformUtils.h
--- include/mlir/Dialect/Transform/IR/CMakeLists.txt
31,34d30
< add_mlir_interface(MatchInterfaces)
< add_dependencies(MLIRMatchInterfacesIncGen MLIRTransformInterfacesIncGen)
< add_mlir_doc(TransformInterfaces MatchOpInterfaces Dialects/ -gen-op-interface-docs)
< 
--- include/mlir/Dialect/Transform/IR/TransformTypes.td
55,63d54
< def Transform_AnyValue : TypeDef<Transform_Dialect, "AnyValue",
<     [DeclareTypeInterfaceMethods<TransformValueHandleTypeInterface>]> {
<   let description = [{
<     Transform IR value that can be associated with a list of Payload IR values.
<   }];
<   let mnemonic = "any_value";
<   let assemblyFormat = "";
< }
< 
70,74d60
< 
< def TransformAnyHandle : Type<
<     Or<[TransformHandleTypeInterface.predicate,
<         TransformValueHandleTypeInterface.predicate]>,
<     "transform operation or value handle">;
--- include/mlir/Dialect/Transform/IR/TransformInterfaces.td
140,143c140,143
<     Types that can be used for the Transform dialect operation handle values.
<     Such types define the properties of Payload IR operations associated with
<     the handle. A user of such a handle can assume that these properties have
<     been verified for any Payload IR operation associated with it.
---
>     Types that can be used for the Transform dialect handle values. Such types
>     define the properties of Payload IR operations associated with the handle.
>     A user of such a handle can assume that these properties have been verified
>     for any Payload IR operation associated with it.
158,168d157
< def TransformValueHandleTypeInterface
<     : TransformTypeInterfaceBase<"TransformValueHandleTypeInterface",
<                                  "::mlir::Value"> {
<   let description = [{
<     Types that can be used for the Transform dialect handle values pointing to
<     Payload IR values. Such types define the properties of Payload IR values
<     associated with the handle. Users of such a handle can assume that these
<     properties have been verified for any Payload IR value associated with it.
<   }];
< }
< 
171,172c160
<              TransformHandleTypeInterface.predicate,
<              TransformValueHandleTypeInterface.predicate]>,
---
>              TransformHandleTypeInterface.predicate]>,
181,184d168
<   let cppNamespace = "::mlir::transform";
< }
< 
< def TransformWithPatternsOpTrait : NativeOpTrait<"TransformWithPatternsOpTrait"> {
--- include/mlir/Dialect/Transform/IR/TransformEffects.td
--- include/mlir/Dialect/Transform/IR/TransformDialect.h
245a246,247
> #include "mlir/Dialect/Transform/IR/TransformDialectEnums.h.inc"
> 
--- include/mlir/Dialect/Transform/IR/TransformOps.td
12d11
< include "mlir/Interfaces/CallInterfaces.td"
16,17d14
< include "mlir/Interfaces/SideEffectInterfaces.td"
< include "mlir/IR/FunctionInterfaces.td"
20d16
< include "mlir/Dialect/Transform/IR/MatchInterfaces.td"
22a19
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
30c27
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
---
>      FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
120,182d116
< def ForeachMatchOp : TransformDialectOp<"foreach_match", [
<     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<     DeclareOpInterfaceMethods<SymbolUserOpInterface>,
<     DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Applies named sequences when a named matcher succeeds";
<   let description = [{
<     Given a pair of co-indexed lists of transform dialect symbols (such as
<     `transform.named_sequence`), walks the payload IR associated with the root
<     handle and interprets the symbols as matcher/action pairs by applying the
<     body of the corresponding symbol definition. The symbol from the first list
<     is the matcher part: if it results in a silenceable error, the error is
<     silenced and the next matcher is attempted. Definite failures from any
<     matcher stop the application immediately and are propagated unconditionally.
<     If none of the matchers succeeds, the next payload operation in walk order
<     (post-order at the moment of writing, double check `Operation::walk`) is
<     matched. If a matcher succeeds, the co-indexed action symbol is applied and
<     the following matchers are not applied to the same payload operation. If the
<     action succeeds, the next payload operation in walk order is matched. If it
<     fails, both silenceable and definite errors are propagated as the result of
<     this op.
< 
<     The matcher symbol must take one operand of a type that implements the same
<     transform dialect interface as the `root` operand (a check is performed at
<     application time to see if the associated payload satisfies the constraints
<     of the actual type). It must not consume the operand as multiple matchers
<     may be applied. The matcher may produce any number of results. The action
<     symbol paired with the matcher must take the same number of arguments as the
<     matcher has results, and these arguments must implement the same transform
<     dialect interfaces, but not necessarily have the exact same type (again, a
<     check is performed at application time to see if the associated payload
<     satisfies the constraints of actual types on both sides). The action symbol
<     may not have results. The actions are expected to only modify payload
<     operations nested in the `root` payload operations associated with the
<     operand of this transform operation.
< 
<     This operation consumes the operand and produces a new handle associated
<     with the same payload. This is necessary to trigger invalidation of handles
<     to any of the payload operations nested in the payload operations associated
<     with the operand, as those are likely to be modified by actions. Note that
<     the root payload operation associated with the operand are not matched.
< 
<     The operation succeeds if none of the matchers produced a definite failure
<     during application and if all of the applied actions produced success. Note
<     that it also succeeds if all the matchers failed on all payload operations,
<     i.e. failure to apply is not an error. The operation produces a silenceable
<     failure if any applied action produced a silenceable failure. In this case,
<     the resulting handle is associated with an empty payload. The operation
<     produces a definite failure if any of the applied matchers or actions
<     produced a definite failure.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$root,
<                        SymbolRefArrayAttr:$matchers,
<                        SymbolRefArrayAttr:$actions);
<   let results = (outs TransformHandleTypeInterface:$updated);
< 
<   let assemblyFormat =
<       "`in` $root custom<ForeachMatchSymbols>($matchers, $actions) "
<       "attr-dict `:` functional-type($root, $updated)";
< 
<   let hasVerifier = 1;
< }
< 
279,295d212
< def GetDefiningOp : TransformDialectOp<"get_defining_op",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
<   let summary = "Get handle to the defining op of a value";
<   let description = [{
<     The handle defined by this Transform op corresponds to the defining op of
<     the targeted value.
< 
<     This transform fails silently if the targeted value is a block argument.
<   }];
< 
<   let arguments = (ins TransformValueHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$result);
<   let assemblyFormat = "$target attr-dict `:` "
<                        "functional-type(operands, results)";
< }
< 
311c228
<   let results = (outs TransformHandleTypeInterface:$producer);
---
>   let results = (outs TransformHandleTypeInterface:$parent);
316,420d232
< def GetResultOp : TransformDialectOp<"get_result",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      NavigationTransformOpTrait, MemoryEffectsOpInterface]> {
<   let summary = "Get handle to the a result of the targeted op";
<   let description = [{
<     The handle defined by this Transform op corresponds to the OpResult with
<     `result_number` that is defined by the given `target` operation.
<     
<     This transform fails silently if the targeted operation does not have enough
<     results. It reads the target handle and produces the result handle.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                        I64Attr:$result_number);
<   let results = (outs TransformValueHandleTypeInterface:$result);
<   let assemblyFormat = "$target `[` $result_number `]` attr-dict `:` "
<                        "functional-type(operands, results)";
< }
< 
< def IncludeOp : TransformDialectOp<"include",
<     [CallOpInterface,
<      MatchOpInterface,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<SymbolUserOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Includes a named transform sequence";
<   let description = [{
<     The application of this transform operation is equivalent to applying the
<     operations contained in the named transform sequence with operands being
<     remapped to block arguments. The behavior of the operation when a
<     transformation in the included named sequence produces a silenceable error
<     is controlled by the `failure_propagation_mode` attribute. When set to
<     `propagate`, the failure of any nested transformation in the sequence
<     implies immediate failure of the entire sequence with a silenceable error,
<     and no further transformation is attempted. When set to `suppress`,
<     silenceable errors in nested operations are ignored and further
<     transformations are applied. Beware that even silenceable errors may leave
<     the payload IR in a state unsuitable for further transformations. It is the
<     responsibility of the user to ensure the following transformations are
<     robust enough when errors are suppressed. Definite errors are propagated
<     immediately regardless of the mode. The objects associated with the results
<     of this operation are the same as those associated with the operands of the
<     `transform.yield` in the referenced named sequence.
<   }];
< 
<   let arguments = (ins SymbolRefAttr:$target,
<                        FailurePropagationMode:$failure_propagation_mode,
<                        Variadic<Transform_AnyHandleOrParamType>:$operands);
<   let results = (outs Variadic<Transform_AnyHandleOrParamType>:$results);
<   
<   let assemblyFormat =
<       "$target `failures` `(` $failure_propagation_mode `)`"
<       "`(` $operands `)` attr-dict `:` functional-type($operands, $results)";
< 
<   let extraClassDeclaration = [{
<     ::mlir::CallInterfaceCallable getCallableForCallee() {
<       return getTarget();
<     }
< 
<     ::mlir::Operation::operand_range getArgOperands() {
<       return getOperands();
<     }
<   }];
< }
< 
< def MatchOperationNameOp : TransformDialectOp<"match.operation_name",
<     [SingleOpMatcher,
<      MatchOpInterface,
<      MemoryEffectsOpInterface]> {
<   let summary = "Matches a single operation of one of the given kinds";
<   let description = [{
<     Succeeds if the operation associated with the operand handle has one of the
<     given operation names. Produces a silenceable failure otherwise.
< 
<     If more than one payload operation is associated with the operand handle,
<     produces a definite failure.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$operand_handle,
<                        StrArrayAttr:$op_names);
<   let assemblyFormat =
<       "$operand_handle $op_names attr-dict `:` type($operand_handle)";
<   let extraClassDeclaration = SingleOpMatcher.extraDeclaration;
< }
< 
< def MatchParamCmpIOp : Op<Transform_Dialect, "match.param.cmpi", [
<     DeclareOpInterfaceMethods<TransformOpInterface>,
<     MatchOpInterface,
<     DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<     SameTypeOperands]> {
<   let summary =
<     "Matches if two parameter lists are associated with the same value";
<   let description = [{
<     Succeeds if all of the co-indexed values associated with the given
<     parameters relate as specified by the predicate (greater than, less than,
<     equal to, or their combinations). Comparison treats all values as signed.
<     Produces a silenceable failure otherwise.
<   }];
<   let arguments = (ins TransformParamTypeInterface:$param,
<                        TransformParamTypeInterface:$reference,
<                        MatchCmpIPredicateAttr:$predicate);
<   let assemblyFormat =
<       "$predicate $param `,` $reference attr-dict `:` type($param)";
< }
< 
444,511d255
< def NamedSequenceOp : TransformDialectOp<"named_sequence",
<     [CallableOpInterface,
<      FunctionOpInterface,
<      IsolatedFromAbove,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let summary = "Named transform sequence that can be included elsewhere";
<   let description = [{
<     Defines a named (callable, function-like) sequence of other Transform
<     dialect operations that can be included using `transform.include` as part of
<     another Transform dialect construct. This sequence is not processed
<     immediately but rather dispatched to when the inclusion is processed. The
<     arguments and results can be used to communicate a subset of mapping into
<     the named sequence. The sequence must consist of a single block and end with
<     a `transform.yield` terminator. The operands of the terminator become the
<     results of the `transform.include`.
< 
<     When dispatched to, the operations in the named sequence are executed one by
<     one, similarly to the regular unnamed sequence. The failure propagation mode
<     is specified on the `transform.include`. Different inclusions may use
<     different failure propagation modes. This transform operation always
<     succeeds by itself, but the inclusion may fail if any of the operations
<     fail.
< 
<     Named sequences can only appear at the top-level of the Transform dialect
<     nesting structure. That is, they cannot be nested in other Transform dialect
<     operations. Furthermore, one of the ancestors must have the `SymbolTable`
<     trait and have the `transform.with_named_sequence` attribute attached.
< 
<     Named sequences may include other named sequences via `transform.include`,
<     but recursion is *not* allowed.
<   }];
<   
<   let arguments = (ins
<     SymbolNameAttr:$sym_name,
<     TypeAttrBase<"::mlir::FunctionType",
<                  "function type attribute">:$function_type,
<     OptionalAttr<StrAttr>:$sym_visibility,
<     OptionalAttr<DictArrayAttr>:$arg_attrs,
<     OptionalAttr<DictArrayAttr>:$res_attrs);
<   let regions = (region MaxSizedRegion<1>:$body);
< 
<   let hasCustomAssemblyFormat = 1;
<   let hasVerifier = 1;
< 
<   let extraClassDeclaration = [{
<     ::llvm::ArrayRef<::mlir::Type> getArgumentTypes() {
<       return getFunctionType().getInputs();
<     }
<     ::llvm::ArrayRef<::mlir::Type> getResultTypes() {
<       return getFunctionType().getResults();
<     }
< 
<     ::mlir::Region *getCallableRegion() {
<       return &getBody();
<     }
<     ::llvm::ArrayRef<::mlir::Type> getCallableResults() {
<       return getFunctionType().getResults();
<     }
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
<   }];
< }
< 
544,561d287
< def ParamConstantOp : Op<Transform_Dialect, "param.constant", [
<     MatchOpInterface,
<     DeclareOpInterfaceMethods<TransformOpInterface>,
<     MemoryEffectsOpInterface,
<     ParamProducerTransformOpTrait]> {
<   let summary = "Produces a new transform dialect parameter value associated "
<                 "with the given attribute";
<   let description = [{
<     Produces a new transform dialect parameter associated with the singleton
<     list containing the given attribute. The operation itself always succeeds,
<     but the general association check may fail if the parameter type does not
<     accept the given kind of attribute as valid.
<   }];
<   let arguments = (ins AnyAttr:$value);
<   let results = (outs TransformParamTypeInterface:$param);
<   let assemblyFormat = "$value attr-dict `->` type($param)";
< }
< 
616a343
> 
660,661c387
<      SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">,
<      AttrSizedOperandSegments]> {
---
>      SingleBlockImplicitTerminator<"::mlir::transform::YieldOp">]> {
666,672c392,397
<     corresponds to a group of operations or values in the payload IR, or to a
<     group of parameters, depending on the type of the value. The behavior of the
<     operation when a nested transformation produces a silenceable error is
<     controlled by the `failure_propagation_mode` attribute. When set to
<     `propagate`, the failure of any nested transformation in the sequence
<     implies immediate failure of the entire sequence with a silenceable error,
<     and no further transformation is attempted. When set to `suppress`,
---
>     corresponds to an operation or a group of operations in the payload IR.
>     The behavior of the operation when a nested transformation produces a
>     silenceable error is controlled by the `failure_propagation_mode` attribute.
>     When set to `propagate`, the failure of any nested transformation in the
>     sequence implies immediate failure of the entire sequence with a silenceable
>     error, and no further transformation is attempted. When set to `suppress`,
675,680c400,406
<     the payload IR in a state unsuitable for further transformations. It is the
<     responsibility of the caller to ensure the following transformations are
<     robust enough when errors are suppressed. Definite errors reported by nested
<     transformations abort the sequence regardless of the propagation mode. The
<     set of modes may be extended in the future, e.g., to collect silenceable
<     errors and report them after attempting all transformations in the sequence.
---
>     the payload IR in a state unsuitable for further transformations. It is
>     the responsibility of the caller to ensure the following transformations
>     are robust enough when errors are suppressed. Definite errors reported by
>     nested transformations abort the sequence regardless of the propagation
>     mode. The set of modes may be extended in the future, e.g., to collect
>     silenceable errors and report them after attempting all transformations in
>     the sequence.
694,695c420
<                        Optional<TransformHandleTypeInterface>:$root,
<                        Variadic<Transform_AnyHandleOrParamType>:$extra_bindings);
---
>                        Optional<TransformHandleTypeInterface>:$root);
700,701c425
<     "custom<SequenceOpOperands>($root, type($root), $extra_bindings, type($extra_bindings))"
<     " (`->` type($results)^)? `failures` `(` "
---
>     "($root^ `:` type($root))? (`->` type($results)^)? `failures` `(` "
711c435
<     // Build a sequence with a root and additional arguments.
---
>     // Build a sequence without a root but a certain bbArg type.
715,729c439
<         "::mlir::Value":$root, "::mlir::ValueRange":$extraBindings,
<         "SequenceBodyBuilderArgsFn":$bodyBuilder)>,
< 
<     // Build a top-level sequence (no root).
<     OpBuilder<(ins
<         "::mlir::TypeRange":$resultTypes,
<         "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
<         "::mlir::Type":$bbArgType, "SequenceBodyBuilderFn":$bodyBuilder)>,
< 
<     // Build a top-level sequence (no root) with extra arguments.
<     OpBuilder<(ins
<         "::mlir::TypeRange":$resultTypes,
<         "::mlir::transform::FailurePropagationMode":$failure_propagation_mode,
<         "::mlir::Type":$bbArgType, "::mlir::TypeRange":$extraBindingTypes,
<         "SequenceBodyBuilderArgsFn":$bodyBuilder)>
---
>         "::mlir::Type":$bbArgType, "SequenceBodyBuilderFn":$bodyBuilder)>
742,743c452
<      OpAsmOpInterface, PossibleTopLevelTransformOpTrait,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
---
>      OpAsmOpInterface, PossibleTopLevelTransformOpTrait, RecursiveMemoryEffects,
782,783c491,492
<     Arg<Optional<TransformHandleTypeInterface>, "Root operation of the Payload IR"
<         >:$root);
---
>     Arg<Optional<TransformHandleTypeInterface>, "Root operation of the Payload IR",
>         [TransformMappingRead]>:$root);
795,796c504
< def YieldOp : TransformDialectOp<"yield",
<     [Terminator, DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
> def YieldOp : TransformDialectOp<"yield", [Terminator]> {
805,807c513,514
<     Arg<Variadic<Transform_AnyHandleOrParamType>,
<         "Transform values yielded back to the parent"
<         >:$operands);
---
>     Arg<Variadic<TransformHandleTypeInterface>, "Operation handles yielded back to the parent",
>         [TransformMappingRead]>:$operands);
--- include/mlir/Dialect/Vector
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Interfaces and include/mlir/Dialect/Vector/Interfaces
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/IR and include/mlir/Dialect/Vector/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/TransformOps and include/mlir/Dialect/Vector/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms and include/mlir/Dialect/Vector/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Utils and include/mlir/Dialect/Vector/Utils
--- include/mlir/Dialect/Vector/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Utils/VectorUtils.h include/mlir/Dialect/Vector/Utils/VectorUtils.h
20a21,22
> class AffineApplyOp;
> class AffineForOp;
30,34d31
< 
< namespace affine {
< class AffineApplyOp;
< class AffineForOp;
< } // namespace affine
--- include/mlir/Dialect/Vector/Utils/VectorUtils.h
20a21,22
> class AffineApplyOp;
> class AffineForOp;
30,34d31
< 
< namespace affine {
< class AffineApplyOp;
< class AffineForOp;
< } // namespace affine
--- include/mlir/Dialect/Vector/CMakeLists.txt
--- include/mlir/Dialect/Vector/Interfaces
--- include/mlir/Dialect/Vector/Interfaces/MaskingOpInterface.td
--- include/mlir/Dialect/Vector/Interfaces/MaskingOpInterface.h
--- include/mlir/Dialect/Vector/Interfaces/CMakeLists.txt
--- include/mlir/Dialect/Vector/Interfaces/MaskableOpInterface.h
--- include/mlir/Dialect/Vector/Interfaces/MaskableOpInterface.td
--- include/mlir/Dialect/Vector/Transforms
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms: LoweringPatterns.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms/Passes.h include/mlir/Dialect/Vector/Transforms/Passes.h
24a25,30
> /// Populates instances of `MaskOpRewritePattern` to lower masked operations
> /// with `vector.mask`. Patterns should rewrite the `vector.mask` operation and
> /// not its nested `MaskableOpInterface`.
> void populateVectorMaskLoweringPatternsForSideEffectingOps(
>     RewritePatternSet &patterns);
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h include/mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h
12d11
< #include <optional>
13a13
> #include <optional>
20d19
< #include "mlir/Support/LogicalResult.h"
26c25,60
< struct VectorTransformsOptions;
---
> 
> //===----------------------------------------------------------------------===//
> // Vector transformation options exposed as auxiliary structs.
> //===----------------------------------------------------------------------===//
> /// Structure to control the behavior of vector transform patterns.
> struct VectorTransformsOptions {
>   /// Option to control the lowering of vector.contract.
>   VectorContractLowering vectorContractLowering = VectorContractLowering::Dot;
>   VectorTransformsOptions &
>   setVectorTransformsOptions(VectorContractLowering opt) {
>     vectorContractLowering = opt;
>     return *this;
>   }
>   /// Option to control the lowering of vector.multi_reduction.
>   VectorMultiReductionLowering vectorMultiReductionLowering =
>       VectorMultiReductionLowering::InnerParallel;
>   VectorTransformsOptions &
>   setVectorMultiReductionLowering(VectorMultiReductionLowering opt) {
>     vectorMultiReductionLowering = opt;
>     return *this;
>   }
>   /// Option to control the lowering of vector.transpose.
>   VectorTransposeLowering vectorTransposeLowering =
>       VectorTransposeLowering::EltWise;
>   VectorTransformsOptions &
>   setVectorTransposeLowering(VectorTransposeLowering opt) {
>     vectorTransposeLowering = opt;
>     return *this;
>   }
>   /// Option to control the splitting of vector transfers.
>   VectorTransferSplit vectorTransferSplit = VectorTransferSplit::None;
>   VectorTransformsOptions &setVectorTransferSplit(VectorTransferSplit opt) {
>     vectorTransferSplit = opt;
>     return *this;
>   }
> };
77,92c111,112
< /// Canonicalization of a `vector.contraction %a, %b, %c` with row-major matmul
< /// semantics to a contraction with MMT semantics (matrix matrix multiplication
< /// with the RHS transposed). This specific form is meant to have the vector
< /// operands are organized such that the reduction dimension is contiguous.
< /// Example:
< /// ```
< /// vector.contract {indexing_maps = [affine_map<(m, n, k) -> (m, k)>,
< ///                                   affine_map<(m, n, k) -> (n, k)>,
< ///                                   affine_map<(m, n, k) -> (m, n)>],
< ///                  iterator_types = ["parallel", "parallel", "reduction"],
< ///                  kind = #vector.kind<add>} %a, %b, %c : ...
< /// ```
< ///
< ///  The `constraint` predicate is used to decide which `vector.contraction` ops
< ///  to filter out.
< void populateVectorContractCanonicalizeMatmulToMMT(
---
> /// Insert TransposeLowering patterns into extraction/insertion.
> void populateVectorTransposeLoweringPatterns(
94,96c114,148
<     std::function<LogicalResult(vector::ContractionOp)> constraint =
<         [](vector::ContractionOp) { return success(); },
<     PatternBenefit = 1);
---
>     VectorTransformsOptions options = VectorTransformsOptions(),
>     PatternBenefit benefit = 1);
> 
> /// Collect a set of patterns to convert vector.multi_reduction op into
> /// a sequence of vector.reduction ops. The patterns comprise:
> /// - InnerOuterDimReductionConversion: rewrites vector.multi_reduction such
> /// that all reduction dimensions are either innermost or outermost, by adding
> /// the proper vector.transpose operations.
> /// - ReduceMultiDimReductionRank: once in innermost or outermost reduction
> /// form, rewrites n-D vector.multi_reduction into 2-D vector.multi_reduction,
> /// by introducing vector.shape_cast ops to collapse + multi-reduce + expand
> /// back.
> /// - TwoDimMultiReductionToElementWise: once in 2-D vector.multi_reduction
> /// form, with an **outermost** reduction dimension, unroll the outer dimension
> /// to obtain a sequence of 1-D vector ops. This also has an opportunity for
> /// tree-reduction (in the future).
> /// - TwoDimMultiReductionToReduction: once in 2-D vector.multi_reduction form,
> /// with an **innermost** reduction dimension, unroll the outer dimension to
> /// obtain a sequence of extract + vector.reduction + insert. This can further
> /// lower to horizontal reduction ops.
> /// - OneDimMultiReductionToTwoDim: for cases that reduce to 1-D vector<k>
> /// reduction (and are thus missing either a parallel or a reduction), we lift
> /// them back up to 2-D with a simple vector.shape_cast to vector<1xk> so that
> /// the other patterns can kick in, thus fully exiting out of the
> /// vector.multi_reduction abstraction.
> void populateVectorMultiReductionLoweringPatterns(
>     RewritePatternSet &patterns, VectorMultiReductionLowering options,
>     PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector contraction ops on high-D
> /// into low-D reduction and product ops.
> void populateVectorContractLoweringPatterns(
>     RewritePatternSet &patterns,
>     VectorTransformsOptions options = VectorTransformsOptions(),
>     PatternBenefit benefit = 1);
103,135c155,157
< /// Populate `patterns` with the following patterns.
< ///
< ///   - VectorTransferFullPartialRewriter
< ///
< /// Split a vector.transfer operation into an in-bounds (i.e., no out-of-bounds
< /// masking) fast path and a slow path.
< ///
< /// Example (a 2-D vector.transfer_read):
< /// ```
< ///    %1 = vector.transfer_read %0[...], %pad : memref<A...>, vector<...>
< /// ```
< /// is transformed into:
< /// ```
< ///    %1:3 = scf.if (%inBounds) {
< ///      // fast path, direct cast
< ///      memref.cast %A: memref<A...> to compatibleMemRefType
< ///      scf.yield %view : compatibleMemRefType, index, index
< ///    } else {
< ///      // slow path, not in-bounds vector.transfer or linalg.copy.
< ///      memref.cast %alloc: memref<B...> to compatibleMemRefType
< ///      scf.yield %4 : compatibleMemRefType, index, index
< //     }
< ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ... true]}
< /// ```
< /// where `alloc` is a top of the function alloca'ed buffer of one vector.
< ///
< /// Preconditions:
< ///  1. `xferOp.permutation_map()` must be a minor identity map
< ///  2. the rank of the `xferOp.memref()` and the rank of the `xferOp.vector()`
< ///  must be equal. This will be relaxed in the future but requires
< ///  rank-reducing subviews.
< void populateVectorTransferFullPartialPatterns(
<     RewritePatternSet &patterns, const VectorTransformsOptions &options);
---
> /// Collect patterns to convert scan op
> void populateVectorScanLoweringPatterns(RewritePatternSet &patterns,
>                                         PatternBenefit benefit = 1);
139a162,215
> /// Collect a set of transfer read/write lowering patterns that simplify the
> /// permutation map (e.g., converting it to a minor identity map) by inserting
> /// broadcasts and transposes. More specifically:
> ///
> /// [TransferReadPermutationLowering]
> /// Lower transfer_read op with permutation into a transfer_read with a
> /// permutation map composed of leading zeros followed by a minor identity +
> /// vector.transpose op.
> /// Ex:
> ///     vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2) -> (0, d1)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2) -> (d1, 0)
> ///     vector.transpose %v, [1, 0]
> ///
> ///     vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, 0, 0, d1, d3)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, 0, d1, 0, d3)
> ///     vector.transpose %v, [0, 1, 3, 2, 4]
> /// Note that an alternative is to transform it to linalg.transpose +
> /// vector.transfer_read to do the transpose in memory instead.
> ///
> /// [TransferWritePermutationLowering]
> /// Lower transfer_write op with permutation into a transfer_write with a
> /// minor identity permutation map. (transfer_write ops cannot have broadcasts.)
> /// Ex:
> ///     vector.transfer_write %v ...
> ///         permutation_map: (d0, d1, d2) -> (d2, d0, d1)
> /// into:
> ///     %tmp = vector.transpose %v, [2, 0, 1]
> ///     vector.transfer_write %tmp ...
> ///         permutation_map: (d0, d1, d2) -> (d0, d1, d2)
> ///
> ///     vector.transfer_write %v ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d3, d2)
> /// into:
> ///     %tmp = vector.transpose %v, [1, 0]
> ///     %v = vector.transfer_write %tmp ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d2, d3)
> ///
> /// [TransferOpReduceRank]
> /// Lower transfer_read op with broadcast in the leading dimensions into
> /// transfer_read of lower rank + vector.broadcast.
> /// Ex: vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, d1, 0, d3)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d1, 0, d3)
> ///     vector.broadcast %v
> void populateVectorTransferPermutationMapLoweringPatterns(
>     RewritePatternSet &patterns, PatternBenefit benefit = 1);
184,199d259
< /// Populate `patterns` with a pattern to break down 1-D vector.bitcast ops
< /// based on the destination vector shape. Bitcasts from a lower bitwidth
< /// element type to a higher bitwidth one are extracted from the lower bitwidth
< /// based on the native destination vector shape and inserted based on the ratio
< /// of the bitwidths.
< ///
< /// This acts as a last resort way to break down vector.bitcast ops to smaller
< /// vector sizes. Because this pattern composes until it is bitcasting to a
< /// single element of the higher bitwidth, the is an optional control function.
< /// If `controlFn` is not nullptr, the pattern will only apply to ops where
< /// `controlFn` returns true, otherwise applies to all bitcast ops.
< void populateBreakDownVectorBitCastOpPatterns(
<     RewritePatternSet &patterns,
<     std::function<bool(BitCastOp)> controlFn = nullptr,
<     PatternBenefit benefit = 1);
< 
253a314,519
> 
> //===----------------------------------------------------------------------===//
> // Finer-grained patterns exposed for more control over individual lowerings.
> //===----------------------------------------------------------------------===//
> /// Apply `splitFullAndPartialTransfer` selectively via a pattern. This pattern
> /// may take an extra filter to perform selection at a finer granularity.
> struct VectorTransferFullPartialRewriter : public RewritePattern {
>   using FilterConstraintType =
>       std::function<LogicalResult(VectorTransferOpInterface op)>;
> 
>   explicit VectorTransferFullPartialRewriter(
>       MLIRContext *context,
>       VectorTransformsOptions options = VectorTransformsOptions(),
>       FilterConstraintType filter =
>           [](VectorTransferOpInterface op) { return success(); },
>       PatternBenefit benefit = 1)
>       : RewritePattern(MatchAnyOpTypeTag(), benefit, context), options(options),
>         filter(std::move(filter)) {}
> 
>   /// Performs the rewrite.
>   LogicalResult matchAndRewrite(Operation *op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   VectorTransformsOptions options;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to:
> /// ```
> ///    %flattened_a = vector.shape_cast %a
> ///    %flattened_b = vector.shape_cast %b
> ///    %flattened_d = vector.matmul %flattened_a, %flattened_b
> ///    %d = vector.shape_cast %%flattened_d
> ///    %e = add %c, %d
> /// ```
> /// `vector.matmul` later lowers to `llvm.matrix.multiply`.
> //
> /// This only kicks in when VectorTransformsOptions is set to OuterProduct and
> /// the vector.contract op is a row-major matrix multiply.
> class ContractionOpToMatmulOpLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToMatmulOpLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to a reduction_size-unrolled sequence:
> /// ```
> ///    %at = vector.transpose %a, [1, 0]
> ///    %bRow0 = vector.extract %b[0]
> ///    %atRow0 = vector.extract %at[0]
> ///    %c0 = vector.outerproduct %atRow0, %bRow0, %c
> ///    ...
> ///    %bRowK = vector.extract %b[K]
> ///    %atRowK = vector.extract %at[K]
> ///    %cK = vector.outerproduct %atRowK, %bRowK, %cK-1
> /// ```
> ///
> /// This only kicks in when VectorTransformsOptions is set to OuterProduct and
> /// the vector.contract op is a row-major matrix multiply.
> class ContractionOpToOuterProductOpLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToOuterProductOpLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to an output-size-unrolled sequence:
> /// ```
> ///    %out = arith.constant ... : vector<MxNxelt_type>
> ///    %bt = vector.transpose %b, [1, 0]
> ///    %aRow0 = vector.extract %a[0]
> ///    %btRow0 = vector.extract %bt[0]
> ///    %c00 = vector.reduce %atRow0, %bRow0
> ///    %out00 = vector.insert %c00, %out[0, 0]
> ///    ...
> ///    %aRowLast = vector.extract %at[M-1]
> ///    %btRowLast = vector.extract %b[N-1]
> ///    %cLastLast = vector.reduce %atRowLast, %bRowLast
> ///    %outcLastLast = vector.insert %cLastLast, %out[M-1, N-1]
> /// ```
> ///
> /// This only kicks in when VectorTransformsOptions is set to Dot and
> /// the vector.contract op is a row-major matmul or matvec.
> class ContractionOpToDotLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToDotLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       const FilterConstraintType &constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions), filter(defaultFilter) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of ContractionOp.
> ///
> /// One:
> ///   %x = vector.contract with at least one free/batch dimension
> /// is replaced by:
> ///   %a = vector.contract with one less free/batch dimension
> ///   %b = vector.contract with one less free/batch dimension
> ///   ..
> ///   %x = combine %a %b ..
> /// until a pure contraction is reached (no free/batch dimensions),
> /// which is replaced by a dot-product.
> ///
> /// This only kicks in when either VectorTransformsOptions is set
> /// to Dot or when other contraction patterns fail.
> class ContractionOpLowering : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpLowering(vector::VectorTransformsOptions vectorTransformOptions,
>                         MLIRContext *context, PatternBenefit benefit = 1,
>                         FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
>   // Lower one parallel dimension.
>   FailureOr<Value> lowerParallel(vector::ContractionOp op, int64_t lhsIndex,
>                                  int64_t rhsIndex,
>                                  PatternRewriter &rewriter) const;
>   // Lower one reduction dimension.
>   FailureOr<Value> lowerReduction(vector::ContractionOp op,
>                                   PatternRewriter &rewriter) const;
> };
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms/VectorTransformsBase.td include/mlir/Dialect/Vector/Transforms/VectorTransformsBase.td
21,26c21,23
< // Lower 2-D transpose to `vector.shuffle` on 1-D vector.
< def VectorTransposeLowering_Shuffle1D:
<   I32EnumAttrCase<"Shuffle1D",  2, "shuffle_1d">;
< // Lower 2-D transpose to `vector.shuffle` on 16x16 vector.
< def VectorTransposeLowering_Shuffle16x16:
<   I32EnumAttrCase<"Shuffle16x16",  3, "shuffle_16x16">;
---
> // Lower 2-D transpose to `vector.shuffle`.
> def VectorTransposeLowering_Shuffle:
>   I32EnumAttrCase<"Shuffle",  2, "shuffle">;
31c28
<      VectorTransposeLowering_Shuffle1D, VectorTransposeLowering_Shuffle16x16]> {
---
>      VectorTransposeLowering_Shuffle]> {
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/Transforms/VectorTransforms.h include/mlir/Dialect/Vector/Transforms/VectorTransforms.h
28,63d27
< // Vector transformation options exposed as auxiliary structs.
< //===----------------------------------------------------------------------===//
< /// Structure to control the behavior of vector transform patterns.
< struct VectorTransformsOptions {
<   /// Option to control the lowering of vector.contract.
<   VectorContractLowering vectorContractLowering = VectorContractLowering::Dot;
<   VectorTransformsOptions &
<   setVectorTransformsOptions(VectorContractLowering opt) {
<     vectorContractLowering = opt;
<     return *this;
<   }
<   /// Option to control the lowering of vector.multi_reduction.
<   VectorMultiReductionLowering vectorMultiReductionLowering =
<       VectorMultiReductionLowering::InnerParallel;
<   VectorTransformsOptions &
<   setVectorMultiReductionLowering(VectorMultiReductionLowering opt) {
<     vectorMultiReductionLowering = opt;
<     return *this;
<   }
<   /// Option to control the lowering of vector.transpose.
<   VectorTransposeLowering vectorTransposeLowering =
<       VectorTransposeLowering::EltWise;
<   VectorTransformsOptions &
<   setVectorTransposeLowering(VectorTransposeLowering opt) {
<     vectorTransposeLowering = opt;
<     return *this;
<   }
<   /// Option to control the splitting of vector transfers.
<   VectorTransferSplit vectorTransferSplit = VectorTransferSplit::None;
<   VectorTransformsOptions &setVectorTransferSplit(VectorTransferSplit opt) {
<     vectorTransferSplit = opt;
<     return *this;
<   }
< };
< 
< //===----------------------------------------------------------------------===//
66,73c30,37
< /// Split a vector.transfer operation into an in-bounds (i.e., no
< /// out-of-bounds masking) fastpath and a slowpath. If `ifOp` is not null and
< /// the result is `success, the `ifOp` points to the newly created conditional
< /// upon function return. To accomodate for the fact that the original
< /// vector.transfer indexing may be arbitrary and the slow path indexes
< /// @[0...0] in the temporary buffer, the scf.if op returns a view and values
< /// of type index. At this time, only vector.transfer_read case is
< /// implemented.
---
> /// Split a vector.transfer operation into an in-bounds (i.e., no out-of-bounds
> /// masking) fastpath and a slowpath.
> /// If `ifOp` is not null and the result is `success, the `ifOp` points to the
> /// newly created conditional upon function return.
> /// To accomodate for the fact that the original vector.transfer indexing may be
> /// arbitrary and the slow path indexes @[0...0] in the temporary buffer, the
> /// scf.if op returns a view and values of type index.
> /// At this time, only vector.transfer_read case is implemented.
90,91c54
< ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ...
< ///    true]}
---
> ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ... true]}
97,99c60,62
< ///  2. the rank of the `xferOp.memref()` and the rank of the
< ///  `xferOp.vector()` must be equal. This will be relaxed in the future but
< ///  requires rank-reducing subviews.
---
> ///  2. the rank of the `xferOp.memref()` and the rank of the `xferOp.vector()`
> ///  must be equal. This will be relaxed in the future but requires
> ///  rank-reducing subviews.
107c70
< void transferOpflowOpt(RewriterBase &rewriter, Operation *rootOp);
---
> void transferOpflowOpt(Operation *rootOp);
--- include/mlir/Dialect/Vector/Transforms/Passes.h
24a25,30
> /// Populates instances of `MaskOpRewritePattern` to lower masked operations
> /// with `vector.mask`. Patterns should rewrite the `vector.mask` operation and
> /// not its nested `MaskableOpInterface`.
> void populateVectorMaskLoweringPatternsForSideEffectingOps(
>     RewritePatternSet &patterns);
> 
--- include/mlir/Dialect/Vector/Transforms/VectorRewritePatterns.h
12d11
< #include <optional>
13a13
> #include <optional>
20d19
< #include "mlir/Support/LogicalResult.h"
26c25,60
< struct VectorTransformsOptions;
---
> 
> //===----------------------------------------------------------------------===//
> // Vector transformation options exposed as auxiliary structs.
> //===----------------------------------------------------------------------===//
> /// Structure to control the behavior of vector transform patterns.
> struct VectorTransformsOptions {
>   /// Option to control the lowering of vector.contract.
>   VectorContractLowering vectorContractLowering = VectorContractLowering::Dot;
>   VectorTransformsOptions &
>   setVectorTransformsOptions(VectorContractLowering opt) {
>     vectorContractLowering = opt;
>     return *this;
>   }
>   /// Option to control the lowering of vector.multi_reduction.
>   VectorMultiReductionLowering vectorMultiReductionLowering =
>       VectorMultiReductionLowering::InnerParallel;
>   VectorTransformsOptions &
>   setVectorMultiReductionLowering(VectorMultiReductionLowering opt) {
>     vectorMultiReductionLowering = opt;
>     return *this;
>   }
>   /// Option to control the lowering of vector.transpose.
>   VectorTransposeLowering vectorTransposeLowering =
>       VectorTransposeLowering::EltWise;
>   VectorTransformsOptions &
>   setVectorTransposeLowering(VectorTransposeLowering opt) {
>     vectorTransposeLowering = opt;
>     return *this;
>   }
>   /// Option to control the splitting of vector transfers.
>   VectorTransferSplit vectorTransferSplit = VectorTransferSplit::None;
>   VectorTransformsOptions &setVectorTransferSplit(VectorTransferSplit opt) {
>     vectorTransferSplit = opt;
>     return *this;
>   }
> };
77,92c111,112
< /// Canonicalization of a `vector.contraction %a, %b, %c` with row-major matmul
< /// semantics to a contraction with MMT semantics (matrix matrix multiplication
< /// with the RHS transposed). This specific form is meant to have the vector
< /// operands are organized such that the reduction dimension is contiguous.
< /// Example:
< /// ```
< /// vector.contract {indexing_maps = [affine_map<(m, n, k) -> (m, k)>,
< ///                                   affine_map<(m, n, k) -> (n, k)>,
< ///                                   affine_map<(m, n, k) -> (m, n)>],
< ///                  iterator_types = ["parallel", "parallel", "reduction"],
< ///                  kind = #vector.kind<add>} %a, %b, %c : ...
< /// ```
< ///
< ///  The `constraint` predicate is used to decide which `vector.contraction` ops
< ///  to filter out.
< void populateVectorContractCanonicalizeMatmulToMMT(
---
> /// Insert TransposeLowering patterns into extraction/insertion.
> void populateVectorTransposeLoweringPatterns(
94,96c114,148
<     std::function<LogicalResult(vector::ContractionOp)> constraint =
<         [](vector::ContractionOp) { return success(); },
<     PatternBenefit = 1);
---
>     VectorTransformsOptions options = VectorTransformsOptions(),
>     PatternBenefit benefit = 1);
> 
> /// Collect a set of patterns to convert vector.multi_reduction op into
> /// a sequence of vector.reduction ops. The patterns comprise:
> /// - InnerOuterDimReductionConversion: rewrites vector.multi_reduction such
> /// that all reduction dimensions are either innermost or outermost, by adding
> /// the proper vector.transpose operations.
> /// - ReduceMultiDimReductionRank: once in innermost or outermost reduction
> /// form, rewrites n-D vector.multi_reduction into 2-D vector.multi_reduction,
> /// by introducing vector.shape_cast ops to collapse + multi-reduce + expand
> /// back.
> /// - TwoDimMultiReductionToElementWise: once in 2-D vector.multi_reduction
> /// form, with an **outermost** reduction dimension, unroll the outer dimension
> /// to obtain a sequence of 1-D vector ops. This also has an opportunity for
> /// tree-reduction (in the future).
> /// - TwoDimMultiReductionToReduction: once in 2-D vector.multi_reduction form,
> /// with an **innermost** reduction dimension, unroll the outer dimension to
> /// obtain a sequence of extract + vector.reduction + insert. This can further
> /// lower to horizontal reduction ops.
> /// - OneDimMultiReductionToTwoDim: for cases that reduce to 1-D vector<k>
> /// reduction (and are thus missing either a parallel or a reduction), we lift
> /// them back up to 2-D with a simple vector.shape_cast to vector<1xk> so that
> /// the other patterns can kick in, thus fully exiting out of the
> /// vector.multi_reduction abstraction.
> void populateVectorMultiReductionLoweringPatterns(
>     RewritePatternSet &patterns, VectorMultiReductionLowering options,
>     PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector contraction ops on high-D
> /// into low-D reduction and product ops.
> void populateVectorContractLoweringPatterns(
>     RewritePatternSet &patterns,
>     VectorTransformsOptions options = VectorTransformsOptions(),
>     PatternBenefit benefit = 1);
103,135c155,157
< /// Populate `patterns` with the following patterns.
< ///
< ///   - VectorTransferFullPartialRewriter
< ///
< /// Split a vector.transfer operation into an in-bounds (i.e., no out-of-bounds
< /// masking) fast path and a slow path.
< ///
< /// Example (a 2-D vector.transfer_read):
< /// ```
< ///    %1 = vector.transfer_read %0[...], %pad : memref<A...>, vector<...>
< /// ```
< /// is transformed into:
< /// ```
< ///    %1:3 = scf.if (%inBounds) {
< ///      // fast path, direct cast
< ///      memref.cast %A: memref<A...> to compatibleMemRefType
< ///      scf.yield %view : compatibleMemRefType, index, index
< ///    } else {
< ///      // slow path, not in-bounds vector.transfer or linalg.copy.
< ///      memref.cast %alloc: memref<B...> to compatibleMemRefType
< ///      scf.yield %4 : compatibleMemRefType, index, index
< //     }
< ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ... true]}
< /// ```
< /// where `alloc` is a top of the function alloca'ed buffer of one vector.
< ///
< /// Preconditions:
< ///  1. `xferOp.permutation_map()` must be a minor identity map
< ///  2. the rank of the `xferOp.memref()` and the rank of the `xferOp.vector()`
< ///  must be equal. This will be relaxed in the future but requires
< ///  rank-reducing subviews.
< void populateVectorTransferFullPartialPatterns(
<     RewritePatternSet &patterns, const VectorTransformsOptions &options);
---
> /// Collect patterns to convert scan op
> void populateVectorScanLoweringPatterns(RewritePatternSet &patterns,
>                                         PatternBenefit benefit = 1);
139a162,215
> /// Collect a set of transfer read/write lowering patterns that simplify the
> /// permutation map (e.g., converting it to a minor identity map) by inserting
> /// broadcasts and transposes. More specifically:
> ///
> /// [TransferReadPermutationLowering]
> /// Lower transfer_read op with permutation into a transfer_read with a
> /// permutation map composed of leading zeros followed by a minor identity +
> /// vector.transpose op.
> /// Ex:
> ///     vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2) -> (0, d1)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2) -> (d1, 0)
> ///     vector.transpose %v, [1, 0]
> ///
> ///     vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, 0, 0, d1, d3)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, 0, d1, 0, d3)
> ///     vector.transpose %v, [0, 1, 3, 2, 4]
> /// Note that an alternative is to transform it to linalg.transpose +
> /// vector.transfer_read to do the transpose in memory instead.
> ///
> /// [TransferWritePermutationLowering]
> /// Lower transfer_write op with permutation into a transfer_write with a
> /// minor identity permutation map. (transfer_write ops cannot have broadcasts.)
> /// Ex:
> ///     vector.transfer_write %v ...
> ///         permutation_map: (d0, d1, d2) -> (d2, d0, d1)
> /// into:
> ///     %tmp = vector.transpose %v, [2, 0, 1]
> ///     vector.transfer_write %tmp ...
> ///         permutation_map: (d0, d1, d2) -> (d0, d1, d2)
> ///
> ///     vector.transfer_write %v ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d3, d2)
> /// into:
> ///     %tmp = vector.transpose %v, [1, 0]
> ///     %v = vector.transfer_write %tmp ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d2, d3)
> ///
> /// [TransferOpReduceRank]
> /// Lower transfer_read op with broadcast in the leading dimensions into
> /// transfer_read of lower rank + vector.broadcast.
> /// Ex: vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (0, d1, 0, d3)
> /// into:
> ///     %v = vector.transfer_read ...
> ///         permutation_map: (d0, d1, d2, d3) -> (d1, 0, d3)
> ///     vector.broadcast %v
> void populateVectorTransferPermutationMapLoweringPatterns(
>     RewritePatternSet &patterns, PatternBenefit benefit = 1);
184,199d259
< /// Populate `patterns` with a pattern to break down 1-D vector.bitcast ops
< /// based on the destination vector shape. Bitcasts from a lower bitwidth
< /// element type to a higher bitwidth one are extracted from the lower bitwidth
< /// based on the native destination vector shape and inserted based on the ratio
< /// of the bitwidths.
< ///
< /// This acts as a last resort way to break down vector.bitcast ops to smaller
< /// vector sizes. Because this pattern composes until it is bitcasting to a
< /// single element of the higher bitwidth, the is an optional control function.
< /// If `controlFn` is not nullptr, the pattern will only apply to ops where
< /// `controlFn` returns true, otherwise applies to all bitcast ops.
< void populateBreakDownVectorBitCastOpPatterns(
<     RewritePatternSet &patterns,
<     std::function<bool(BitCastOp)> controlFn = nullptr,
<     PatternBenefit benefit = 1);
< 
253a314,519
> 
> //===----------------------------------------------------------------------===//
> // Finer-grained patterns exposed for more control over individual lowerings.
> //===----------------------------------------------------------------------===//
> /// Apply `splitFullAndPartialTransfer` selectively via a pattern. This pattern
> /// may take an extra filter to perform selection at a finer granularity.
> struct VectorTransferFullPartialRewriter : public RewritePattern {
>   using FilterConstraintType =
>       std::function<LogicalResult(VectorTransferOpInterface op)>;
> 
>   explicit VectorTransferFullPartialRewriter(
>       MLIRContext *context,
>       VectorTransformsOptions options = VectorTransformsOptions(),
>       FilterConstraintType filter =
>           [](VectorTransferOpInterface op) { return success(); },
>       PatternBenefit benefit = 1)
>       : RewritePattern(MatchAnyOpTypeTag(), benefit, context), options(options),
>         filter(std::move(filter)) {}
> 
>   /// Performs the rewrite.
>   LogicalResult matchAndRewrite(Operation *op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   VectorTransformsOptions options;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to:
> /// ```
> ///    %flattened_a = vector.shape_cast %a
> ///    %flattened_b = vector.shape_cast %b
> ///    %flattened_d = vector.matmul %flattened_a, %flattened_b
> ///    %d = vector.shape_cast %%flattened_d
> ///    %e = add %c, %d
> /// ```
> /// `vector.matmul` later lowers to `llvm.matrix.multiply`.
> //
> /// This only kicks in when VectorTransformsOptions is set to OuterProduct and
> /// the vector.contract op is a row-major matrix multiply.
> class ContractionOpToMatmulOpLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToMatmulOpLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to a reduction_size-unrolled sequence:
> /// ```
> ///    %at = vector.transpose %a, [1, 0]
> ///    %bRow0 = vector.extract %b[0]
> ///    %atRow0 = vector.extract %at[0]
> ///    %c0 = vector.outerproduct %atRow0, %bRow0, %c
> ///    ...
> ///    %bRowK = vector.extract %b[K]
> ///    %atRowK = vector.extract %at[K]
> ///    %cK = vector.outerproduct %atRowK, %bRowK, %cK-1
> /// ```
> ///
> /// This only kicks in when VectorTransformsOptions is set to OuterProduct and
> /// the vector.contract op is a row-major matrix multiply.
> class ContractionOpToOuterProductOpLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToOuterProductOpLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of a `vector.contract %a, %b, %c` with row-major matmul
> /// semantics to an output-size-unrolled sequence:
> /// ```
> ///    %out = arith.constant ... : vector<MxNxelt_type>
> ///    %bt = vector.transpose %b, [1, 0]
> ///    %aRow0 = vector.extract %a[0]
> ///    %btRow0 = vector.extract %bt[0]
> ///    %c00 = vector.reduce %atRow0, %bRow0
> ///    %out00 = vector.insert %c00, %out[0, 0]
> ///    ...
> ///    %aRowLast = vector.extract %at[M-1]
> ///    %btRowLast = vector.extract %b[N-1]
> ///    %cLastLast = vector.reduce %atRowLast, %bRowLast
> ///    %outcLastLast = vector.insert %cLastLast, %out[M-1, N-1]
> /// ```
> ///
> /// This only kicks in when VectorTransformsOptions is set to Dot and
> /// the vector.contract op is a row-major matmul or matvec.
> class ContractionOpToDotLowering
>     : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
> 
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpToDotLowering(
>       vector::VectorTransformsOptions vectorTransformOptions,
>       MLIRContext *context, PatternBenefit benefit = 1,
>       const FilterConstraintType &constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions), filter(defaultFilter) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
> };
> 
> /// Progressive lowering of ContractionOp.
> ///
> /// One:
> ///   %x = vector.contract with at least one free/batch dimension
> /// is replaced by:
> ///   %a = vector.contract with one less free/batch dimension
> ///   %b = vector.contract with one less free/batch dimension
> ///   ..
> ///   %x = combine %a %b ..
> /// until a pure contraction is reached (no free/batch dimensions),
> /// which is replaced by a dot-product.
> ///
> /// This only kicks in when either VectorTransformsOptions is set
> /// to Dot or when other contraction patterns fail.
> class ContractionOpLowering : public OpRewritePattern<vector::ContractionOp> {
> public:
>   using OpRewritePattern::OpRewritePattern;
>   using FilterConstraintType =
>       std::function<LogicalResult(vector::ContractionOp op)>;
> 
>   static LogicalResult defaultFilter(vector::ContractionOp op) {
>     return success();
>   }
> 
>   ContractionOpLowering(vector::VectorTransformsOptions vectorTransformOptions,
>                         MLIRContext *context, PatternBenefit benefit = 1,
>                         FilterConstraintType constraint = defaultFilter)
>       : OpRewritePattern<vector::ContractionOp>(context, benefit),
>         vectorTransformOptions(vectorTransformOptions),
>         filter(std::move(constraint)) {}
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> 
> private:
>   /// Options to control the vector patterns.
>   vector::VectorTransformsOptions vectorTransformOptions;
>   FilterConstraintType filter;
>   // Lower one parallel dimension.
>   FailureOr<Value> lowerParallel(vector::ContractionOp op, int64_t lhsIndex,
>                                  int64_t rhsIndex,
>                                  PatternRewriter &rewriter) const;
>   // Lower one reduction dimension.
>   FailureOr<Value> lowerReduction(vector::ContractionOp op,
>                                   PatternRewriter &rewriter) const;
> };
--- include/mlir/Dialect/Vector/Transforms/VectorTransformsBase.td
21,26c21,23
< // Lower 2-D transpose to `vector.shuffle` on 1-D vector.
< def VectorTransposeLowering_Shuffle1D:
<   I32EnumAttrCase<"Shuffle1D",  2, "shuffle_1d">;
< // Lower 2-D transpose to `vector.shuffle` on 16x16 vector.
< def VectorTransposeLowering_Shuffle16x16:
<   I32EnumAttrCase<"Shuffle16x16",  3, "shuffle_16x16">;
---
> // Lower 2-D transpose to `vector.shuffle`.
> def VectorTransposeLowering_Shuffle:
>   I32EnumAttrCase<"Shuffle",  2, "shuffle">;
31c28
<      VectorTransposeLowering_Shuffle1D, VectorTransposeLowering_Shuffle16x16]> {
---
>      VectorTransposeLowering_Shuffle]> {
--- include/mlir/Dialect/Vector/Transforms/Passes.td
--- include/mlir/Dialect/Vector/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/Vector/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Vector/Transforms/VectorTransforms.h
28,63d27
< // Vector transformation options exposed as auxiliary structs.
< //===----------------------------------------------------------------------===//
< /// Structure to control the behavior of vector transform patterns.
< struct VectorTransformsOptions {
<   /// Option to control the lowering of vector.contract.
<   VectorContractLowering vectorContractLowering = VectorContractLowering::Dot;
<   VectorTransformsOptions &
<   setVectorTransformsOptions(VectorContractLowering opt) {
<     vectorContractLowering = opt;
<     return *this;
<   }
<   /// Option to control the lowering of vector.multi_reduction.
<   VectorMultiReductionLowering vectorMultiReductionLowering =
<       VectorMultiReductionLowering::InnerParallel;
<   VectorTransformsOptions &
<   setVectorMultiReductionLowering(VectorMultiReductionLowering opt) {
<     vectorMultiReductionLowering = opt;
<     return *this;
<   }
<   /// Option to control the lowering of vector.transpose.
<   VectorTransposeLowering vectorTransposeLowering =
<       VectorTransposeLowering::EltWise;
<   VectorTransformsOptions &
<   setVectorTransposeLowering(VectorTransposeLowering opt) {
<     vectorTransposeLowering = opt;
<     return *this;
<   }
<   /// Option to control the splitting of vector transfers.
<   VectorTransferSplit vectorTransferSplit = VectorTransferSplit::None;
<   VectorTransformsOptions &setVectorTransferSplit(VectorTransferSplit opt) {
<     vectorTransferSplit = opt;
<     return *this;
<   }
< };
< 
< //===----------------------------------------------------------------------===//
66,73c30,37
< /// Split a vector.transfer operation into an in-bounds (i.e., no
< /// out-of-bounds masking) fastpath and a slowpath. If `ifOp` is not null and
< /// the result is `success, the `ifOp` points to the newly created conditional
< /// upon function return. To accomodate for the fact that the original
< /// vector.transfer indexing may be arbitrary and the slow path indexes
< /// @[0...0] in the temporary buffer, the scf.if op returns a view and values
< /// of type index. At this time, only vector.transfer_read case is
< /// implemented.
---
> /// Split a vector.transfer operation into an in-bounds (i.e., no out-of-bounds
> /// masking) fastpath and a slowpath.
> /// If `ifOp` is not null and the result is `success, the `ifOp` points to the
> /// newly created conditional upon function return.
> /// To accomodate for the fact that the original vector.transfer indexing may be
> /// arbitrary and the slow path indexes @[0...0] in the temporary buffer, the
> /// scf.if op returns a view and values of type index.
> /// At this time, only vector.transfer_read case is implemented.
90,91c54
< ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ...
< ///    true]}
---
> ///    %0 = vector.transfer_read %1#0[%1#1, %1#2] {in_bounds = [true ... true]}
97,99c60,62
< ///  2. the rank of the `xferOp.memref()` and the rank of the
< ///  `xferOp.vector()` must be equal. This will be relaxed in the future but
< ///  requires rank-reducing subviews.
---
> ///  2. the rank of the `xferOp.memref()` and the rank of the `xferOp.vector()`
> ///  must be equal. This will be relaxed in the future but requires
> ///  rank-reducing subviews.
107c70
< void transferOpflowOpt(RewriterBase &rewriter, Operation *rootOp);
---
> void transferOpflowOpt(Operation *rootOp);
--- include/mlir/Dialect/Vector/Transforms/VectorDistribution.h
--- include/mlir/Dialect/Vector/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/IR/VectorOps.h include/mlir/Dialect/Vector/IR/VectorOps.h
47d46
< class ContractionOp;
80,84d78
< /// Cast away the leading unit dim, if exists, for the given contract op.
< /// Return success if the transformation applies; return failure otherwise.
< LogicalResult castAwayContractionLeadingOneDim(vector::ContractionOp contractOp,
<                                                RewriterBase &rewriter);
< 
118a113,123
> /// Collect a set of transfer read/write lowering patterns.
> ///
> /// These patterns lower transfer ops to simpler ops like `vector.load`,
> /// `vector.store` and `vector.broadcast`. Only transfers with a transfer rank
> /// of a most `maxTransferRank` are lowered. This is useful when combined with
> /// VectorToSCF, which reduces the rank of vector transfer ops.
> void populateVectorTransferLoweringPatterns(
>     RewritePatternSet &patterns,
>     std::optional<unsigned> maxTransferRank = std::nullopt,
>     PatternBenefit benefit = 1);
> 
123a129,149
> /// Collects patterns to progressively lower vector.broadcast ops on high-D
> /// vectors to low-D vector ops.
> void populateVectorBroadcastLoweringPatterns(RewritePatternSet &patterns,
>                                              PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector mask ops into elementary
> /// selection and insertion ops.
> void populateVectorMaskOpLoweringPatterns(RewritePatternSet &patterns,
>                                           PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector.shape_cast ops on high-D
> /// vectors into 1-D/2-D vector ops by generating data movement extract/insert
> /// ops.
> void populateVectorShapeCastLoweringPatterns(RewritePatternSet &patterns,
>                                              PatternBenefit benefit = 1);
> 
> /// Collects patterns that lower scalar vector transfer ops to memref loads and
> /// stores when beneficial.
> void populateScalarVectorTransferLoweringPatterns(RewritePatternSet &patterns,
>                                                   PatternBenefit benefit = 1);
> 
168c194
<                          Value v1, Value acc, Value mask = Value());
---
>                          Value v1, Value v2);
191,201c217,218
< Operation *maskOperation(OpBuilder &builder, Operation *maskableOp, Value mask,
<                          Value passthru = Value());
< 
< /// Creates a vector select operation that picks values from `newValue` or
< /// `passthru` for each result vector lane based on `mask`. This utility is used
< /// to propagate the pass-thru value for masked-out or expeculatively executed
< /// lanes. VP intrinsics do not support pass-thru values and every mask-out lane
< /// is set to poison. LLVM backends are usually able to match op + select
< /// patterns and fold them into a native target instructions.
< Value selectPassthru(OpBuilder &builder, Value mask, Value newValue,
<                      Value passthru);
---
> Operation *maskOperation(RewriterBase &rewriter, Operation *maskableOp,
>                          Value mask);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/IR/VectorOps.td include/mlir/Dialect/Vector/IR/VectorOps.td
32a33
>   let useFoldAPI = kEmitFoldAdaptorFolder;
94d94
<       DeclareOpInterfaceMethods<MaskableOpInterface>,
97a98
>                Variadic<VectorOf<[I1]>>:$masks,
112,115c113,115
<     If operands and the result have types of different bitwidths, operands are
<     promoted to have the same bitwidth as the result before performing the
<     contraction. For integer types, only signless integer types are supported,
<     and the promotion happens via sign extension.
---
>     Optional vector mask arguments (produced by CreateMaskOp or ConstantMaskOp)
>     specify the dynamic dimension sizes of valid data within the lhs/rhs vector
>     arguments.
192a193,200
>     // 4D vector contraction with two contracting dimensions and optional
>     // vector mask arguments.
>     %lhs_mask = vector.constant_mask [7, 8, 16, 15] : vector<7x8x16x15xi1>
>     %rhs_mask = vector.constant_mask [8, 16, 7, 5] : vector<8x16x7x5xi1>
> 
>     %5 = vector.contract #contraction_trait %0, %1, %2, %lhs_mask, %rhs_mask
>        : vector<7x8x16x15xf32>, vector<8x16x7x5xf32> into vector<8x15x8x5xf32>
> 
195c203
<     %5 = vector.contract #contraction_trait %0, %1, %2
---
>     %6 = vector.contract #contraction_trait %0, %1, %2
209c217
<     %6 = vector.contract #contraction_trait %0, %1, %2
---
>     %7 = vector.contract #contraction_trait %0, %1, %2
230a239,246
>     VectorType getLHSVectorMaskType() {
>       if (llvm::size(getMasks()) != 2) return VectorType();
>       return getOperand(3).getType().cast<VectorType>();
>     }
>     VectorType getRHSVectorMaskType() {
>       if (llvm::size(getMasks()) != 2) return VectorType();
>       return getOperand(4).getType().cast<VectorType>();
>     }
303c319
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
431c447
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
448c464
<     ///   2. `broadcastedDims` must be confined to [0 .. rank(value.getResultVectorType)]
---
>     ///   2. `broadcastedDims` must be confined to [0 .. rank(value.getVectorType)]
519c535
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
532c548
<                     "$_self.cast<VectorType>().getElementType()">]>,
---
>                     "$_self.cast<ShapedType>().getElementType()">]>,
566c582
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
601c617
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
647c663
<                     "$_self.cast<VectorType>().getElementType()">,
---
>                     "$_self.cast<ShapedType>().getElementType()">,
906,907c922
<                 TCresVTEtIsSameAsOpBase<0, 1>>,
<     DeclareOpInterfaceMethods<MaskableOpInterface>]>,
---
>                 TCresVTEtIsSameAsOpBase<0, 1>>]>,
977c992
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
1153,1155c1168
<     VectorType getSourceVectorType() {
<       return getVector().getType().cast<VectorType>();
<     }
---
>     VectorType getVectorType(){ return getVector().getType().cast<VectorType>(); }
1733,1734c1746,1747
<     result[0] := if mask[0] then base[i + 0] else pass_thru[0]
<     result[1] := if mask[1] then base[i + 1] else pass_thru[1]
---
>     result[0] := mask[0] ? base[i+0] : pass_thru[0]
>     result[1] := mask[1] ? base[i+1] : pass_thru[1]
1737,1742d1749
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
< 
1799,1804d1805
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
1841,1844c1842
<   Vector_Op<"gather", [
<     DeclareOpInterfaceMethods<MaskableOpInterface>,
<     DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
<   ]>,
---
>   Vector_Op<"gather">,
1854c1852
<     index vector and a mask vector
---
>     index vector and mask
1858,1864c1856,1860
<     The gather operation returns an n-D vector whose elements are either loaded
<     from memory or ranked tensor, or taken from a pass-through vector, depending
<     on the values of an n-D mask vector.
<     If a mask bit is set, the corresponding result element is defined by the base
<     with indices and the n-D index vector (each index is a 1-D offset on the base).
<     Otherwise, the corresponding element is taken from the n-D pass-through vector.
<     Informally the semantics are:
---
>     The gather operation gathers elements from memory or ranked tensor into a
>     n-D vector as defined by a base with indices and an additional n-D index
>     vector (each index is a 1-D offset on the base), but only if the
>     corresponding bit is set in a n-D mask vector. Otherwise, the element is
>     taken from a n-D pass-through vector. Informally the semantics are:
1866,1867c1862,1863
<     result[0] := if mask[0] then base[index[0]] else pass_thru[0]
<     result[1] := if mask[1] then base[index[1]] else pass_thru[1]
---
>     result[0] := mask[0] ? base[index[0]] : pass_thru[0]
>     result[1] := mask[1] ? base[index[1]] : pass_thru[1]
1870,1874c1866
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
---
>     The vector dialect leaves out-of-bounds behavior undefined.
1890d1881
< 
1892,1896c1883,1897
<     ShapedType getBaseType() { return getBase().getType(); }
<     VectorType getIndexVectorType() { return getIndexVec().getType(); }
<     VectorType getMaskVectorType() { return getMask().getType(); }
<     VectorType getPassThruVectorType() { return getPassThru().getType(); }
<     VectorType getVectorType() { return getResult().getType(); }
---
>     ShapedType getBaseType() {
>       return getBase().getType().cast<ShapedType>();
>     }
>     VectorType getIndexVectorType() {
>       return getIndexVec().getType().cast<VectorType>();
>     }
>     VectorType getMaskVectorType() {
>       return getMask().getType().cast<VectorType>();
>     }
>     VectorType getPassThruVectorType() {
>       return getPassThru().getType().cast<VectorType>();
>     }
>     VectorType getVectorType() {
>       return getResult().getType().cast<VectorType>();
>     }
1898d1898
< 
1916,1919c1916
<   let summary = [{
<     scatters elements from a vector into memory as defined by an index vector
<     and a mask vector
<   }];
---
>   let summary = "scatters elements from a vector into memory as defined by an index vector and mask";
1922c1919
<     The scatter operation stores elements from a 1-D vector into memory as
---
>     The scatter operation scatters elements from a 1-D vector into memory as
1931,1940c1928,1931
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
<     If the index vector contains two or more duplicate indices, the behavior is
<     undefined. Underlying implementation may enforce strict sequential
<     semantics.
<     TODO: always enforce strict sequential semantics?
---
>     The vector dialect leaves out-of-bounds and repeated index behavior
>     undefined. Underlying implementations may enforce strict sequential
>     semantics for the latter, though.
>     TODO: enforce the latter always?
1958d1948
< 
1960,1963c1950,1961
<     MemRefType getMemRefType() { return getBase().getType(); }
<     VectorType getIndexVectorType() { return getIndexVec().getType(); }
<     VectorType getMaskVectorType() { return getMask().getType(); }
<     VectorType getVectorType() { return getValueToStore().getType(); }
---
>     MemRefType getMemRefType() {
>       return getBase().getType().cast<MemRefType>();
>     }
>     VectorType getIndexVectorType() {
>       return getIndexVec().getType().cast<VectorType>();
>     }
>     VectorType getMaskVectorType() {
>       return getMask().getType().cast<VectorType>();
>     }
>     VectorType getVectorType() {
>       return getValueToStore().getType().cast<VectorType>();
>     }
1965d1962
< 
1991,1992c1988,1989
<     result[0] := if mask[0] then base[index++] else pass_thru[0]
<     result[1] := if mask[1] then base[index++] else pass_thru[1]
---
>     result[0] := mask[0] ? base[index++] : pass_thru[0]
>     result[1] := mask[1] ? base[index++] : pass_thru[1]
1997,2001d1993
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
< 
2060,2064d2051
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
2314,2318d2300
<   let builders = [
<     // Build with mixed static/dynamic operands.
<     OpBuilder<(ins "VectorType":$type, "ArrayRef<OpFoldResult>":$mixedOperands)>
<   ];
< 
2395d2376
<     Block *getMaskBlock() { return &getMaskRegion().front(); }
2399d2379
<   let hasCanonicalizer = 1;
2440c2420
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
2443c2423
<     VectorType getResultVectorType() {
---
>     VectorType getResultType() {
2458,2462c2438
<   Vector_Op<"print", []>,
<   Arguments<(ins Type<Or<[
<     AnyVectorOfAnyRank.predicate,
<     AnyInteger.predicate, Index.predicate, AnyFloat.predicate
<   ]>>:$source)> {
---
>   Vector_Op<"print", []>, Arguments<(ins AnyType:$source)> {
2590c2566
<     The `vector.flat_transpose` op treats the 1-D input `matrix` as
---
>     The vector.flat_transpose op treats the 1-D input `matrix` as
2601,2602c2577,2578
<     %1 = vector.flat_transpose %0 {columns = 4 : i32, rows = 4 : i32}
<        : vector<16xf32> -> vector<16xf32>
---
>     %1 = vector.flat_transpose %0 { rows = 4: i32, columns = 4: i32 }
>        : (vector<16xf32>) -> vector<16xf32>
--- include/mlir/Dialect/Vector/IR/VectorOps.h
47d46
< class ContractionOp;
80,84d78
< /// Cast away the leading unit dim, if exists, for the given contract op.
< /// Return success if the transformation applies; return failure otherwise.
< LogicalResult castAwayContractionLeadingOneDim(vector::ContractionOp contractOp,
<                                                RewriterBase &rewriter);
< 
118a113,123
> /// Collect a set of transfer read/write lowering patterns.
> ///
> /// These patterns lower transfer ops to simpler ops like `vector.load`,
> /// `vector.store` and `vector.broadcast`. Only transfers with a transfer rank
> /// of a most `maxTransferRank` are lowered. This is useful when combined with
> /// VectorToSCF, which reduces the rank of vector transfer ops.
> void populateVectorTransferLoweringPatterns(
>     RewritePatternSet &patterns,
>     std::optional<unsigned> maxTransferRank = std::nullopt,
>     PatternBenefit benefit = 1);
> 
123a129,149
> /// Collects patterns to progressively lower vector.broadcast ops on high-D
> /// vectors to low-D vector ops.
> void populateVectorBroadcastLoweringPatterns(RewritePatternSet &patterns,
>                                              PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector mask ops into elementary
> /// selection and insertion ops.
> void populateVectorMaskOpLoweringPatterns(RewritePatternSet &patterns,
>                                           PatternBenefit benefit = 1);
> 
> /// Collects patterns to progressively lower vector.shape_cast ops on high-D
> /// vectors into 1-D/2-D vector ops by generating data movement extract/insert
> /// ops.
> void populateVectorShapeCastLoweringPatterns(RewritePatternSet &patterns,
>                                              PatternBenefit benefit = 1);
> 
> /// Collects patterns that lower scalar vector transfer ops to memref loads and
> /// stores when beneficial.
> void populateScalarVectorTransferLoweringPatterns(RewritePatternSet &patterns,
>                                                   PatternBenefit benefit = 1);
> 
168c194
<                          Value v1, Value acc, Value mask = Value());
---
>                          Value v1, Value v2);
191,201c217,218
< Operation *maskOperation(OpBuilder &builder, Operation *maskableOp, Value mask,
<                          Value passthru = Value());
< 
< /// Creates a vector select operation that picks values from `newValue` or
< /// `passthru` for each result vector lane based on `mask`. This utility is used
< /// to propagate the pass-thru value for masked-out or expeculatively executed
< /// lanes. VP intrinsics do not support pass-thru values and every mask-out lane
< /// is set to poison. LLVM backends are usually able to match op + select
< /// patterns and fold them into a native target instructions.
< Value selectPassthru(OpBuilder &builder, Value mask, Value newValue,
<                      Value passthru);
---
> Operation *maskOperation(RewriterBase &rewriter, Operation *maskableOp,
>                          Value mask);
--- include/mlir/Dialect/Vector/IR/CMakeLists.txt
--- include/mlir/Dialect/Vector/IR/VectorOps.td
32a33
>   let useFoldAPI = kEmitFoldAdaptorFolder;
94d94
<       DeclareOpInterfaceMethods<MaskableOpInterface>,
97a98
>                Variadic<VectorOf<[I1]>>:$masks,
112,115c113,115
<     If operands and the result have types of different bitwidths, operands are
<     promoted to have the same bitwidth as the result before performing the
<     contraction. For integer types, only signless integer types are supported,
<     and the promotion happens via sign extension.
---
>     Optional vector mask arguments (produced by CreateMaskOp or ConstantMaskOp)
>     specify the dynamic dimension sizes of valid data within the lhs/rhs vector
>     arguments.
192a193,200
>     // 4D vector contraction with two contracting dimensions and optional
>     // vector mask arguments.
>     %lhs_mask = vector.constant_mask [7, 8, 16, 15] : vector<7x8x16x15xi1>
>     %rhs_mask = vector.constant_mask [8, 16, 7, 5] : vector<8x16x7x5xi1>
> 
>     %5 = vector.contract #contraction_trait %0, %1, %2, %lhs_mask, %rhs_mask
>        : vector<7x8x16x15xf32>, vector<8x16x7x5xf32> into vector<8x15x8x5xf32>
> 
195c203
<     %5 = vector.contract #contraction_trait %0, %1, %2
---
>     %6 = vector.contract #contraction_trait %0, %1, %2
209c217
<     %6 = vector.contract #contraction_trait %0, %1, %2
---
>     %7 = vector.contract #contraction_trait %0, %1, %2
230a239,246
>     VectorType getLHSVectorMaskType() {
>       if (llvm::size(getMasks()) != 2) return VectorType();
>       return getOperand(3).getType().cast<VectorType>();
>     }
>     VectorType getRHSVectorMaskType() {
>       if (llvm::size(getMasks()) != 2) return VectorType();
>       return getOperand(4).getType().cast<VectorType>();
>     }
303c319
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
431c447
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
448c464
<     ///   2. `broadcastedDims` must be confined to [0 .. rank(value.getResultVectorType)]
---
>     ///   2. `broadcastedDims` must be confined to [0 .. rank(value.getVectorType)]
519c535
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
532c548
<                     "$_self.cast<VectorType>().getElementType()">]>,
---
>                     "$_self.cast<ShapedType>().getElementType()">]>,
566c582
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
601c617
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
647c663
<                     "$_self.cast<VectorType>().getElementType()">,
---
>                     "$_self.cast<ShapedType>().getElementType()">,
906,907c922
<                 TCresVTEtIsSameAsOpBase<0, 1>>,
<     DeclareOpInterfaceMethods<MaskableOpInterface>]>,
---
>                 TCresVTEtIsSameAsOpBase<0, 1>>]>,
977c992
<     VectorType getResultVectorType() {
---
>     VectorType getVectorType() {
1153,1155c1168
<     VectorType getSourceVectorType() {
<       return getVector().getType().cast<VectorType>();
<     }
---
>     VectorType getVectorType(){ return getVector().getType().cast<VectorType>(); }
1733,1734c1746,1747
<     result[0] := if mask[0] then base[i + 0] else pass_thru[0]
<     result[1] := if mask[1] then base[i + 1] else pass_thru[1]
---
>     result[0] := mask[0] ? base[i+0] : pass_thru[0]
>     result[1] := mask[1] ? base[i+1] : pass_thru[1]
1737,1742d1749
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
< 
1799,1804d1805
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
1841,1844c1842
<   Vector_Op<"gather", [
<     DeclareOpInterfaceMethods<MaskableOpInterface>,
<     DeclareOpInterfaceMethods<VectorUnrollOpInterface, ["getShapeForUnroll"]>
<   ]>,
---
>   Vector_Op<"gather">,
1854c1852
<     index vector and a mask vector
---
>     index vector and mask
1858,1864c1856,1860
<     The gather operation returns an n-D vector whose elements are either loaded
<     from memory or ranked tensor, or taken from a pass-through vector, depending
<     on the values of an n-D mask vector.
<     If a mask bit is set, the corresponding result element is defined by the base
<     with indices and the n-D index vector (each index is a 1-D offset on the base).
<     Otherwise, the corresponding element is taken from the n-D pass-through vector.
<     Informally the semantics are:
---
>     The gather operation gathers elements from memory or ranked tensor into a
>     n-D vector as defined by a base with indices and an additional n-D index
>     vector (each index is a 1-D offset on the base), but only if the
>     corresponding bit is set in a n-D mask vector. Otherwise, the element is
>     taken from a n-D pass-through vector. Informally the semantics are:
1866,1867c1862,1863
<     result[0] := if mask[0] then base[index[0]] else pass_thru[0]
<     result[1] := if mask[1] then base[index[1]] else pass_thru[1]
---
>     result[0] := mask[0] ? base[index[0]] : pass_thru[0]
>     result[1] := mask[1] ? base[index[1]] : pass_thru[1]
1870,1874c1866
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
---
>     The vector dialect leaves out-of-bounds behavior undefined.
1890d1881
< 
1892,1896c1883,1897
<     ShapedType getBaseType() { return getBase().getType(); }
<     VectorType getIndexVectorType() { return getIndexVec().getType(); }
<     VectorType getMaskVectorType() { return getMask().getType(); }
<     VectorType getPassThruVectorType() { return getPassThru().getType(); }
<     VectorType getVectorType() { return getResult().getType(); }
---
>     ShapedType getBaseType() {
>       return getBase().getType().cast<ShapedType>();
>     }
>     VectorType getIndexVectorType() {
>       return getIndexVec().getType().cast<VectorType>();
>     }
>     VectorType getMaskVectorType() {
>       return getMask().getType().cast<VectorType>();
>     }
>     VectorType getPassThruVectorType() {
>       return getPassThru().getType().cast<VectorType>();
>     }
>     VectorType getVectorType() {
>       return getResult().getType().cast<VectorType>();
>     }
1898d1898
< 
1916,1919c1916
<   let summary = [{
<     scatters elements from a vector into memory as defined by an index vector
<     and a mask vector
<   }];
---
>   let summary = "scatters elements from a vector into memory as defined by an index vector and mask";
1922c1919
<     The scatter operation stores elements from a 1-D vector into memory as
---
>     The scatter operation scatters elements from a 1-D vector into memory as
1931,1940c1928,1931
< 
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
<     If the index vector contains two or more duplicate indices, the behavior is
<     undefined. Underlying implementation may enforce strict sequential
<     semantics.
<     TODO: always enforce strict sequential semantics?
---
>     The vector dialect leaves out-of-bounds and repeated index behavior
>     undefined. Underlying implementations may enforce strict sequential
>     semantics for the latter, though.
>     TODO: enforce the latter always?
1958d1948
< 
1960,1963c1950,1961
<     MemRefType getMemRefType() { return getBase().getType(); }
<     VectorType getIndexVectorType() { return getIndexVec().getType(); }
<     VectorType getMaskVectorType() { return getMask().getType(); }
<     VectorType getVectorType() { return getValueToStore().getType(); }
---
>     MemRefType getMemRefType() {
>       return getBase().getType().cast<MemRefType>();
>     }
>     VectorType getIndexVectorType() {
>       return getIndexVec().getType().cast<VectorType>();
>     }
>     VectorType getMaskVectorType() {
>       return getMask().getType().cast<VectorType>();
>     }
>     VectorType getVectorType() {
>       return getValueToStore().getType().cast<VectorType>();
>     }
1965d1962
< 
1991,1992c1988,1989
<     result[0] := if mask[0] then base[index++] else pass_thru[0]
<     result[1] := if mask[1] then base[index++] else pass_thru[1]
---
>     result[0] := mask[0] ? base[index++] : pass_thru[0]
>     result[1] := mask[1] ? base[index++] : pass_thru[1]
1997,2001d1993
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, the value
<     comes from the pass-through vector regardless of the index, and the index is
<     allowed to be out-of-bounds.
< 
2060,2064d2051
<     If a mask bit is set and the corresponding index is out-of-bounds for the
<     given base, the behavior is undefined. If a mask bit is not set, no value
<     is stored regardless of the index, and the index is allowed to be
<     out-of-bounds.
< 
2314,2318d2300
<   let builders = [
<     // Build with mixed static/dynamic operands.
<     OpBuilder<(ins "VectorType":$type, "ArrayRef<OpFoldResult>":$mixedOperands)>
<   ];
< 
2395d2376
<     Block *getMaskBlock() { return &getMaskRegion().front(); }
2399d2379
<   let hasCanonicalizer = 1;
2440c2420
<     VectorType getSourceVectorType() {
---
>     VectorType getVectorType() {
2443c2423
<     VectorType getResultVectorType() {
---
>     VectorType getResultType() {
2458,2462c2438
<   Vector_Op<"print", []>,
<   Arguments<(ins Type<Or<[
<     AnyVectorOfAnyRank.predicate,
<     AnyInteger.predicate, Index.predicate, AnyFloat.predicate
<   ]>>:$source)> {
---
>   Vector_Op<"print", []>, Arguments<(ins AnyType:$source)> {
2590c2566
<     The `vector.flat_transpose` op treats the 1-D input `matrix` as
---
>     The vector.flat_transpose op treats the 1-D input `matrix` as
2601,2602c2577,2578
<     %1 = vector.flat_transpose %0 {columns = 4 : i32, rows = 4 : i32}
<        : vector<16xf32> -> vector<16xf32>
---
>     %1 = vector.flat_transpose %0 { rows = 4: i32, columns = 4: i32 }
>        : (vector<16xf32>) -> vector<16xf32>
--- include/mlir/Dialect/Vector/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.h include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.h
11a12
> #include "mlir/Dialect/PDL/IR/PDLTypes.h"
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.td include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
13a15
> include "mlir/Dialect/PDL/IR/PDLTypes.td"
18,33c20,22
< // TODO: not isolated from above and better targetability of the op.
< // TODO: not a functional-style transform.
< class TransformWithPatternsOp<string opname, list<Trait> traits = []>
<     : Op<Transform_Dialect, opname,
<          !listconcat([TransformOpInterface,
<                       TransformEachOpTrait,
<                       TransformWithPatternsOpTrait,
<                       MemoryEffectsOpInterface,
<                       FunctionalStyleTransformOpTrait], traits)> {
<   let extraClassDeclaration = [{
<     void populatePatterns(RewritePatternSet &patterns);
<   }];
< }
< 
< def ApplyRankReducingSubviewPatternsOp : 
<   TransformWithPatternsOp<"vector.apply_rank_reducing_subview_patterns"> {
---
> def LowerVectorsOp : Op<Transform_Dialect, "vector.lower_vectors",
>     [DeclareOpInterfaceMethods<TransformOpInterface>,
>      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
35,79c24,25
<     Apply opt-in vector transfer permutation patterns that include:
<       - TransferReadDropUnitDimsPattern
<       - TransferWriteDropUnitDimsPattern
<     
<     These patterns have the effect of rewriting a vector.transfer with unit 
<     dimensions into a rank-reduced version thanks to subview operations.
<     This is complemented by shape_cast folding patterns.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def ApplyTransferPermutationPatternsOp : 
<   TransformWithPatternsOp<"vector.apply_transfer_permutation_patterns"> {
<   let description = [{
<     Apply opt-in vector transfer permutation patterns that include:
<       - TransferReadPermutationLowering
<       - TransferWritePermutationLowering
<       - TransferOpReduceRank
<       - TransferWriteNonPermutationLowering
<     
<     These patterns have the effect of rewriting a vector.transfer with an 
<     arbitrary permutation_map to a vector.transfer with a permutation_map that is
<     a minor identity followed by a vector.transpose.
< 
<     In other words, this makes the vector.transfer contiguous on the most minor
<     dimensions and materializes the permutation_map as a vector.transpose.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
---
>     Indicates that the vector operations nested under the isolated from above op
>     `target` should be lowered to finer-grained vector primitives.
81,84c27
< def LowerBroadcastOp : TransformWithPatternsOp<"vector.lower_broadcast"> {
<   let description = [{
<     Indicates that the vector outerproduct operations nested under the isolated
<     from above op `target` should be lowered to finer-grained vector primitives.
---
>     At this time, the transform is all or nothing.
90,111c33,34
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerContractionOp : TransformWithPatternsOp<"vector.lower_contraction"> {
<   let description = [{
<     Indicates that the vector contraction-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>   // TODO: evolve this to proper enums.
>   let arguments = (ins PDL_Operation:$target,
113,196c36
<        "vector::VectorContractLowering::OuterProduct">:$lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`lowering_strategy` `=` $lowering_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerMasksOp : TransformWithPatternsOp<"vector.lower_masks"> {
<   let description = [{
<     Indicates that the vector.create_mask and vector.constant_mask operations
<     nested under the isolated from above op `target` should be lowered to
<     finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerMaskedTransfersOp : TransformWithPatternsOp<"vector.lower_masked_transfers"> {
<   let description = [{
<     Indicates that masked vector.transfer and vector.gather operations nested
<     under the isolated from above op `target` should be lowered to finer-grained
<     vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def MaterializeMasksOp : TransformWithPatternsOp<"vector.materialize_masks"> {
<   let description = [{
<     Indicates that mask operations nested under the isolated from above op 
<     `target` should be lowered to fine-grained arithemtic operations.
< 
<     This is usually the last step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerMultiReductionOp
<     : TransformWithPatternsOp<"vector.lower_multi_reduction"> {
<   let description = [{
<     Indicates that the vector multi_reduction-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>        "vector::VectorContractLowering::OuterProduct">:$contraction_lowering,
199,313c39
<          $lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`lowering_strategy` `=` $lowering_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerOuterProductOp : TransformWithPatternsOp<"vector.lower_outerproduct"> {
<   let description = [{
<     Indicates that the vector outerproduct operations nested under the isolated
<     from above op `target` should be lowered to finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerShapeCastOp : TransformWithPatternsOp<"vector.lower_shape_cast"> {
<   let description = [{
<     Indicates that the vector shape_cast operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerTransferOp : TransformWithPatternsOp<"vector.lower_transfer"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`max_transfer_rank` `=` $max_transfer_rank^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerTransposeOp : TransformWithPatternsOp<"vector.lower_transpose"> {
<   let description = [{
<     Indicates that the vector transpose-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<VectorTransposeLoweringAttr,
<        "vector::VectorTransposeLowering::EltWise">:$lowering_strategy,
<      DefaultValuedAttr<BoolAttr, "false">:$avx2_lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     oilist (
<       `lowering_strategy` `=` $lowering_strategy
<       | `avx2_lowering_strategy` `=` $avx2_lowering_strategy
<     )
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve split_transfer_strategy to proper enums.
< def SplitTransferFullPartialOp
<     : TransformWithPatternsOp<"vector.split_transfer_full_partial"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be split to full and partial parts.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>          $multireduction_lowering,
315,341c41,59
<        "vector::VectorTransferSplit::LinalgCopy">:$split_transfer_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`split_transfer_strategy` `=` $split_transfer_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def TransferToScfOp : TransformWithPatternsOp<"vector.transfer_to_scf"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be rewritten with scf.for loops over
<     finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank,
<      DefaultValuedAttr<BoolAttr, "false">:$full_unroll
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
---
>        "vector::VectorTransferSplit::LinalgCopy">:$split_transfers,
>      DefaultValuedAttr<VectorTransposeLoweringAttr,
>        "vector::VectorTransposeLowering::EltWise">:$transpose_lowering,
>      DefaultValuedAttr<BoolAttr, "false">:$transpose_avx2_lowering,
>      DefaultValuedAttr<BoolAttr, "true">:$unroll_vector_transfers
>   );
>   let results = (outs PDL_Operation:$results);
> 
>   let builders = [
>     OpBuilder<(ins "Type":$resultType, "Value":$target,
>       "const vector::LowerVectorsOptions &":$options), [{
>         return build($_builder, $_state, resultType, target,
>           options.vectorContractLowering,
>           options.vectorMultiReductionLowering, options.vectorTransferSplit,
>           options.vectorTransposeLowering, options.transposeAVX2Lowering,
>           options.unrollVectorTransfers);
>       }]
>     >
>   ];
346,347c64,67
<         `max_transfer_rank` `=` $max_transfer_rank
<       | `full_unroll` `=` $full_unroll
---
>       `contraction_lowering` `=` $contraction_lowering
>       | `multireduction_lowering` `=` $multireduction_lowering
>       | `split_transfers` `=` $split_transfers
>       | `transpose_lowering` `=` $transpose_lowering
350d69
<     `:` functional-type($target, results)
--- include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
13a15
> include "mlir/Dialect/PDL/IR/PDLTypes.td"
18,33c20,22
< // TODO: not isolated from above and better targetability of the op.
< // TODO: not a functional-style transform.
< class TransformWithPatternsOp<string opname, list<Trait> traits = []>
<     : Op<Transform_Dialect, opname,
<          !listconcat([TransformOpInterface,
<                       TransformEachOpTrait,
<                       TransformWithPatternsOpTrait,
<                       MemoryEffectsOpInterface,
<                       FunctionalStyleTransformOpTrait], traits)> {
<   let extraClassDeclaration = [{
<     void populatePatterns(RewritePatternSet &patterns);
<   }];
< }
< 
< def ApplyRankReducingSubviewPatternsOp : 
<   TransformWithPatternsOp<"vector.apply_rank_reducing_subview_patterns"> {
---
> def LowerVectorsOp : Op<Transform_Dialect, "vector.lower_vectors",
>     [DeclareOpInterfaceMethods<TransformOpInterface>,
>      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
35,79c24,25
<     Apply opt-in vector transfer permutation patterns that include:
<       - TransferReadDropUnitDimsPattern
<       - TransferWriteDropUnitDimsPattern
<     
<     These patterns have the effect of rewriting a vector.transfer with unit 
<     dimensions into a rank-reduced version thanks to subview operations.
<     This is complemented by shape_cast folding patterns.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def ApplyTransferPermutationPatternsOp : 
<   TransformWithPatternsOp<"vector.apply_transfer_permutation_patterns"> {
<   let description = [{
<     Apply opt-in vector transfer permutation patterns that include:
<       - TransferReadPermutationLowering
<       - TransferWritePermutationLowering
<       - TransferOpReduceRank
<       - TransferWriteNonPermutationLowering
<     
<     These patterns have the effect of rewriting a vector.transfer with an 
<     arbitrary permutation_map to a vector.transfer with a permutation_map that is
<     a minor identity followed by a vector.transpose.
< 
<     In other words, this makes the vector.transfer contiguous on the most minor
<     dimensions and materializes the permutation_map as a vector.transpose.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
---
>     Indicates that the vector operations nested under the isolated from above op
>     `target` should be lowered to finer-grained vector primitives.
81,84c27
< def LowerBroadcastOp : TransformWithPatternsOp<"vector.lower_broadcast"> {
<   let description = [{
<     Indicates that the vector outerproduct operations nested under the isolated
<     from above op `target` should be lowered to finer-grained vector primitives.
---
>     At this time, the transform is all or nothing.
90,111c33,34
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerContractionOp : TransformWithPatternsOp<"vector.lower_contraction"> {
<   let description = [{
<     Indicates that the vector contraction-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>   // TODO: evolve this to proper enums.
>   let arguments = (ins PDL_Operation:$target,
113,196c36
<        "vector::VectorContractLowering::OuterProduct">:$lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`lowering_strategy` `=` $lowering_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerMasksOp : TransformWithPatternsOp<"vector.lower_masks"> {
<   let description = [{
<     Indicates that the vector.create_mask and vector.constant_mask operations
<     nested under the isolated from above op `target` should be lowered to
<     finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerMaskedTransfersOp : TransformWithPatternsOp<"vector.lower_masked_transfers"> {
<   let description = [{
<     Indicates that masked vector.transfer and vector.gather operations nested
<     under the isolated from above op `target` should be lowered to finer-grained
<     vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def MaterializeMasksOp : TransformWithPatternsOp<"vector.materialize_masks"> {
<   let description = [{
<     Indicates that mask operations nested under the isolated from above op 
<     `target` should be lowered to fine-grained arithemtic operations.
< 
<     This is usually the last step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerMultiReductionOp
<     : TransformWithPatternsOp<"vector.lower_multi_reduction"> {
<   let description = [{
<     Indicates that the vector multi_reduction-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>        "vector::VectorContractLowering::OuterProduct">:$contraction_lowering,
199,313c39
<          $lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`lowering_strategy` `=` $lowering_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerOuterProductOp : TransformWithPatternsOp<"vector.lower_outerproduct"> {
<   let description = [{
<     Indicates that the vector outerproduct operations nested under the isolated
<     from above op `target` should be lowered to finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerShapeCastOp : TransformWithPatternsOp<"vector.lower_shape_cast"> {
<   let description = [{
<     Indicates that the vector shape_cast operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def LowerTransferOp : TransformWithPatternsOp<"vector.lower_transfer"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`max_transfer_rank` `=` $max_transfer_rank^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve lowering_strategy to proper enums.
< def LowerTransposeOp : TransformWithPatternsOp<"vector.lower_transpose"> {
<   let description = [{
<     Indicates that the vector transpose-like operations nested under the 
<     isolated from above op `target` should be lowered to finer-grained vector
<     primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<VectorTransposeLoweringAttr,
<        "vector::VectorTransposeLowering::EltWise">:$lowering_strategy,
<      DefaultValuedAttr<BoolAttr, "false">:$avx2_lowering_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     oilist (
<       `lowering_strategy` `=` $lowering_strategy
<       | `avx2_lowering_strategy` `=` $avx2_lowering_strategy
<     )
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< // TODO: evolve split_transfer_strategy to proper enums.
< def SplitTransferFullPartialOp
<     : TransformWithPatternsOp<"vector.split_transfer_full_partial"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be split to full and partial parts.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>          $multireduction_lowering,
315,341c41,59
<        "vector::VectorTransferSplit::LinalgCopy">:$split_transfer_strategy
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
< 
<   let assemblyFormat = [{
<     $target
<     (`split_transfer_strategy` `=` $split_transfer_strategy^)?
<     attr-dict
<     `:` functional-type($target, results)
<   }];
< }
< 
< def TransferToScfOp : TransformWithPatternsOp<"vector.transfer_to_scf"> {
<   let description = [{
<     Indicates that the vector transfer operations nested under the 
<     isolated from above op `target` should be rewritten with scf.for loops over
<     finer-grained vector primitives.
< 
<     This is usually a late step that is run after bufferization as part of the
<     process of lowering to e.g. LLVM or NVVM.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target,
<      DefaultValuedAttr<I64Attr, "1">:$max_transfer_rank,
<      DefaultValuedAttr<BoolAttr, "false">:$full_unroll
<   );
<   let results = (outs TransformHandleTypeInterface:$results);
---
>        "vector::VectorTransferSplit::LinalgCopy">:$split_transfers,
>      DefaultValuedAttr<VectorTransposeLoweringAttr,
>        "vector::VectorTransposeLowering::EltWise">:$transpose_lowering,
>      DefaultValuedAttr<BoolAttr, "false">:$transpose_avx2_lowering,
>      DefaultValuedAttr<BoolAttr, "true">:$unroll_vector_transfers
>   );
>   let results = (outs PDL_Operation:$results);
> 
>   let builders = [
>     OpBuilder<(ins "Type":$resultType, "Value":$target,
>       "const vector::LowerVectorsOptions &":$options), [{
>         return build($_builder, $_state, resultType, target,
>           options.vectorContractLowering,
>           options.vectorMultiReductionLowering, options.vectorTransferSplit,
>           options.vectorTransposeLowering, options.transposeAVX2Lowering,
>           options.unrollVectorTransfers);
>       }]
>     >
>   ];
346,347c64,67
<         `max_transfer_rank` `=` $max_transfer_rank
<       | `full_unroll` `=` $full_unroll
---
>       `contraction_lowering` `=` $contraction_lowering
>       | `multireduction_lowering` `=` $multireduction_lowering
>       | `split_transfers` `=` $split_transfers
>       | `transpose_lowering` `=` $transpose_lowering
350d69
<     `:` functional-type($target, results)
--- include/mlir/Dialect/Vector/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/Vector/TransformOps/VectorTransformOps.h
11a12
> #include "mlir/Dialect/PDL/IR/PDLTypes.h"
--- include/mlir/Dialect/Index
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Index/IR and include/mlir/Dialect/Index/IR
--- include/mlir/Dialect/Index/CMakeLists.txt
--- include/mlir/Dialect/Index/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Index/IR/IndexDialect.td include/mlir/Dialect/Index/IR/IndexDialect.td
85a86
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Index/IR/IndexOps.h include/mlir/Dialect/Index/IR/IndexOps.h
16d15
< #include "mlir/Interfaces/InferIntRangeInterface.h"
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Index/IR/IndexOps.td include/mlir/Dialect/Index/IR/IndexOps.td
15d14
< include "mlir/Interfaces/InferIntRangeInterface.td"
27,28c26
<     : Op<IndexDialect, mnemonic,
<       [Pure, DeclareOpInterfaceMethods<InferIntRangeInterface>] # traits>;
---
>     : Op<IndexDialect, mnemonic, [Pure] # traits>;
--- include/mlir/Dialect/Index/IR/CMakeLists.txt
--- include/mlir/Dialect/Index/IR/IndexEnums.td
--- include/mlir/Dialect/Index/IR/IndexOps.td
15d14
< include "mlir/Interfaces/InferIntRangeInterface.td"
27,28c26
<     : Op<IndexDialect, mnemonic,
<       [Pure, DeclareOpInterfaceMethods<InferIntRangeInterface>] # traits>;
---
>     : Op<IndexDialect, mnemonic, [Pure] # traits>;
--- include/mlir/Dialect/Index/IR/IndexDialect.td
85a86
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Index/IR/IndexDialect.h
--- include/mlir/Dialect/Index/IR/IndexOps.h
16d15
< #include "mlir/Interfaces/InferIntRangeInterface.h"
--- include/mlir/Dialect/Index/IR/IndexAttrs.h
--- include/mlir/Dialect/SCF
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/IR and include/mlir/Dialect/SCF/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/TransformOps and include/mlir/Dialect/SCF/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms and include/mlir/Dialect/SCF/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Utils and include/mlir/Dialect/SCF/Utils
--- include/mlir/Dialect/SCF/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Utils/AffineCanonicalizationUtils.h include/mlir/Dialect/SCF/Utils/AffineCanonicalizationUtils.h
20a21
> class AffineApplyOp;
21a23
> class FlatAffineValueConstraints;
29,32d30
< namespace affine {
< class FlatAffineValueConstraints;
< } // namespace affine
< 
43,52d40
< 
< /// Match "for loop"-like operations from the SCF dialect.
< LogicalResult matchForLikeLoop(Value iv, OpFoldResult &lb, OpFoldResult &ub,
<                                OpFoldResult &step);
< 
< /// Populate the given constraint set with induction variable constraints of a
< /// "for" loop with the given range and step.
< LogicalResult addLoopRangeConstraints(affine::FlatAffineValueConstraints &cstr,
<                                       Value iv, OpFoldResult lb,
<                                       OpFoldResult ub, OpFoldResult step);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Utils/Utils.h include/mlir/Dialect/SCF/Utils/Utils.h
57,68d56
< // Simpler API if the new yields are just a list of values that can be
< // determined ahead of time.
< inline scf::ForOp
< replaceLoopWithNewYields(OpBuilder &builder, scf::ForOp loop,
<                          ValueRange newIterOperands, ValueRange newYields,
<                          bool replaceIterOperandsUsesInLoop = true) {
<   auto fn = [&](OpBuilder &b, Location loc, ArrayRef<BlockArgument> newBBArgs) {
<     return SmallVector<Value>(newYields.begin(), newYields.end());
<   };
<   return replaceLoopWithNewYields(builder, loop, newIterOperands, fn,
<                                   replaceIterOperandsUsesInLoop);
< }
--- include/mlir/Dialect/SCF/Utils/Utils.h
57,68d56
< // Simpler API if the new yields are just a list of values that can be
< // determined ahead of time.
< inline scf::ForOp
< replaceLoopWithNewYields(OpBuilder &builder, scf::ForOp loop,
<                          ValueRange newIterOperands, ValueRange newYields,
<                          bool replaceIterOperandsUsesInLoop = true) {
<   auto fn = [&](OpBuilder &b, Location loc, ArrayRef<BlockArgument> newBBArgs) {
<     return SmallVector<Value>(newYields.begin(), newYields.end());
<   };
<   return replaceLoopWithNewYields(builder, loop, newIterOperands, fn,
<                                   replaceIterOperandsUsesInLoop);
< }
--- include/mlir/Dialect/SCF/Utils/AffineCanonicalizationUtils.h
20a21
> class AffineApplyOp;
21a23
> class FlatAffineValueConstraints;
29,32d30
< namespace affine {
< class FlatAffineValueConstraints;
< } // namespace affine
< 
43,52d40
< 
< /// Match "for loop"-like operations from the SCF dialect.
< LogicalResult matchForLikeLoop(Value iv, OpFoldResult &lb, OpFoldResult &ub,
<                                OpFoldResult &step);
< 
< /// Populate the given constraint set with induction variable constraints of a
< /// "for" loop with the given range and step.
< LogicalResult addLoopRangeConstraints(affine::FlatAffineValueConstraints &cstr,
<                                       Value iv, OpFoldResult lb,
<                                       OpFoldResult ub, OpFoldResult step);
--- include/mlir/Dialect/SCF/CMakeLists.txt
--- include/mlir/Dialect/SCF/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms/Passes.h include/mlir/Dialect/SCF/Transforms/Passes.h
40c40
< std::unique_ptr<Pass> createTestSCFParallelLoopCollapsingPass();
---
> std::unique_ptr<Pass> createParallelLoopCollapsingPass();
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms/Passes.td include/mlir/Dialect/SCF/Transforms/Passes.td
27c27
<   let dependentDialects = ["affine::AffineDialect", "tensor::TensorDialect",
---
>   let dependentDialects = ["AffineDialect", "tensor::TensorDialect",
40c40
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
53,84c53,55
< def TestSCFParallelLoopCollapsing : Pass<"test-scf-parallel-loop-collapsing"> {
<   let summary = "Test parallel loops collapsing transformation";
<   let constructor = "mlir::createTestSCFParallelLoopCollapsingPass()";
<   let description = [{
<       This pass is purely for testing the scf::collapseParallelLoops
<       transformation. The transformation does not have opinions on how a
<       parallel loop should be collapsed, so this pass is structured for the
<       common case on GPUs of collapsing to a 3d parallel loop. 3 lists can be
<       provided to collapsed-indices-{0,1,2} to represent how the loop should be
<       collapsed and must reference evrey iterator in the original parallel loop.
< 
<     ```mlir
<     # Before:
<     scf.parallel (%arg0, %arg1)
<                  = (%c0, %c0) to (%c2, %c2) step (%c1, %c1) {
<       "test.sink"(%5, %3) : (index, index) -> ()
<       scf.yield
<     }
< 
<     # After:
<     scf.parallel (%arg0) = (%c0) to (%c4) step (%c1) {
<       %0 = arith.remsi %arg0, %c2 : index
<       %1 = arith.divsi %arg0, %c2 : index
<       %2 = arith.muli %0, %c7 : index
<       %3 = arith.addi %2, %c3 : index
<       %4 = arith.muli %1, %c7 : index
<       %5 = arith.addi %4, %c3 : index
<       "test.sink"(%5, %3) : (index, index) -> ()
<     }
<     ```
<   }];
< 
---
> def SCFParallelLoopCollapsing : Pass<"scf-parallel-loop-collapsing"> {
>   let summary = "Collapse parallel loops to use less induction variables";
>   let constructor = "mlir::createParallelLoopCollapsingPass()";
112c83
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms/Patterns.h include/mlir/Dialect/SCF/Transforms/Patterns.h
34,37d33
< FailureOr<ForOp> pipelineForLoop(RewriterBase &rewriter, ForOp forOp,
<                                  const PipeliningOption &options);
< 
< // TODO: such patterns should be auto-generated.
49,51c45
<                                             PatternRewriter &rewriter) const {
<     return pipelineForLoop(rewriter, forOp, options);
<   }
---
>                                             PatternRewriter &rewriter) const;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms/TileUsingInterface.h include/mlir/Dialect/SCF/Transforms/TileUsingInterface.h
21c21
< class RewriterBase;
---
> class PatternRewriter;
99d98
<   SmallVector<Operation *> tiledOps;
247c246
< tileReductionUsingScf(RewriterBase &b, PartialReductionOpInterface op,
---
> tileReductionUsingScf(PatternRewriter &b, PartialReductionOpInterface op,
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/Transforms/Transforms.h include/mlir/Dialect/SCF/Transforms/Transforms.h
73,77c73,77
< /// After loop peeling, this function tries to simplify affine.min and
< /// affine.max ops in the body of the peeled loop and in the body of the partial
< /// iteration loop, taking advantage of the fact that the peeled loop has only
< /// "full" iterations. This simplification is expected to enable further
< /// canonicalization opportunities through other patterns.
---
> /// After loop peeling, this function tries to simplify/canonicalize affine.min
> /// and affine.max ops in the body of the peeled loop and in the body of the
> /// partial iteration loop, taking advantage of the fact that the peeled loop
> /// has only "full" iterations. This canonicalization is expected to enable
> /// further canonicalization opportunities through other patterns.
88,89c88,89
< LogicalResult peelForLoopAndSimplifyBounds(RewriterBase &rewriter, ForOp forOp,
<                                            scf::ForOp &partialIteration);
---
> LogicalResult peelAndCanonicalizeForLoop(RewriterBase &rewriter, ForOp forOp,
>                                          scf::ForOp &partialIteration);
123,128d122
< /// Populates the provided pattern set with patterns that do 1:N type
< /// conversions on (some) SCF ops. This is intended to be used with
< /// applyPartialOneToNConversion.
< void populateSCFStructuralOneToNTypeConversions(TypeConverter &typeConverter,
<                                                 RewritePatternSet &patterns);
< 
161c155
<       std::function<Operation *(RewriterBase &, Operation *, Value)>;
---
>       std::function<Operation *(Operation *, Value, PatternRewriter &)>;
--- include/mlir/Dialect/SCF/Transforms/Passes.h
40c40
< std::unique_ptr<Pass> createTestSCFParallelLoopCollapsingPass();
---
> std::unique_ptr<Pass> createParallelLoopCollapsingPass();
--- include/mlir/Dialect/SCF/Transforms/Passes.td
27c27
<   let dependentDialects = ["affine::AffineDialect", "tensor::TensorDialect",
---
>   let dependentDialects = ["AffineDialect", "tensor::TensorDialect",
40c40
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
53,84c53,55
< def TestSCFParallelLoopCollapsing : Pass<"test-scf-parallel-loop-collapsing"> {
<   let summary = "Test parallel loops collapsing transformation";
<   let constructor = "mlir::createTestSCFParallelLoopCollapsingPass()";
<   let description = [{
<       This pass is purely for testing the scf::collapseParallelLoops
<       transformation. The transformation does not have opinions on how a
<       parallel loop should be collapsed, so this pass is structured for the
<       common case on GPUs of collapsing to a 3d parallel loop. 3 lists can be
<       provided to collapsed-indices-{0,1,2} to represent how the loop should be
<       collapsed and must reference evrey iterator in the original parallel loop.
< 
<     ```mlir
<     # Before:
<     scf.parallel (%arg0, %arg1)
<                  = (%c0, %c0) to (%c2, %c2) step (%c1, %c1) {
<       "test.sink"(%5, %3) : (index, index) -> ()
<       scf.yield
<     }
< 
<     # After:
<     scf.parallel (%arg0) = (%c0) to (%c4) step (%c1) {
<       %0 = arith.remsi %arg0, %c2 : index
<       %1 = arith.divsi %arg0, %c2 : index
<       %2 = arith.muli %0, %c7 : index
<       %3 = arith.addi %2, %c3 : index
<       %4 = arith.muli %1, %c7 : index
<       %5 = arith.addi %4, %c3 : index
<       "test.sink"(%5, %3) : (index, index) -> ()
<     }
<     ```
<   }];
< 
---
> def SCFParallelLoopCollapsing : Pass<"scf-parallel-loop-collapsing"> {
>   let summary = "Collapse parallel loops to use less induction variables";
>   let constructor = "mlir::createParallelLoopCollapsingPass()";
112c83
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
--- include/mlir/Dialect/SCF/Transforms/Patterns.h
34,37d33
< FailureOr<ForOp> pipelineForLoop(RewriterBase &rewriter, ForOp forOp,
<                                  const PipeliningOption &options);
< 
< // TODO: such patterns should be auto-generated.
49,51c45
<                                             PatternRewriter &rewriter) const {
<     return pipelineForLoop(rewriter, forOp, options);
<   }
---
>                                             PatternRewriter &rewriter) const;
--- include/mlir/Dialect/SCF/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/SCF/Transforms/CMakeLists.txt
--- include/mlir/Dialect/SCF/Transforms/TileUsingInterface.h
21c21
< class RewriterBase;
---
> class PatternRewriter;
99d98
<   SmallVector<Operation *> tiledOps;
247c246
< tileReductionUsingScf(RewriterBase &b, PartialReductionOpInterface op,
---
> tileReductionUsingScf(PatternRewriter &b, PartialReductionOpInterface op,
--- include/mlir/Dialect/SCF/Transforms/Transforms.h
73,77c73,77
< /// After loop peeling, this function tries to simplify affine.min and
< /// affine.max ops in the body of the peeled loop and in the body of the partial
< /// iteration loop, taking advantage of the fact that the peeled loop has only
< /// "full" iterations. This simplification is expected to enable further
< /// canonicalization opportunities through other patterns.
---
> /// After loop peeling, this function tries to simplify/canonicalize affine.min
> /// and affine.max ops in the body of the peeled loop and in the body of the
> /// partial iteration loop, taking advantage of the fact that the peeled loop
> /// has only "full" iterations. This canonicalization is expected to enable
> /// further canonicalization opportunities through other patterns.
88,89c88,89
< LogicalResult peelForLoopAndSimplifyBounds(RewriterBase &rewriter, ForOp forOp,
<                                            scf::ForOp &partialIteration);
---
> LogicalResult peelAndCanonicalizeForLoop(RewriterBase &rewriter, ForOp forOp,
>                                          scf::ForOp &partialIteration);
123,128d122
< /// Populates the provided pattern set with patterns that do 1:N type
< /// conversions on (some) SCF ops. This is intended to be used with
< /// applyPartialOneToNConversion.
< void populateSCFStructuralOneToNTypeConversions(TypeConverter &typeConverter,
<                                                 RewritePatternSet &patterns);
< 
161c155
<       std::function<Operation *(RewriterBase &, Operation *, Value)>;
---
>       std::function<Operation *(Operation *, Value, PatternRewriter &)>;
--- include/mlir/Dialect/SCF/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/IR/DeviceMappingInterface.td include/mlir/Dialect/SCF/IR/DeviceMappingInterface.td
33c33
<     Currently, `scf.forall` uses this interface to express the mapping
---
>     Currently, `scf.foreach_thread` uses this interface to express the mapping
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/IR/SCF.h include/mlir/Dialect/SCF/IR/SCF.h
16d15
< #include "mlir/Dialect/Arith/Utils/Utils.h"
22d20
< #include "mlir/Interfaces/InferTypeOpInterface.h"
56c54
< /// Returns the ForallOp parent of an thread index variable.
---
> /// Returns the ForeachThreadOp parent of an thread index variable.
58c56
< ForallOp getForallOpThreadIndexOwner(Value val);
---
> ForeachThreadOp getForeachThreadOpThreadIndexOwner(Value val);
64,71d61
< 
< /// Promotes the loop body of a scf::ForallOp to its containing block if the
< /// loop was known to have a single iteration.
< LogicalResult promoteIfSingleIteration(PatternRewriter &rewriter,
<                                        scf::ForallOp forallOp);
< 
< /// Promotes the loop body of a scf::ForallOp to its containing block.
< void promote(PatternRewriter &rewriter, scf::ForallOp forallOp);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/IR/SCFOps.td include/mlir/Dialect/SCF/IR/SCFOps.td
20d19
< include "mlir/Interfaces/InferTypeOpInterface.td"
28a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
124d123
<        AllTypesMatch<["lowerBound", "upperBound", "step"]>,
131c130
<     The `scf.for` operation represents a loop taking 3 SSA value as operands
---
>     The "scf.for" operation represents a loop taking 3 SSA value as operands
135,138c134,138
<     argument of this region. This SSA value is a signless integer or index.
<     The step is a value of same type but required to be positive. The lower and
<     upper bounds specify a half-open range: the range includes the lower bound
<     but does not include the upper bound.
---
>     argument of this region. This SSA value always has type index, which is the
>     size of the machine word. The step is a value of type index, required to be
>     positive.
>     The lower and upper bounds specify a half-open range: the range includes
>     the lower bound but does not include the upper bound.
141c141
<     `scf.yield`. Calling ForOp::build will create such a region and insert
---
>     "scf.yield". Calling ForOp::build will create such a region and insert
146d145
<     // Index case.
150,154d148
<     ...
<     // Integer case.
<     scf.for %iv_32 = %lb_32 to %ub_32 step %step_32 : i32 {
<       ... // body
<     }
159c153
<     passed as additional SSA operands to the `scf.for` following the 3 loop
---
>     passed as additional SSA operands to the "scf.for" following the 3 loop
165c159
<     The region must terminate with a `scf.yield` that passes the current
---
>     The region must terminate with a "scf.yield" that passes the current
167c161
<     `scf.for` result, if at the last iteration. The static type of a
---
>     "scf.for" result, if at the last iteration. The static type of a
171c165
<     must insert `scf.yield` in that case.
---
>     must insert "scf.yield" in that case.
173c167
<     `scf.for` results hold the final values after the last iteration.
---
>     "scf.for" results hold the final values after the last iteration.
194,196c188,190
<     If the `scf.for` defines any values, a yield must be explicitly present.
<     The number and types of the `scf.for` results must match the initial
<     values in the `iter_args` binding and the yield operands.
---
>     If the "scf.for" defines any values, a yield must be explicitly present.
>     The number and types of the "scf.for" results must match the initial
>     values in the "iter_args" binding and the yield operands.
198c192
<     Another example with a nested `scf.if` (see `scf.if` for details) to
---
>     Another example with a nested "scf.if" (see "scf.if" for details) to
222,224c216,218
<   let arguments = (ins AnySignlessIntegerOrIndex:$lowerBound,
<                        AnySignlessIntegerOrIndex:$upperBound,
<                        AnySignlessIntegerOrIndex:$step,
---
>   let arguments = (ins Index:$lowerBound,
>                        Index:$upperBound,
>                        Index:$step,
357c351
< // ForallOp
---
> // ForeachThreadOp
360c354
< def ForallOp : SCF_Op<"forall", [
---
> def ForeachThreadOp : SCF_Op<"foreach_thread", [
362c356
<        AutomaticAllocationScope,
---
>        SingleBlockImplicitTerminator<"scf::PerformConcurrentlyOp">,
364,365c358,359
<        SingleBlockImplicitTerminator<"scf::InParallelOp">,
<      ]> {
---
>        AutomaticAllocationScope,
>       ]> {
368c362
<     `scf.forall` is a target-independent multi-dimensional parallel
---
>     `scf.foreach_thread` is a target-independent multi-dimensional parallel
370,371c364,365
<     parallel body and it takes index operands that specify lower bounds, upper
<     bounds and steps.
---
>     parallel body and it takes index operands that indicate how many parallel
>     instances of that function are created.
398,400c392,394
<     The only allowed terminator is `scf.forall.in_parallel`.
<     `scf.forall` returns one value per `shared_out` operand. The
<     actions of the `in_parallel` terminators specify how to combine the
---
>     The only allowed terminator is `scf.foreach_thread.perform_concurrently`.
>     `scf.foreach_thread` returns one value per `shared_out` operand. The
>     actions of the `perform_concurrently` terminators specify how to combine the
403c397
<     block argument of the `scf.forall` op.
---
>     block argument of the `scf.foreach_thread` op.
408c402
<     `scf.forall` acts as an implicit synchronization point.
---
>     `scf.foreach_thread` acts as an implicit synchronization point.
413,418c407
<     `scf.forall` can be printed in two different ways depending on
<     whether the loop is normalized or not. The loop is 'normalized' when all
<     lower bounds are equal to zero and steps are equal to one. In that case,
<     `lowerBound` and `step` operands will be omitted during printing.
< 
<     Normalized loop example:
---
>     Example:
424c413
<     %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
---
>     %matmul_and_pointwise:2 = scf.foreach_thread (%thread_id_1, %thread_id_2) in
443,444c432,433
<       scf.forall.in_parallel {
<         scf.forall.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
---
>       scf.foreach_thread.perform_concurrently {
>         scf.foreach_thread.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
447c436
<         scf.forall.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
---
>         scf.foreach_thread.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
456,487d444
<     Loop with loop bounds example:
< 
<     ```mlir
<     //
<     // Sequential context.
<     //
<     %pointwise = scf.forall (%i, %j) = (0, 0) to (%dim1, %dim2)
<       step (%tileSize1, %tileSize2) shared_outs(%o1 = %out)
<       -> (tensor<?x?xT>, tensor<?xT>) {
<       //
<       // Parallel context.
<       //
<       %sA = tensor.extract_slice %A[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
<       %sB = tensor.extract_slice %B[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
<       %sC = tensor.extract_slice %o[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
< 
<       %add = map {"arith.addf"} ins(%sA, %sB) outs(%sC)
< 
<       scf.forall.in_parallel {
<         scf.forall.parallel_insert_slice %add into
<           %o[%i, %j][%tileSize1, %tileSize2][1, 1]
<           : tensor<?x?xT> into tensor<?x?xT>
<       }
<     }
<     // Implicit synchronization point.
<     // Sequential context.
<     //
<     ```
< 
495c452
<     %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
---
>     %matmul_and_pointwise:2 = scf.foreach_thread (%thread_id_1, %thread_id_2) in
502c459
<        scf.forall.in_parallel {
---
>        scf.foreach_thread.perform_concurrently {
516c473
<     %r = scf.forall ... shared_outs(%o = t0) -> tensor<?xf32> {
---
>     %r = scf.foreach_thread ... shared_outs(%o = t0) -> tensor<?xf32> {
518c475
<       // because the scf.forall op's %t0 use bufferizes to a memory
---
>       // because the scf.foreach_thread op's %t0 use bufferizes to a memory
527,535c484,486
<   let arguments = (ins
<     Variadic<Index>:$dynamicLowerBound,
<     Variadic<Index>:$dynamicUpperBound,
<     Variadic<Index>:$dynamicStep,
<     DenseI64ArrayAttr:$staticLowerBound,
<     DenseI64ArrayAttr:$staticUpperBound,
<     DenseI64ArrayAttr:$staticStep,
<     Variadic<AnyRankedTensor>:$outputs,
<     OptionalAttr<DeviceMappingArrayAttr>:$mapping);
---
>   let arguments = (ins Variadic<Index>:$num_threads,
>                        Variadic<AnyRankedTensor>:$outputs,
>                        OptionalAttr<DeviceMappingArrayAttr>:$mapping);
547,558c498,504
<     // Builder that takes loop bounds.
<     OpBuilder<(ins "ArrayRef<OpFoldResult>":$lbs,
<        "ArrayRef<OpFoldResult>":$ubs, "ArrayRef<OpFoldResult>":$steps,
<        "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
<        CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
<             "nullptr"> :$bodyBuilderFn)>,
< 
<     // Builder for normalized loop that takes only upper bounds.
<     OpBuilder<(ins "ArrayRef<OpFoldResult>":$ubs,
<        "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
<        CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
<             "nullptr"> :$bodyBuilderFn)>,
---
>     // Bodyless builder, outputs must be specified.
>     OpBuilder<(ins "ValueRange":$outputs, "ValueRange":$num_threads,
>                    "std::optional<ArrayAttr>":$mapping)>,
>     // Builder that takes a bodyBuilder lambda.
>     OpBuilder<(ins "ValueRange":$outputs, "ValueRange":$num_threads,
>                    "ArrayRef<Attribute>":$mapping,
>                    "function_ref<void(OpBuilder &, Location, ValueRange)>":$bodyBuilder)>
560d505
< 
562,603c507
<     // Get lower bounds as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedLowerBound() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticLowerBound(), getDynamicLowerBound(), b);
<     }
< 
<     // Get upper bounds as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedUpperBound() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticUpperBound(), getDynamicUpperBound(), b);
<     }
< 
<     // Get steps as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedStep() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticStep(), getDynamicStep(), b);
<     }
< 
<     /// Get lower bounds as values.
<     SmallVector<Value> getLowerBound(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedLowerBound());
<     }
< 
<     /// Get upper bounds as values.
<     SmallVector<Value> getUpperBound(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedUpperBound());
<     }
< 
<     /// Get steps as values.
<     SmallVector<Value> getStep(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedStep());
<     }
< 
<     int64_t getRank() { return getStaticLowerBound().size(); }
< 
<     /// Number of operands controlling the loop: lbs, ubs, steps
<     unsigned getNumControlOperands() { return 3 * getRank(); }
< 
<     /// Number of dynamic operands controlling the loop: lbs, ubs, steps
<     unsigned getNumDynamicControlOperands() {
<       return getODSOperandIndexAndLength(3).first;
<     }
---
>     int64_t getRank() { return getNumThreads().size(); }
606,607c510
<       assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
<              "invalid operand");
---
>       assert(opOperand->getOperandNumber() >= getRank() && "invalid operand");
609c512
<           opOperand->getOperandNumber() - getNumDynamicControlOperands());
---
>           opOperand->getOperandNumber() - getRank());
616,618c519
< 
<       return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
<                                            bbArg.getArgNumber() - getRank());
---
>       return &getOperation()->getOpOperand(bbArg.getArgNumber());
624,625c525,526
<       return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
<                                            opResult.getResultNumber());
---
>       return &getOperation()->getOpOperand(
>           opResult.getResultNumber() + getRank());
629,633c530,531
<       assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
<              "invalid operand");
< 
<       return getBody()->getArgument(opOperand->getOperandNumber() -
<                                     getNumDynamicControlOperands() + getRank());
---
>       assert(opOperand->getOperandNumber() >= getRank() && "invalid operand");
>       return getBody()->getArgument(opOperand->getOperandNumber());
640c538
<     ::mlir::ValueRange getInductionVars() {
---
>     ::mlir::ValueRange getThreadIndices() {
644,645c542,543
<     ::mlir::Value getInductionVar(int64_t idx) {
<       return getInductionVars()[idx];
---
>     ::mlir::Value getThreadIndex(int64_t idx) {
>       return getThreadIndices()[idx];
652,653c550,557
<     /// Checks if the lbs are zeros and steps are ones.
<     bool isNormalized();
---
>     /// Helper to sort `values` according to matching `keys`.
>     /// Take a custom `compare` binary comparator which returns true if the first
>     /// element is smaller than the second (i.e. compatible with std::sort).
>     /// This is a helper typically used to sort numThreads values before they are
>     /// mapped to concrete physical dimensions of hardware.
>     static SmallVector<Value> getValuesSortedByKey(
>       ArrayRef<Attribute> keys, ValueRange values,
>       llvm::function_ref<bool(Attribute, Attribute)> compare);
658,659c562
<     static void ensureTerminator(Region & region, OpBuilder & builder,
<                                  Location loc);
---
>     static void ensureTerminator(Region &region, OpBuilder &builder, Location loc);
661c564
<     InParallelOp getTerminator();
---
>     PerformConcurrentlyOp getTerminator();
666c569
< // InParallelOp
---
> // PerformConcurrentlyOp
669c572
< def InParallelOp : SCF_Op<"forall.in_parallel", [
---
> def PerformConcurrentlyOp : SCF_Op<"foreach_thread.perform_concurrently", [
673c576
<        HasParent<"ForallOp">,
---
>        HasParent<"ForeachThreadOp">,
675c578
<   let summary = "terminates a `forall` block";
---
>   let summary = "terminates a `foreach_thread` block";
677,678c580,581
<     `scf.forall.in_parallel` is a designated terminator for
<     the `scf.forall` operation.
---
>     `scf.foreach_thread.perform_concurrently` is a designated terminator for
>     the `scf.foreach_thread` operation.
682c585
<     the enclosing `scf.forall`.
---
>     the enclosing `scf.foreach_thread`.
697,698c600,601
<   // TODO: Add a `InParallelOpInterface` interface for ops that can
<   // appear inside in_parallel.
---
>   // TODO: Add a `PerformConcurrentlyOpInterface` interface for ops that can
>   // appear inside perform_concurrently.
710,714c613,618
< def IfOp : SCF_Op<"if", [DeclareOpInterfaceMethods<RegionBranchOpInterface, [
<     "getNumRegionInvocations", "getRegionInvocationBounds"]>,
<     DeclareOpInterfaceMethods<InferTypeOpInterface>,
<     SingleBlockImplicitTerminator<"scf::YieldOp">, RecursiveMemoryEffects,
<     NoRegionArguments]> {
---
> def IfOp : SCF_Op<"if",
>       [DeclareOpInterfaceMethods<RegionBranchOpInterface,
>                                  ["getNumRegionInvocations",
>                                   "getRegionInvocationBounds"]>,
>        SingleBlockImplicitTerminator<"scf::YieldOp">, RecursiveMemoryEffects,
>        NoRegionArguments]> {
729,730c633,634
<     `scf.if` may also produce results. Which values are returned depends on
<     which execution path is taken.
---
>     `scf.if` may also return results that are defined in its regions. The
>     values defined are determined by which execution path is taken.
746,752c650,654
<     The "then" region has exactly 1 block. The "else" region may have 0 or 1
<     block. In case the `scf.if` produces results, the "else" region must also
<     have exactly 1 block.
< 
<     The blocks are always terminated with `scf.yield`. If `scf.if` defines no
<     values, the `scf.yield` can be left out, and will be inserted implicitly.
<     Otherwise, it must be explicit.
---
>     `scf.if` regions are always terminated with "scf.yield". If "scf.if"
>     defines no values, the "scf.yield" can be left out, and will be inserted
>     implicitly. Otherwise, it must be explicit.
>     Also, if "scf.if" defines one or more values, the 'else' block cannot be
>     omitted.
761,763d662
< 
<     The types of the yielded values must match the result types of the
<     `scf.if`.
767,768c666
<   let regions = (region SizedRegion<1>:$thenRegion,
<                         MaxSizedRegion<1>:$elseRegion);
---
>   let regions = (region SizedRegion<1>:$thenRegion, AnyRegion:$elseRegion);
772,774d669
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond)>,
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
<       "bool":$addThenBlock, "bool":$addElseBlock)>,
778c673,675
<     OpBuilder<(ins "Value":$cond,
---
>     // TODO: Remove builder when it is no longer used to create invalid `if` ops
>     // (with a type mispatch between the op and it's inner `yield` op).
>     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
782a680,684
>     OpBuilder<(ins "Value":$cond,
>       CArg<"function_ref<void(OpBuilder &, Location)>",
>            "buildTerminatedBody">:$thenBuilder,
>       CArg<"function_ref<void(OpBuilder &, Location)>",
>            "nullptr">:$elseBuilder)>
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/SCF/IR/DeviceMappingInterface.h
--- include/mlir/Dialect/SCF/IR/SCF.h
16d15
< #include "mlir/Dialect/Arith/Utils/Utils.h"
22d20
< #include "mlir/Interfaces/InferTypeOpInterface.h"
56c54
< /// Returns the ForallOp parent of an thread index variable.
---
> /// Returns the ForeachThreadOp parent of an thread index variable.
58c56
< ForallOp getForallOpThreadIndexOwner(Value val);
---
> ForeachThreadOp getForeachThreadOpThreadIndexOwner(Value val);
64,71d61
< 
< /// Promotes the loop body of a scf::ForallOp to its containing block if the
< /// loop was known to have a single iteration.
< LogicalResult promoteIfSingleIteration(PatternRewriter &rewriter,
<                                        scf::ForallOp forallOp);
< 
< /// Promotes the loop body of a scf::ForallOp to its containing block.
< void promote(PatternRewriter &rewriter, scf::ForallOp forallOp);
--- include/mlir/Dialect/SCF/IR/CMakeLists.txt
--- include/mlir/Dialect/SCF/IR/SCFOps.td
20d19
< include "mlir/Interfaces/InferTypeOpInterface.td"
28a28
>   let useFoldAPI = kEmitFoldAdaptorFolder;
124d123
<        AllTypesMatch<["lowerBound", "upperBound", "step"]>,
131c130
<     The `scf.for` operation represents a loop taking 3 SSA value as operands
---
>     The "scf.for" operation represents a loop taking 3 SSA value as operands
135,138c134,138
<     argument of this region. This SSA value is a signless integer or index.
<     The step is a value of same type but required to be positive. The lower and
<     upper bounds specify a half-open range: the range includes the lower bound
<     but does not include the upper bound.
---
>     argument of this region. This SSA value always has type index, which is the
>     size of the machine word. The step is a value of type index, required to be
>     positive.
>     The lower and upper bounds specify a half-open range: the range includes
>     the lower bound but does not include the upper bound.
141c141
<     `scf.yield`. Calling ForOp::build will create such a region and insert
---
>     "scf.yield". Calling ForOp::build will create such a region and insert
146d145
<     // Index case.
150,154d148
<     ...
<     // Integer case.
<     scf.for %iv_32 = %lb_32 to %ub_32 step %step_32 : i32 {
<       ... // body
<     }
159c153
<     passed as additional SSA operands to the `scf.for` following the 3 loop
---
>     passed as additional SSA operands to the "scf.for" following the 3 loop
165c159
<     The region must terminate with a `scf.yield` that passes the current
---
>     The region must terminate with a "scf.yield" that passes the current
167c161
<     `scf.for` result, if at the last iteration. The static type of a
---
>     "scf.for" result, if at the last iteration. The static type of a
171c165
<     must insert `scf.yield` in that case.
---
>     must insert "scf.yield" in that case.
173c167
<     `scf.for` results hold the final values after the last iteration.
---
>     "scf.for" results hold the final values after the last iteration.
194,196c188,190
<     If the `scf.for` defines any values, a yield must be explicitly present.
<     The number and types of the `scf.for` results must match the initial
<     values in the `iter_args` binding and the yield operands.
---
>     If the "scf.for" defines any values, a yield must be explicitly present.
>     The number and types of the "scf.for" results must match the initial
>     values in the "iter_args" binding and the yield operands.
198c192
<     Another example with a nested `scf.if` (see `scf.if` for details) to
---
>     Another example with a nested "scf.if" (see "scf.if" for details) to
222,224c216,218
<   let arguments = (ins AnySignlessIntegerOrIndex:$lowerBound,
<                        AnySignlessIntegerOrIndex:$upperBound,
<                        AnySignlessIntegerOrIndex:$step,
---
>   let arguments = (ins Index:$lowerBound,
>                        Index:$upperBound,
>                        Index:$step,
357c351
< // ForallOp
---
> // ForeachThreadOp
360c354
< def ForallOp : SCF_Op<"forall", [
---
> def ForeachThreadOp : SCF_Op<"foreach_thread", [
362c356
<        AutomaticAllocationScope,
---
>        SingleBlockImplicitTerminator<"scf::PerformConcurrentlyOp">,
364,365c358,359
<        SingleBlockImplicitTerminator<"scf::InParallelOp">,
<      ]> {
---
>        AutomaticAllocationScope,
>       ]> {
368c362
<     `scf.forall` is a target-independent multi-dimensional parallel
---
>     `scf.foreach_thread` is a target-independent multi-dimensional parallel
370,371c364,365
<     parallel body and it takes index operands that specify lower bounds, upper
<     bounds and steps.
---
>     parallel body and it takes index operands that indicate how many parallel
>     instances of that function are created.
398,400c392,394
<     The only allowed terminator is `scf.forall.in_parallel`.
<     `scf.forall` returns one value per `shared_out` operand. The
<     actions of the `in_parallel` terminators specify how to combine the
---
>     The only allowed terminator is `scf.foreach_thread.perform_concurrently`.
>     `scf.foreach_thread` returns one value per `shared_out` operand. The
>     actions of the `perform_concurrently` terminators specify how to combine the
403c397
<     block argument of the `scf.forall` op.
---
>     block argument of the `scf.foreach_thread` op.
408c402
<     `scf.forall` acts as an implicit synchronization point.
---
>     `scf.foreach_thread` acts as an implicit synchronization point.
413,418c407
<     `scf.forall` can be printed in two different ways depending on
<     whether the loop is normalized or not. The loop is 'normalized' when all
<     lower bounds are equal to zero and steps are equal to one. In that case,
<     `lowerBound` and `step` operands will be omitted during printing.
< 
<     Normalized loop example:
---
>     Example:
424c413
<     %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
---
>     %matmul_and_pointwise:2 = scf.foreach_thread (%thread_id_1, %thread_id_2) in
443,444c432,433
<       scf.forall.in_parallel {
<         scf.forall.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
---
>       scf.foreach_thread.perform_concurrently {
>         scf.foreach_thread.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:
447c436
<         scf.forall.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
---
>         scf.foreach_thread.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:
456,487d444
<     Loop with loop bounds example:
< 
<     ```mlir
<     //
<     // Sequential context.
<     //
<     %pointwise = scf.forall (%i, %j) = (0, 0) to (%dim1, %dim2)
<       step (%tileSize1, %tileSize2) shared_outs(%o1 = %out)
<       -> (tensor<?x?xT>, tensor<?xT>) {
<       //
<       // Parallel context.
<       //
<       %sA = tensor.extract_slice %A[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
<       %sB = tensor.extract_slice %B[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
<       %sC = tensor.extract_slice %o[%i, %j][%tileSize1, %tileSize2][1, 1]
<         : tensor<?x?xT> to tensor<?x?xT>
< 
<       %add = map {"arith.addf"} ins(%sA, %sB) outs(%sC)
< 
<       scf.forall.in_parallel {
<         scf.forall.parallel_insert_slice %add into
<           %o[%i, %j][%tileSize1, %tileSize2][1, 1]
<           : tensor<?x?xT> into tensor<?x?xT>
<       }
<     }
<     // Implicit synchronization point.
<     // Sequential context.
<     //
<     ```
< 
495c452
<     %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in
---
>     %matmul_and_pointwise:2 = scf.foreach_thread (%thread_id_1, %thread_id_2) in
502c459
<        scf.forall.in_parallel {
---
>        scf.foreach_thread.perform_concurrently {
516c473
<     %r = scf.forall ... shared_outs(%o = t0) -> tensor<?xf32> {
---
>     %r = scf.foreach_thread ... shared_outs(%o = t0) -> tensor<?xf32> {
518c475
<       // because the scf.forall op's %t0 use bufferizes to a memory
---
>       // because the scf.foreach_thread op's %t0 use bufferizes to a memory
527,535c484,486
<   let arguments = (ins
<     Variadic<Index>:$dynamicLowerBound,
<     Variadic<Index>:$dynamicUpperBound,
<     Variadic<Index>:$dynamicStep,
<     DenseI64ArrayAttr:$staticLowerBound,
<     DenseI64ArrayAttr:$staticUpperBound,
<     DenseI64ArrayAttr:$staticStep,
<     Variadic<AnyRankedTensor>:$outputs,
<     OptionalAttr<DeviceMappingArrayAttr>:$mapping);
---
>   let arguments = (ins Variadic<Index>:$num_threads,
>                        Variadic<AnyRankedTensor>:$outputs,
>                        OptionalAttr<DeviceMappingArrayAttr>:$mapping);
547,558c498,504
<     // Builder that takes loop bounds.
<     OpBuilder<(ins "ArrayRef<OpFoldResult>":$lbs,
<        "ArrayRef<OpFoldResult>":$ubs, "ArrayRef<OpFoldResult>":$steps,
<        "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
<        CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
<             "nullptr"> :$bodyBuilderFn)>,
< 
<     // Builder for normalized loop that takes only upper bounds.
<     OpBuilder<(ins "ArrayRef<OpFoldResult>":$ubs,
<        "ValueRange":$outputs, "std::optional<ArrayAttr>":$mapping,
<        CArg<"function_ref<void(OpBuilder &, Location, ValueRange)>",
<             "nullptr"> :$bodyBuilderFn)>,
---
>     // Bodyless builder, outputs must be specified.
>     OpBuilder<(ins "ValueRange":$outputs, "ValueRange":$num_threads,
>                    "std::optional<ArrayAttr>":$mapping)>,
>     // Builder that takes a bodyBuilder lambda.
>     OpBuilder<(ins "ValueRange":$outputs, "ValueRange":$num_threads,
>                    "ArrayRef<Attribute>":$mapping,
>                    "function_ref<void(OpBuilder &, Location, ValueRange)>":$bodyBuilder)>
560d505
< 
562,603c507
<     // Get lower bounds as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedLowerBound() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticLowerBound(), getDynamicLowerBound(), b);
<     }
< 
<     // Get upper bounds as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedUpperBound() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticUpperBound(), getDynamicUpperBound(), b);
<     }
< 
<     // Get steps as OpFoldResult.
<     SmallVector<OpFoldResult> getMixedStep() {
<       Builder b(getOperation()->getContext());
<       return getMixedValues(getStaticStep(), getDynamicStep(), b);
<     }
< 
<     /// Get lower bounds as values.
<     SmallVector<Value> getLowerBound(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedLowerBound());
<     }
< 
<     /// Get upper bounds as values.
<     SmallVector<Value> getUpperBound(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedUpperBound());
<     }
< 
<     /// Get steps as values.
<     SmallVector<Value> getStep(OpBuilder &b) {
<       return getValueOrCreateConstantIndexOp(b, getLoc(), getMixedStep());
<     }
< 
<     int64_t getRank() { return getStaticLowerBound().size(); }
< 
<     /// Number of operands controlling the loop: lbs, ubs, steps
<     unsigned getNumControlOperands() { return 3 * getRank(); }
< 
<     /// Number of dynamic operands controlling the loop: lbs, ubs, steps
<     unsigned getNumDynamicControlOperands() {
<       return getODSOperandIndexAndLength(3).first;
<     }
---
>     int64_t getRank() { return getNumThreads().size(); }
606,607c510
<       assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
<              "invalid operand");
---
>       assert(opOperand->getOperandNumber() >= getRank() && "invalid operand");
609c512
<           opOperand->getOperandNumber() - getNumDynamicControlOperands());
---
>           opOperand->getOperandNumber() - getRank());
616,618c519
< 
<       return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
<                                            bbArg.getArgNumber() - getRank());
---
>       return &getOperation()->getOpOperand(bbArg.getArgNumber());
624,625c525,526
<       return &getOperation()->getOpOperand(getNumDynamicControlOperands() +
<                                            opResult.getResultNumber());
---
>       return &getOperation()->getOpOperand(
>           opResult.getResultNumber() + getRank());
629,633c530,531
<       assert(opOperand->getOperandNumber() >= getNumDynamicControlOperands() &&
<              "invalid operand");
< 
<       return getBody()->getArgument(opOperand->getOperandNumber() -
<                                     getNumDynamicControlOperands() + getRank());
---
>       assert(opOperand->getOperandNumber() >= getRank() && "invalid operand");
>       return getBody()->getArgument(opOperand->getOperandNumber());
640c538
<     ::mlir::ValueRange getInductionVars() {
---
>     ::mlir::ValueRange getThreadIndices() {
644,645c542,543
<     ::mlir::Value getInductionVar(int64_t idx) {
<       return getInductionVars()[idx];
---
>     ::mlir::Value getThreadIndex(int64_t idx) {
>       return getThreadIndices()[idx];
652,653c550,557
<     /// Checks if the lbs are zeros and steps are ones.
<     bool isNormalized();
---
>     /// Helper to sort `values` according to matching `keys`.
>     /// Take a custom `compare` binary comparator which returns true if the first
>     /// element is smaller than the second (i.e. compatible with std::sort).
>     /// This is a helper typically used to sort numThreads values before they are
>     /// mapped to concrete physical dimensions of hardware.
>     static SmallVector<Value> getValuesSortedByKey(
>       ArrayRef<Attribute> keys, ValueRange values,
>       llvm::function_ref<bool(Attribute, Attribute)> compare);
658,659c562
<     static void ensureTerminator(Region & region, OpBuilder & builder,
<                                  Location loc);
---
>     static void ensureTerminator(Region &region, OpBuilder &builder, Location loc);
661c564
<     InParallelOp getTerminator();
---
>     PerformConcurrentlyOp getTerminator();
666c569
< // InParallelOp
---
> // PerformConcurrentlyOp
669c572
< def InParallelOp : SCF_Op<"forall.in_parallel", [
---
> def PerformConcurrentlyOp : SCF_Op<"foreach_thread.perform_concurrently", [
673c576
<        HasParent<"ForallOp">,
---
>        HasParent<"ForeachThreadOp">,
675c578
<   let summary = "terminates a `forall` block";
---
>   let summary = "terminates a `foreach_thread` block";
677,678c580,581
<     `scf.forall.in_parallel` is a designated terminator for
<     the `scf.forall` operation.
---
>     `scf.foreach_thread.perform_concurrently` is a designated terminator for
>     the `scf.foreach_thread` operation.
682c585
<     the enclosing `scf.forall`.
---
>     the enclosing `scf.foreach_thread`.
697,698c600,601
<   // TODO: Add a `InParallelOpInterface` interface for ops that can
<   // appear inside in_parallel.
---
>   // TODO: Add a `PerformConcurrentlyOpInterface` interface for ops that can
>   // appear inside perform_concurrently.
710,714c613,618
< def IfOp : SCF_Op<"if", [DeclareOpInterfaceMethods<RegionBranchOpInterface, [
<     "getNumRegionInvocations", "getRegionInvocationBounds"]>,
<     DeclareOpInterfaceMethods<InferTypeOpInterface>,
<     SingleBlockImplicitTerminator<"scf::YieldOp">, RecursiveMemoryEffects,
<     NoRegionArguments]> {
---
> def IfOp : SCF_Op<"if",
>       [DeclareOpInterfaceMethods<RegionBranchOpInterface,
>                                  ["getNumRegionInvocations",
>                                   "getRegionInvocationBounds"]>,
>        SingleBlockImplicitTerminator<"scf::YieldOp">, RecursiveMemoryEffects,
>        NoRegionArguments]> {
729,730c633,634
<     `scf.if` may also produce results. Which values are returned depends on
<     which execution path is taken.
---
>     `scf.if` may also return results that are defined in its regions. The
>     values defined are determined by which execution path is taken.
746,752c650,654
<     The "then" region has exactly 1 block. The "else" region may have 0 or 1
<     block. In case the `scf.if` produces results, the "else" region must also
<     have exactly 1 block.
< 
<     The blocks are always terminated with `scf.yield`. If `scf.if` defines no
<     values, the `scf.yield` can be left out, and will be inserted implicitly.
<     Otherwise, it must be explicit.
---
>     `scf.if` regions are always terminated with "scf.yield". If "scf.if"
>     defines no values, the "scf.yield" can be left out, and will be inserted
>     implicitly. Otherwise, it must be explicit.
>     Also, if "scf.if" defines one or more values, the 'else' block cannot be
>     omitted.
761,763d662
< 
<     The types of the yielded values must match the result types of the
<     `scf.if`.
767,768c666
<   let regions = (region SizedRegion<1>:$thenRegion,
<                         MaxSizedRegion<1>:$elseRegion);
---
>   let regions = (region SizedRegion<1>:$thenRegion, AnyRegion:$elseRegion);
772,774d669
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond)>,
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
<       "bool":$addThenBlock, "bool":$addElseBlock)>,
778c673,675
<     OpBuilder<(ins "Value":$cond,
---
>     // TODO: Remove builder when it is no longer used to create invalid `if` ops
>     // (with a type mispatch between the op and it's inner `yield` op).
>     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$cond,
782a680,684
>     OpBuilder<(ins "Value":$cond,
>       CArg<"function_ref<void(OpBuilder &, Location)>",
>            "buildTerminatedBody">:$thenBuilder,
>       CArg<"function_ref<void(OpBuilder &, Location)>",
>            "nullptr">:$elseBuilder)>
--- include/mlir/Dialect/SCF/IR/DeviceMappingInterface.td
33c33
<     Currently, `scf.forall` uses this interface to express the mapping
---
>     Currently, `scf.foreach_thread` uses this interface to express the mapping
--- include/mlir/Dialect/SCF/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.h include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.h
22d21
< class IfOp;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.td include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
213,253d213
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< def TakeAssumedBranchOp : Op<Transform_Dialect, "scf.take_assumed_branch", [
<   DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<   TransformOpInterface, TransformEachOpTrait]> {
<   let description = [{
<     Given an scf.if conditional, inject user-defined information that it is
<     always safe to execute only the if or else branch. 
<     
<     This is achieved by just replacing the scf.if by the content of one of its
<     branches.
< 
<     This is particularly useful for user-controlled rewriting of conditionals
<     that exist solely to guard against out-of-bounds behavior.
< 
<     At the moment, no assume or assert operation is emitted as it is not always
<     desirable. In the future, this may be controlled by a dedicated attribute.
< 
<     #### Return modes
< 
<     The transform only consumes its operand and does not produce any result.
<     The transform definitely fails if `take_else_branch` is specified and the
<     `else` region is empty.
<   }];
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                        OptionalAttr<UnitAttr>:$take_else_branch);
<   let results = (outs);
< 
<   let assemblyFormat = [{
<       $target
<       (`take_else_branch` $take_else_branch^)?
<       attr-dict
<        `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::scf::IfOp ifOp,
--- include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.h
22d21
< class IfOp;
--- include/mlir/Dialect/SCF/TransformOps/SCFTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
213,253d213
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< def TakeAssumedBranchOp : Op<Transform_Dialect, "scf.take_assumed_branch", [
<   DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<   TransformOpInterface, TransformEachOpTrait]> {
<   let description = [{
<     Given an scf.if conditional, inject user-defined information that it is
<     always safe to execute only the if or else branch. 
<     
<     This is achieved by just replacing the scf.if by the content of one of its
<     branches.
< 
<     This is particularly useful for user-controlled rewriting of conditionals
<     that exist solely to guard against out-of-bounds behavior.
< 
<     At the moment, no assume or assert operation is emitted as it is not always
<     desirable. In the future, this may be controlled by a dedicated attribute.
< 
<     #### Return modes
< 
<     The transform only consumes its operand and does not produce any result.
<     The transform definitely fails if `take_else_branch` is specified and the
<     `else` region is empty.
<   }];
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                        OptionalAttr<UnitAttr>:$take_else_branch);
<   let results = (outs);
< 
<   let assemblyFormat = [{
<       $target
<       (`take_else_branch` $take_else_branch^)?
<       attr-dict
<        `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::scf::IfOp ifOp,
--- include/mlir/Dialect/SCF/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/Async
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Async/IR and include/mlir/Dialect/Async/IR
--- include/mlir/Dialect/Async/Passes.h
--- include/mlir/Dialect/Async/Passes.td
--- include/mlir/Dialect/Async/CMakeLists.txt
--- include/mlir/Dialect/Async/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Async/IR/AsyncDialect.td include/mlir/Dialect/Async/IR/AsyncDialect.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Async/IR/AsyncOps.td include/mlir/Dialect/Async/IR/AsyncOps.td
17a18
> include "mlir/Interfaces/CallInterfaces.td"
109c110
<      IsolatedFromAbove, OpAsmOpInterface]> {
---
>      IsolatedFromAbove, OpAsmOpInterface, Symbol]> {
171,182d171
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
218,219c207
<     [DeclareOpInterfaceMethods<CallOpInterface>,
<      DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
---
>     [CallOpInterface, DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
261a250,265
> 
>     /// Get the argument operands to the called function.
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
>     operand_range getCallOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
> 
>     operand_iterator arg_operand_begin() { return operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
>     /// Return the callee of this operation.
>     CallInterfaceCallable getCallableForCallee() {
>       return (*this)->getAttrOfType<SymbolRefAttr>("callee");
>     }
--- include/mlir/Dialect/Async/IR/Async.h
--- include/mlir/Dialect/Async/IR/AsyncOps.td
17a18
> include "mlir/Interfaces/CallInterfaces.td"
109c110
<      IsolatedFromAbove, OpAsmOpInterface]> {
---
>      IsolatedFromAbove, OpAsmOpInterface, Symbol]> {
171,182d171
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
218,219c207
<     [DeclareOpInterfaceMethods<CallOpInterface>,
<      DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
---
>     [CallOpInterface, DeclareOpInterfaceMethods<SymbolUserOpInterface>]> {
261a250,265
> 
>     /// Get the argument operands to the called function.
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
>     operand_range getCallOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
> 
>     operand_iterator arg_operand_begin() { return operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
>     /// Return the callee of this operation.
>     CallInterfaceCallable getCallableForCallee() {
>       return (*this)->getAttrOfType<SymbolRefAttr>("callee");
>     }
--- include/mlir/Dialect/Async/IR/AsyncTypes.h
--- include/mlir/Dialect/Async/IR/CMakeLists.txt
--- include/mlir/Dialect/Async/IR/AsyncTypes.td
--- include/mlir/Dialect/Async/IR/AsyncDialect.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Async/Transforms.h
--- include/mlir/Dialect/MLProgram
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MLProgram/IR and include/mlir/Dialect/MLProgram/IR
--- include/mlir/Dialect/MLProgram/CMakeLists.txt
--- include/mlir/Dialect/MLProgram/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MLProgram/IR/MLProgramBase.td include/mlir/Dialect/MLProgram/IR/MLProgramBase.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MLProgram/IR/MLProgramOps.td include/mlir/Dialect/MLProgram/IR/MLProgramOps.td
76,87d75
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
436,447d423
< 
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
--- include/mlir/Dialect/MLProgram/IR/MLProgramBase.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/MLProgram/IR/MLProgramAttributes.td
--- include/mlir/Dialect/MLProgram/IR/MLProgramTypes.td
--- include/mlir/Dialect/MLProgram/IR/CMakeLists.txt
--- include/mlir/Dialect/MLProgram/IR/MLProgramOps.td
76,87d75
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
436,447d423
< 
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
--- include/mlir/Dialect/MLProgram/IR/MLProgramAttributes.h
--- include/mlir/Dialect/MLProgram/IR/MLProgram.h
--- include/mlir/Dialect/MLProgram/IR/MLProgramTypes.h
--- include/mlir/Dialect/PDL
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDL/IR and include/mlir/Dialect/PDL/IR
--- include/mlir/Dialect/PDL/CMakeLists.txt
--- include/mlir/Dialect/PDL/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/PDL/IR/PDLDialect.td include/mlir/Dialect/PDL/IR/PDLDialect.td
68a69
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/PDL/IR/PDLOps.td
--- include/mlir/Dialect/PDL/IR/PDL.h
--- include/mlir/Dialect/PDL/IR/CMakeLists.txt
--- include/mlir/Dialect/PDL/IR/PDLTypes.td
--- include/mlir/Dialect/PDL/IR/PDLOps.h
--- include/mlir/Dialect/PDL/IR/PDLTypes.h
--- include/mlir/Dialect/PDL/IR/PDLDialect.td
68a69
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/CMakeLists.txt
16d15
< add_subdirectory(IRDL)
--- include/mlir/Dialect/DLTI
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/DLTI/DLTIBase.td include/mlir/Dialect/DLTI/DLTIBase.td
26d25
<     // Top level attribute name.
30d28
<     // Constants used in entries.
39,44d36
< 
<     constexpr const static ::llvm::StringLiteral
<     kDataLayoutAllocaMemorySpaceKey = "dlti.alloca_memory_space";
< 
<     constexpr const static ::llvm::StringLiteral
<     kDataLayoutStackAlignmentKey = "dlti.stack_alignment";
47a40
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/DLTI/DLTI.h include/mlir/Dialect/DLTI/DLTI.h
101,106d100
<   /// Returns the alloca memory space identifier.
<   StringAttr getAllocaMemorySpaceIdentifier(MLIRContext *context) const;
< 
<   /// Returns the stack alignment identifier.
<   StringAttr getStackAlignmentIdentifier(MLIRContext *context) const;
< 
--- include/mlir/Dialect/DLTI/Traits.h
--- include/mlir/Dialect/DLTI/DLTIBase.td
26d25
<     // Top level attribute name.
30d28
<     // Constants used in entries.
39,44d36
< 
<     constexpr const static ::llvm::StringLiteral
<     kDataLayoutAllocaMemorySpaceKey = "dlti.alloca_memory_space";
< 
<     constexpr const static ::llvm::StringLiteral
<     kDataLayoutStackAlignmentKey = "dlti.stack_alignment";
47a40
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/DLTI/DLTI.h
101,106d100
<   /// Returns the alloca memory space identifier.
<   StringAttr getAllocaMemorySpaceIdentifier(MLIRContext *context) const;
< 
<   /// Returns the stack alignment identifier.
<   StringAttr getStackAlignmentIdentifier(MLIRContext *context) const;
< 
--- include/mlir/Dialect/DLTI/DLTI.td
--- include/mlir/Dialect/DLTI/CMakeLists.txt
--- include/mlir/Dialect/OpenMP
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP/OpenMPDialect.h include/mlir/Dialect/OpenMP/OpenMPDialect.h
25a26
> #include "mlir/Dialect/OpenMP/OpenMPOpsInterfaces.h.inc"
30,31d30
< 
< #include "mlir/Dialect/OpenMP/OpenMPInterfaces.h"
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP: OpenMPInterfaces.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP/OpenMPOpsInterfaces.td include/mlir/Dialect/OpenMP/OpenMPOpsInterfaces.td
50,171d49
< def OffloadModuleInterface : OpInterface<"OffloadModuleInterface"> {
<   let description = [{
<     Operations that represent a module for offloading (host or device) 
<     should have this interface.
<   }];
< 
<   let cppNamespace = "::mlir::omp";
< 
<   let methods = [
<     InterfaceMethod<
<       /*description=*/[{
<         Set the attribute IsDeviceAttr on the current module with the 
<         specified boolean argument.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setIsDevice",
<       (ins "bool":$isDevice), [{}], [{
<         $_op->setAttr(
<           mlir::StringAttr::get($_op->getContext(), llvm::Twine{"omp.is_device"}),
<             mlir::omp::IsDeviceAttr::get($_op->getContext(), isDevice));
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Get the IsDeviceAttr attribute on the current module if it exists and return
<         its value, if it doesn't exist it returns false by default.
<       }],
<       /*retTy=*/"bool",
<       /*methodName=*/"getIsDevice",
<       (ins), [{}], [{
<         if (Attribute isDevice = $_op->getAttr("omp.is_device"))
<           if (isDevice.isa<mlir::omp::IsDeviceAttr>())
<             return isDevice.dyn_cast<IsDeviceAttr>().getIsDevice();
<         return false;
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Get the FlagsAttr attribute on the current module if it exists 
<         and return the attribute, if it doesn't exit it returns a nullptr
<       }],
<       /*retTy=*/"mlir::omp::FlagsAttr",
<       /*methodName=*/"getFlags", 
<       (ins), [{}], [{
<         if (Attribute flags = $_op->getAttr("omp.flags"))
<           return flags.dyn_cast_or_null<mlir::omp::FlagsAttr>();
<         return nullptr;
<       }]>,  
<       InterfaceMethod<
<       /*description=*/[{
<         Apply an omp.FlagsAttr to a module with the specified values 
<         for the flags
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setFlags", 
<       (ins "uint32_t":$debugKind,
<             "bool":$assumeTeamsOversubscription,
<             "bool":$assumeThreadsOversubscription,
<             "bool":$assumeNoThreadState,
<             "bool":$assumeNoNestedParallelism), [{}], [{
<         $_op->setAttr(("omp." + mlir::omp::FlagsAttr::getMnemonic()).str(),
<                   mlir::omp::FlagsAttr::get($_op->getContext(), debugKind,
<                       assumeTeamsOversubscription, assumeThreadsOversubscription, 
<                       assumeNoThreadState, assumeNoNestedParallelism));
<       }]>,
<     InterfaceMethod<
<       /*description=*/[{
<         Get the Target attribute on the current module if it exists
<         and return the attribute, if it doesn't exist it returns a nullptr.
<       }],
<       /*retTy=*/"mlir::omp::TargetAttr",
<       /*methodName=*/"getTarget",
<       (ins), [{}], [{
<         if (Attribute flags = $_op->getAttr("omp.target"))
<           return flags.dyn_cast_or_null<mlir::omp::TargetAttr>();
<         return nullptr;
<       }]>,
<     InterfaceMethod<
<       /*description=*/[{
<         Set the attribute target on the current module with the
<         specified string arguments - name of cpu and corresponding features.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setTarget",
<       (ins "llvm::StringRef":$targetCPU,
<            "llvm::StringRef":$targetFeatures), [{}], [{
<         if (targetCPU.empty())
<           return;
<         $_op->setAttr(("omp." + mlir::omp::TargetAttr::getMnemonic()).str(),
<                   mlir::omp::TargetAttr::get($_op->getContext(),
<                                              targetCPU.str(),
<                                              targetFeatures.str()));
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Set a StringAttr on the current module containing the host IR file path. This 
<         file path is used in two-phase compilation during the device phase to generate
<         device side LLVM IR when lowering MLIR. 
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setHostIRFilePath", 
<       (ins "std::string":$hostIRFilePath), [{}], [{
<         $_op->setAttr(
<           mlir::StringAttr::get($_op->getContext(), llvm::Twine{"omp.host_ir_filepath"}),
<             mlir::StringAttr::get($_op->getContext(), hostIRFilePath));
<        }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Find the host-ir file path StringAttr from the current module if it exists and 
<         return its contained value, if it doesn't exist it returns an empty string. This 
<         file path is used in two-phase compilation during the device phase to generate
<         device side LLVM IR when lowering MLIR. 
<       }],
<       /*retTy=*/"llvm::StringRef",
<       /*methodName=*/"getHostIRFilePath", 
<       (ins), [{}], [{
<         if (Attribute filepath = $_op->getAttr("omp.host_ir_filepath"))
<           if (filepath.isa<mlir::StringAttr>())
<             return filepath.dyn_cast<mlir::StringAttr>().getValue();
<         return {};
<       }]>
<   ];
< }
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP/OpenMPOps.td include/mlir/Dialect/OpenMP/OpenMPOps.td
30a31
>   let useFoldAPI = kEmitFoldAdaptorFolder;
36,80d36
< //===----------------------------------------------------------------------===//
< //  OpenMP Attributes
< //===----------------------------------------------------------------------===//
< 
< class OpenMP_Attr<string name, string attrMnemonic,
<                 list<Trait> traits = [],
<                 string baseCppClass = "::mlir::Attribute">
<     : AttrDef<OpenMP_Dialect, name, traits, baseCppClass> {
<   let mnemonic = attrMnemonic;
< }
< 
< def IsDeviceAttr : OpenMP_Attr<"IsDevice", "isdevice"> {
<   let parameters = (ins
<     "bool":$is_device
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< //===----------------------------------------------------------------------===//
< // Runtime library flag's attribute that holds information for lowering to LLVM
< //===----------------------------------------------------------------------===//
< 
< def FlagsAttr : OpenMP_Attr<"Flags", "flags"> {
<   let parameters = (ins
<     DefaultValuedParameter<"uint32_t", "0">:$debug_kind,
<     DefaultValuedParameter<"bool", "false">:$assume_teams_oversubscription,
<     DefaultValuedParameter<"bool", "false">:$assume_threads_oversubscription,
<     DefaultValuedParameter<"bool", "false">:$assume_no_thread_state,
<     DefaultValuedParameter<"bool", "false">:$assume_no_nested_parallelism
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def TargetAttr : OpenMP_Attr<"Target", "target"> {
<   let parameters = (ins
<     StringRefParameter<>:$target_cpu,
<     StringRefParameter<>:$target_features
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< 
526,545d481
< def ClauseTaskDependIn    : I32EnumAttrCase<"taskdependin",    0>;
< def ClauseTaskDependOut   : I32EnumAttrCase<"taskdependout",   1>;
< def ClauseTaskDependInOut : I32EnumAttrCase<"taskdependinout", 2>;
< 
< def ClauseTaskDepend : I32EnumAttr<
<     "ClauseTaskDepend",
<     "task depend clause",
<     [ClauseTaskDependIn, ClauseTaskDependOut, ClauseTaskDependInOut]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = "::mlir::omp";
< }
< def ClauseTaskDependAttr :
<   EnumAttr<OpenMP_Dialect, ClauseTaskDepend, "clause_task_depend"> {
<   let assemblyFormat = "`(` $value `)`";
< }
< def TaskDependArrayAttr :
<   TypedArrayAttrBase<ClauseTaskDependAttr, "clause_task_depend array attr"> {
<     let constBuilderCall = ?;
<   }
< 
586,589d521
<     The `depends` and `depend_vars` arguments are variadic lists of values
<     that specify the dependencies of this particular task in relation to
<     other tasks.
< 
604,605d535
<                        OptionalAttr<TaskDependArrayAttr>:$depends,
<                        Variadic<OpenMP_PointerLikeType>:$depend_vars,
624,627d553
<           |`depend` `(`
<               custom<DependVarList>(
<                 $depend_vars, type($depend_vars), $depends
<               ) `)`
894,1045d819
< 
< //===---------------------------------------------------------------------===//
< // 2.14.2 target data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_DataOp: OpenMP_Op<"target_data", [AttrSizedOperandSegments]>{
<   let summary = "target data construct";
<   let description = [{
<     Map variables to a device data environment for the extent of the region.
< 
<     The omp target data directive maps variables to a device data
<     environment, and defines the lexical scope of the data environment
<     that is created. The omp target data directive can reduce data copies
<     to and from the offloading device when multiple target regions are using
<     the same data.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed
<     on the host device.
< 
<     The optional $device parameter specifies the device number for the target
<     region.
< 
<     The optional $use_device_ptr specifies the device pointers to the
<     corresponding list items in the device data environment.
< 
<     The optional $use_device_addr specifies the address of the objects in the
<     device data enviornment.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        Variadic<AnyType>:$use_device_ptr,
<                        Variadic<AnyType>:$use_device_addr,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `use_device_ptr` `(` $use_device_ptr `:` type($use_device_ptr) `)`
<     | `use_device_addr` `(` $use_device_addr `:` type($use_device_addr) `)`)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     $region attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===---------------------------------------------------------------------===//
< // 2.14.3 target enter data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_EnterDataOp: OpenMP_Op<"target_enter_data",
<                                                  [AttrSizedOperandSegments]>{
<   let  summary = "target enter data construct";
<   let description = [{
<     The target enter data directive specifies that variables are mapped to
<     a device data environment. The target enter data directive is a
<     stand-alone directive.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed on
<     the host device.
< 
<     The optional $device parameter specifies the device number for the
<     target region.
< 
<     The optional $nowait eliminates the implicit barrier so the parent task
<     can make progress even if the target task is not yet completed.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `nowait` $nowait)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     attr-dict
<    }];
< 
<   let hasVerifier = 1;
< }
< 
< //===---------------------------------------------------------------------===//
< // 2.14.4 target exit data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_ExitDataOp: OpenMP_Op<"target_exit_data",
<                                                  [AttrSizedOperandSegments]>{
<   let  summary = "target exit data construct";
<   let description = [{
<     The target exit data directive specifies that variables are mapped to a
<     device data environment. The target exit data directive is
<     a stand-alone directive.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed
<     on the host device.
< 
<     The optional $device parameter specifies the device number for the
<     target region.
< 
<     The optional $nowait eliminates the implicit barrier so the parent
<     task can make progress even if the target task is not yet completed.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `nowait` $nowait)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     attr-dict
<    }];
< 
<   let hasVerifier = 1;
< }
< 
1068c842
<     TODO:  is_device_ptr, depend, defaultmap, in_reduction
---
>     TODO:  map, is_device_ptr, depend, defaultmap, in_reduction
1075,1077c849
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        OptionalAttr<I64ArrayAttr>:$map_types);
---
>                        UnitAttr:$nowait);
1083,1086c855,857
<     | `device` `(` $device `:` type($device) `)`
<     | `thread_limit` `(` $thread_limit `:` type($thread_limit) `)`
<     | `nowait` $nowait
<     | `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
---
>           | `device` `(` $device `:` type($device) `)`
>           | `thread_limit` `(` $thread_limit `:` type($thread_limit) `)`
>           | `nowait` $nowait
1089,1090d859
< 
<   let hasVerifier = 1;
1277d1045
<                        TypeAttr:$element_type,
1284c1052
<     `:` type($x) `,` $element_type attr-dict
---
>     `:` type($x) attr-dict
1412,1423d1179
< 
<     /// The number of variable operands.
<     unsigned getNumVariableOperands() {
<       assert(getX() && "expected 'x' operand");
<       return 1;
<     }
< 
<     /// The i-th variable operand passed.
<     Value getVariableOperand(unsigned i) {
<       assert(i == 0 && "invalid index position for an operand");
<       return getX();
<     }
1621c1377
<       return cast<PointerLikeType>(getAtomicReductionRegion().front().getArgument(0).getType());
---
>       return getAtomicReductionRegion().front().getArgument(0).getType();
1631c1387,1391
< def ReductionOp : OpenMP_Op<"reduction"> {
---
> def ReductionOp : OpenMP_Op<"reduction", [
>     TypesMatchWith<"value types matches accumulator element type",
>                    "accumulator", "operand",
>                  "$_self.cast<::mlir::omp::PointerLikeType>().getElementType()">
>   ]> {
1641,1643c1401,1402
<   let assemblyFormat = [{
<     $operand `,` $accumulator attr-dict `:` type($operand) `,` type($accumulator)
<   }];
---
>   let assemblyFormat =
>     "$operand `,` $accumulator attr-dict `:` type($accumulator)";
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/OpenMP/OpenMPTypeInterfaces.td include/mlir/Dialect/OpenMP/OpenMPTypeInterfaces.td
24,26c24
<       /*description=*/[{
<         Returns the pointee type or null if the pointer has no pointee type
<       }],
---
>       /*description=*/"Returns the pointee type.",
--- include/mlir/Dialect/OpenMP/OpenMPOpsInterfaces.td
50,171d49
< def OffloadModuleInterface : OpInterface<"OffloadModuleInterface"> {
<   let description = [{
<     Operations that represent a module for offloading (host or device) 
<     should have this interface.
<   }];
< 
<   let cppNamespace = "::mlir::omp";
< 
<   let methods = [
<     InterfaceMethod<
<       /*description=*/[{
<         Set the attribute IsDeviceAttr on the current module with the 
<         specified boolean argument.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setIsDevice",
<       (ins "bool":$isDevice), [{}], [{
<         $_op->setAttr(
<           mlir::StringAttr::get($_op->getContext(), llvm::Twine{"omp.is_device"}),
<             mlir::omp::IsDeviceAttr::get($_op->getContext(), isDevice));
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Get the IsDeviceAttr attribute on the current module if it exists and return
<         its value, if it doesn't exist it returns false by default.
<       }],
<       /*retTy=*/"bool",
<       /*methodName=*/"getIsDevice",
<       (ins), [{}], [{
<         if (Attribute isDevice = $_op->getAttr("omp.is_device"))
<           if (isDevice.isa<mlir::omp::IsDeviceAttr>())
<             return isDevice.dyn_cast<IsDeviceAttr>().getIsDevice();
<         return false;
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Get the FlagsAttr attribute on the current module if it exists 
<         and return the attribute, if it doesn't exit it returns a nullptr
<       }],
<       /*retTy=*/"mlir::omp::FlagsAttr",
<       /*methodName=*/"getFlags", 
<       (ins), [{}], [{
<         if (Attribute flags = $_op->getAttr("omp.flags"))
<           return flags.dyn_cast_or_null<mlir::omp::FlagsAttr>();
<         return nullptr;
<       }]>,  
<       InterfaceMethod<
<       /*description=*/[{
<         Apply an omp.FlagsAttr to a module with the specified values 
<         for the flags
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setFlags", 
<       (ins "uint32_t":$debugKind,
<             "bool":$assumeTeamsOversubscription,
<             "bool":$assumeThreadsOversubscription,
<             "bool":$assumeNoThreadState,
<             "bool":$assumeNoNestedParallelism), [{}], [{
<         $_op->setAttr(("omp." + mlir::omp::FlagsAttr::getMnemonic()).str(),
<                   mlir::omp::FlagsAttr::get($_op->getContext(), debugKind,
<                       assumeTeamsOversubscription, assumeThreadsOversubscription, 
<                       assumeNoThreadState, assumeNoNestedParallelism));
<       }]>,
<     InterfaceMethod<
<       /*description=*/[{
<         Get the Target attribute on the current module if it exists
<         and return the attribute, if it doesn't exist it returns a nullptr.
<       }],
<       /*retTy=*/"mlir::omp::TargetAttr",
<       /*methodName=*/"getTarget",
<       (ins), [{}], [{
<         if (Attribute flags = $_op->getAttr("omp.target"))
<           return flags.dyn_cast_or_null<mlir::omp::TargetAttr>();
<         return nullptr;
<       }]>,
<     InterfaceMethod<
<       /*description=*/[{
<         Set the attribute target on the current module with the
<         specified string arguments - name of cpu and corresponding features.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setTarget",
<       (ins "llvm::StringRef":$targetCPU,
<            "llvm::StringRef":$targetFeatures), [{}], [{
<         if (targetCPU.empty())
<           return;
<         $_op->setAttr(("omp." + mlir::omp::TargetAttr::getMnemonic()).str(),
<                   mlir::omp::TargetAttr::get($_op->getContext(),
<                                              targetCPU.str(),
<                                              targetFeatures.str()));
<       }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Set a StringAttr on the current module containing the host IR file path. This 
<         file path is used in two-phase compilation during the device phase to generate
<         device side LLVM IR when lowering MLIR. 
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"setHostIRFilePath", 
<       (ins "std::string":$hostIRFilePath), [{}], [{
<         $_op->setAttr(
<           mlir::StringAttr::get($_op->getContext(), llvm::Twine{"omp.host_ir_filepath"}),
<             mlir::StringAttr::get($_op->getContext(), hostIRFilePath));
<        }]>,
<       InterfaceMethod<
<       /*description=*/[{
<         Find the host-ir file path StringAttr from the current module if it exists and 
<         return its contained value, if it doesn't exist it returns an empty string. This 
<         file path is used in two-phase compilation during the device phase to generate
<         device side LLVM IR when lowering MLIR. 
<       }],
<       /*retTy=*/"llvm::StringRef",
<       /*methodName=*/"getHostIRFilePath", 
<       (ins), [{}], [{
<         if (Attribute filepath = $_op->getAttr("omp.host_ir_filepath"))
<           if (filepath.isa<mlir::StringAttr>())
<             return filepath.dyn_cast<mlir::StringAttr>().getValue();
<         return {};
<       }]>
<   ];
< }
< 
--- include/mlir/Dialect/OpenMP/OpenMPDialect.h
25a26
> #include "mlir/Dialect/OpenMP/OpenMPOpsInterfaces.h.inc"
30,31d30
< 
< #include "mlir/Dialect/OpenMP/OpenMPInterfaces.h"
--- include/mlir/Dialect/OpenMP/OpenMPOps.td
30a31
>   let useFoldAPI = kEmitFoldAdaptorFolder;
36,80d36
< //===----------------------------------------------------------------------===//
< //  OpenMP Attributes
< //===----------------------------------------------------------------------===//
< 
< class OpenMP_Attr<string name, string attrMnemonic,
<                 list<Trait> traits = [],
<                 string baseCppClass = "::mlir::Attribute">
<     : AttrDef<OpenMP_Dialect, name, traits, baseCppClass> {
<   let mnemonic = attrMnemonic;
< }
< 
< def IsDeviceAttr : OpenMP_Attr<"IsDevice", "isdevice"> {
<   let parameters = (ins
<     "bool":$is_device
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< //===----------------------------------------------------------------------===//
< // Runtime library flag's attribute that holds information for lowering to LLVM
< //===----------------------------------------------------------------------===//
< 
< def FlagsAttr : OpenMP_Attr<"Flags", "flags"> {
<   let parameters = (ins
<     DefaultValuedParameter<"uint32_t", "0">:$debug_kind,
<     DefaultValuedParameter<"bool", "false">:$assume_teams_oversubscription,
<     DefaultValuedParameter<"bool", "false">:$assume_threads_oversubscription,
<     DefaultValuedParameter<"bool", "false">:$assume_no_thread_state,
<     DefaultValuedParameter<"bool", "false">:$assume_no_nested_parallelism
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< def TargetAttr : OpenMP_Attr<"Target", "target"> {
<   let parameters = (ins
<     StringRefParameter<>:$target_cpu,
<     StringRefParameter<>:$target_features
<   );
< 
<   let assemblyFormat = "`<` struct(params) `>`";
< }
< 
< 
526,545d481
< def ClauseTaskDependIn    : I32EnumAttrCase<"taskdependin",    0>;
< def ClauseTaskDependOut   : I32EnumAttrCase<"taskdependout",   1>;
< def ClauseTaskDependInOut : I32EnumAttrCase<"taskdependinout", 2>;
< 
< def ClauseTaskDepend : I32EnumAttr<
<     "ClauseTaskDepend",
<     "task depend clause",
<     [ClauseTaskDependIn, ClauseTaskDependOut, ClauseTaskDependInOut]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = "::mlir::omp";
< }
< def ClauseTaskDependAttr :
<   EnumAttr<OpenMP_Dialect, ClauseTaskDepend, "clause_task_depend"> {
<   let assemblyFormat = "`(` $value `)`";
< }
< def TaskDependArrayAttr :
<   TypedArrayAttrBase<ClauseTaskDependAttr, "clause_task_depend array attr"> {
<     let constBuilderCall = ?;
<   }
< 
586,589d521
<     The `depends` and `depend_vars` arguments are variadic lists of values
<     that specify the dependencies of this particular task in relation to
<     other tasks.
< 
604,605d535
<                        OptionalAttr<TaskDependArrayAttr>:$depends,
<                        Variadic<OpenMP_PointerLikeType>:$depend_vars,
624,627d553
<           |`depend` `(`
<               custom<DependVarList>(
<                 $depend_vars, type($depend_vars), $depends
<               ) `)`
894,1045d819
< 
< //===---------------------------------------------------------------------===//
< // 2.14.2 target data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_DataOp: OpenMP_Op<"target_data", [AttrSizedOperandSegments]>{
<   let summary = "target data construct";
<   let description = [{
<     Map variables to a device data environment for the extent of the region.
< 
<     The omp target data directive maps variables to a device data
<     environment, and defines the lexical scope of the data environment
<     that is created. The omp target data directive can reduce data copies
<     to and from the offloading device when multiple target regions are using
<     the same data.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed
<     on the host device.
< 
<     The optional $device parameter specifies the device number for the target
<     region.
< 
<     The optional $use_device_ptr specifies the device pointers to the
<     corresponding list items in the device data environment.
< 
<     The optional $use_device_addr specifies the address of the objects in the
<     device data enviornment.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        Variadic<AnyType>:$use_device_ptr,
<                        Variadic<AnyType>:$use_device_addr,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let regions = (region AnyRegion:$region);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `use_device_ptr` `(` $use_device_ptr `:` type($use_device_ptr) `)`
<     | `use_device_addr` `(` $use_device_addr `:` type($use_device_addr) `)`)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     $region attr-dict
<   }];
< 
<   let hasVerifier = 1;
< }
< 
< //===---------------------------------------------------------------------===//
< // 2.14.3 target enter data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_EnterDataOp: OpenMP_Op<"target_enter_data",
<                                                  [AttrSizedOperandSegments]>{
<   let  summary = "target enter data construct";
<   let description = [{
<     The target enter data directive specifies that variables are mapped to
<     a device data environment. The target enter data directive is a
<     stand-alone directive.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed on
<     the host device.
< 
<     The optional $device parameter specifies the device number for the
<     target region.
< 
<     The optional $nowait eliminates the implicit barrier so the parent task
<     can make progress even if the target task is not yet completed.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `nowait` $nowait)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     attr-dict
<    }];
< 
<   let hasVerifier = 1;
< }
< 
< //===---------------------------------------------------------------------===//
< // 2.14.4 target exit data Construct
< //===---------------------------------------------------------------------===//
< 
< def Target_ExitDataOp: OpenMP_Op<"target_exit_data",
<                                                  [AttrSizedOperandSegments]>{
<   let  summary = "target exit data construct";
<   let description = [{
<     The target exit data directive specifies that variables are mapped to a
<     device data environment. The target exit data directive is
<     a stand-alone directive.
< 
<     The optional $if_expr parameter specifies a boolean result of a
<     conditional check. If this value is 1 or is not provided then the target
<     region runs on a device, if it is 0 then the target region is executed
<     on the host device.
< 
<     The optional $device parameter specifies the device number for the
<     target region.
< 
<     The optional $nowait eliminates the implicit barrier so the parent
<     task can make progress even if the target task is not yet completed.
< 
<     The $map_operands specifies the locator-list operands of the map clause.
< 
<     The $map_types specifies the types and modifiers for the map clause.
< 
<     TODO:  depend clause and map_type_modifier values iterator and mapper.
<   }];
< 
<   let arguments = (ins Optional<I1>:$if_expr,
<                        Optional<AnyInteger>:$device,
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        I64ArrayAttr:$map_types);
< 
<   let assemblyFormat = [{
<     oilist(`if` `(` $if_expr `:` type($if_expr) `)`
<     | `device` `(` $device `:` type($device) `)`
<     | `nowait` $nowait)
<     `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
<     attr-dict
<    }];
< 
<   let hasVerifier = 1;
< }
< 
1068c842
<     TODO:  is_device_ptr, depend, defaultmap, in_reduction
---
>     TODO:  map, is_device_ptr, depend, defaultmap, in_reduction
1075,1077c849
<                        UnitAttr:$nowait,
<                        Variadic<OpenMP_PointerLikeType>:$map_operands,
<                        OptionalAttr<I64ArrayAttr>:$map_types);
---
>                        UnitAttr:$nowait);
1083,1086c855,857
<     | `device` `(` $device `:` type($device) `)`
<     | `thread_limit` `(` $thread_limit `:` type($thread_limit) `)`
<     | `nowait` $nowait
<     | `map` `(` custom<MapClause>($map_operands, type($map_operands), $map_types) `)`
---
>           | `device` `(` $device `:` type($device) `)`
>           | `thread_limit` `(` $thread_limit `:` type($thread_limit) `)`
>           | `nowait` $nowait
1089,1090d859
< 
<   let hasVerifier = 1;
1277d1045
<                        TypeAttr:$element_type,
1284c1052
<     `:` type($x) `,` $element_type attr-dict
---
>     `:` type($x) attr-dict
1412,1423d1179
< 
<     /// The number of variable operands.
<     unsigned getNumVariableOperands() {
<       assert(getX() && "expected 'x' operand");
<       return 1;
<     }
< 
<     /// The i-th variable operand passed.
<     Value getVariableOperand(unsigned i) {
<       assert(i == 0 && "invalid index position for an operand");
<       return getX();
<     }
1621c1377
<       return cast<PointerLikeType>(getAtomicReductionRegion().front().getArgument(0).getType());
---
>       return getAtomicReductionRegion().front().getArgument(0).getType();
1631c1387,1391
< def ReductionOp : OpenMP_Op<"reduction"> {
---
> def ReductionOp : OpenMP_Op<"reduction", [
>     TypesMatchWith<"value types matches accumulator element type",
>                    "accumulator", "operand",
>                  "$_self.cast<::mlir::omp::PointerLikeType>().getElementType()">
>   ]> {
1641,1643c1401,1402
<   let assemblyFormat = [{
<     $operand `,` $accumulator attr-dict `:` type($operand) `,` type($accumulator)
<   }];
---
>   let assemblyFormat =
>     "$operand `,` $accumulator attr-dict `:` type($accumulator)";
--- include/mlir/Dialect/OpenMP/OpenMPTypeInterfaces.td
24,26c24
<       /*description=*/[{
<         Returns the pointee type or null if the pointer has no pointee type
<       }],
---
>       /*description=*/"Returns the pointee type.",
--- include/mlir/Dialect/OpenMP/CMakeLists.txt
--- include/mlir/Dialect/NVGPU
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU/IR and include/mlir/Dialect/NVGPU/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU/Transforms and include/mlir/Dialect/NVGPU/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU/Utils and include/mlir/Dialect/NVGPU/Utils
--- include/mlir/Dialect/NVGPU/Passes.h
--- include/mlir/Dialect/NVGPU/Passes.td
--- include/mlir/Dialect/NVGPU/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU/Utils/MMAUtils.h include/mlir/Dialect/NVGPU/Utils/MMAUtils.h
72c72
< getLaneIdAndValueIdToOperandCoord(OpBuilder &builder, Location loc,
---
> getLaneIdAndValueIdToOperandCoord(Location loc, OpBuilder &builder,
93c93
< getLaneIdToLdMatrixMatrixCoord(OpBuilder &builder, Location loc,
---
> getLaneIdToLdMatrixMatrixCoord(Location loc, OpBuilder &builder,
94a95,106
> 
> /// Transform `vector.contract` into (m,k)x(n,k)x(m,n) form so that it can be
> /// converted to `nvgpu.mma.sync`. This specific form is meant to indicate that
> /// the vector operands are organized such that the reduction dimension is
> /// contiguous.
> struct PrepareContractToGPUMMASync
>     : public OpRewritePattern<vector::ContractionOp> {
>   using OpRewritePattern<vector::ContractionOp>::OpRewritePattern;
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> };
--- include/mlir/Dialect/NVGPU/Utils/MMAUtils.h
72c72
< getLaneIdAndValueIdToOperandCoord(OpBuilder &builder, Location loc,
---
> getLaneIdAndValueIdToOperandCoord(Location loc, OpBuilder &builder,
93c93
< getLaneIdToLdMatrixMatrixCoord(OpBuilder &builder, Location loc,
---
> getLaneIdToLdMatrixMatrixCoord(Location loc, OpBuilder &builder,
94a95,106
> 
> /// Transform `vector.contract` into (m,k)x(n,k)x(m,n) form so that it can be
> /// converted to `nvgpu.mma.sync`. This specific form is meant to indicate that
> /// the vector operands are organized such that the reduction dimension is
> /// contiguous.
> struct PrepareContractToGPUMMASync
>     : public OpRewritePattern<vector::ContractionOp> {
>   using OpRewritePattern<vector::ContractionOp>::OpRewritePattern;
> 
>   LogicalResult matchAndRewrite(vector::ContractionOp op,
>                                 PatternRewriter &rewriter) const override;
> };
--- include/mlir/Dialect/NVGPU/CMakeLists.txt
--- include/mlir/Dialect/NVGPU/Transforms
--- include/mlir/Dialect/NVGPU/Transforms/Transforms.h
--- include/mlir/Dialect/NVGPU/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/NVGPU/IR/NVGPU.td include/mlir/Dialect/NVGPU/IR/NVGPU.td
38a39
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/NVGPU/IR/NVGPUDialect.h
--- include/mlir/Dialect/NVGPU/IR/CMakeLists.txt
--- include/mlir/Dialect/NVGPU/IR/NVGPU.td
38a39
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Tosa
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR and include/mlir/Dialect/Tosa/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/Transforms and include/mlir/Dialect/Tosa/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/Utils and include/mlir/Dialect/Tosa/Utils
--- include/mlir/Dialect/Tosa/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/Utils/ShapeUtils.h include/mlir/Dialect/Tosa/Utils/ShapeUtils.h
141c141
<     if (!lhs || !rhs || lhs.dtype != rhs.dtype)
---
>     if (!rhs || !rhs || lhs.dtype != rhs.dtype)
--- include/mlir/Dialect/Tosa/Utils/QuantUtils.h
--- include/mlir/Dialect/Tosa/Utils/ShapeUtils.h
141c141
<     if (!lhs || !rhs || lhs.dtype != rhs.dtype)
---
>     if (!rhs || !rhs || lhs.dtype != rhs.dtype)
--- include/mlir/Dialect/Tosa/Utils/ConversionUtils.h
--- include/mlir/Dialect/Tosa/CMakeLists.txt
--- include/mlir/Dialect/Tosa/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/Transforms/Passes.td include/mlir/Dialect/Tosa/Transforms/Passes.td
55a56,74
> def TosaPartition : Pass<"tosa-partition", "ModuleOp"> {
>   let summary = "Outline TOSA Conv2D ops and adjacent element-wise ops";
>   let description = [{
>     Outline kernels of tosa::Conv2D and surrounding elementwise ops.
>   }];
> 
>   let options = [
>     ListOption<"anchorOps", "anchor-ops", "std::string",
>                "One or more operations to be used as focus of partitioned "
>                "kernels",
>                "llvm::cl::ZeroOrMore">,
>     Option<"partitionTagOpt", "partition-tag", "std::string",
>            /*default=*/"\"kernel\"", "Attribute for outlined functions">,
>     Option<"trailingOnly", "trailing-only", "bool", /*default=*/"true",
>            "Don't gather ops ahead of anchor op">
>   ];
>   let dependentDialects = ["tosa::TosaDialect"];
> }
> 
87,92c106,107
<              /*default=*/"\"undefined\"",
<              "Validate if operations match for the given profile">,
<       Option<"StrictOperationSpecAlignment", "strict-op-spec-alignment", "bool",
<              /*default=*/"false",
<              "Verify if the properties of certain operations align the spec requirement">,
<    ];
---
>       /*default=*/"\"undefined\"",
>       "Validation if ops match for given profile">];
--- include/mlir/Dialect/Tosa/Transforms/Passes.h
--- include/mlir/Dialect/Tosa/Transforms/Passes.td
55a56,74
> def TosaPartition : Pass<"tosa-partition", "ModuleOp"> {
>   let summary = "Outline TOSA Conv2D ops and adjacent element-wise ops";
>   let description = [{
>     Outline kernels of tosa::Conv2D and surrounding elementwise ops.
>   }];
> 
>   let options = [
>     ListOption<"anchorOps", "anchor-ops", "std::string",
>                "One or more operations to be used as focus of partitioned "
>                "kernels",
>                "llvm::cl::ZeroOrMore">,
>     Option<"partitionTagOpt", "partition-tag", "std::string",
>            /*default=*/"\"kernel\"", "Attribute for outlined functions">,
>     Option<"trailingOnly", "trailing-only", "bool", /*default=*/"true",
>            "Don't gather ops ahead of anchor op">
>   ];
>   let dependentDialects = ["tosa::TosaDialect"];
> }
> 
87,92c106,107
<              /*default=*/"\"undefined\"",
<              "Validate if operations match for the given profile">,
<       Option<"StrictOperationSpecAlignment", "strict-op-spec-alignment", "bool",
<              /*default=*/"false",
<              "Verify if the properties of certain operations align the spec requirement">,
<    ];
---
>       /*default=*/"\"undefined\"",
>       "Validation if ops match for given profile">];
--- include/mlir/Dialect/Tosa/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Tosa/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR/TosaOpBase.td include/mlir/Dialect/Tosa/IR/TosaOpBase.td
47a48
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR/TosaOps.h include/mlir/Dialect/Tosa/IR/TosaOps.h
17d16
< #include "mlir/IR/OpImplementation.h"
21d19
< #include "mlir/Interfaces/VectorInterfaces.h"
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR/TosaOps.td include/mlir/Dialect/Tosa/IR/TosaOps.td
187,216d186
< // Operator: fft2d
< //===----------------------------------------------------------------------===//
< def Tosa_FFT2dOp : Tosa_Op<"fft2d", [
<     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
<                               ["inferReturnTypeComponents"]>,
<     Pure]> {
<   let summary = "Performs FFT2D operation on the input.";
< 
<   let description = [{
<     Performs a batched complex 2D Fast Fourier Transform over the input. The
<     complex input values are constructed from the corresponding values in the
<     input_real and input_imag tensors. The resulting values in the output are
<     split into the output_real and output_imag tensors. No normalization is
<     applied on either the forward or inverse versions of the operation.
<   }];
< 
<   let arguments = (ins
<     Tosa_Tensor3D:$input_real,
<     Tosa_Tensor3D:$input_imag,
< 
<     BoolAttr:$inverse
<   );
< 
<   let results = (outs
<     Tosa_Tensor3D:$output_real,
<     Tosa_Tensor3D:$output_imag
<   );
< }
< 
< //===----------------------------------------------------------------------===//
304,331d273
< // Operator: rfft2d
< //===----------------------------------------------------------------------===//
< def Tosa_RFFT2dOp : Tosa_Op<"rfft2d", [
<     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
<                               ["inferReturnTypeComponents"]>,
<     Pure]> {
<   let summary = "Performs RFFT2D operation on the input.";
< 
<   let description = [{
<     Performs a batched 2D real-valued Fast Fourier Transform over the input where
<     the input tensor consists of real values producing complex valued output. The
<     complex output values will be split into the output_real and output_imag
<     tensor arguments. RFFT2D takes advantage of Hermitian symmetry to only
<     calculate the first half of the final output axis. Imaginary values with
<     locations (0,0), (0,W/2), (H/2,0) and (H/2,W/2) are zero.
<   }];
< 
<   let arguments = (ins
<     Tosa_Tensor3D:$input
<   );
< 
<   let results = (outs
<     Tosa_Tensor3D:$output_real,
<     Tosa_Tensor3D:$output_imag
<   );
< }
< 
< //===----------------------------------------------------------------------===//
450a393,421
> 
> //===----------------------------------------------------------------------===//
> // Operator: erf
> //===----------------------------------------------------------------------===//
> def Tosa_ErfOp : Tosa_Op<"erf", [
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
>   let summary = "Computes gauss error function of input";
> 
>   let description = [{
>     Gauss error function:
>                            - x
>     erf(x) = 2 / sqrt(pi) /  e^(-t^2) dt
>                          - 0
>     For quantized integer data types, the TABLE operator should be used instead
>     with the following definition.  The erf_table has 513 entries each of
>     16-bit/8-bit precision and covering the input range -4.0 to +4.0 in steps of 1/64.
>   }];
> 
>   let arguments = (ins
>     Tosa_Tensor:$input
>   );
> 
>   let results = (outs
>     Tosa_Tensor:$output
>   );
> }
> 
479a451
>   let hasCanonicalizer = 1;
797a770
>   let hasCanonicalizer = 1;
1265c1238,1240
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1282,1287d1256
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1294c1263,1265
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1311,1316d1281
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1323c1288,1290
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1340,1345d1306
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1352c1313,1315
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1369,1374d1331
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1381c1338,1340
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1398,1403d1356
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1410c1363,1365
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1427,1432d1381
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1444c1393,1395
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1462,1467d1412
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1505c1450,1452
<   InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1526,1531d1472
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1584d1524
<   let hasCanonicalizer = 1;
1769,1770d1708
<         float 32 to float 64    float32 float64
<         float 64 to float 32    float64 float32
1774c1712
<     Tosa_Tensor_Plus_F64:$input
---
>     Tosa_Tensor:$input
1778c1716
<     Tosa_Tensor_Plus_F64:$output
---
>     Tosa_Tensor:$output
1849c1787
<     Tosa_Tensor_Plus_F64:$output
---
>     Tosa_Tensor:$output
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR/TosaTypesBase.td include/mlir/Dialect/Tosa/IR/TosaTypesBase.td
98,100d97
< // Add F64 type support just for tosa::CastOp and tosa::ConstOp
< def Tosa_AnyNumber_Plus_F64 : AnyTypeOf<[Tosa_Int, Tosa_QuantizedInt, Tosa_Float, F64],
<                                "number_plus_f64">;
111d107
< def Tosa_Tensor_Plus_F64 : TensorOf<[Tosa_AnyNumber_Plus_F64]>;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Tosa/IR/TosaUtilOps.td include/mlir/Dialect/Tosa/IR/TosaUtilOps.td
22d21
< include "mlir/Interfaces/VectorInterfaces.td"
28,31c27
< def Tosa_ApplyScaleOp :
<   Tosa_Op<"apply_scale",
<           [Pure, DeclareOpInterfaceMethods<VectorUnrollOpInterface>] #
<           ElementwiseMappable.traits> {
---
> def Tosa_ApplyScaleOp: Tosa_Op<"apply_scale", [Pure] # ElementwiseMappable.traits> {
53,56d48
< 
<   let extraClassDeclaration = [{
<     std::optional<SmallVector<int64_t, 4>> getShapeForUnroll();
<   }];
--- include/mlir/Dialect/Tosa/IR/TosaUtilOps.td
22d21
< include "mlir/Interfaces/VectorInterfaces.td"
28,31c27
< def Tosa_ApplyScaleOp :
<   Tosa_Op<"apply_scale",
<           [Pure, DeclareOpInterfaceMethods<VectorUnrollOpInterface>] #
<           ElementwiseMappable.traits> {
---
> def Tosa_ApplyScaleOp: Tosa_Op<"apply_scale", [Pure] # ElementwiseMappable.traits> {
53,56d48
< 
<   let extraClassDeclaration = [{
<     std::optional<SmallVector<int64_t, 4>> getShapeForUnroll();
<   }];
--- include/mlir/Dialect/Tosa/IR/TosaOps.td
187,216d186
< // Operator: fft2d
< //===----------------------------------------------------------------------===//
< def Tosa_FFT2dOp : Tosa_Op<"fft2d", [
<     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
<                               ["inferReturnTypeComponents"]>,
<     Pure]> {
<   let summary = "Performs FFT2D operation on the input.";
< 
<   let description = [{
<     Performs a batched complex 2D Fast Fourier Transform over the input. The
<     complex input values are constructed from the corresponding values in the
<     input_real and input_imag tensors. The resulting values in the output are
<     split into the output_real and output_imag tensors. No normalization is
<     applied on either the forward or inverse versions of the operation.
<   }];
< 
<   let arguments = (ins
<     Tosa_Tensor3D:$input_real,
<     Tosa_Tensor3D:$input_imag,
< 
<     BoolAttr:$inverse
<   );
< 
<   let results = (outs
<     Tosa_Tensor3D:$output_real,
<     Tosa_Tensor3D:$output_imag
<   );
< }
< 
< //===----------------------------------------------------------------------===//
304,331d273
< // Operator: rfft2d
< //===----------------------------------------------------------------------===//
< def Tosa_RFFT2dOp : Tosa_Op<"rfft2d", [
<     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
<                               ["inferReturnTypeComponents"]>,
<     Pure]> {
<   let summary = "Performs RFFT2D operation on the input.";
< 
<   let description = [{
<     Performs a batched 2D real-valued Fast Fourier Transform over the input where
<     the input tensor consists of real values producing complex valued output. The
<     complex output values will be split into the output_real and output_imag
<     tensor arguments. RFFT2D takes advantage of Hermitian symmetry to only
<     calculate the first half of the final output axis. Imaginary values with
<     locations (0,0), (0,W/2), (H/2,0) and (H/2,W/2) are zero.
<   }];
< 
<   let arguments = (ins
<     Tosa_Tensor3D:$input
<   );
< 
<   let results = (outs
<     Tosa_Tensor3D:$output_real,
<     Tosa_Tensor3D:$output_imag
<   );
< }
< 
< //===----------------------------------------------------------------------===//
450a393,421
> 
> //===----------------------------------------------------------------------===//
> // Operator: erf
> //===----------------------------------------------------------------------===//
> def Tosa_ErfOp : Tosa_Op<"erf", [
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
>   let summary = "Computes gauss error function of input";
> 
>   let description = [{
>     Gauss error function:
>                            - x
>     erf(x) = 2 / sqrt(pi) /  e^(-t^2) dt
>                          - 0
>     For quantized integer data types, the TABLE operator should be used instead
>     with the following definition.  The erf_table has 513 entries each of
>     16-bit/8-bit precision and covering the input range -4.0 to +4.0 in steps of 1/64.
>   }];
> 
>   let arguments = (ins
>     Tosa_Tensor:$input
>   );
> 
>   let results = (outs
>     Tosa_Tensor:$output
>   );
> }
> 
479a451
>   let hasCanonicalizer = 1;
797a770
>   let hasCanonicalizer = 1;
1265c1238,1240
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1282,1287d1256
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1294c1263,1265
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1311,1316d1281
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1323c1288,1290
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1340,1345d1306
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1352c1313,1315
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1369,1374d1331
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1381c1338,1340
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1398,1403d1356
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1410c1363,1365
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1427,1432d1381
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1444c1393,1395
<     InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1462,1467d1412
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1505c1450,1452
<   InferTensorType, Pure]> {
---
>     DeclareOpInterfaceMethods<InferShapedTypeOpInterface,
>                               ["inferReturnTypeComponents"]>,
>     Pure]> {
1526,1531d1472
< 
<   let extraClassDeclaration = [{
<     /// Returns true when two result types are compatible for this op;
<     /// Method used by InferTypeOpInterface.
<     static bool isCompatibleReturnTypes(TypeRange l, TypeRange r);
<   }];
1584d1524
<   let hasCanonicalizer = 1;
1769,1770d1708
<         float 32 to float 64    float32 float64
<         float 64 to float 32    float64 float32
1774c1712
<     Tosa_Tensor_Plus_F64:$input
---
>     Tosa_Tensor:$input
1778c1716
<     Tosa_Tensor_Plus_F64:$output
---
>     Tosa_Tensor:$output
1849c1787
<     Tosa_Tensor_Plus_F64:$output
---
>     Tosa_Tensor:$output
--- include/mlir/Dialect/Tosa/IR/TosaOps.h
17d16
< #include "mlir/IR/OpImplementation.h"
21d19
< #include "mlir/Interfaces/VectorInterfaces.h"
--- include/mlir/Dialect/Tosa/IR/TosaInterfaces.td
--- include/mlir/Dialect/Tosa/IR/CMakeLists.txt
--- include/mlir/Dialect/Tosa/IR/TosaOpBase.td
47a48
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Tosa/IR/TosaTypesBase.td
98,100d97
< // Add F64 type support just for tosa::CastOp and tosa::ConstOp
< def Tosa_AnyNumber_Plus_F64 : AnyTypeOf<[Tosa_Int, Tosa_QuantizedInt, Tosa_Float, F64],
<                                "number_plus_f64">;
111d107
< def Tosa_Tensor_Plus_F64 : TensorOf<[Tosa_AnyNumber_Plus_F64]>;
--- include/mlir/Dialect/Affine
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis and include/mlir/Dialect/Affine/Analysis
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR and include/mlir/Dialect/Affine/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/LoopFusionUtils.h include/mlir/Dialect/Affine/LoopFusionUtils.h
24,26d23
< class Operation;
< 
< namespace affine {
28a26
> class Operation;
170,171d167
< 
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/LoopUtils.h include/mlir/Dialect/Affine/LoopUtils.h
24a25
> class AffineForOp;
26a28
> struct MemRefRegion;
40,43d41
< namespace affine {
< class AffineForOp;
< struct MemRefRegion;
< 
109,110c107
< /// and intra-tile loops. A band is a contiguous set of loops. This utility
< /// doesn't check for the validity of tiling itself, but just performs it.
---
> /// and intra-tile loops. A band is a contiguous set of loops.
190,192c187
< /// encountered. For memrefs for whose element types a size in bytes can't be
< /// computed (`index` type), their capacity is not accounted for and the
< /// `fastMemCapacityBytes` copy option would be non-functional in such cases.
---
> /// encountered.
350d344
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Passes.h include/mlir/Dialect/Affine/Passes.h
21d20
< 
26d24
< namespace affine {
114a113,116
> /// Populate patterns that expand affine index operations into more fundamental
> /// operations (not necessarily restricted to Affine dialect).
> void populateAffineExpandIndexOpsPatterns(RewritePatternSet &patterns);
> 
127d128
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Passes.td include/mlir/Dialect/Affine/Passes.td
20c20
<   let constructor = "mlir::affine::createAffineDataCopyGenerationPass()";
---
>   let constructor = "mlir::createAffineDataCopyGenerationPass()";
155c155
<   let constructor = "mlir::affine::createLoopFusionPass()";
---
>   let constructor = "mlir::createLoopFusionPass()";
169,170c169,170
<            "mlir::affine::FusionMode::Greedy", "fusion mode to attempt",
<            "llvm::cl::values(clEnumValN(mlir::affine::FusionMode::Greedy,"
---
>            "mlir::FusionMode::Greedy", "fusion mode to attempt",
>            "llvm::cl::values(clEnumValN(mlir::FusionMode::Greedy,"
172c172
<            "clEnumValN( mlir::affine::FusionMode::ProducerConsumer, "
---
>            "clEnumValN( mlir::FusionMode::ProducerConsumer, "
174c174
<            "clEnumValN( mlir::affine::FusionMode::Sibling, "
---
>            "clEnumValN( mlir::FusionMode::Sibling, "
183c183
<   let constructor = "mlir::affine::createAffineLoopInvariantCodeMotionPass()";
---
>   let constructor = "mlir::createAffineLoopInvariantCodeMotionPass()";
188c188
<   let constructor = "mlir::affine::createLoopTilingPass()";
---
>   let constructor = "mlir::createLoopTilingPass()";
204c204
<   let constructor = "mlir::affine::createLoopUnrollPass()";
---
>   let constructor = "mlir::createLoopUnrollPass()";
224c224
<   let constructor = "mlir::affine::createLoopUnrollAndJamPass()";
---
>   let constructor = "mlir::createLoopUnrollAndJamPass()";
298c298
<   let constructor = "mlir::affine::createPipelineDataTransferPass()";
---
>   let constructor = "mlir::createPipelineDataTransferPass()";
344c344
<   let constructor = "mlir::affine::createAffineScalarReplacementPass()";
---
>   let constructor = "mlir::createAffineScalarReplacementPass()";
372c372
<   let constructor = "mlir::affine::createAffineParallelizePass()";
---
>   let constructor = "mlir::createAffineParallelizePass()";
385c385
<   let constructor = "mlir::affine::createAffineLoopNormalizePass()";
---
>   let constructor = "mlir::createAffineLoopNormalizePass()";
395c395
<   let constructor = "mlir::affine::createLoopCoalescingPass()";
---
>   let constructor = "mlir::createLoopCoalescingPass()";
402c402
<   let constructor = "mlir::affine::createSimplifyAffineStructuresPass()";
---
>   let constructor = "mlir::createSimplifyAffineStructuresPass()";
407c407
<   let constructor = "mlir::affine::createAffineExpandIndexOpsPass()";
---
>   let constructor = "mlir::createAffineExpandIndexOpsPass()";
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/TransformOps and include/mlir/Dialect/Affine/TransformOps
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine: Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Utils.h include/mlir/Dialect/Affine/Utils.h
20a21,24
> 
> class AffineForOp;
> class AffineIfOp;
> class AffineParallelOp;
35,39d38
< namespace affine {
< class AffineForOp;
< class AffineIfOp;
< class AffineParallelOp;
< 
388d386
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/ViewLikeInterfaceUtils.h include/mlir/Dialect/Affine/ViewLikeInterfaceUtils.h
12d11
< #include "mlir/Dialect/Utils/StaticValueUtils.h"
17,19d15
< class RewriterBase;
< 
< namespace affine {
28,29d23
< // TODO: unify this API with resolveIndicesIntoOpWithOffsetsAndStrides or
< // deprecate.
45,46d38
< // TODO: unify this API with resolveIndicesIntoOpWithOffsetsAndStrides or
< // deprecate.
56,104d47
< /// Given the 'consumerIndices' of a load/store operation operating on an op
< /// with offsets and strides, return the combined indices.
< ///
< /// For example, using `memref.load` and `memref.subview` as an illustration:
< ///
< /// ```
< ///    %0 = ... : memref<12x42xf32>
< ///    %1 = memref.subview %0[%arg0, %arg1][...][%stride1, %stride2] :
< ///      memref<12x42xf32> to memref<4x4xf32, offset=?, strides=[?, ?]>
< ///    %2 = load %1[%i1, %i2] : memref<4x4xf32, offset=?, strides=[?, ?]>
< /// ```
< ///
< /// could be folded into:
< ///
< /// ```
< ///    %2 = load %0[%arg0 + %i1 * %stride1][%arg1 + %i2 * %stride2] :
< ///         memref<12x42xf32>
< /// ```
< void resolveIndicesIntoOpWithOffsetsAndStrides(
<     RewriterBase &rewriter, Location loc,
<     ArrayRef<OpFoldResult> mixedSourceOffsets,
<     ArrayRef<OpFoldResult> mixedSourceStrides,
<     const llvm::SmallBitVector &rankReducedDims,
<     ArrayRef<OpFoldResult> consumerIndices,
<     SmallVectorImpl<Value> &resolvedIndices);
< 
< inline void resolveIndicesIntoOpWithOffsetsAndStrides(
<     RewriterBase &rewriter, Location loc,
<     ArrayRef<OpFoldResult> mixedSourceOffsets,
<     ArrayRef<OpFoldResult> mixedSourceStrides,
<     const llvm::SmallBitVector &rankReducedDims, ValueRange consumerIndices,
<     SmallVectorImpl<Value> &resolvedIndices) {
<   return resolveIndicesIntoOpWithOffsetsAndStrides(
<       rewriter, loc, mixedSourceOffsets, mixedSourceStrides, rankReducedDims,
<       getAsOpFoldResult(consumerIndices), resolvedIndices);
< }
< 
< /// Given `sourceSizes`, `destSizes` and information about which dimensions are
< /// dropped by the source: `rankReducedSourceDims`, compute the resolved sizes
< /// that correspond to dest_op(source_op).
< /// In practice, this amounts to filtering by `rankReducedSourceDims` and taking
< /// from `sourceSizes` if a dimension is dropped, otherwise taking from
< /// `destSizes`.
< void resolveSizesIntoOpWithSizes(
<     ArrayRef<OpFoldResult> sourceSizes, ArrayRef<OpFoldResult> destSizes,
<     const llvm::SmallBitVector &rankReducedSourceDims,
<     SmallVectorImpl<OpFoldResult> &resolvedSizes);
< 
< } // namespace affine
--- include/mlir/Dialect/Affine/Passes.h
21d20
< 
26d24
< namespace affine {
114a113,116
> /// Populate patterns that expand affine index operations into more fundamental
> /// operations (not necessarily restricted to Affine dialect).
> void populateAffineExpandIndexOpsPatterns(RewritePatternSet &patterns);
> 
127d128
< } // namespace affine
--- include/mlir/Dialect/Affine/Passes.td
20c20
<   let constructor = "mlir::affine::createAffineDataCopyGenerationPass()";
---
>   let constructor = "mlir::createAffineDataCopyGenerationPass()";
155c155
<   let constructor = "mlir::affine::createLoopFusionPass()";
---
>   let constructor = "mlir::createLoopFusionPass()";
169,170c169,170
<            "mlir::affine::FusionMode::Greedy", "fusion mode to attempt",
<            "llvm::cl::values(clEnumValN(mlir::affine::FusionMode::Greedy,"
---
>            "mlir::FusionMode::Greedy", "fusion mode to attempt",
>            "llvm::cl::values(clEnumValN(mlir::FusionMode::Greedy,"
172c172
<            "clEnumValN( mlir::affine::FusionMode::ProducerConsumer, "
---
>            "clEnumValN( mlir::FusionMode::ProducerConsumer, "
174c174
<            "clEnumValN( mlir::affine::FusionMode::Sibling, "
---
>            "clEnumValN( mlir::FusionMode::Sibling, "
183c183
<   let constructor = "mlir::affine::createAffineLoopInvariantCodeMotionPass()";
---
>   let constructor = "mlir::createAffineLoopInvariantCodeMotionPass()";
188c188
<   let constructor = "mlir::affine::createLoopTilingPass()";
---
>   let constructor = "mlir::createLoopTilingPass()";
204c204
<   let constructor = "mlir::affine::createLoopUnrollPass()";
---
>   let constructor = "mlir::createLoopUnrollPass()";
224c224
<   let constructor = "mlir::affine::createLoopUnrollAndJamPass()";
---
>   let constructor = "mlir::createLoopUnrollAndJamPass()";
298c298
<   let constructor = "mlir::affine::createPipelineDataTransferPass()";
---
>   let constructor = "mlir::createPipelineDataTransferPass()";
344c344
<   let constructor = "mlir::affine::createAffineScalarReplacementPass()";
---
>   let constructor = "mlir::createAffineScalarReplacementPass()";
372c372
<   let constructor = "mlir::affine::createAffineParallelizePass()";
---
>   let constructor = "mlir::createAffineParallelizePass()";
385c385
<   let constructor = "mlir::affine::createAffineLoopNormalizePass()";
---
>   let constructor = "mlir::createAffineLoopNormalizePass()";
395c395
<   let constructor = "mlir::affine::createLoopCoalescingPass()";
---
>   let constructor = "mlir::createLoopCoalescingPass()";
402c402
<   let constructor = "mlir::affine::createSimplifyAffineStructuresPass()";
---
>   let constructor = "mlir::createSimplifyAffineStructuresPass()";
407c407
<   let constructor = "mlir::affine::createAffineExpandIndexOpsPass()";
---
>   let constructor = "mlir::createAffineExpandIndexOpsPass()";
--- include/mlir/Dialect/Affine/Utils.h
20a21,24
> 
> class AffineForOp;
> class AffineIfOp;
> class AffineParallelOp;
35,39d38
< namespace affine {
< class AffineForOp;
< class AffineIfOp;
< class AffineParallelOp;
< 
388d386
< } // namespace affine
--- include/mlir/Dialect/Affine/CMakeLists.txt
--- include/mlir/Dialect/Affine/ViewLikeInterfaceUtils.h
12d11
< #include "mlir/Dialect/Utils/StaticValueUtils.h"
17,19d15
< class RewriterBase;
< 
< namespace affine {
28,29d23
< // TODO: unify this API with resolveIndicesIntoOpWithOffsetsAndStrides or
< // deprecate.
45,46d38
< // TODO: unify this API with resolveIndicesIntoOpWithOffsetsAndStrides or
< // deprecate.
56,104d47
< /// Given the 'consumerIndices' of a load/store operation operating on an op
< /// with offsets and strides, return the combined indices.
< ///
< /// For example, using `memref.load` and `memref.subview` as an illustration:
< ///
< /// ```
< ///    %0 = ... : memref<12x42xf32>
< ///    %1 = memref.subview %0[%arg0, %arg1][...][%stride1, %stride2] :
< ///      memref<12x42xf32> to memref<4x4xf32, offset=?, strides=[?, ?]>
< ///    %2 = load %1[%i1, %i2] : memref<4x4xf32, offset=?, strides=[?, ?]>
< /// ```
< ///
< /// could be folded into:
< ///
< /// ```
< ///    %2 = load %0[%arg0 + %i1 * %stride1][%arg1 + %i2 * %stride2] :
< ///         memref<12x42xf32>
< /// ```
< void resolveIndicesIntoOpWithOffsetsAndStrides(
<     RewriterBase &rewriter, Location loc,
<     ArrayRef<OpFoldResult> mixedSourceOffsets,
<     ArrayRef<OpFoldResult> mixedSourceStrides,
<     const llvm::SmallBitVector &rankReducedDims,
<     ArrayRef<OpFoldResult> consumerIndices,
<     SmallVectorImpl<Value> &resolvedIndices);
< 
< inline void resolveIndicesIntoOpWithOffsetsAndStrides(
<     RewriterBase &rewriter, Location loc,
<     ArrayRef<OpFoldResult> mixedSourceOffsets,
<     ArrayRef<OpFoldResult> mixedSourceStrides,
<     const llvm::SmallBitVector &rankReducedDims, ValueRange consumerIndices,
<     SmallVectorImpl<Value> &resolvedIndices) {
<   return resolveIndicesIntoOpWithOffsetsAndStrides(
<       rewriter, loc, mixedSourceOffsets, mixedSourceStrides, rankReducedDims,
<       getAsOpFoldResult(consumerIndices), resolvedIndices);
< }
< 
< /// Given `sourceSizes`, `destSizes` and information about which dimensions are
< /// dropped by the source: `rankReducedSourceDims`, compute the resolved sizes
< /// that correspond to dest_op(source_op).
< /// In practice, this amounts to filtering by `rankReducedSourceDims` and taking
< /// from `sourceSizes` if a dimension is dropped, otherwise taking from
< /// `destSizes`.
< void resolveSizesIntoOpWithSizes(
<     ArrayRef<OpFoldResult> sourceSizes, ArrayRef<OpFoldResult> destSizes,
<     const llvm::SmallBitVector &rankReducedSourceDims,
<     SmallVectorImpl<OpFoldResult> &resolvedSizes);
< 
< } // namespace affine
--- include/mlir/Dialect/Affine/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.h include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.h
19a20
> namespace mlir {
20a22
> } // namespace mlir
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.td include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.td
23d22
<   let cppNamespace = "::mlir::affine";
28c27
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
33c32,33
<         return $_op.getOperand($_op.getMemRefOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getMemRefOperandIndex());
38c38
<       /*retTy=*/"::mlir::MemRefType",
---
>       /*retTy=*/"MemRefType",
43c43,44
<         return $_op.getMemRef().getType().template cast<::mlir::MemRefType>();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getMemRef().getType().template cast<MemRefType>();
48c49
<       /*retTy=*/"::mlir::Operation::operand_range",
---
>       /*retTy=*/"Operation::operand_range",
53c54,55
<         return llvm::drop_begin($_op.getOperands(), 1);
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return llvm::drop_begin(op.getOperands(), 1);
60c62
<       /*retTy=*/"::mlir::AffineMap",
---
>       /*retTy=*/"AffineMap",
65c67,68
<         return $_op.getAffineMapAttr().getValue();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getAffineMapAttr().getValue();
70c73
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
75c78
<         return $_op;
---
>         return cast<ConcreteOp>(this->getOperation());
86d88
<   let cppNamespace = "::mlir::affine";
91c93
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
96c98,99
<         return $_op.getOperand($_op.getMemRefOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getMemRefOperandIndex());
101c104
<       /*retTy=*/"::mlir::MemRefType",
---
>       /*retTy=*/"MemRefType",
106c109,110
<         return $_op.getMemRef().getType().template cast<::mlir::MemRefType>();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getMemRef().getType().template cast<MemRefType>();
111c115
<       /*retTy=*/"::mlir::Operation::operand_range",
---
>       /*retTy=*/"Operation::operand_range",
116c120,121
<         return llvm::drop_begin($_op.getOperands(), 2);
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return llvm::drop_begin(op.getOperands(), 2);
123c128
<       /*retTy=*/"::mlir::AffineMap",
---
>       /*retTy=*/"AffineMap",
128c133,134
<         return $_op.getAffineMapAttr().getValue();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getAffineMapAttr().getValue();
133c139
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
138c144,145
<         return $_op.getOperand($_op.getStoredValOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getStoredValOperandIndex());
151d157
<   let cppNamespace = "::mlir::affine";
156c162
<       /*retTy=*/"::mlir::NamedAttribute",
---
>       /*retTy=*/"NamedAttribute",
158c164
<       /*args=*/(ins "::mlir::Value":$memref),
---
>       /*args=*/(ins "Value":$memref),
161c167,168
<         assert(memref == $_op.getMemRef() &&
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         assert(memref == op.getMemRef() &&
163,165c170,171
<         return {::mlir::StringAttr::get(
<                     $_op.getContext(), $_op.getMapAttrStrName()),
<                     $_op.getAffineMapAttr()};
---
>         return {StringAttr::get(op.getContext(), op.getMapAttrStrName()),
>                 op.getAffineMapAttr()};
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR/AffineOps.h include/mlir/Dialect/Affine/IR/AffineOps.h
25,26d24
< namespace affine {
< 
30a29,31
> /// TODO: These should be renamed if they are on the mlir namespace.
> ///       Ideally, they should go in a mlir::affine:: namespace.
> 
91,92c92
<                 OpTrait::OpInvariants, AffineMapAccessInterface::Trait,
<                 MemoryEffectOpInterface::Trait> {
---
>                 OpTrait::OpInvariants, AffineMapAccessInterface::Trait> {
236,239d235
<   void
<   getEffects(SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
<                  &effects);
< 
340,342d335
<   void
<   getEffects(SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
<                  &effects);
440,441d432
< 
< } // namespace affine
443d433
< 
450,451d439
< namespace affine {
< 
456,462d443
< /// Returns true if `val` is the induction variable of an AffineParallelOp.
< bool isAffineParallelInductionVar(Value val);
< 
< /// Returns true if the provided value is the induction variable of an
< /// AffineForOp or AffineParallelOp.
< bool isAffineInductionVar(Value val);
< 
467,470d447
< /// Returns true if the provided value is among the induction variables of an
< /// AffineParallelOp.
< AffineParallelOp getAffineParallelInductionVarOwner(Value val);
< 
544d520
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR/AffineOps.td include/mlir/Dialect/Affine/IR/AffineOps.td
24c24
<   let cppNamespace = "::mlir::affine";
---
>   let cppNamespace = "mlir";
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
40c41
<     The `affine.apply` operation applies an [affine mapping](#affine-maps)
---
>     The affine.apply operation applies an [affine mapping](#affine-expressions)
99,110d99
< 
<     /// Returns all dimension operands.
<     ValueRange getDimOperands() {
<       return OperandRange{getOperands().begin(),
<                           getOperands().begin() + getMap().getNumDims()};
<     }
< 
<     /// Returns all symbol operands.
<     ValueRange getSymbolOperands() {
<       return OperandRange{getOperands().begin() + getMap().getNumDims(),
<                           getOperands().end()};
<     }
141c130
<     [`affine.yield`](#affineyield-mliraffineyieldop). *Note:* when
---
>     [`affine.yield`](#affineyield-affineyieldop). *Note:* when
520,526c509
<     Syntax:
< 
<     ```
<     operation ::= ssa-id `=` `affine.load` ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
<     ```
< 
<     The `affine.load` op reads an element from a memref, where the index
---
>     The "affine.load" op reads an element from a memref, where the index
528c511
<     variables and symbols. The output of `affine.load` is a new value with the
---
>     variables and symbols. The output of 'affine.load' is a new value with the
531c514
<     `symbol` can be used to indicate SSA identifiers which are symbolic.
---
>     'symbol' can be used to indicate SSA identifiers which are symbolic.
539c522
<     Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: Uses 'symbol' keyword for symbols '%n' and '%m'.
624c607
<     The `affine.max` operation computes the maximum value result from a multi-result
---
>     The "max" operation computes the maximum value result from a multi-result
641c624
<     The `affine.parallel` operation represents a hyper-rectangular affine
---
>     The "affine.parallel" operation represents a hyper-rectangular affine
649c632
<     region must contain exactly one block that terminates with `affine.yield`.
---
>     region must contain exactly one block that terminates with "affine.yield".
662c645
<     Each value yielded by `affine.yield` will be accumulated/reduced via one of
---
>     Each value yielded by affine.yield will be accumulated/reduced via one of
669c652
<     Note: Calling `AffineParallelOp::build` will create the required region and
---
>     Note: Calling AffineParallelOp::build will create the required region and
784c767
<     The `affine.prefetch` op prefetches data from a memref location described
---
>     The "affine.prefetch" op prefetches data from a memref location described
887,893c870
<     Syntax:
< 
<     ```
<     operation ::= `affine.store` ssa-use, ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
<     ```
< 
<     The `affine.store` op writes an element to a memref, where the index
---
>     The "affine.store" op writes an element to a memref, where the index
895c872
<     variables and symbols. The `affine.store` op stores a new value which is the
---
>     variables and symbols. The 'affine.store' op stores a new value which is the
898c875
<     `symbol` can be used to indicate SSA identifiers which are symbolic.
---
>     'symbol' can be used to indicate SSA identifiers which are symbolic.
906c883
<     Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: Uses 'symbol' keyword for symbols '%n' and '%m'.
937c914
<     The `affine.yield` yields zero or more SSA values from an affine op region and
---
>     "affine.yield" yields zero or more SSA values from an affine op region and
940c917
<     If `affine.yield` has any operands, the operands must match the parent
---
>     If "affine.yield" has any operands, the operands must match the parent
942c919
<     If the parent operation defines no values, then the `affine.yield` may be
---
>     If the parent operation defines no values, then the "affine.yield" may be
945a923
>     ```
961,962c939,940
<     The `affine.vector_load` is the vector counterpart of
<     [affine.load](#affineload-mliraffineloadop). It reads a slice from a
---
>     The "affine.vector_load" is the vector counterpart of
>     [affine.load](#affineload-affineloadop). It reads a slice from a
971c949
<     dimension of the memref. The keyword `symbol` can be used to indicate SSA
---
>     dimension of the memref. The keyword 'symbol' can be used to indicate SSA
980c958
<     Example 2: 4-wide f32 vector load. Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: 4-wide f32 vector load. Uses 'symbol' keyword for symbols '%n' and '%m'.
995c973
<     (see [vector.transfer_read](../Vector/#vectortransfer_read-mlirvectortransferreadop)).
---
>     (see [vector.transfer_read](../Vector/#vectortransfer_read-vectortransferreadop)).
1026,1027c1004,1005
<     The `affine.vector_store` is the vector counterpart of
<     [affine.store](#affinestore-mliraffinestoreop). It writes a
---
>     The "affine.vector_store" is the vector counterpart of
>     [affine.store](#affinestore-affinestoreop). It writes a
1038c1016
<     dimension of the memref. The keyword `symbol` can be used to indicate SSA
---
>     dimension of the memref. The keyword 'symbol' can be used to indicate SSA
1047c1025
<     Example 2: 4-wide f32 vector store. Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: 4-wide f32 vector store. Uses 'symbol' keyword for symbols '%n' and '%m'.
1062c1040
<     (see [vector.transfer_write](../Vector/#vectortransfer_write-mlirvectortransferwriteop)).
---
>     (see [vector.transfer_write](../Vector/#vectortransfer_write-vectortransferwriteop)).
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR/AffineValueMap.h include/mlir/Dialect/Affine/IR/AffineValueMap.h
21d20
< namespace affine {
93d91
< } // namespace affine
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/Affine/IR/CMakeLists.txt
--- include/mlir/Dialect/Affine/IR/AffineOps.td
24c24
<   let cppNamespace = "::mlir::affine";
---
>   let cppNamespace = "mlir";
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
40c41
<     The `affine.apply` operation applies an [affine mapping](#affine-maps)
---
>     The affine.apply operation applies an [affine mapping](#affine-expressions)
99,110d99
< 
<     /// Returns all dimension operands.
<     ValueRange getDimOperands() {
<       return OperandRange{getOperands().begin(),
<                           getOperands().begin() + getMap().getNumDims()};
<     }
< 
<     /// Returns all symbol operands.
<     ValueRange getSymbolOperands() {
<       return OperandRange{getOperands().begin() + getMap().getNumDims(),
<                           getOperands().end()};
<     }
141c130
<     [`affine.yield`](#affineyield-mliraffineyieldop). *Note:* when
---
>     [`affine.yield`](#affineyield-affineyieldop). *Note:* when
520,526c509
<     Syntax:
< 
<     ```
<     operation ::= ssa-id `=` `affine.load` ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
<     ```
< 
<     The `affine.load` op reads an element from a memref, where the index
---
>     The "affine.load" op reads an element from a memref, where the index
528c511
<     variables and symbols. The output of `affine.load` is a new value with the
---
>     variables and symbols. The output of 'affine.load' is a new value with the
531c514
<     `symbol` can be used to indicate SSA identifiers which are symbolic.
---
>     'symbol' can be used to indicate SSA identifiers which are symbolic.
539c522
<     Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: Uses 'symbol' keyword for symbols '%n' and '%m'.
624c607
<     The `affine.max` operation computes the maximum value result from a multi-result
---
>     The "max" operation computes the maximum value result from a multi-result
641c624
<     The `affine.parallel` operation represents a hyper-rectangular affine
---
>     The "affine.parallel" operation represents a hyper-rectangular affine
649c632
<     region must contain exactly one block that terminates with `affine.yield`.
---
>     region must contain exactly one block that terminates with "affine.yield".
662c645
<     Each value yielded by `affine.yield` will be accumulated/reduced via one of
---
>     Each value yielded by affine.yield will be accumulated/reduced via one of
669c652
<     Note: Calling `AffineParallelOp::build` will create the required region and
---
>     Note: Calling AffineParallelOp::build will create the required region and
784c767
<     The `affine.prefetch` op prefetches data from a memref location described
---
>     The "affine.prefetch" op prefetches data from a memref location described
887,893c870
<     Syntax:
< 
<     ```
<     operation ::= `affine.store` ssa-use, ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type
<     ```
< 
<     The `affine.store` op writes an element to a memref, where the index
---
>     The "affine.store" op writes an element to a memref, where the index
895c872
<     variables and symbols. The `affine.store` op stores a new value which is the
---
>     variables and symbols. The 'affine.store' op stores a new value which is the
898c875
<     `symbol` can be used to indicate SSA identifiers which are symbolic.
---
>     'symbol' can be used to indicate SSA identifiers which are symbolic.
906c883
<     Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: Uses 'symbol' keyword for symbols '%n' and '%m'.
937c914
<     The `affine.yield` yields zero or more SSA values from an affine op region and
---
>     "affine.yield" yields zero or more SSA values from an affine op region and
940c917
<     If `affine.yield` has any operands, the operands must match the parent
---
>     If "affine.yield" has any operands, the operands must match the parent
942c919
<     If the parent operation defines no values, then the `affine.yield` may be
---
>     If the parent operation defines no values, then the "affine.yield" may be
945a923
>     ```
961,962c939,940
<     The `affine.vector_load` is the vector counterpart of
<     [affine.load](#affineload-mliraffineloadop). It reads a slice from a
---
>     The "affine.vector_load" is the vector counterpart of
>     [affine.load](#affineload-affineloadop). It reads a slice from a
971c949
<     dimension of the memref. The keyword `symbol` can be used to indicate SSA
---
>     dimension of the memref. The keyword 'symbol' can be used to indicate SSA
980c958
<     Example 2: 4-wide f32 vector load. Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: 4-wide f32 vector load. Uses 'symbol' keyword for symbols '%n' and '%m'.
995c973
<     (see [vector.transfer_read](../Vector/#vectortransfer_read-mlirvectortransferreadop)).
---
>     (see [vector.transfer_read](../Vector/#vectortransfer_read-vectortransferreadop)).
1026,1027c1004,1005
<     The `affine.vector_store` is the vector counterpart of
<     [affine.store](#affinestore-mliraffinestoreop). It writes a
---
>     The "affine.vector_store" is the vector counterpart of
>     [affine.store](#affinestore-affinestoreop). It writes a
1038c1016
<     dimension of the memref. The keyword `symbol` can be used to indicate SSA
---
>     dimension of the memref. The keyword 'symbol' can be used to indicate SSA
1047c1025
<     Example 2: 4-wide f32 vector store. Uses `symbol` keyword for symbols `%n` and `%m`.
---
>     Example 2: 4-wide f32 vector store. Uses 'symbol' keyword for symbols '%n' and '%m'.
1062c1040
<     (see [vector.transfer_write](../Vector/#vectortransfer_write-mlirvectortransferwriteop)).
---
>     (see [vector.transfer_write](../Vector/#vectortransfer_write-vectortransferwriteop)).
--- include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.td
23d22
<   let cppNamespace = "::mlir::affine";
28c27
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
33c32,33
<         return $_op.getOperand($_op.getMemRefOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getMemRefOperandIndex());
38c38
<       /*retTy=*/"::mlir::MemRefType",
---
>       /*retTy=*/"MemRefType",
43c43,44
<         return $_op.getMemRef().getType().template cast<::mlir::MemRefType>();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getMemRef().getType().template cast<MemRefType>();
48c49
<       /*retTy=*/"::mlir::Operation::operand_range",
---
>       /*retTy=*/"Operation::operand_range",
53c54,55
<         return llvm::drop_begin($_op.getOperands(), 1);
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return llvm::drop_begin(op.getOperands(), 1);
60c62
<       /*retTy=*/"::mlir::AffineMap",
---
>       /*retTy=*/"AffineMap",
65c67,68
<         return $_op.getAffineMapAttr().getValue();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getAffineMapAttr().getValue();
70c73
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
75c78
<         return $_op;
---
>         return cast<ConcreteOp>(this->getOperation());
86d88
<   let cppNamespace = "::mlir::affine";
91c93
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
96c98,99
<         return $_op.getOperand($_op.getMemRefOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getMemRefOperandIndex());
101c104
<       /*retTy=*/"::mlir::MemRefType",
---
>       /*retTy=*/"MemRefType",
106c109,110
<         return $_op.getMemRef().getType().template cast<::mlir::MemRefType>();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getMemRef().getType().template cast<MemRefType>();
111c115
<       /*retTy=*/"::mlir::Operation::operand_range",
---
>       /*retTy=*/"Operation::operand_range",
116c120,121
<         return llvm::drop_begin($_op.getOperands(), 2);
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return llvm::drop_begin(op.getOperands(), 2);
123c128
<       /*retTy=*/"::mlir::AffineMap",
---
>       /*retTy=*/"AffineMap",
128c133,134
<         return $_op.getAffineMapAttr().getValue();
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getAffineMapAttr().getValue();
133c139
<       /*retTy=*/"::mlir::Value",
---
>       /*retTy=*/"Value",
138c144,145
<         return $_op.getOperand($_op.getStoredValOperandIndex());
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         return op.getOperand(op.getStoredValOperandIndex());
151d157
<   let cppNamespace = "::mlir::affine";
156c162
<       /*retTy=*/"::mlir::NamedAttribute",
---
>       /*retTy=*/"NamedAttribute",
158c164
<       /*args=*/(ins "::mlir::Value":$memref),
---
>       /*args=*/(ins "Value":$memref),
161c167,168
<         assert(memref == $_op.getMemRef() &&
---
>         ConcreteOp op = cast<ConcreteOp>(this->getOperation());
>         assert(memref == op.getMemRef() &&
163,165c170,171
<         return {::mlir::StringAttr::get(
<                     $_op.getContext(), $_op.getMapAttrStrName()),
<                     $_op.getAffineMapAttr()};
---
>         return {StringAttr::get(op.getContext(), op.getMapAttrStrName()),
>                 op.getAffineMapAttr()};
--- include/mlir/Dialect/Affine/IR/AffineOps.h
25,26d24
< namespace affine {
< 
30a29,31
> /// TODO: These should be renamed if they are on the mlir namespace.
> ///       Ideally, they should go in a mlir::affine:: namespace.
> 
91,92c92
<                 OpTrait::OpInvariants, AffineMapAccessInterface::Trait,
<                 MemoryEffectOpInterface::Trait> {
---
>                 OpTrait::OpInvariants, AffineMapAccessInterface::Trait> {
236,239d235
<   void
<   getEffects(SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
<                  &effects);
< 
340,342d335
<   void
<   getEffects(SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>
<                  &effects);
440,441d432
< 
< } // namespace affine
443d433
< 
450,451d439
< namespace affine {
< 
456,462d443
< /// Returns true if `val` is the induction variable of an AffineParallelOp.
< bool isAffineParallelInductionVar(Value val);
< 
< /// Returns true if the provided value is the induction variable of an
< /// AffineForOp or AffineParallelOp.
< bool isAffineInductionVar(Value val);
< 
467,470d447
< /// Returns true if the provided value is among the induction variables of an
< /// AffineParallelOp.
< AffineParallelOp getAffineParallelInductionVarOwner(Value val);
< 
544d520
< } // namespace affine
--- include/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.h
19a20
> namespace mlir {
20a22
> } // namespace mlir
--- include/mlir/Dialect/Affine/IR/AffineValueMap.h
21d20
< namespace affine {
93d91
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis/AffineAnalysis.h include/mlir/Dialect/Affine/Analysis/AffineAnalysis.h
24d23
< class Operation;
26d24
< namespace affine {
31a30
> class Operation;
171,173c170,171
<     unsigned loopDepth,
<     FlatAffineValueConstraints *dependenceConstraints = nullptr,
<     SmallVector<DependenceComponent, 2> *dependenceComponents = nullptr,
---
>     unsigned loopDepth, FlatAffineValueConstraints *dependenceConstraints,
>     SmallVector<DependenceComponent, 2> *dependenceComponents,
195d192
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis/AffineStructures.h include/mlir/Dialect/Affine/Analysis/AffineStructures.h
16d15
< #include "mlir/Analysis/FlatLinearValueConstraints.h"
24a24,28
> 
> class AffineCondition;
> class AffineForOp;
> class AffineIfOp;
> class AffineParallelOp;
25a30
> class AffineValueMap;
27d31
< class MemRefType;
29d32
< struct MutableAffineMap;
30a34,35
> class MemRefType;
> struct MutableAffineMap;
36,45c41,43
< namespace affine {
< class AffineCondition;
< class AffineForOp;
< class AffineIfOp;
< class AffineParallelOp;
< class AffineValueMap;
< 
< /// FlatAffineValueConstraints is an extension of FlatLinearValueConstraints
< /// with helper functions for Affine dialect ops.
< class FlatAffineValueConstraints : public FlatLinearValueConstraints {
---
> /// FlatAffineValueConstraints represents an extension of IntegerPolyhedron
> /// where each non-local variable can have an SSA Value attached to it.
> class FlatAffineValueConstraints : public presburger::IntegerPolyhedron {
47c45,84
<   using FlatLinearValueConstraints::FlatLinearValueConstraints;
---
>   /// Constructs a constraint system reserving memory for the specified number
>   /// of constraints and variables.
>   FlatAffineValueConstraints(unsigned numReservedInequalities,
>                              unsigned numReservedEqualities,
>                              unsigned numReservedCols, unsigned numDims,
>                              unsigned numSymbols, unsigned numLocals,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : IntegerPolyhedron(numReservedInequalities, numReservedEqualities,
>                           numReservedCols,
>                           presburger::PresburgerSpace::getSetSpace(
>                               numDims, numSymbols, numLocals)) {
>     assert(numReservedCols >= getNumVars() + 1);
>     assert(valArgs.empty() || valArgs.size() == getNumDimAndSymbolVars());
>     values.reserve(numReservedCols);
>     if (valArgs.empty())
>       values.resize(getNumDimAndSymbolVars(), std::nullopt);
>     else
>       values.append(valArgs.begin(), valArgs.end());
>   }
> 
>   /// Constructs a constraint system with the specified number of
>   /// dimensions and symbols.
>   FlatAffineValueConstraints(unsigned numDims = 0, unsigned numSymbols = 0,
>                              unsigned numLocals = 0,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : FlatAffineValueConstraints(/*numReservedInequalities=*/0,
>                                    /*numReservedEqualities=*/0,
>                                    /*numReservedCols=*/numDims + numSymbols +
>                                        numLocals + 1,
>                                    numDims, numSymbols, numLocals, valArgs) {}
> 
>   FlatAffineValueConstraints(const IntegerPolyhedron &fac,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : IntegerPolyhedron(fac) {
>     assert(valArgs.empty() || valArgs.size() == getNumDimAndSymbolVars());
>     if (valArgs.empty())
>       values.resize(getNumDimAndSymbolVars(), std::nullopt);
>     else
>       values.append(valArgs.begin(), valArgs.end());
>   }
49c86,108
<   /// Return the kind of this object.
---
>   /// Create a flat affine constraint system from an AffineValueMap or a list of
>   /// these. The constructed system will only include equalities.
>   explicit FlatAffineValueConstraints(const AffineValueMap &avm);
>   explicit FlatAffineValueConstraints(ArrayRef<const AffineValueMap *> avmRef);
> 
>   /// Creates an affine constraint system from an IntegerSet.
>   explicit FlatAffineValueConstraints(IntegerSet set, ValueRange operands = {});
> 
>   // Construct a hyperrectangular constraint set from ValueRanges that represent
>   // induction variables, lower and upper bounds. `ivs`, `lbs` and `ubs` are
>   // expected to match one to one. The order of variables and constraints is:
>   //
>   // ivs | lbs | ubs | eq/ineq
>   // ----+-----+-----+---------
>   //   1   -1     0      >= 0
>   // ----+-----+-----+---------
>   //  -1    0     1      >= 0
>   //
>   // All dimensions as set as VarKind::SetDim.
>   static FlatAffineValueConstraints
>   getHyperrectangular(ValueRange ivs, ValueRange lbs, ValueRange ubs);
> 
>   /// Return the kind of this FlatAffineConstraints.
53,54c112
<     return cst->getKind() >= Kind::FlatAffineValueConstraints &&
<            cst->getKind() <= Kind::FlatAffineRelation;
---
>     return cst->getKind() == Kind::FlatAffineValueConstraints;
56a115,130
>   /// Clears any existing data and reserves memory for the specified
>   /// constraints.
>   void reset(unsigned numReservedInequalities, unsigned numReservedEqualities,
>              unsigned numReservedCols, unsigned numDims, unsigned numSymbols,
>              unsigned numLocals = 0);
>   void reset(unsigned numDims = 0, unsigned numSymbols = 0,
>              unsigned numLocals = 0);
>   void reset(unsigned numReservedInequalities, unsigned numReservedEqualities,
>              unsigned numReservedCols, unsigned numDims, unsigned numSymbols,
>              unsigned numLocals, ArrayRef<Value> valArgs);
>   void reset(unsigned numDims, unsigned numSymbols, unsigned numLocals,
>              ArrayRef<Value> valArgs);
> 
>   /// Clones this object.
>   std::unique_ptr<FlatAffineValueConstraints> clone() const;
> 
99a174,199
>   /// being drawn from the specified bound map. In case of an EQ bound, the
>   /// bound map is expected to have exactly one result. In case of a LB/UB, the
>   /// bound map may have more than one result, for each of which an inequality
>   /// is added.
>   ///
>   /// The bound can be added as open or closed by specifying isClosedBound. In
>   /// case of a LB/UB, isClosedBound = false means the bound is added internally
>   /// as a closed bound by +1/-1 respectively. In case of an EQ bound, it can
>   /// only be added as a closed bound.
>   ///
>   /// Note: The dimensions/symbols of this FlatAffineConstraints must match the
>   /// dimensions/symbols of the affine map.
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap,
>                          bool isClosedBound);
> 
>   /// Adds a bound for the variable at the specified position with constraints
>   /// being drawn from the specified bound map. In case of an EQ bound, the
>   /// bound map is expected to have exactly one result. In case of a LB/UB, the
>   /// bound map may have more than one result, for each of which an inequality
>   /// is added.
>   /// Note: The dimensions/symbols of this FlatAffineConstraints must match the
>   /// dimensions/symbols of the affine map. By default the lower bound is closed
>   /// and the upper bound is open.
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap);
> 
>   /// Adds a bound for the variable at the specified position with constraints
104,106c204,205
<   LogicalResult addBound(presburger::BoundType type, unsigned pos,
<                          AffineMap boundMap, ValueRange operands);
<   using FlatLinearValueConstraints::addBound;
---
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap,
>                          ValueRange operands);
108,114c207,259
<   /// Add the specified values as a dim or symbol var depending on its nature,
<   /// if it already doesn't exist in the system. `val` has to be either a
<   /// terminal symbol or a loop IV, i.e., it cannot be the result affine.apply
<   /// of any symbols or loop IVs. The variable is added to the end of the
<   /// existing dims or symbols. Additional information on the variable is
<   /// extracted from the IR and added to the constraint system.
<   void addInductionVarOrTerminalSymbol(Value val);
---
>   /// Adds a constant bound for the variable associated with the given Value.
>   void addBound(BoundType type, Value val, int64_t value);
> 
>   /// The `addBound` overload above hides the inherited overloads by default, so
>   /// we explicitly introduce them here.
>   using IntegerPolyhedron::addBound;
> 
>   /// Returns the constraint system as an integer set. Returns a null integer
>   /// set if the system has no constraints, or if an integer set couldn't be
>   /// constructed as a result of a local variable's explicit representation not
>   /// being known and such a local variable appearing in any of the constraints.
>   IntegerSet getAsIntegerSet(MLIRContext *context) const;
> 
>   /// Computes the lower and upper bounds of the first `num` dimensional
>   /// variables (starting at `offset`) as an affine map of the remaining
>   /// variables (dimensional and symbolic). This method is able to detect
>   /// variables as floordiv's and mod's of affine expressions of other
>   /// variables with respect to (positive) constants. Sets bound map to a
>   /// null AffineMap if such a bound can't be found (or yet unimplemented).
>   ///
>   /// By default the returned lower bounds are closed and upper bounds are open.
>   /// This can be changed by getClosedUB.
>   void getSliceBounds(unsigned offset, unsigned num, MLIRContext *context,
>                       SmallVectorImpl<AffineMap> *lbMaps,
>                       SmallVectorImpl<AffineMap> *ubMaps,
>                       bool getClosedUB = false);
> 
>   /// Composes an affine map whose dimensions and symbols match one to one with
>   /// the dimensions and symbols of this FlatAffineConstraints. The results of
>   /// the map `other` are added as the leading dimensions of this constraint
>   /// system. Returns failure if `other` is a semi-affine map.
>   LogicalResult composeMatchingMap(AffineMap other);
> 
>   /// Gets the lower and upper bound of the `offset` + `pos`th variable
>   /// treating [0, offset) U [offset + num, symStartPos) as dimensions and
>   /// [symStartPos, getNumDimAndSymbolVars) as symbols, and `pos` lies in
>   /// [0, num). The multi-dimensional maps in the returned pair represent the
>   /// max and min of potentially multiple affine expressions. The upper bound is
>   /// exclusive. `localExprs` holds pre-computed AffineExpr's for all local
>   /// variables in the system.
>   std::pair<AffineMap, AffineMap>
>   getLowerAndUpperBound(unsigned pos, unsigned offset, unsigned num,
>                         unsigned symStartPos, ArrayRef<AffineExpr> localExprs,
>                         MLIRContext *context) const;
> 
>   /// Returns the bound for the variable at `pos` from the inequality at
>   /// `ineqPos` as a 1-d affine value map (affine map + operands). The returned
>   /// affine value map can either be a lower bound or an upper bound depending
>   /// on the sign of atIneq(ineqPos, pos). Asserts if the row at `ineqPos` does
>   /// not involve the `pos`th variable.
>   void getIneqAsAffineValueMap(unsigned pos, unsigned ineqPos,
>                                AffineValueMap &vmap,
>                                MLIRContext *context) const;
127,128c272,325
<   /// Changes all symbol variables which are loop IVs to dim variables.
<   void convertLoopIVSymbolsToDims();
---
>   /// Looks up the position of the variable with the specified Value. Returns
>   /// true if found (false otherwise). `pos` is set to the (column) position of
>   /// the variable.
>   bool findVar(Value val, unsigned *pos) const;
> 
>   /// Returns true if an variable with the specified Value exists, false
>   /// otherwise.
>   bool containsVar(Value val) const;
> 
>   /// Swap the posA^th variable with the posB^th variable.
>   void swapVar(unsigned posA, unsigned posB) override;
> 
>   /// Insert variables of the specified kind at position `pos`. Positions are
>   /// relative to the kind of variable. The coefficient columns corresponding
>   /// to the added variables are initialized to zero. `vals` are the Values
>   /// corresponding to the variables. Values should not be used with
>   /// VarKind::Local since values can only be attached to non-local variables.
>   /// Return the absolute column position (i.e., not relative to the kind of
>   /// variable) of the first added variable.
>   ///
>   /// Note: Empty Values are allowed in `vals`.
>   unsigned insertDimVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::SetDim, pos, num);
>   }
>   unsigned insertSymbolVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::Symbol, pos, num);
>   }
>   unsigned insertLocalVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::Local, pos, num);
>   }
>   unsigned insertDimVar(unsigned pos, ValueRange vals);
>   unsigned insertSymbolVar(unsigned pos, ValueRange vals);
>   unsigned insertVar(presburger::VarKind kind, unsigned pos,
>                      unsigned num = 1) override;
>   unsigned insertVar(presburger::VarKind kind, unsigned pos, ValueRange vals);
> 
>   /// Append variables of the specified kind after the last variable of that
>   /// kind. The coefficient columns corresponding to the added variables are
>   /// initialized to zero. `vals` are the Values corresponding to the
>   /// variables. Return the absolute column position (i.e., not relative to the
>   /// kind of variable) of the first appended variable.
>   ///
>   /// Note: Empty Values are allowed in `vals`.
>   unsigned appendDimVar(ValueRange vals);
>   unsigned appendSymbolVar(ValueRange vals);
>   unsigned appendDimVar(unsigned num = 1) {
>     return appendVar(VarKind::SetDim, num);
>   }
>   unsigned appendSymbolVar(unsigned num = 1) {
>     return appendVar(VarKind::Symbol, num);
>   }
>   unsigned appendLocalVar(unsigned num = 1) {
>     return appendVar(VarKind::Local, num);
>   }
130,137c327,344
<   /// Returns the bound for the variable at `pos` from the inequality at
<   /// `ineqPos` as a 1-d affine value map (affine map + operands). The returned
<   /// affine value map can either be a lower bound or an upper bound depending
<   /// on the sign of atIneq(ineqPos, pos). Asserts if the row at `ineqPos` does
<   /// not involve the `pos`th variable.
<   void getIneqAsAffineValueMap(unsigned pos, unsigned ineqPos,
<                                AffineValueMap &vmap,
<                                MLIRContext *context) const;
---
>   /// Removes variables in the column range [varStart, varLimit), and copies any
>   /// remaining valid data into place, updates member variables, and resizes
>   /// arrays as needed.
>   void removeVarRange(presburger::VarKind kind, unsigned varStart,
>                       unsigned varLimit) override;
>   using IntegerPolyhedron::removeVarRange;
> 
>   /// Add the specified values as a dim or symbol var depending on its nature,
>   /// if it already doesn't exist in the system. `val` has to be either a
>   /// terminal symbol or a loop IV, i.e., it cannot be the result affine.apply
>   /// of any symbols or loop IVs. The variable is added to the end of the
>   /// existing dims or symbols. Additional information on the variable is
>   /// extracted from the IR and added to the constraint system.
>   void addInductionVarOrTerminalSymbol(Value val);
> 
>   /// Align `map` with this constraint system based on `operands`. Each operand
>   /// must already have a corresponding dim/symbol in this constraint system.
>   AffineMap computeAlignedMap(AffineMap map, ValueRange operands) const;
146,148c353,355
<   /// the FlatAffineValueConstraints with which to associate. Every operand of
<   /// vMap should have a matching dim/symbol column in this constraint system
<   /// (with the same associated Value).
---
>   /// the FlatAffineConstraints with which to associate. Every operand of vMap
>   /// should have a matching dim/symbol column in this constraint system (with
>   /// the same associated Value).
149a357,514
> 
>   /// Projects out the variable that is associate with Value.
>   void projectOut(Value val);
>   using IntegerPolyhedron::projectOut;
> 
>   /// Changes all symbol variables which are loop IVs to dim variables.
>   void convertLoopIVSymbolsToDims();
> 
>   /// Updates the constraints to be the smallest bounding (enclosing) box that
>   /// contains the points of `this` set and that of `other`, with the symbols
>   /// being treated specially. For each of the dimensions, the min of the lower
>   /// bounds (symbolic) and the max of the upper bounds (symbolic) is computed
>   /// to determine such a bounding box. `other` is expected to have the same
>   /// dimensional variables as this constraint system (in the same order).
>   ///
>   /// E.g.:
>   /// 1) this   = {0 <= d0 <= 127},
>   ///    other  = {16 <= d0 <= 192},
>   ///    output = {0 <= d0 <= 192}
>   /// 2) this   = {s0 + 5 <= d0 <= s0 + 20},
>   ///    other  = {s0 + 1 <= d0 <= s0 + 9},
>   ///    output = {s0 + 1 <= d0 <= s0 + 20}
>   /// 3) this   = {0 <= d0 <= 5, 1 <= d1 <= 9}
>   ///    other  = {2 <= d0 <= 6, 5 <= d1 <= 15},
>   ///    output = {0 <= d0 <= 6, 1 <= d1 <= 15}
>   LogicalResult unionBoundingBox(const FlatAffineValueConstraints &other);
>   using IntegerPolyhedron::unionBoundingBox;
> 
>   /// Merge and align the variables of `this` and `other` starting at
>   /// `offset`, so that both constraint systems get the union of the contained
>   /// variables that is dimension-wise and symbol-wise unique; both
>   /// constraint systems are updated so that they have the union of all
>   /// variables, with `this`'s original variables appearing first followed
>   /// by any of `other`'s variables that didn't appear in `this`. Local
>   /// variables in `other` that have the same division representation as local
>   /// variables in `this` are merged into one.
>   //  E.g.: Input: `this`  has (%i, %j) [%M, %N]
>   //               `other` has (%k, %j) [%P, %N, %M]
>   //        Output: both `this`, `other` have (%i, %j, %k) [%M, %N, %P]
>   //
>   void mergeAndAlignVarsWithOther(unsigned offset,
>                                   FlatAffineValueConstraints *other);
> 
>   /// Returns true if this constraint system and `other` are in the same
>   /// space, i.e., if they are associated with the same set of variables,
>   /// appearing in the same order. Returns false otherwise.
>   bool areVarsAlignedWithOther(const FlatAffineValueConstraints &other);
> 
>   /// Replaces the contents of this FlatAffineValueConstraints with `other`.
>   void clearAndCopyFrom(const IntegerRelation &other) override;
> 
>   /// Returns the Value associated with the pos^th variable. Asserts if
>   /// no Value variable was associated.
>   inline Value getValue(unsigned pos) const {
>     assert(pos < getNumDimAndSymbolVars() && "Invalid position");
>     assert(hasValue(pos) && "variable's Value not set");
>     return *values[pos];
>   }
> 
>   /// Returns true if the pos^th variable has an associated Value.
>   inline bool hasValue(unsigned pos) const {
>     assert(pos < getNumDimAndSymbolVars() && "Invalid position");
>     return values[pos].has_value();
>   }
> 
>   /// Returns true if at least one variable has an associated Value.
>   bool hasValues() const;
> 
>   /// Returns the Values associated with variables in range [start, end).
>   /// Asserts if no Value was associated with one of these variables.
>   inline void getValues(unsigned start, unsigned end,
>                         SmallVectorImpl<Value> *values) const {
>     assert(end <= getNumDimAndSymbolVars() && "invalid end position");
>     assert(start <= end && "invalid start position");
>     values->clear();
>     values->reserve(end - start);
>     for (unsigned i = start; i < end; i++)
>       values->push_back(getValue(i));
>   }
>   inline void getAllValues(SmallVectorImpl<Value> *values) const {
>     getValues(0, getNumDimAndSymbolVars(), values);
>   }
> 
>   inline ArrayRef<std::optional<Value>> getMaybeValues() const {
>     return {values.data(), values.size()};
>   }
> 
>   inline ArrayRef<std::optional<Value>>
>   getMaybeValues(presburger::VarKind kind) const {
>     assert(kind != VarKind::Local &&
>            "Local variables do not have any value attached to them.");
>     return {values.data() + getVarKindOffset(kind), getNumVarKind(kind)};
>   }
> 
>   /// Sets the Value associated with the pos^th variable.
>   inline void setValue(unsigned pos, Value val) {
>     assert(pos < getNumDimAndSymbolVars() && "invalid var position");
>     values[pos] = val;
>   }
> 
>   /// Sets the Values associated with the variables in the range [start, end).
>   /// The range must contain only dim and symbol variables.
>   void setValues(unsigned start, unsigned end, ArrayRef<Value> values) {
>     assert(end <= getNumVars() && "invalid end position");
>     assert(start <= end && "invalid start position");
>     assert(values.size() == end - start &&
>            "value should be provided for each variable in the range.");
>     for (unsigned i = start; i < end; ++i)
>       setValue(i, values[i - start]);
>   }
> 
>   /// Merge and align symbols of `this` and `other` such that both get union of
>   /// of symbols that are unique. Symbols in `this` and `other` should be
>   /// unique. Symbols with Value as `None` are considered to be inequal to all
>   /// other symbols.
>   void mergeSymbolVars(FlatAffineValueConstraints &other);
> 
> protected:
>   using VarKind = presburger::VarKind;
> 
>   /// Returns false if the fields corresponding to various variable counts, or
>   /// equality/inequality buffer sizes aren't consistent; true otherwise. This
>   /// is meant to be used within an assert internally.
>   bool hasConsistentState() const override;
> 
>   /// Given an affine map that is aligned with this constraint system:
>   /// * Flatten the map.
>   /// * Add newly introduced local columns at the beginning of this constraint
>   ///   system (local column pos 0).
>   /// * Add equalities that define the new local columns to this constraint
>   ///   system.
>   /// * Return the flattened expressions via `flattenedExprs`.
>   ///
>   /// Note: This is a shared helper function of `addLowerOrUpperBound` and
>   ///       `composeMatchingMap`.
>   LogicalResult flattenAlignedMapAndMergeLocals(
>       AffineMap map, std::vector<SmallVector<int64_t, 8>> *flattenedExprs);
> 
>   /// Eliminates the variable at the specified position using Fourier-Motzkin
>   /// variable elimination, but uses Gaussian elimination if there is an
>   /// equality involving that variable. If the result of the elimination is
>   /// integer exact, `*isResultIntegerExact` is set to true. If `darkShadow` is
>   /// set to true, a potential under approximation (subset) of the rational
>   /// shadow / exact integer shadow is computed.
>   // See implementation comments for more details.
>   void fourierMotzkinEliminate(unsigned pos, bool darkShadow = false,
>                                bool *isResultIntegerExact = nullptr) override;
> 
>   /// Prints the number of constraints, dimensions, symbols and locals in the
>   /// FlatAffineConstraints. Also, prints for each variable whether there is
>   /// an SSA Value attached to it.
>   void printSpace(raw_ostream &os) const override;
> 
>   /// Values corresponding to the (column) non-local variables of this
>   /// constraint system appearing in the order the variables correspond to
>   /// columns. Variables that aren't associated with any Value are set to
>   /// None.
>   SmallVector<std::optional<Value>, 8> values;
185,191d549
<   /// Return the kind of this object.
<   Kind getKind() const override { return Kind::FlatAffineRelation; }
< 
<   static bool classof(const IntegerRelation *cst) {
<     return cst->getKind() == Kind::FlatAffineRelation;
<   }
< 
237a596,655
> /// Flattens 'expr' into 'flattenedExpr', which contains the coefficients of the
> /// dimensions, symbols, and additional variables that represent floor divisions
> /// of dimensions, symbols, and in turn other floor divisions.  Returns failure
> /// if 'expr' could not be flattened (i.e., semi-affine is not yet handled).
> /// 'cst' contains constraints that connect newly introduced local variables
> /// to existing dimensional and symbolic variables. See documentation for
> /// AffineExprFlattener on how mod's and div's are flattened.
> LogicalResult getFlattenedAffineExpr(AffineExpr expr, unsigned numDims,
>                                      unsigned numSymbols,
>                                      SmallVectorImpl<int64_t> *flattenedExpr,
>                                      FlatAffineValueConstraints *cst = nullptr);
> 
> /// Flattens the result expressions of the map to their corresponding flattened
> /// forms and set in 'flattenedExprs'. Returns failure if any expression in the
> /// map could not be flattened (i.e., semi-affine is not yet handled). 'cst'
> /// contains constraints that connect newly introduced local variables to
> /// existing dimensional and / symbolic variables. See documentation for
> /// AffineExprFlattener on how mod's and div's are flattened. For all affine
> /// expressions that share the same operands (like those of an affine map), this
> /// method should be used instead of repeatedly calling getFlattenedAffineExpr
> /// since local variables added to deal with div's and mod's will be reused
> /// across expressions.
> LogicalResult
> getFlattenedAffineExprs(AffineMap map,
>                         std::vector<SmallVector<int64_t, 8>> *flattenedExprs,
>                         FlatAffineValueConstraints *cst = nullptr);
> LogicalResult
> getFlattenedAffineExprs(IntegerSet set,
>                         std::vector<SmallVector<int64_t, 8>> *flattenedExprs,
>                         FlatAffineValueConstraints *cst = nullptr);
> 
> LogicalResult
> getMultiAffineFunctionFromMap(AffineMap map,
>                               presburger::MultiAffineFunction &multiAff);
> 
> /// Re-indexes the dimensions and symbols of an affine map with given `operands`
> /// values to align with `dims` and `syms` values.
> ///
> /// Each dimension/symbol of the map, bound to an operand `o`, is replaced with
> /// dimension `i`, where `i` is the position of `o` within `dims`. If `o` is not
> /// in `dims`, replace it with symbol `i`, where `i` is the position of `o`
> /// within `syms`. If `o` is not in `syms` either, replace it with a new symbol.
> ///
> /// Note: If a value appears multiple times as a dimension/symbol (or both), all
> /// corresponding dim/sym expressions are replaced with the first dimension
> /// bound to that value (or first symbol if no such dimension exists).
> ///
> /// The resulting affine map has `dims.size()` many dimensions and at least
> /// `syms.size()` many symbols.
> ///
> /// The SSA values of the symbols of the resulting map are optionally returned
> /// via `newSyms`. This is a concatenation of `syms` with the SSA values of the
> /// newly added symbols.
> ///
> /// Note: As part of this re-indexing, dimensions may turn into symbols, or vice
> /// versa.
> AffineMap alignAffineMapWithValues(AffineMap map, ValueRange operands,
>                                    ValueRange dims, ValueRange syms,
>                                    SmallVector<Value> *newSyms = nullptr);
> 
258,259c676
< } // namespace affine
< } // namespace mlir
---
> } // namespace mlir.
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis/LoopAnalysis.h include/mlir/Dialect/Affine/Analysis/LoopAnalysis.h
20a21
> 
21a23
> class AffineForOp;
24a27
> class NestedPattern;
28,31d30
< namespace affine {
< class AffineForOp;
< class NestedPattern;
< 
87d85
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis/NestedMatcher.h include/mlir/Dialect/Affine/Analysis/NestedMatcher.h
17d16
< class Operation;
19d17
< namespace affine {
20a19
> class Operation;
195d193
< } // namespace affine
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/Analysis/Utils.h include/mlir/Dialect/Affine/Analysis/Utils.h
20c20,24
< #include "mlir/Dialect/Affine/IR/AffineOps.h"
---
> #include "mlir/IR/AffineMap.h"
> #include "mlir/IR/Block.h"
> #include "mlir/IR/Location.h"
> #include "mlir/Support/LLVM.h"
> #include "llvm/ADT/SmallVector.h"
25,28d28
< class Block;
< class Location;
< class Operation;
< class Value;
30d29
< namespace affine {
32a32,33
> class Block;
> class Location;
34,213c35,36
< 
< // LoopNestStateCollector walks loop nests and collects load and store
< // operations, and whether or not a region holding op other than ForOp and IfOp
< // was encountered in the loop nest.
< struct LoopNestStateCollector {
<   SmallVector<AffineForOp, 4> forOps;
<   SmallVector<Operation *, 4> loadOpInsts;
<   SmallVector<Operation *, 4> storeOpInsts;
<   bool hasNonAffineRegionOp = false;
< 
<   // Collects load and store operations, and whether or not a region holding op
<   // other than ForOp and IfOp was encountered in the loop nest.
<   void collect(Operation *opToWalk);
< };
< 
< // MemRefDependenceGraph is a graph data structure where graph nodes are
< // top-level operations in a `Block` which contain load/store ops, and edges
< // are memref dependences between the nodes.
< // TODO: Add a more flexible dependence graph representation.
< // TODO: Add a depth parameter to dependence graph construction.
< struct MemRefDependenceGraph {
< public:
<   // Node represents a node in the graph. A Node is either an entire loop nest
<   // rooted at the top level which contains loads/stores, or a top level
<   // load/store.
<   struct Node {
<     // The unique identifier of this node in the graph.
<     unsigned id;
<     // The top-level statement which is (or contains) a load/store.
<     Operation *op;
<     // List of load operations.
<     SmallVector<Operation *, 4> loads;
<     // List of store op insts.
<     SmallVector<Operation *, 4> stores;
< 
<     Node(unsigned id, Operation *op) : id(id), op(op) {}
< 
<     // Returns the load op count for 'memref'.
<     unsigned getLoadOpCount(Value memref) const;
< 
<     // Returns the store op count for 'memref'.
<     unsigned getStoreOpCount(Value memref) const;
< 
<     // Returns all store ops in 'storeOps' which access 'memref'.
<     void getStoreOpsForMemref(Value memref,
<                               SmallVectorImpl<Operation *> *storeOps) const;
< 
<     // Returns all load ops in 'loadOps' which access 'memref'.
<     void getLoadOpsForMemref(Value memref,
<                              SmallVectorImpl<Operation *> *loadOps) const;
< 
<     // Returns all memrefs in 'loadAndStoreMemrefSet' for which this node
<     // has at least one load and store operation.
<     void getLoadAndStoreMemrefSet(DenseSet<Value> *loadAndStoreMemrefSet) const;
<   };
< 
<   // Edge represents a data dependence between nodes in the graph.
<   struct Edge {
<     // The id of the node at the other end of the edge.
<     // If this edge is stored in Edge = Node.inEdges[i], then
<     // 'Node.inEdges[i].id' is the identifier of the source node of the edge.
<     // If this edge is stored in Edge = Node.outEdges[i], then
<     // 'Node.outEdges[i].id' is the identifier of the dest node of the edge.
<     unsigned id;
<     // The SSA value on which this edge represents a dependence.
<     // If the value is a memref, then the dependence is between graph nodes
<     // which contain accesses to the same memref 'value'. If the value is a
<     // non-memref value, then the dependence is between a graph node which
<     // defines an SSA value and another graph node which uses the SSA value
<     // (e.g. a constant or load operation defining a value which is used inside
<     // a loop nest).
<     Value value;
<   };
< 
<   // Map from node id to Node.
<   DenseMap<unsigned, Node> nodes;
<   // Map from node id to list of input edges.
<   DenseMap<unsigned, SmallVector<Edge, 2>> inEdges;
<   // Map from node id to list of output edges.
<   DenseMap<unsigned, SmallVector<Edge, 2>> outEdges;
<   // Map from memref to a count on the dependence edges associated with that
<   // memref.
<   DenseMap<Value, unsigned> memrefEdgeCount;
<   // The next unique identifier to use for newly created graph nodes.
<   unsigned nextNodeId = 0;
< 
<   MemRefDependenceGraph(Block &block) : block(block) {}
< 
<   // Initializes the dependence graph based on operations in `block'.
<   // Returns true on success, false otherwise.
<   bool init();
< 
<   // Returns the graph node for 'id'.
<   Node *getNode(unsigned id);
< 
<   // Returns the graph node for 'forOp'.
<   Node *getForOpNode(AffineForOp forOp);
< 
<   // Adds a node with 'op' to the graph and returns its unique identifier.
<   unsigned addNode(Operation *op);
< 
<   // Remove node 'id' (and its associated edges) from graph.
<   void removeNode(unsigned id);
< 
<   // Returns true if node 'id' writes to any memref which escapes (or is an
<   // argument to) the block. Returns false otherwise.
<   bool writesToLiveInOrEscapingMemrefs(unsigned id);
< 
<   // Returns true iff there is an edge from node 'srcId' to node 'dstId' which
<   // is for 'value' if non-null, or for any value otherwise. Returns false
<   // otherwise.
<   bool hasEdge(unsigned srcId, unsigned dstId, Value value = nullptr);
< 
<   // Adds an edge from node 'srcId' to node 'dstId' for 'value'.
<   void addEdge(unsigned srcId, unsigned dstId, Value value);
< 
<   // Removes an edge from node 'srcId' to node 'dstId' for 'value'.
<   void removeEdge(unsigned srcId, unsigned dstId, Value value);
< 
<   // Returns true if there is a path in the dependence graph from node 'srcId'
<   // to node 'dstId'. Returns false otherwise. `srcId`, `dstId`, and the
<   // operations that the edges connected are expected to be from the same block.
<   bool hasDependencePath(unsigned srcId, unsigned dstId);
< 
<   // Returns the input edge count for node 'id' and 'memref' from src nodes
<   // which access 'memref' with a store operation.
<   unsigned getIncomingMemRefAccesses(unsigned id, Value memref);
< 
<   // Returns the output edge count for node 'id' and 'memref' (if non-null),
<   // otherwise returns the total output edge count from node 'id'.
<   unsigned getOutEdgeCount(unsigned id, Value memref = nullptr);
< 
<   /// Return all nodes which define SSA values used in node 'id'.
<   void gatherDefiningNodes(unsigned id, DenseSet<unsigned> &definingNodes);
< 
<   // Computes and returns an insertion point operation, before which the
<   // the fused <srcId, dstId> loop nest can be inserted while preserving
<   // dependences. Returns nullptr if no such insertion point is found.
<   Operation *getFusedLoopNestInsertionPoint(unsigned srcId, unsigned dstId);
< 
<   // Updates edge mappings from node 'srcId' to node 'dstId' after fusing them,
<   // taking into account that:
<   //   *) if 'removeSrcId' is true, 'srcId' will be removed after fusion,
<   //   *) memrefs in 'privateMemRefs' has been replaced in node at 'dstId' by a
<   //      private memref.
<   void updateEdges(unsigned srcId, unsigned dstId,
<                    const DenseSet<Value> &privateMemRefs, bool removeSrcId);
< 
<   // Update edge mappings for nodes 'sibId' and 'dstId' to reflect fusion
<   // of sibling node 'sibId' into node 'dstId'.
<   void updateEdges(unsigned sibId, unsigned dstId);
< 
<   // Adds ops in 'loads' and 'stores' to node at 'id'.
<   void addToNode(unsigned id, const SmallVectorImpl<Operation *> &loads,
<                  const SmallVectorImpl<Operation *> &stores);
< 
<   void clearNodeLoadAndStores(unsigned id);
< 
<   // Calls 'callback' for each input edge incident to node 'id' which carries a
<   // memref dependence.
<   void forEachMemRefInputEdge(unsigned id,
<                               const std::function<void(Edge)> &callback);
< 
<   // Calls 'callback' for each output edge from node 'id' which carries a
<   // memref dependence.
<   void forEachMemRefOutputEdge(unsigned id,
<                                const std::function<void(Edge)> &callback);
< 
<   // Calls 'callback' for each edge in 'edges' which carries a memref
<   // dependence.
<   void forEachMemRefEdge(ArrayRef<Edge> edges,
<                          const std::function<void(Edge)> &callback);
< 
<   void print(raw_ostream &os) const;
< 
<   void dump() const { print(llvm::errs()); }
< 
<   /// The block for which this graph is created to perform fusion.
<   Block &block;
< };
---
> class Operation;
> class Value;
219,222d41
< /// Populates 'ivs' with IVs of the surrounding affine.for and affine.parallel
< /// ops ordered from the outermost one to the innermost.
< void getAffineIVs(Operation &op, SmallVectorImpl<Value> &ivs);
< 
539,541c358,360
< /// Returns the size of a memref with element type int or float in bytes if it's
< /// statically shaped, std::nullopt otherwise.
< std::optional<uint64_t> getIntOrFloatMemRefSizeInBytes(MemRefType memRefType);
---
> /// Returns the size of memref data in bytes if it's statically shaped,
> /// std::nullopt otherwise.
> std::optional<uint64_t> getMemRefSizeInBytes(MemRefType memRefType);
558,561d376
< /// Returns the memref's element type's size in bytes where the elemental type
< /// is an int or float or a vector of such types.
< std::optional<int64_t> getMemRefIntOrFloatEltSizeInBytes(MemRefType memRefType);
< 
580d394
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis/NestedMatcher.h
17d16
< class Operation;
19d17
< namespace affine {
20a19
> class Operation;
195d193
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis/Utils.h
20c20,24
< #include "mlir/Dialect/Affine/IR/AffineOps.h"
---
> #include "mlir/IR/AffineMap.h"
> #include "mlir/IR/Block.h"
> #include "mlir/IR/Location.h"
> #include "mlir/Support/LLVM.h"
> #include "llvm/ADT/SmallVector.h"
25,28d28
< class Block;
< class Location;
< class Operation;
< class Value;
30d29
< namespace affine {
32a32,33
> class Block;
> class Location;
34,213c35,36
< 
< // LoopNestStateCollector walks loop nests and collects load and store
< // operations, and whether or not a region holding op other than ForOp and IfOp
< // was encountered in the loop nest.
< struct LoopNestStateCollector {
<   SmallVector<AffineForOp, 4> forOps;
<   SmallVector<Operation *, 4> loadOpInsts;
<   SmallVector<Operation *, 4> storeOpInsts;
<   bool hasNonAffineRegionOp = false;
< 
<   // Collects load and store operations, and whether or not a region holding op
<   // other than ForOp and IfOp was encountered in the loop nest.
<   void collect(Operation *opToWalk);
< };
< 
< // MemRefDependenceGraph is a graph data structure where graph nodes are
< // top-level operations in a `Block` which contain load/store ops, and edges
< // are memref dependences between the nodes.
< // TODO: Add a more flexible dependence graph representation.
< // TODO: Add a depth parameter to dependence graph construction.
< struct MemRefDependenceGraph {
< public:
<   // Node represents a node in the graph. A Node is either an entire loop nest
<   // rooted at the top level which contains loads/stores, or a top level
<   // load/store.
<   struct Node {
<     // The unique identifier of this node in the graph.
<     unsigned id;
<     // The top-level statement which is (or contains) a load/store.
<     Operation *op;
<     // List of load operations.
<     SmallVector<Operation *, 4> loads;
<     // List of store op insts.
<     SmallVector<Operation *, 4> stores;
< 
<     Node(unsigned id, Operation *op) : id(id), op(op) {}
< 
<     // Returns the load op count for 'memref'.
<     unsigned getLoadOpCount(Value memref) const;
< 
<     // Returns the store op count for 'memref'.
<     unsigned getStoreOpCount(Value memref) const;
< 
<     // Returns all store ops in 'storeOps' which access 'memref'.
<     void getStoreOpsForMemref(Value memref,
<                               SmallVectorImpl<Operation *> *storeOps) const;
< 
<     // Returns all load ops in 'loadOps' which access 'memref'.
<     void getLoadOpsForMemref(Value memref,
<                              SmallVectorImpl<Operation *> *loadOps) const;
< 
<     // Returns all memrefs in 'loadAndStoreMemrefSet' for which this node
<     // has at least one load and store operation.
<     void getLoadAndStoreMemrefSet(DenseSet<Value> *loadAndStoreMemrefSet) const;
<   };
< 
<   // Edge represents a data dependence between nodes in the graph.
<   struct Edge {
<     // The id of the node at the other end of the edge.
<     // If this edge is stored in Edge = Node.inEdges[i], then
<     // 'Node.inEdges[i].id' is the identifier of the source node of the edge.
<     // If this edge is stored in Edge = Node.outEdges[i], then
<     // 'Node.outEdges[i].id' is the identifier of the dest node of the edge.
<     unsigned id;
<     // The SSA value on which this edge represents a dependence.
<     // If the value is a memref, then the dependence is between graph nodes
<     // which contain accesses to the same memref 'value'. If the value is a
<     // non-memref value, then the dependence is between a graph node which
<     // defines an SSA value and another graph node which uses the SSA value
<     // (e.g. a constant or load operation defining a value which is used inside
<     // a loop nest).
<     Value value;
<   };
< 
<   // Map from node id to Node.
<   DenseMap<unsigned, Node> nodes;
<   // Map from node id to list of input edges.
<   DenseMap<unsigned, SmallVector<Edge, 2>> inEdges;
<   // Map from node id to list of output edges.
<   DenseMap<unsigned, SmallVector<Edge, 2>> outEdges;
<   // Map from memref to a count on the dependence edges associated with that
<   // memref.
<   DenseMap<Value, unsigned> memrefEdgeCount;
<   // The next unique identifier to use for newly created graph nodes.
<   unsigned nextNodeId = 0;
< 
<   MemRefDependenceGraph(Block &block) : block(block) {}
< 
<   // Initializes the dependence graph based on operations in `block'.
<   // Returns true on success, false otherwise.
<   bool init();
< 
<   // Returns the graph node for 'id'.
<   Node *getNode(unsigned id);
< 
<   // Returns the graph node for 'forOp'.
<   Node *getForOpNode(AffineForOp forOp);
< 
<   // Adds a node with 'op' to the graph and returns its unique identifier.
<   unsigned addNode(Operation *op);
< 
<   // Remove node 'id' (and its associated edges) from graph.
<   void removeNode(unsigned id);
< 
<   // Returns true if node 'id' writes to any memref which escapes (or is an
<   // argument to) the block. Returns false otherwise.
<   bool writesToLiveInOrEscapingMemrefs(unsigned id);
< 
<   // Returns true iff there is an edge from node 'srcId' to node 'dstId' which
<   // is for 'value' if non-null, or for any value otherwise. Returns false
<   // otherwise.
<   bool hasEdge(unsigned srcId, unsigned dstId, Value value = nullptr);
< 
<   // Adds an edge from node 'srcId' to node 'dstId' for 'value'.
<   void addEdge(unsigned srcId, unsigned dstId, Value value);
< 
<   // Removes an edge from node 'srcId' to node 'dstId' for 'value'.
<   void removeEdge(unsigned srcId, unsigned dstId, Value value);
< 
<   // Returns true if there is a path in the dependence graph from node 'srcId'
<   // to node 'dstId'. Returns false otherwise. `srcId`, `dstId`, and the
<   // operations that the edges connected are expected to be from the same block.
<   bool hasDependencePath(unsigned srcId, unsigned dstId);
< 
<   // Returns the input edge count for node 'id' and 'memref' from src nodes
<   // which access 'memref' with a store operation.
<   unsigned getIncomingMemRefAccesses(unsigned id, Value memref);
< 
<   // Returns the output edge count for node 'id' and 'memref' (if non-null),
<   // otherwise returns the total output edge count from node 'id'.
<   unsigned getOutEdgeCount(unsigned id, Value memref = nullptr);
< 
<   /// Return all nodes which define SSA values used in node 'id'.
<   void gatherDefiningNodes(unsigned id, DenseSet<unsigned> &definingNodes);
< 
<   // Computes and returns an insertion point operation, before which the
<   // the fused <srcId, dstId> loop nest can be inserted while preserving
<   // dependences. Returns nullptr if no such insertion point is found.
<   Operation *getFusedLoopNestInsertionPoint(unsigned srcId, unsigned dstId);
< 
<   // Updates edge mappings from node 'srcId' to node 'dstId' after fusing them,
<   // taking into account that:
<   //   *) if 'removeSrcId' is true, 'srcId' will be removed after fusion,
<   //   *) memrefs in 'privateMemRefs' has been replaced in node at 'dstId' by a
<   //      private memref.
<   void updateEdges(unsigned srcId, unsigned dstId,
<                    const DenseSet<Value> &privateMemRefs, bool removeSrcId);
< 
<   // Update edge mappings for nodes 'sibId' and 'dstId' to reflect fusion
<   // of sibling node 'sibId' into node 'dstId'.
<   void updateEdges(unsigned sibId, unsigned dstId);
< 
<   // Adds ops in 'loads' and 'stores' to node at 'id'.
<   void addToNode(unsigned id, const SmallVectorImpl<Operation *> &loads,
<                  const SmallVectorImpl<Operation *> &stores);
< 
<   void clearNodeLoadAndStores(unsigned id);
< 
<   // Calls 'callback' for each input edge incident to node 'id' which carries a
<   // memref dependence.
<   void forEachMemRefInputEdge(unsigned id,
<                               const std::function<void(Edge)> &callback);
< 
<   // Calls 'callback' for each output edge from node 'id' which carries a
<   // memref dependence.
<   void forEachMemRefOutputEdge(unsigned id,
<                                const std::function<void(Edge)> &callback);
< 
<   // Calls 'callback' for each edge in 'edges' which carries a memref
<   // dependence.
<   void forEachMemRefEdge(ArrayRef<Edge> edges,
<                          const std::function<void(Edge)> &callback);
< 
<   void print(raw_ostream &os) const;
< 
<   void dump() const { print(llvm::errs()); }
< 
<   /// The block for which this graph is created to perform fusion.
<   Block &block;
< };
---
> class Operation;
> class Value;
219,222d41
< /// Populates 'ivs' with IVs of the surrounding affine.for and affine.parallel
< /// ops ordered from the outermost one to the innermost.
< void getAffineIVs(Operation &op, SmallVectorImpl<Value> &ivs);
< 
539,541c358,360
< /// Returns the size of a memref with element type int or float in bytes if it's
< /// statically shaped, std::nullopt otherwise.
< std::optional<uint64_t> getIntOrFloatMemRefSizeInBytes(MemRefType memRefType);
---
> /// Returns the size of memref data in bytes if it's statically shaped,
> /// std::nullopt otherwise.
> std::optional<uint64_t> getMemRefSizeInBytes(MemRefType memRefType);
558,561d376
< /// Returns the memref's element type's size in bytes where the elemental type
< /// is an int or float or a vector of such types.
< std::optional<int64_t> getMemRefIntOrFloatEltSizeInBytes(MemRefType memRefType);
< 
580d394
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis/AffineAnalysis.h
24d23
< class Operation;
26d24
< namespace affine {
31a30
> class Operation;
171,173c170,171
<     unsigned loopDepth,
<     FlatAffineValueConstraints *dependenceConstraints = nullptr,
<     SmallVector<DependenceComponent, 2> *dependenceComponents = nullptr,
---
>     unsigned loopDepth, FlatAffineValueConstraints *dependenceConstraints,
>     SmallVector<DependenceComponent, 2> *dependenceComponents,
195d192
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis/LoopAnalysis.h
20a21
> 
21a23
> class AffineForOp;
24a27
> class NestedPattern;
28,31d30
< namespace affine {
< class AffineForOp;
< class NestedPattern;
< 
87d85
< } // namespace affine
--- include/mlir/Dialect/Affine/Analysis/AffineStructures.h
16d15
< #include "mlir/Analysis/FlatLinearValueConstraints.h"
24a24,28
> 
> class AffineCondition;
> class AffineForOp;
> class AffineIfOp;
> class AffineParallelOp;
25a30
> class AffineValueMap;
27d31
< class MemRefType;
29d32
< struct MutableAffineMap;
30a34,35
> class MemRefType;
> struct MutableAffineMap;
36,45c41,43
< namespace affine {
< class AffineCondition;
< class AffineForOp;
< class AffineIfOp;
< class AffineParallelOp;
< class AffineValueMap;
< 
< /// FlatAffineValueConstraints is an extension of FlatLinearValueConstraints
< /// with helper functions for Affine dialect ops.
< class FlatAffineValueConstraints : public FlatLinearValueConstraints {
---
> /// FlatAffineValueConstraints represents an extension of IntegerPolyhedron
> /// where each non-local variable can have an SSA Value attached to it.
> class FlatAffineValueConstraints : public presburger::IntegerPolyhedron {
47c45,84
<   using FlatLinearValueConstraints::FlatLinearValueConstraints;
---
>   /// Constructs a constraint system reserving memory for the specified number
>   /// of constraints and variables.
>   FlatAffineValueConstraints(unsigned numReservedInequalities,
>                              unsigned numReservedEqualities,
>                              unsigned numReservedCols, unsigned numDims,
>                              unsigned numSymbols, unsigned numLocals,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : IntegerPolyhedron(numReservedInequalities, numReservedEqualities,
>                           numReservedCols,
>                           presburger::PresburgerSpace::getSetSpace(
>                               numDims, numSymbols, numLocals)) {
>     assert(numReservedCols >= getNumVars() + 1);
>     assert(valArgs.empty() || valArgs.size() == getNumDimAndSymbolVars());
>     values.reserve(numReservedCols);
>     if (valArgs.empty())
>       values.resize(getNumDimAndSymbolVars(), std::nullopt);
>     else
>       values.append(valArgs.begin(), valArgs.end());
>   }
> 
>   /// Constructs a constraint system with the specified number of
>   /// dimensions and symbols.
>   FlatAffineValueConstraints(unsigned numDims = 0, unsigned numSymbols = 0,
>                              unsigned numLocals = 0,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : FlatAffineValueConstraints(/*numReservedInequalities=*/0,
>                                    /*numReservedEqualities=*/0,
>                                    /*numReservedCols=*/numDims + numSymbols +
>                                        numLocals + 1,
>                                    numDims, numSymbols, numLocals, valArgs) {}
> 
>   FlatAffineValueConstraints(const IntegerPolyhedron &fac,
>                              ArrayRef<std::optional<Value>> valArgs = {})
>       : IntegerPolyhedron(fac) {
>     assert(valArgs.empty() || valArgs.size() == getNumDimAndSymbolVars());
>     if (valArgs.empty())
>       values.resize(getNumDimAndSymbolVars(), std::nullopt);
>     else
>       values.append(valArgs.begin(), valArgs.end());
>   }
49c86,108
<   /// Return the kind of this object.
---
>   /// Create a flat affine constraint system from an AffineValueMap or a list of
>   /// these. The constructed system will only include equalities.
>   explicit FlatAffineValueConstraints(const AffineValueMap &avm);
>   explicit FlatAffineValueConstraints(ArrayRef<const AffineValueMap *> avmRef);
> 
>   /// Creates an affine constraint system from an IntegerSet.
>   explicit FlatAffineValueConstraints(IntegerSet set, ValueRange operands = {});
> 
>   // Construct a hyperrectangular constraint set from ValueRanges that represent
>   // induction variables, lower and upper bounds. `ivs`, `lbs` and `ubs` are
>   // expected to match one to one. The order of variables and constraints is:
>   //
>   // ivs | lbs | ubs | eq/ineq
>   // ----+-----+-----+---------
>   //   1   -1     0      >= 0
>   // ----+-----+-----+---------
>   //  -1    0     1      >= 0
>   //
>   // All dimensions as set as VarKind::SetDim.
>   static FlatAffineValueConstraints
>   getHyperrectangular(ValueRange ivs, ValueRange lbs, ValueRange ubs);
> 
>   /// Return the kind of this FlatAffineConstraints.
53,54c112
<     return cst->getKind() >= Kind::FlatAffineValueConstraints &&
<            cst->getKind() <= Kind::FlatAffineRelation;
---
>     return cst->getKind() == Kind::FlatAffineValueConstraints;
56a115,130
>   /// Clears any existing data and reserves memory for the specified
>   /// constraints.
>   void reset(unsigned numReservedInequalities, unsigned numReservedEqualities,
>              unsigned numReservedCols, unsigned numDims, unsigned numSymbols,
>              unsigned numLocals = 0);
>   void reset(unsigned numDims = 0, unsigned numSymbols = 0,
>              unsigned numLocals = 0);
>   void reset(unsigned numReservedInequalities, unsigned numReservedEqualities,
>              unsigned numReservedCols, unsigned numDims, unsigned numSymbols,
>              unsigned numLocals, ArrayRef<Value> valArgs);
>   void reset(unsigned numDims, unsigned numSymbols, unsigned numLocals,
>              ArrayRef<Value> valArgs);
> 
>   /// Clones this object.
>   std::unique_ptr<FlatAffineValueConstraints> clone() const;
> 
99a174,199
>   /// being drawn from the specified bound map. In case of an EQ bound, the
>   /// bound map is expected to have exactly one result. In case of a LB/UB, the
>   /// bound map may have more than one result, for each of which an inequality
>   /// is added.
>   ///
>   /// The bound can be added as open or closed by specifying isClosedBound. In
>   /// case of a LB/UB, isClosedBound = false means the bound is added internally
>   /// as a closed bound by +1/-1 respectively. In case of an EQ bound, it can
>   /// only be added as a closed bound.
>   ///
>   /// Note: The dimensions/symbols of this FlatAffineConstraints must match the
>   /// dimensions/symbols of the affine map.
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap,
>                          bool isClosedBound);
> 
>   /// Adds a bound for the variable at the specified position with constraints
>   /// being drawn from the specified bound map. In case of an EQ bound, the
>   /// bound map is expected to have exactly one result. In case of a LB/UB, the
>   /// bound map may have more than one result, for each of which an inequality
>   /// is added.
>   /// Note: The dimensions/symbols of this FlatAffineConstraints must match the
>   /// dimensions/symbols of the affine map. By default the lower bound is closed
>   /// and the upper bound is open.
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap);
> 
>   /// Adds a bound for the variable at the specified position with constraints
104,106c204,205
<   LogicalResult addBound(presburger::BoundType type, unsigned pos,
<                          AffineMap boundMap, ValueRange operands);
<   using FlatLinearValueConstraints::addBound;
---
>   LogicalResult addBound(BoundType type, unsigned pos, AffineMap boundMap,
>                          ValueRange operands);
108,114c207,259
<   /// Add the specified values as a dim or symbol var depending on its nature,
<   /// if it already doesn't exist in the system. `val` has to be either a
<   /// terminal symbol or a loop IV, i.e., it cannot be the result affine.apply
<   /// of any symbols or loop IVs. The variable is added to the end of the
<   /// existing dims or symbols. Additional information on the variable is
<   /// extracted from the IR and added to the constraint system.
<   void addInductionVarOrTerminalSymbol(Value val);
---
>   /// Adds a constant bound for the variable associated with the given Value.
>   void addBound(BoundType type, Value val, int64_t value);
> 
>   /// The `addBound` overload above hides the inherited overloads by default, so
>   /// we explicitly introduce them here.
>   using IntegerPolyhedron::addBound;
> 
>   /// Returns the constraint system as an integer set. Returns a null integer
>   /// set if the system has no constraints, or if an integer set couldn't be
>   /// constructed as a result of a local variable's explicit representation not
>   /// being known and such a local variable appearing in any of the constraints.
>   IntegerSet getAsIntegerSet(MLIRContext *context) const;
> 
>   /// Computes the lower and upper bounds of the first `num` dimensional
>   /// variables (starting at `offset`) as an affine map of the remaining
>   /// variables (dimensional and symbolic). This method is able to detect
>   /// variables as floordiv's and mod's of affine expressions of other
>   /// variables with respect to (positive) constants. Sets bound map to a
>   /// null AffineMap if such a bound can't be found (or yet unimplemented).
>   ///
>   /// By default the returned lower bounds are closed and upper bounds are open.
>   /// This can be changed by getClosedUB.
>   void getSliceBounds(unsigned offset, unsigned num, MLIRContext *context,
>                       SmallVectorImpl<AffineMap> *lbMaps,
>                       SmallVectorImpl<AffineMap> *ubMaps,
>                       bool getClosedUB = false);
> 
>   /// Composes an affine map whose dimensions and symbols match one to one with
>   /// the dimensions and symbols of this FlatAffineConstraints. The results of
>   /// the map `other` are added as the leading dimensions of this constraint
>   /// system. Returns failure if `other` is a semi-affine map.
>   LogicalResult composeMatchingMap(AffineMap other);
> 
>   /// Gets the lower and upper bound of the `offset` + `pos`th variable
>   /// treating [0, offset) U [offset + num, symStartPos) as dimensions and
>   /// [symStartPos, getNumDimAndSymbolVars) as symbols, and `pos` lies in
>   /// [0, num). The multi-dimensional maps in the returned pair represent the
>   /// max and min of potentially multiple affine expressions. The upper bound is
>   /// exclusive. `localExprs` holds pre-computed AffineExpr's for all local
>   /// variables in the system.
>   std::pair<AffineMap, AffineMap>
>   getLowerAndUpperBound(unsigned pos, unsigned offset, unsigned num,
>                         unsigned symStartPos, ArrayRef<AffineExpr> localExprs,
>                         MLIRContext *context) const;
> 
>   /// Returns the bound for the variable at `pos` from the inequality at
>   /// `ineqPos` as a 1-d affine value map (affine map + operands). The returned
>   /// affine value map can either be a lower bound or an upper bound depending
>   /// on the sign of atIneq(ineqPos, pos). Asserts if the row at `ineqPos` does
>   /// not involve the `pos`th variable.
>   void getIneqAsAffineValueMap(unsigned pos, unsigned ineqPos,
>                                AffineValueMap &vmap,
>                                MLIRContext *context) const;
127,128c272,325
<   /// Changes all symbol variables which are loop IVs to dim variables.
<   void convertLoopIVSymbolsToDims();
---
>   /// Looks up the position of the variable with the specified Value. Returns
>   /// true if found (false otherwise). `pos` is set to the (column) position of
>   /// the variable.
>   bool findVar(Value val, unsigned *pos) const;
> 
>   /// Returns true if an variable with the specified Value exists, false
>   /// otherwise.
>   bool containsVar(Value val) const;
> 
>   /// Swap the posA^th variable with the posB^th variable.
>   void swapVar(unsigned posA, unsigned posB) override;
> 
>   /// Insert variables of the specified kind at position `pos`. Positions are
>   /// relative to the kind of variable. The coefficient columns corresponding
>   /// to the added variables are initialized to zero. `vals` are the Values
>   /// corresponding to the variables. Values should not be used with
>   /// VarKind::Local since values can only be attached to non-local variables.
>   /// Return the absolute column position (i.e., not relative to the kind of
>   /// variable) of the first added variable.
>   ///
>   /// Note: Empty Values are allowed in `vals`.
>   unsigned insertDimVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::SetDim, pos, num);
>   }
>   unsigned insertSymbolVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::Symbol, pos, num);
>   }
>   unsigned insertLocalVar(unsigned pos, unsigned num = 1) {
>     return insertVar(VarKind::Local, pos, num);
>   }
>   unsigned insertDimVar(unsigned pos, ValueRange vals);
>   unsigned insertSymbolVar(unsigned pos, ValueRange vals);
>   unsigned insertVar(presburger::VarKind kind, unsigned pos,
>                      unsigned num = 1) override;
>   unsigned insertVar(presburger::VarKind kind, unsigned pos, ValueRange vals);
> 
>   /// Append variables of the specified kind after the last variable of that
>   /// kind. The coefficient columns corresponding to the added variables are
>   /// initialized to zero. `vals` are the Values corresponding to the
>   /// variables. Return the absolute column position (i.e., not relative to the
>   /// kind of variable) of the first appended variable.
>   ///
>   /// Note: Empty Values are allowed in `vals`.
>   unsigned appendDimVar(ValueRange vals);
>   unsigned appendSymbolVar(ValueRange vals);
>   unsigned appendDimVar(unsigned num = 1) {
>     return appendVar(VarKind::SetDim, num);
>   }
>   unsigned appendSymbolVar(unsigned num = 1) {
>     return appendVar(VarKind::Symbol, num);
>   }
>   unsigned appendLocalVar(unsigned num = 1) {
>     return appendVar(VarKind::Local, num);
>   }
130,137c327,344
<   /// Returns the bound for the variable at `pos` from the inequality at
<   /// `ineqPos` as a 1-d affine value map (affine map + operands). The returned
<   /// affine value map can either be a lower bound or an upper bound depending
<   /// on the sign of atIneq(ineqPos, pos). Asserts if the row at `ineqPos` does
<   /// not involve the `pos`th variable.
<   void getIneqAsAffineValueMap(unsigned pos, unsigned ineqPos,
<                                AffineValueMap &vmap,
<                                MLIRContext *context) const;
---
>   /// Removes variables in the column range [varStart, varLimit), and copies any
>   /// remaining valid data into place, updates member variables, and resizes
>   /// arrays as needed.
>   void removeVarRange(presburger::VarKind kind, unsigned varStart,
>                       unsigned varLimit) override;
>   using IntegerPolyhedron::removeVarRange;
> 
>   /// Add the specified values as a dim or symbol var depending on its nature,
>   /// if it already doesn't exist in the system. `val` has to be either a
>   /// terminal symbol or a loop IV, i.e., it cannot be the result affine.apply
>   /// of any symbols or loop IVs. The variable is added to the end of the
>   /// existing dims or symbols. Additional information on the variable is
>   /// extracted from the IR and added to the constraint system.
>   void addInductionVarOrTerminalSymbol(Value val);
> 
>   /// Align `map` with this constraint system based on `operands`. Each operand
>   /// must already have a corresponding dim/symbol in this constraint system.
>   AffineMap computeAlignedMap(AffineMap map, ValueRange operands) const;
146,148c353,355
<   /// the FlatAffineValueConstraints with which to associate. Every operand of
<   /// vMap should have a matching dim/symbol column in this constraint system
<   /// (with the same associated Value).
---
>   /// the FlatAffineConstraints with which to associate. Every operand of vMap
>   /// should have a matching dim/symbol column in this constraint system (with
>   /// the same associated Value).
149a357,514
> 
>   /// Projects out the variable that is associate with Value.
>   void projectOut(Value val);
>   using IntegerPolyhedron::projectOut;
> 
>   /// Changes all symbol variables which are loop IVs to dim variables.
>   void convertLoopIVSymbolsToDims();
> 
>   /// Updates the constraints to be the smallest bounding (enclosing) box that
>   /// contains the points of `this` set and that of `other`, with the symbols
>   /// being treated specially. For each of the dimensions, the min of the lower
>   /// bounds (symbolic) and the max of the upper bounds (symbolic) is computed
>   /// to determine such a bounding box. `other` is expected to have the same
>   /// dimensional variables as this constraint system (in the same order).
>   ///
>   /// E.g.:
>   /// 1) this   = {0 <= d0 <= 127},
>   ///    other  = {16 <= d0 <= 192},
>   ///    output = {0 <= d0 <= 192}
>   /// 2) this   = {s0 + 5 <= d0 <= s0 + 20},
>   ///    other  = {s0 + 1 <= d0 <= s0 + 9},
>   ///    output = {s0 + 1 <= d0 <= s0 + 20}
>   /// 3) this   = {0 <= d0 <= 5, 1 <= d1 <= 9}
>   ///    other  = {2 <= d0 <= 6, 5 <= d1 <= 15},
>   ///    output = {0 <= d0 <= 6, 1 <= d1 <= 15}
>   LogicalResult unionBoundingBox(const FlatAffineValueConstraints &other);
>   using IntegerPolyhedron::unionBoundingBox;
> 
>   /// Merge and align the variables of `this` and `other` starting at
>   /// `offset`, so that both constraint systems get the union of the contained
>   /// variables that is dimension-wise and symbol-wise unique; both
>   /// constraint systems are updated so that they have the union of all
>   /// variables, with `this`'s original variables appearing first followed
>   /// by any of `other`'s variables that didn't appear in `this`. Local
>   /// variables in `other` that have the same division representation as local
>   /// variables in `this` are merged into one.
>   //  E.g.: Input: `this`  has (%i, %j) [%M, %N]
>   //               `other` has (%k, %j) [%P, %N, %M]
>   //        Output: both `this`, `other` have (%i, %j, %k) [%M, %N, %P]
>   //
>   void mergeAndAlignVarsWithOther(unsigned offset,
>                                   FlatAffineValueConstraints *other);
> 
>   /// Returns true if this constraint system and `other` are in the same
>   /// space, i.e., if they are associated with the same set of variables,
>   /// appearing in the same order. Returns false otherwise.
>   bool areVarsAlignedWithOther(const FlatAffineValueConstraints &other);
> 
>   /// Replaces the contents of this FlatAffineValueConstraints with `other`.
>   void clearAndCopyFrom(const IntegerRelation &other) override;
> 
>   /// Returns the Value associated with the pos^th variable. Asserts if
>   /// no Value variable was associated.
>   inline Value getValue(unsigned pos) const {
>     assert(pos < getNumDimAndSymbolVars() && "Invalid position");
>     assert(hasValue(pos) && "variable's Value not set");
>     return *values[pos];
>   }
> 
>   /// Returns true if the pos^th variable has an associated Value.
>   inline bool hasValue(unsigned pos) const {
>     assert(pos < getNumDimAndSymbolVars() && "Invalid position");
>     return values[pos].has_value();
>   }
> 
>   /// Returns true if at least one variable has an associated Value.
>   bool hasValues() const;
> 
>   /// Returns the Values associated with variables in range [start, end).
>   /// Asserts if no Value was associated with one of these variables.
>   inline void getValues(unsigned start, unsigned end,
>                         SmallVectorImpl<Value> *values) const {
>     assert(end <= getNumDimAndSymbolVars() && "invalid end position");
>     assert(start <= end && "invalid start position");
>     values->clear();
>     values->reserve(end - start);
>     for (unsigned i = start; i < end; i++)
>       values->push_back(getValue(i));
>   }
>   inline void getAllValues(SmallVectorImpl<Value> *values) const {
>     getValues(0, getNumDimAndSymbolVars(), values);
>   }
> 
>   inline ArrayRef<std::optional<Value>> getMaybeValues() const {
>     return {values.data(), values.size()};
>   }
> 
>   inline ArrayRef<std::optional<Value>>
>   getMaybeValues(presburger::VarKind kind) const {
>     assert(kind != VarKind::Local &&
>            "Local variables do not have any value attached to them.");
>     return {values.data() + getVarKindOffset(kind), getNumVarKind(kind)};
>   }
> 
>   /// Sets the Value associated with the pos^th variable.
>   inline void setValue(unsigned pos, Value val) {
>     assert(pos < getNumDimAndSymbolVars() && "invalid var position");
>     values[pos] = val;
>   }
> 
>   /// Sets the Values associated with the variables in the range [start, end).
>   /// The range must contain only dim and symbol variables.
>   void setValues(unsigned start, unsigned end, ArrayRef<Value> values) {
>     assert(end <= getNumVars() && "invalid end position");
>     assert(start <= end && "invalid start position");
>     assert(values.size() == end - start &&
>            "value should be provided for each variable in the range.");
>     for (unsigned i = start; i < end; ++i)
>       setValue(i, values[i - start]);
>   }
> 
>   /// Merge and align symbols of `this` and `other` such that both get union of
>   /// of symbols that are unique. Symbols in `this` and `other` should be
>   /// unique. Symbols with Value as `None` are considered to be inequal to all
>   /// other symbols.
>   void mergeSymbolVars(FlatAffineValueConstraints &other);
> 
> protected:
>   using VarKind = presburger::VarKind;
> 
>   /// Returns false if the fields corresponding to various variable counts, or
>   /// equality/inequality buffer sizes aren't consistent; true otherwise. This
>   /// is meant to be used within an assert internally.
>   bool hasConsistentState() const override;
> 
>   /// Given an affine map that is aligned with this constraint system:
>   /// * Flatten the map.
>   /// * Add newly introduced local columns at the beginning of this constraint
>   ///   system (local column pos 0).
>   /// * Add equalities that define the new local columns to this constraint
>   ///   system.
>   /// * Return the flattened expressions via `flattenedExprs`.
>   ///
>   /// Note: This is a shared helper function of `addLowerOrUpperBound` and
>   ///       `composeMatchingMap`.
>   LogicalResult flattenAlignedMapAndMergeLocals(
>       AffineMap map, std::vector<SmallVector<int64_t, 8>> *flattenedExprs);
> 
>   /// Eliminates the variable at the specified position using Fourier-Motzkin
>   /// variable elimination, but uses Gaussian elimination if there is an
>   /// equality involving that variable. If the result of the elimination is
>   /// integer exact, `*isResultIntegerExact` is set to true. If `darkShadow` is
>   /// set to true, a potential under approximation (subset) of the rational
>   /// shadow / exact integer shadow is computed.
>   // See implementation comments for more details.
>   void fourierMotzkinEliminate(unsigned pos, bool darkShadow = false,
>                                bool *isResultIntegerExact = nullptr) override;
> 
>   /// Prints the number of constraints, dimensions, symbols and locals in the
>   /// FlatAffineConstraints. Also, prints for each variable whether there is
>   /// an SSA Value attached to it.
>   void printSpace(raw_ostream &os) const override;
> 
>   /// Values corresponding to the (column) non-local variables of this
>   /// constraint system appearing in the order the variables correspond to
>   /// columns. Variables that aren't associated with any Value are set to
>   /// None.
>   SmallVector<std::optional<Value>, 8> values;
185,191d549
<   /// Return the kind of this object.
<   Kind getKind() const override { return Kind::FlatAffineRelation; }
< 
<   static bool classof(const IntegerRelation *cst) {
<     return cst->getKind() == Kind::FlatAffineRelation;
<   }
< 
237a596,655
> /// Flattens 'expr' into 'flattenedExpr', which contains the coefficients of the
> /// dimensions, symbols, and additional variables that represent floor divisions
> /// of dimensions, symbols, and in turn other floor divisions.  Returns failure
> /// if 'expr' could not be flattened (i.e., semi-affine is not yet handled).
> /// 'cst' contains constraints that connect newly introduced local variables
> /// to existing dimensional and symbolic variables. See documentation for
> /// AffineExprFlattener on how mod's and div's are flattened.
> LogicalResult getFlattenedAffineExpr(AffineExpr expr, unsigned numDims,
>                                      unsigned numSymbols,
>                                      SmallVectorImpl<int64_t> *flattenedExpr,
>                                      FlatAffineValueConstraints *cst = nullptr);
> 
> /// Flattens the result expressions of the map to their corresponding flattened
> /// forms and set in 'flattenedExprs'. Returns failure if any expression in the
> /// map could not be flattened (i.e., semi-affine is not yet handled). 'cst'
> /// contains constraints that connect newly introduced local variables to
> /// existing dimensional and / symbolic variables. See documentation for
> /// AffineExprFlattener on how mod's and div's are flattened. For all affine
> /// expressions that share the same operands (like those of an affine map), this
> /// method should be used instead of repeatedly calling getFlattenedAffineExpr
> /// since local variables added to deal with div's and mod's will be reused
> /// across expressions.
> LogicalResult
> getFlattenedAffineExprs(AffineMap map,
>                         std::vector<SmallVector<int64_t, 8>> *flattenedExprs,
>                         FlatAffineValueConstraints *cst = nullptr);
> LogicalResult
> getFlattenedAffineExprs(IntegerSet set,
>                         std::vector<SmallVector<int64_t, 8>> *flattenedExprs,
>                         FlatAffineValueConstraints *cst = nullptr);
> 
> LogicalResult
> getMultiAffineFunctionFromMap(AffineMap map,
>                               presburger::MultiAffineFunction &multiAff);
> 
> /// Re-indexes the dimensions and symbols of an affine map with given `operands`
> /// values to align with `dims` and `syms` values.
> ///
> /// Each dimension/symbol of the map, bound to an operand `o`, is replaced with
> /// dimension `i`, where `i` is the position of `o` within `dims`. If `o` is not
> /// in `dims`, replace it with symbol `i`, where `i` is the position of `o`
> /// within `syms`. If `o` is not in `syms` either, replace it with a new symbol.
> ///
> /// Note: If a value appears multiple times as a dimension/symbol (or both), all
> /// corresponding dim/sym expressions are replaced with the first dimension
> /// bound to that value (or first symbol if no such dimension exists).
> ///
> /// The resulting affine map has `dims.size()` many dimensions and at least
> /// `syms.size()` many symbols.
> ///
> /// The SSA values of the symbols of the resulting map are optionally returned
> /// via `newSyms`. This is a concatenation of `syms` with the SSA values of the
> /// newly added symbols.
> ///
> /// Note: As part of this re-indexing, dimensions may turn into symbols, or vice
> /// versa.
> AffineMap alignAffineMapWithValues(AffineMap map, ValueRange operands,
>                                    ValueRange dims, ValueRange syms,
>                                    SmallVector<Value> *newSyms = nullptr);
> 
258,259c676
< } // namespace affine
< } // namespace mlir
---
> } // namespace mlir.
--- include/mlir/Dialect/Affine/LoopFusionUtils.h
24,26d23
< class Operation;
< 
< namespace affine {
28a26
> class Operation;
170,171d167
< 
< } // namespace affine
--- include/mlir/Dialect/Affine/LoopUtils.h
24a25
> class AffineForOp;
26a28
> struct MemRefRegion;
40,43d41
< namespace affine {
< class AffineForOp;
< struct MemRefRegion;
< 
109,110c107
< /// and intra-tile loops. A band is a contiguous set of loops. This utility
< /// doesn't check for the validity of tiling itself, but just performs it.
---
> /// and intra-tile loops. A band is a contiguous set of loops.
190,192c187
< /// encountered. For memrefs for whose element types a size in bytes can't be
< /// computed (`index` type), their capacity is not accounted for and the
< /// `fastMemCapacityBytes` copy option would be non-functional in such cases.
---
> /// encountered.
350d344
< } // namespace affine
--- include/mlir/Dialect/Affine/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.h include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.h
17a18
> class AffineForOp;
22c23
< class AffineForOp;
---
> class ForOp;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.td include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.td
13a14
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
--- include/mlir/Dialect/Affine/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.td
13a14
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
--- include/mlir/Dialect/Affine/TransformOps/AffineTransformOps.h
17a18
> class AffineForOp;
22c23
< class AffineForOp;
---
> class ForOp;
--- include/mlir/Dialect/MemRef
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/IR and include/mlir/Dialect/MemRef/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/TransformOps and include/mlir/Dialect/MemRef/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Transforms and include/mlir/Dialect/MemRef/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Utils and include/mlir/Dialect/MemRef/Utils
--- include/mlir/Dialect/MemRef/Utils
--- include/mlir/Dialect/MemRef/Utils/MemRefUtils.h
--- include/mlir/Dialect/MemRef/CMakeLists.txt
--- include/mlir/Dialect/MemRef/Transforms
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Transforms: BufferizableOpInterfaceImpl.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Transforms/Passes.h include/mlir/Dialect/MemRef/Transforms/Passes.h
22a23,26
> namespace arith {
> class WideIntEmulationConverter;
> } // namespace arith
> 
33a38,104
> class AllocOp;
> //===----------------------------------------------------------------------===//
> // Patterns
> //===----------------------------------------------------------------------===//
> 
> /// Collects a set of patterns to rewrite ops within the memref dialect.
> void populateExpandOpsPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for folding memref aliasing ops into consumer load/store
> /// ops into `patterns`.
> void populateFoldMemRefAliasOpPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns that resolve `memref.dim` operations with values that are
> /// defined by operations that implement the
> /// `ReifyRankedShapeTypeShapeOpInterface`, in terms of shapes of its input
> /// operands.
> void populateResolveRankedShapeTypeResultDimsPatterns(
>     RewritePatternSet &patterns);
> 
> /// Appends patterns that resolve `memref.dim` operations with values that are
> /// defined by operations that implement the `InferShapedTypeOpInterface`, in
> /// terms of shapes of its input operands.
> void populateResolveShapedTypeResultDimsPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for expanding memref operations that modify the metadata
> /// (sizes, offset, strides) of a memref into easier to analyze constructs.
> void populateExpandStridedMetadataPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for emulating wide integer memref operations with ops over
> /// narrower integer types.
> void populateMemRefWideIntEmulationPatterns(
>     arith::WideIntEmulationConverter &typeConverter,
>     RewritePatternSet &patterns);
> 
> /// Appends type converions for emulating wide integer memref operations with
> /// ops over narrowe integer types.
> void populateMemRefWideIntEmulationConversions(
>     arith::WideIntEmulationConverter &typeConverter);
> 
> /// Transformation to do multi-buffering/array expansion to remove dependencies
> /// on the temporary allocation between consecutive loop iterations.
> /// It returns the new allocation if the original allocation was multi-buffered
> /// and returns failure() otherwise.
> /// Example:
> /// ```
> /// %0 = memref.alloc() : memref<4x128xf32>
> /// scf.for %iv = %c1 to %c1024 step %c3 {
> ///   memref.copy %1, %0 : memref<4x128xf32> to memref<4x128xf32>
> ///   "some_use"(%0) : (memref<4x128xf32>) -> ()
> /// }
> /// ```
> /// into:
> /// ```
> /// %0 = memref.alloc() : memref<5x4x128xf32>
> /// scf.for %iv = %c1 to %c1024 step %c3 {
> ///   %s = arith.subi %iv, %c1 : index
> ///   %d = arith.divsi %s, %c3 : index
> ///   %i = arith.remsi %d, %c5 : index
> ///   %sv = memref.subview %0[%i, 0, 0] [1, 4, 128] [1, 1, 1] :
> ///     memref<5x4x128xf32> to memref<4x128xf32, strided<[128, 1], offset: ?>>
> ///   memref.copy %1, %sv : memref<4x128xf32> to memref<4x128xf32, strided<...>>
> ///   "some_use"(%sv) : (memref<4x128xf32, strided<...>) -> ()
> /// }
> /// ```
> FailureOr<memref::AllocOp> multiBuffer(memref::AllocOp allocOp,
>                                        unsigned multiplier);
> 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Transforms/Passes.td include/mlir/Dialect/MemRef/Transforms/Passes.td
27c27
<       "affine::AffineDialect", "memref::MemRefDialect", "vector::VectorDialect"
---
>       "AffineDialect", "memref::MemRefDialect", "vector::VectorDialect"
51,52c51,52
<     [layout map](https://mlir.llvm.org/docs/Dialects/Builtin/#affine-map-layout)
<     into memref types with an identity layout map, e.g. (i, j) -> (i, j). This
---
>     [layout map](https://mlir.llvm.org/docs/LangRef/#layout-map) into
>     memref types with an identity layout map, e.g. (i, j) -> (i, j). This
57c57,58
<     For instance, an [AffineLoadOp](https://mlir.llvm.org/docs/Dialects/Affine/#affineload-mliraffineloadop)
---
>     For instance, an [AffineLoadOp]
>     (https://mlir.llvm.org/docs/Dialects/Affine/#affineload-affineloadop)
59,61c60,62
<     contained in the op. Operations marked with the
<     [MemRefsNormalizable](https://mlir.llvm.org/docs/Traits/#memrefsnormalizable)
<     trait are expected to be normalizable. Supported operations include affine
---
>     contained in the op. Operations marked with the [MemRefsNormalizable]
>     (https://mlir.llvm.org/docs/Traits/#memrefsnormalizable) trait are
>     expected to be normalizable. Supported operations include affine
159c160
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
187c188
<     "affine::AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
---
>     "AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
202c203
<       "affine::AffineDialect", "memref::MemRefDialect"
---
>       "AffineDialect", "memref::MemRefDialect"
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/Transforms: Transforms.h
--- include/mlir/Dialect/MemRef/Transforms/Passes.h
22a23,26
> namespace arith {
> class WideIntEmulationConverter;
> } // namespace arith
> 
33a38,104
> class AllocOp;
> //===----------------------------------------------------------------------===//
> // Patterns
> //===----------------------------------------------------------------------===//
> 
> /// Collects a set of patterns to rewrite ops within the memref dialect.
> void populateExpandOpsPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for folding memref aliasing ops into consumer load/store
> /// ops into `patterns`.
> void populateFoldMemRefAliasOpPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns that resolve `memref.dim` operations with values that are
> /// defined by operations that implement the
> /// `ReifyRankedShapeTypeShapeOpInterface`, in terms of shapes of its input
> /// operands.
> void populateResolveRankedShapeTypeResultDimsPatterns(
>     RewritePatternSet &patterns);
> 
> /// Appends patterns that resolve `memref.dim` operations with values that are
> /// defined by operations that implement the `InferShapedTypeOpInterface`, in
> /// terms of shapes of its input operands.
> void populateResolveShapedTypeResultDimsPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for expanding memref operations that modify the metadata
> /// (sizes, offset, strides) of a memref into easier to analyze constructs.
> void populateExpandStridedMetadataPatterns(RewritePatternSet &patterns);
> 
> /// Appends patterns for emulating wide integer memref operations with ops over
> /// narrower integer types.
> void populateMemRefWideIntEmulationPatterns(
>     arith::WideIntEmulationConverter &typeConverter,
>     RewritePatternSet &patterns);
> 
> /// Appends type converions for emulating wide integer memref operations with
> /// ops over narrowe integer types.
> void populateMemRefWideIntEmulationConversions(
>     arith::WideIntEmulationConverter &typeConverter);
> 
> /// Transformation to do multi-buffering/array expansion to remove dependencies
> /// on the temporary allocation between consecutive loop iterations.
> /// It returns the new allocation if the original allocation was multi-buffered
> /// and returns failure() otherwise.
> /// Example:
> /// ```
> /// %0 = memref.alloc() : memref<4x128xf32>
> /// scf.for %iv = %c1 to %c1024 step %c3 {
> ///   memref.copy %1, %0 : memref<4x128xf32> to memref<4x128xf32>
> ///   "some_use"(%0) : (memref<4x128xf32>) -> ()
> /// }
> /// ```
> /// into:
> /// ```
> /// %0 = memref.alloc() : memref<5x4x128xf32>
> /// scf.for %iv = %c1 to %c1024 step %c3 {
> ///   %s = arith.subi %iv, %c1 : index
> ///   %d = arith.divsi %s, %c3 : index
> ///   %i = arith.remsi %d, %c5 : index
> ///   %sv = memref.subview %0[%i, 0, 0] [1, 4, 128] [1, 1, 1] :
> ///     memref<5x4x128xf32> to memref<4x128xf32, strided<[128, 1], offset: ?>>
> ///   memref.copy %1, %sv : memref<4x128xf32> to memref<4x128xf32, strided<...>>
> ///   "some_use"(%sv) : (memref<4x128xf32, strided<...>) -> ()
> /// }
> /// ```
> FailureOr<memref::AllocOp> multiBuffer(memref::AllocOp allocOp,
>                                        unsigned multiplier);
> 
--- include/mlir/Dialect/MemRef/Transforms/Passes.td
27c27
<       "affine::AffineDialect", "memref::MemRefDialect", "vector::VectorDialect"
---
>       "AffineDialect", "memref::MemRefDialect", "vector::VectorDialect"
51,52c51,52
<     [layout map](https://mlir.llvm.org/docs/Dialects/Builtin/#affine-map-layout)
<     into memref types with an identity layout map, e.g. (i, j) -> (i, j). This
---
>     [layout map](https://mlir.llvm.org/docs/LangRef/#layout-map) into
>     memref types with an identity layout map, e.g. (i, j) -> (i, j). This
57c57,58
<     For instance, an [AffineLoadOp](https://mlir.llvm.org/docs/Dialects/Affine/#affineload-mliraffineloadop)
---
>     For instance, an [AffineLoadOp]
>     (https://mlir.llvm.org/docs/Dialects/Affine/#affineload-affineloadop)
59,61c60,62
<     contained in the op. Operations marked with the
<     [MemRefsNormalizable](https://mlir.llvm.org/docs/Traits/#memrefsnormalizable)
<     trait are expected to be normalizable. Supported operations include affine
---
>     contained in the op. Operations marked with the [MemRefsNormalizable]
>     (https://mlir.llvm.org/docs/Traits/#memrefsnormalizable) trait are
>     expected to be normalizable. Supported operations include affine
159c160
<   let dependentDialects = ["affine::AffineDialect"];
---
>   let dependentDialects = ["AffineDialect"];
187c188
<     "affine::AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
---
>     "AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
202c203
<       "affine::AffineDialect", "memref::MemRefDialect"
---
>       "AffineDialect", "memref::MemRefDialect"
--- include/mlir/Dialect/MemRef/Transforms/RuntimeOpVerification.h
--- include/mlir/Dialect/MemRef/Transforms/CMakeLists.txt
--- include/mlir/Dialect/MemRef/Transforms/ComposeSubView.h
--- include/mlir/Dialect/MemRef/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/IR/MemRefBase.td include/mlir/Dialect/MemRef/IR/MemRefBase.td
23a24
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/IR/MemRefOps.td include/mlir/Dialect/MemRef/IR/MemRefOps.td
103,116d102
< 
<     SmallVector<OpFoldResult> getMixedSizes() {
<       SmallVector<OpFoldResult> result;
<       unsigned ctr = 0;
<       OpBuilder b(getContext());
<       for (int64_t i = 0, e = getType().getRank(); i < e; ++i) {
<         if (getType().isDynamicDim(i)) {
<           result.push_back(getDynamicSizes()[ctr++]);
<         } else {
<           result.push_back(b.getIndexAttr(getType().getShape()[i]));
<         }
<       }
<       return result;
<     }
250c236
<     %3 = memref.realloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>
---
>     %3 = memref.ralloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>
263,267c249
<   // Note that we conceptually mark the operands as freeing the incoming
<   // memref and allocating the outcoming memref, even though this may not
<   // physically happen on each execution.
< 
<   let arguments = (ins Arg<MemRefRankOf<[AnyType], [1]>, "", [MemFree]>:$source,
---
>   let arguments = (ins MemRefRankOf<[AnyType], [1]>:$source,
272c254
<   let results = (outs Res<MemRefRankOf<[AnyType], [1]>, "", [MemAlloc<DefaultResource>]>);
---
>   let results = (outs MemRefRankOf<[AnyType], [1]>);
593c575
<   let arguments = (ins AnyNon0RankedOrUnrankedMemRef:$source,
---
>   let arguments = (ins AnyRankedOrUnrankedMemRef:$source,
620a603
>   let hasVerifier = 1;
778,788d760
< 
<     void getEffects(
<         SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
<         effects) {
<       effects.emplace_back(MemoryEffects::Read::get(), getSrcMemRef(),
<                            SideEffects::DefaultResource::get());
<       effects.emplace_back(MemoryEffects::Write::get(), getDstMemRef(),
<                            SideEffects::DefaultResource::get());
<       effects.emplace_back(MemoryEffects::Read::get(), getTagMemRef(),
<                            SideEffects::DefaultResource::get());
<     }
831,836d802
<     void getEffects(
<         SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
<         effects) {
<       effects.emplace_back(MemoryEffects::Read::get(), getTagMemRef(),
<                            SideEffects::DefaultResource::get());
<     }
1192,1193c1158
<                        Variadic<Index>:$indices,
<                        DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
---
>                        Variadic<Index>:$indices);
1211,1258d1175
< // MemorySpaceCastOp
< //===----------------------------------------------------------------------===//
< def MemRef_MemorySpaceCastOp : MemRef_Op<"memory_space_cast", [
<       DeclareOpInterfaceMethods<CastOpInterface>,
<       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
<       MemRefsNormalizable,
<       Pure,
<       SameOperandsAndResultElementType,
<       SameOperandsAndResultShape,
<       ViewLikeOpInterface
<     ]> {
<   let summary = "memref memory space cast operation";
<   let description = [{
<     This operation casts memref values between memory spaces.
<     The input and result will be memrefs of the same types and shape that alias
<     the same underlying memory, though, for some casts on some targets,
<     the underlying values of the pointer stored in the memref may be affected
<     by the cast.
< 
<     The input and result must have the same shape, element type, rank, and layout.
< 
<     If the source and target address spaces are the same, this operation is a noop.
< 
<     Example:
< 
<     ```mlir
<     // Cast a GPU private memory attribution into a generic pointer
<     %2 = memref.memory_space_cast %1 : memref<?xf32, 5> to memref<?xf32>
<     // Cast a generic pointer to workgroup-local memory
<     %4 = memref.memory_space_cast %3 : memref<5x4xi32> to memref<5x34xi32, 3>
<     // Cast between two non-default memory spaces
<     %6 = memref.memory_space_cast %5
<       : memref<*xmemref<?xf32>, 5> to memref<*xmemref<?xf32>, 3>
<     ```
<   }];
< 
<   let arguments = (ins AnyRankedOrUnrankedMemRef:$source);
<   let results = (outs AnyRankedOrUnrankedMemRef:$dest);
<   let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";
< 
<   let extraClassDeclaration = [{
<     Value getViewSource() { return getSource(); }
<   }];
< 
<   let hasFolder = 1;
< }
< 
< //===----------------------------------------------------------------------===//
1393c1310
<     MemRefType getType() { return getResult().getType(); }
---
>     MemRefType getType() { return getResult().getType().cast<MemRefType>(); }
1396,1397c1313,1316
<     /// Return the rank of the result type.
<     unsigned getResultRank() { return getType().getRank(); }
---
>     /// Return the rank of the source ShapedType.
>     unsigned getResultRank() {
>       return getResult().getType().cast<ShapedType>().getRank();
>     }
1402c1321
<       unsigned resultRank = getType().getRank();
---
>       unsigned resultRank = getResult().getType().cast<ShapedType>().getRank();
1774,1775c1693
<                        Variadic<Index>:$indices,
<                        DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
---
>                        Variadic<Index>:$indices);
1831c1749,1750
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
1963,1964c1882,1883
<     // Build a SubViewOp with mixed static and dynamic entries and inferred
<     // result type.
---
>     // Build a SubViewOp with mixed static and dynamic entries and custom
>     // result type. If the type passed is nullptr, it is inferred.
1968,1969c1887,1888
<     // Build a SubViewOp with mixed static and dynamic entries and custom
<     // result type. If the type passed is nullptr, it is inferred.
---
>     // Build a SubViewOp with mixed static and dynamic entries and inferred
>     // result type.
2121a2041
>     ShapedType getShapedType() { return getIn().getType().cast<ShapedType>(); }
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/MemRef/IR/MemRefBase.td
23a24
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/MemRef/IR/CMakeLists.txt
--- include/mlir/Dialect/MemRef/IR/MemRefOps.td
103,116d102
< 
<     SmallVector<OpFoldResult> getMixedSizes() {
<       SmallVector<OpFoldResult> result;
<       unsigned ctr = 0;
<       OpBuilder b(getContext());
<       for (int64_t i = 0, e = getType().getRank(); i < e; ++i) {
<         if (getType().isDynamicDim(i)) {
<           result.push_back(getDynamicSizes()[ctr++]);
<         } else {
<           result.push_back(b.getIndexAttr(getType().getShape()[i]));
<         }
<       }
<       return result;
<     }
250c236
<     %3 = memref.realloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>
---
>     %3 = memref.ralloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>
263,267c249
<   // Note that we conceptually mark the operands as freeing the incoming
<   // memref and allocating the outcoming memref, even though this may not
<   // physically happen on each execution.
< 
<   let arguments = (ins Arg<MemRefRankOf<[AnyType], [1]>, "", [MemFree]>:$source,
---
>   let arguments = (ins MemRefRankOf<[AnyType], [1]>:$source,
272c254
<   let results = (outs Res<MemRefRankOf<[AnyType], [1]>, "", [MemAlloc<DefaultResource>]>);
---
>   let results = (outs MemRefRankOf<[AnyType], [1]>);
593c575
<   let arguments = (ins AnyNon0RankedOrUnrankedMemRef:$source,
---
>   let arguments = (ins AnyRankedOrUnrankedMemRef:$source,
620a603
>   let hasVerifier = 1;
778,788d760
< 
<     void getEffects(
<         SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
<         effects) {
<       effects.emplace_back(MemoryEffects::Read::get(), getSrcMemRef(),
<                            SideEffects::DefaultResource::get());
<       effects.emplace_back(MemoryEffects::Write::get(), getDstMemRef(),
<                            SideEffects::DefaultResource::get());
<       effects.emplace_back(MemoryEffects::Read::get(), getTagMemRef(),
<                            SideEffects::DefaultResource::get());
<     }
831,836d802
<     void getEffects(
<         SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>> &
<         effects) {
<       effects.emplace_back(MemoryEffects::Read::get(), getTagMemRef(),
<                            SideEffects::DefaultResource::get());
<     }
1192,1193c1158
<                        Variadic<Index>:$indices,
<                        DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
---
>                        Variadic<Index>:$indices);
1211,1258d1175
< // MemorySpaceCastOp
< //===----------------------------------------------------------------------===//
< def MemRef_MemorySpaceCastOp : MemRef_Op<"memory_space_cast", [
<       DeclareOpInterfaceMethods<CastOpInterface>,
<       DeclareOpInterfaceMethods<OpAsmOpInterface, ["getAsmResultNames"]>,
<       MemRefsNormalizable,
<       Pure,
<       SameOperandsAndResultElementType,
<       SameOperandsAndResultShape,
<       ViewLikeOpInterface
<     ]> {
<   let summary = "memref memory space cast operation";
<   let description = [{
<     This operation casts memref values between memory spaces.
<     The input and result will be memrefs of the same types and shape that alias
<     the same underlying memory, though, for some casts on some targets,
<     the underlying values of the pointer stored in the memref may be affected
<     by the cast.
< 
<     The input and result must have the same shape, element type, rank, and layout.
< 
<     If the source and target address spaces are the same, this operation is a noop.
< 
<     Example:
< 
<     ```mlir
<     // Cast a GPU private memory attribution into a generic pointer
<     %2 = memref.memory_space_cast %1 : memref<?xf32, 5> to memref<?xf32>
<     // Cast a generic pointer to workgroup-local memory
<     %4 = memref.memory_space_cast %3 : memref<5x4xi32> to memref<5x34xi32, 3>
<     // Cast between two non-default memory spaces
<     %6 = memref.memory_space_cast %5
<       : memref<*xmemref<?xf32>, 5> to memref<*xmemref<?xf32>, 3>
<     ```
<   }];
< 
<   let arguments = (ins AnyRankedOrUnrankedMemRef:$source);
<   let results = (outs AnyRankedOrUnrankedMemRef:$dest);
<   let assemblyFormat = "$source attr-dict `:` type($source) `to` type($dest)";
< 
<   let extraClassDeclaration = [{
<     Value getViewSource() { return getSource(); }
<   }];
< 
<   let hasFolder = 1;
< }
< 
< //===----------------------------------------------------------------------===//
1393c1310
<     MemRefType getType() { return getResult().getType(); }
---
>     MemRefType getType() { return getResult().getType().cast<MemRefType>(); }
1396,1397c1313,1316
<     /// Return the rank of the result type.
<     unsigned getResultRank() { return getType().getRank(); }
---
>     /// Return the rank of the source ShapedType.
>     unsigned getResultRank() {
>       return getResult().getType().cast<ShapedType>().getRank();
>     }
1402c1321
<       unsigned resultRank = getType().getRank();
---
>       unsigned resultRank = getResult().getType().cast<ShapedType>().getRank();
1774,1775c1693
<                        Variadic<Index>:$indices,
<                        DefaultValuedOptionalAttr<BoolAttr, "false">:$nontemporal);
---
>                        Variadic<Index>:$indices);
1831c1749,1750
<     sentinel value ShapedType::kDynamic encodes that the corresponding entry has
---
>     sentinel value ShapedType::kDynamic and
>     ShapedType::kDynamic encodes that the corresponding entry has
1963,1964c1882,1883
<     // Build a SubViewOp with mixed static and dynamic entries and inferred
<     // result type.
---
>     // Build a SubViewOp with mixed static and dynamic entries and custom
>     // result type. If the type passed is nullptr, it is inferred.
1968,1969c1887,1888
<     // Build a SubViewOp with mixed static and dynamic entries and custom
<     // result type. If the type passed is nullptr, it is inferred.
---
>     // Build a SubViewOp with mixed static and dynamic entries and inferred
>     // result type.
2121a2041
>     ShapedType getShapedType() { return getIn().getType().cast<ShapedType>(); }
--- include/mlir/Dialect/MemRef/IR/MemRef.h
--- include/mlir/Dialect/MemRef/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/MemRef/TransformOps/MemRefTransformOps.td include/mlir/Dialect/MemRef/TransformOps/MemRefTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
14d14
< include "mlir/Dialect/Transform/IR/TransformTypes.td"
19,20d18
< def Transform_MemRefAllocOp : Transform_ConcreteOpType<"memref.alloc">;
< 
23c21
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
---
>      TransformOpInterface, TransformEachOpTrait]> {
31,33d28
<      If skip analysis is not set the transformation will only apply
<      if it can prove that there is no data being carried across loop
<      iterations.
42,44c37,38
<       (ins Transform_MemRefAllocOp:$target,
<            ConfinedAttr<I64Attr, [IntPositive]>:$factor,
<            UnitAttr:$skip_analysis);
---
>       (ins PDL_Operation:$target,
>            ConfinedAttr<I64Attr, [IntPositive]>:$factor);
48,87c42
<   let assemblyFormat =
<     "$target attr-dict `:` functional-type(operands, results)";
< }
< 
< def MemRefExtractAddressComputationsOp :
<     Op<Transform_Dialect, "memref.extract_address_computations",
<        [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<         TransformEachOpTrait, TransformOpInterface]> {
<   let summary = "Extract address computations from memory accesses";
<   let description = [{
<     Transformation that extracts address computations from instructions
<     with memory accesses such that these memory accesses use only a base
<     pointer.
< 
<     For instance,
<     ```mlir
<     memref.load %base[%off0, ...]
<     ```
< 
<     Will be rewritten in:
<     ```mlir
<     %new_base = memref.subview %base[%off0,...][1,...][1,...]
<     memref.load %new_base[%c0,...]
<     ```
< 
<     Note: The current implementation requires that the input operation
<     is "isolated from above".
< 
<     #### Return modes
< 
<     This operation produces `definiteFailure` if the extraction fails for any
<     reason.
<     The operation always returns the handle to the target op that is expected
<     to be isolated from above.
<   }];
< 
<   let arguments = (ins PDL_Operation:$target);
<   let results = (outs PDL_Operation:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$target attr-dict";
91,92c46,47
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
---
>         memref::AllocOp target,
>         ::mlir::transform::ApplyToEachResultList &results,
95a51
> 
--- include/mlir/Dialect/MemRef/TransformOps/MemRefTransformOps.td
12a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
14d14
< include "mlir/Dialect/Transform/IR/TransformTypes.td"
19,20d18
< def Transform_MemRefAllocOp : Transform_ConcreteOpType<"memref.alloc">;
< 
23c21
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
---
>      TransformOpInterface, TransformEachOpTrait]> {
31,33d28
<      If skip analysis is not set the transformation will only apply
<      if it can prove that there is no data being carried across loop
<      iterations.
42,44c37,38
<       (ins Transform_MemRefAllocOp:$target,
<            ConfinedAttr<I64Attr, [IntPositive]>:$factor,
<            UnitAttr:$skip_analysis);
---
>       (ins PDL_Operation:$target,
>            ConfinedAttr<I64Attr, [IntPositive]>:$factor);
48,87c42
<   let assemblyFormat =
<     "$target attr-dict `:` functional-type(operands, results)";
< }
< 
< def MemRefExtractAddressComputationsOp :
<     Op<Transform_Dialect, "memref.extract_address_computations",
<        [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<         TransformEachOpTrait, TransformOpInterface]> {
<   let summary = "Extract address computations from memory accesses";
<   let description = [{
<     Transformation that extracts address computations from instructions
<     with memory accesses such that these memory accesses use only a base
<     pointer.
< 
<     For instance,
<     ```mlir
<     memref.load %base[%off0, ...]
<     ```
< 
<     Will be rewritten in:
<     ```mlir
<     %new_base = memref.subview %base[%off0,...][1,...][1,...]
<     memref.load %new_base[%c0,...]
<     ```
< 
<     Note: The current implementation requires that the input operation
<     is "isolated from above".
< 
<     #### Return modes
< 
<     This operation produces `definiteFailure` if the extraction fails for any
<     reason.
<     The operation always returns the handle to the target op that is expected
<     to be isolated from above.
<   }];
< 
<   let arguments = (ins PDL_Operation:$target);
<   let results = (outs PDL_Operation:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results)";
---
>   let assemblyFormat = "$target attr-dict";
91,92c46,47
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
---
>         memref::AllocOp target,
>         ::mlir::transform::ApplyToEachResultList &results,
95a51
> 
--- include/mlir/Dialect/MemRef/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/MemRef/TransformOps/MemRefTransformOps.h
--- include/mlir/Dialect/Linalg
Only in include/mlir/Dialect/Linalg: Analysis
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR and include/mlir/Dialect/Linalg/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Passes.td include/mlir/Dialect/Linalg/Passes.td
40c40
<     "linalg::LinalgDialect", "affine::AffineDialect", "memref::MemRefDialect"
---
>     "linalg::LinalgDialect", "AffineDialect", "memref::MemRefDialect"
48c48
<     "affine::AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"
---
>     "AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"
71c71
<     "affine::AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"];
---
>     "AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"];
80c80
<     "affine::AffineDialect"
---
>     "AffineDialect"
90c90
<     "affine::AffineDialect",
---
>     "AffineDialect",
101c101
<     "affine::AffineDialect",
---
>     "AffineDialect",
114c114
< def LinalgDetensorize : InterfacePass<"linalg-detensorize", "FunctionOpInterface"> {
---
> def LinalgDetensorize : Pass<"linalg-detensorize", ""> {
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps and include/mlir/Dialect/Linalg/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Transforms and include/mlir/Dialect/Linalg/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Utils and include/mlir/Dialect/Linalg/Utils
--- include/mlir/Dialect/Linalg/Passes.h
--- include/mlir/Dialect/Linalg/Passes.td
40c40
<     "linalg::LinalgDialect", "affine::AffineDialect", "memref::MemRefDialect"
---
>     "linalg::LinalgDialect", "AffineDialect", "memref::MemRefDialect"
48c48
<     "affine::AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"
---
>     "AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"
71c71
<     "affine::AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"];
---
>     "AffineDialect", "linalg::LinalgDialect", "memref::MemRefDialect"];
80c80
<     "affine::AffineDialect"
---
>     "AffineDialect"
90c90
<     "affine::AffineDialect",
---
>     "AffineDialect",
101c101
<     "affine::AffineDialect",
---
>     "AffineDialect",
114c114
< def LinalgDetensorize : InterfacePass<"linalg-detensorize", "FunctionOpInterface"> {
---
> def LinalgDetensorize : Pass<"linalg-detensorize", ""> {
--- include/mlir/Dialect/Linalg/Utils
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Utils: IndexingUtils.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Utils/Utils.h include/mlir/Dialect/Linalg/Utils/Utils.h
11a12
> #include "mlir/Dialect/Linalg/Analysis/DependenceAnalysis.h"
13d13
< #include "mlir/Dialect/Linalg/Utils/IndexingUtils.h"
15c15
< #include "mlir/Dialect/Utils/StructuredOpsUtils.h"
---
> #include "llvm/ADT/MapVector.h"
20a21
> class AffineForOp;
24,27d24
< namespace affine {
< class AffineForOp;
< } // namespace affine
< 
33,70c30
< 
< //===----------------------------------------------------------------------===//
< // Utilities for inferring various semantics properties of Linalg ops.
< //===----------------------------------------------------------------------===//
< 
< /// Possible dimension candidates that define a matmul embedded in the indexing
< /// maps of a LinalgOp.
< struct EmbeddedMatmulDimsCandidates {
<   DenseSet<int64_t> mPos, nPos, kPos;
< };
< 
< /// Given a `linalgOp` and one of its `opOperand`, returns the positions of the
< /// iterators of type `iter` that index the `opOperand` as a permutation.
< /// This is useful to infer various subcomputations on a given `linalgOp`.
< /// This is performed by looking up each result in the matching indexing map and
< /// determining whether:
< ///   - It is a single AffineDimExpr.
< ///   - It is the only result involving this AffineDimExpr.
< DenseSet<int64_t> findPermutationsIndexingOperand(LinalgOp linalgOp,
<                                                   OpOperand *opOperand,
<                                                   utils::IteratorType iter);
< 
< /// Return true if `linalgOp` contains an embedded matmul subcomputation in its
< /// most minor dimensions.
< bool containsMostMinorMatmul(linalg::LinalgOp linalgOp);
< 
< /// Find 2 parallel (m and n) and 1 reduction (k) dimension candidates that form
< /// a matmul subcomputation within `linalgOp`. These dimensions are such that:
< ///   1. The m dimension is involved in an outer-product along LHS
< ///      (i.e. it is a permutation on RES and LHS and does not appear in RHS).
< ///   2. The n dimension is involved in an outer-product along RHS
< ///      (i.e. it is a permutation on RES and RHS and does not appear in LHS).
< ///   3. The k dimension appears as a permutation on LHS and RHS.
< ///   4. m, n and k appear only once in any given indexing.
< /// This allows detecting that some matmul is embedded within `linalgOp` with
< /// some orthogonal heuristic.
< FailureOr<EmbeddedMatmulDimsCandidates>
< inferMatmulDims(linalg::LinalgOp linalgOp);
---
> class LinalgDependenceGraph;
90a51,98
> /// Helper function that creates a memref::DimOp or tensor::DimOp depending on
> /// the type of `source`.
> Value createOrFoldDimOp(OpBuilder &b, Location loc, Value source, int64_t dim);
> OpFoldResult createFoldedDimOp(OpBuilder &b, Location loc, Value source,
>                                int64_t dim);
> 
> /// Given an operation, retrieves the value of each dynamic dimension through
> /// constructing the necessary DimOp operators.
> SmallVector<Value, 4> getDynOperands(Location loc, Value val, OpBuilder &b);
> 
> /// Computes an upper bound for the result `value` of an index computation.
> /// Translates AffineMinOps and AffineApplyOps along the use-def chains of the
> /// index computation to affine constraints and projects out intermediate
> /// values. The method sets `boundMap` to an affine map that given
> /// `boundOperands` evaluates to an upper bound for the index computation.
> ///
> /// If constantRequired is true, only returns the constant bounds (potentially
> /// over-approximating) and fails when not possible.
> ///
> /// Example:
> /// ```
> /// %dim0 = dim %tensor, %c0
> /// %dim1 = dim %tensor, %c1
> /// %0 = affine.min affine.map<(d0) -> (40, d0)> (%dim0)
> /// %1 = affine.apply affine.map<(d0, d1) -> (d0 + d1)> (%0, %dim1)
> /// ```
> /// getUpperBoundForIndex(%1, boundMap, boundOperands)
> /// set the output parameters to:
> /// - boundMap = affine.map<(d0) -> (d0 + 40)>
> /// - boundOperands = [%dim1]
> void getUpperBoundForIndex(Value value, AffineMap &boundMap,
>                            SmallVectorImpl<Value> &boundOperands,
>                            bool constantRequired = false);
> 
> /// Returns a constant upper bound for the result `value` of an index
> /// computation. Calls `getUpperBoundForIndex` and returns a constant upper
> /// bound if the result of `boundMap` is a constant expression and failure
> /// otherwise.
> ///
> /// Example:
> /// ```
> /// %0 = affine.min affine.map<(d0) -> (40, d0)> (%d0)
> /// %1 = affine.apply affine.map<(d0) -> (d0 + 2)> (%0)
> /// ```
> /// getConstantUpperBoundForIndex(%1) returns 42
> /// (boundsMap = affine.map<() -> (42)>)
> FailureOr<int64_t> getConstantUpperBoundForIndex(Value value);
> 
93,96c101,104
< /// `source` size. The padding introduces trailing `pad` values until the
< /// target size is met. If `source` is defined by one or more LinalgOps that
< /// have been padded with the same value and sizes, return their padded result
< /// instead of creating a tensor::PadOp.
---
> /// `source` size. The padding introduces trailing `pad` values until the target
> /// size is met. If `source` is defined by one or more LinalgOps that have been
> /// padded with the same value and sizes, return their padded result instead of
> /// creating a tensor::PadOp.
113,114c121,122
< /// Returns a GenericOp that transposes `inputTensor` into `outputTensor`
< /// using `transposeVector` to permute the `inputTensor` dimensions.
---
> /// Returns a GenericOp that tansposes `inputTensor` into `outputTensor` using
> /// `transposeVector` to permute the `inputTensor` dimensions.
124,125c132,133
< /// Get the reassociation maps to fold the result of a extract_slice (or
< /// source of a insert_slice) operation with given offsets, and sizes to its
---
> /// Get the reassociation maps to fold the result of a extract_slice (or source
> /// of a insert_slice) operation with given offsets, and sizes to its
127,129c135,137
< /// and offset is 0. Strictly speaking the offset 0 is not required in
< /// general, but non-zero offsets are not handled by SPIR-V backend at this
< /// point (and potentially cannot be handled).
---
> /// and offset is 0. Strictly speaking the offset 0 is not required in general,
> /// but non-zero offsets are not handled by SPIR-V backend at this point (and
> /// potentially cannot be handled).
135c143
< std::optional<TypedAttr> getNeutralElement(Operation *op);
---
> std::optional<Attribute> getNeutralElement(Operation *op);
148,150c156,170
< /// Computes tile offsets, given a list of loop `ivs` and `tileSizes`. In case
< /// a tile size is zero (i.e., no tiling), the corresponding offset is also
< /// zero.
---
> /// Checks whether the specific `producer` is the last write to exactly the
> /// whole `consumedView`. This checks structural dominance, that the dependence
> /// is a RAW without any interleaved write to any piece of `consumedView`.
> bool isProducerLastWriteOfView(const LinalgDependenceGraph &graph,
>                                LinalgOp consumer, Value consumedView,
>                                LinalgOp producer);
> 
> /// Checks whether fusing the specific `producer` of the `consumedView` is
> /// feasible. This checks `producer` is the last write of `consumedView` and
> /// that no interleaved dependence would be violated (RAW, WAR or WAW).
> bool isFusableInto(const LinalgDependenceGraph &graph, LinalgOp consumer,
>                    Value consumedView, LinalgOp producer);
> 
> /// Computes tile offsets, given a list of loop `ivs` and `tileSizes`. In case a
> /// tile size is zero (i.e., no tiling), the corresponding offset is also zero.
164,165c184,185
< /// operation `op` is applied to the given `operands`. Note that `operands`
< /// are not necessarily the actual operands of `op`.
---
> /// operation `op` is applied to the given `operands`. Note that `operands` are
> /// not necessarily the actual operands of `op`.
169,173c189,192
< /// they were originally extracted from with `extract_slice` before being
< /// passed as `operands` to the given structured operation `op` or its clone.
< /// Note that `operands` are not necessarily the actual operands of `op`, the
< /// operation serves only as metadata container for operand types and
< /// positions.
---
> /// they were originally extracted from with `extract_slice` before being passed
> /// as `operands` to the given structured operation `op` or its clone. Note that
> /// `operands` are not necessarily the actual operands of `op`, the operation
> /// serves only as metadata container for operand types and positions.
186,187c205,206
< /// is being tiled with the given loop bounds `lbs` and `ubs` and the tile
< /// sizes `tileSizes`.
---
> /// is being tiled with the given loop bounds `lbs` and `ubs` and the tile sizes
> /// `tileSizes`.
218,219c237,238
< /// controls whether to omit the partial/boundary tile condition check in
< /// cases where we statically know that it is unnecessary.
---
> /// controls whether to omit the partial/boundary tile condition check in cases
> /// where we statically know that it is unnecessary.
231,232c250,251
< /// omit the partial/boundary tile condition check in cases where we
< /// statically know that it is unnecessary.
---
> /// omit the partial/boundary tile condition check in cases where we statically
> /// know that it is unnecessary.
251a271,277
> using FusableOpDependencesTy = llvm::MapVector<
>     Operation *,
>     SmallVector<LinalgDependenceGraph::LinalgDependenceGraphElem, 1>>;
> FusableOpDependencesTy
> findAllFusableDependences(ArrayRef<LinalgOp> ops,
>                           const LinalgDependenceGraph &dependenceGraph);
> 
253,255c279,280
< /// When operating on tensors, `fusedProducer` may feed into a `tensor.cast`
< /// op before the consumer Linalg op, until enough canonicalizations have
< /// applied.
---
> /// When operating on tensors, `fusedProducer` may feed into a `tensor.cast` op
> /// before the consumer Linalg op, until enough canonicalizations have applied.
260a286,293
> /// Fuses producer into consumer if the producer is structurally feasible and
> /// the fusion would not violate dependencies.
> /// Implements the fusion part of the "tileAndFuse on buffers" transformation
> /// and thus requires the `consumerOpOperand` to be a `subview` op (generally
> /// obtained by applying the tiling transformation).
> FailureOr<FusionInfo> fuseProducerOfBuffer(OpBuilder &b,
>                                            OpOperand &consumerOpOperand,
>                                            const LinalgDependenceGraph &graph);
268d300
< 
286,287c318
<   /// relationship between number of processors and number of iterations of
<   /// the
---
>   /// relationship between number of processors and number of iterations of the
294,295c325
<   /// scf.parallel(%iv)= (%lb + %procId * %step) to (%ub) step (%step *
<   /// %nprocs)
---
>   /// scf.parallel(%iv)= (%lb + %procId * %step) to (%ub) step (%step * %nprocs)
299,302c329,330
<   /// more than or equal to the number of iterations of the distributed loop.
<   /// In
<   /// such cases, a simple in-bounds check is enough (instead of materializing
<   /// a
---
>   /// more than or equal to the number of iterations of the distributed loop. In
>   /// such cases, a simple in-bounds check is enough (instead of materializing a
317,318c345
<   ///  equal to the number of iterations of the distributed loop. In such
<   ///  cases,
---
>   ///  equal to the number of iterations of the distributed loop. In such cases,
345,350c372,377
<   /// Callback function that returns the Values for processor ID (`procId`),
<   /// and number of processors (`nprocs`) used to execute the parallel loops.
<   /// The number of `{procId, nprocs}` pairs returned must be equal to the
<   /// number of `parallelLoopRanges` passed into the callback. The
<   /// `parallelLoopRanges` are ranges of the outer parallel loops of the
<   /// operation that do have non-zero tile sizes specified.
---
>   /// Callback function that returns the Values for processor ID (`procId`), and
>   /// number of processors (`nprocs`) used to execute the parallel loops. The
>   /// number of `{procId, nprocs}` pairs returned must be equal to the number of
>   /// `parallelLoopRanges` passed into the callback. The `parallelLoopRanges`
>   /// are ranges of the outer parallel loops of the operation that
>   /// do have non-zero tile sizes specified.
354,355c381
< /// Update the `lb`, `ub` and `step` to get per processor `lb`, `ub` and
< /// `step`.
---
> /// Update the `lb`, `ub` and `step` to get per processor `lb`, `ub` and `step`.
369,370c395,396
<   /// Tile the root operation using the given `tileSizes` and
<   /// `tileInterchange`, and `tileDistribution`.
---
>   /// Tile the root operation using the given `tileSizes` and `tileInterchange`,
>   /// and `tileDistribution`.
376,377c402,403
<   /// Fuse the producer of `consumerOpOperand` into the tile loop nest.
<   /// Returns the fused producer or fails if fusion is not possible.
---
>   /// Fuse the producer of `consumerOpOperand` into the tile loop nest. Returns
>   /// the fused producer or fails if fusion is not possible.
433,434c459,460
<   /// Matches the given linalg op if its body is performing binary operation
<   /// on int or float scalar values and returns the binary op kind.
---
>   /// Matches the given linalg op if its body is performing binary operation on
>   /// int or float scalar values and returns the binary op kind.
452,455c478,480
< /// `loopRanges` and loop type described by the `iteratorTypes`.
< /// `bodyBuilderFn` is used to generate the body of the innermost loop. It is
< /// passed a range of loop induction variables and a range of operand values
< /// to use.
---
> /// `loopRanges` and loop type described by the `iteratorTypes`. `bodyBuilderFn`
> /// is used to generate the body of the innermost loop. It is passed a range
> /// of loop induction variables and a range of operand values to use.
470c495,497
<   auto elidedAttrs = llvm::to_vector(op.getAttributeNames());
---
>   llvm::StringSet<> elidedAttrs;
>   elidedAttrs.insert(op.getAttributeNames().begin(),
>                      op.getAttributeNames().end());
472,473c499,506
<     elidedAttrs.push_back(LinalgDialect::kMemoizedIndexingMapsAttrName);
<   return getPrunedAttributeList(op, elidedAttrs);
---
>     elidedAttrs.insert(LinalgDialect::kMemoizedIndexingMapsAttrName);
>   SmallVector<NamedAttribute> attrs;
>   for (auto attr : op->getAttrs()) {
>     if (elidedAttrs.count(attr.getName()))
>       continue;
>     attrs.push_back(attr);
>   }
>   return attrs;
--- include/mlir/Dialect/Linalg/Utils/Utils.h
11a12
> #include "mlir/Dialect/Linalg/Analysis/DependenceAnalysis.h"
13d13
< #include "mlir/Dialect/Linalg/Utils/IndexingUtils.h"
15c15
< #include "mlir/Dialect/Utils/StructuredOpsUtils.h"
---
> #include "llvm/ADT/MapVector.h"
20a21
> class AffineForOp;
24,27d24
< namespace affine {
< class AffineForOp;
< } // namespace affine
< 
33,70c30
< 
< //===----------------------------------------------------------------------===//
< // Utilities for inferring various semantics properties of Linalg ops.
< //===----------------------------------------------------------------------===//
< 
< /// Possible dimension candidates that define a matmul embedded in the indexing
< /// maps of a LinalgOp.
< struct EmbeddedMatmulDimsCandidates {
<   DenseSet<int64_t> mPos, nPos, kPos;
< };
< 
< /// Given a `linalgOp` and one of its `opOperand`, returns the positions of the
< /// iterators of type `iter` that index the `opOperand` as a permutation.
< /// This is useful to infer various subcomputations on a given `linalgOp`.
< /// This is performed by looking up each result in the matching indexing map and
< /// determining whether:
< ///   - It is a single AffineDimExpr.
< ///   - It is the only result involving this AffineDimExpr.
< DenseSet<int64_t> findPermutationsIndexingOperand(LinalgOp linalgOp,
<                                                   OpOperand *opOperand,
<                                                   utils::IteratorType iter);
< 
< /// Return true if `linalgOp` contains an embedded matmul subcomputation in its
< /// most minor dimensions.
< bool containsMostMinorMatmul(linalg::LinalgOp linalgOp);
< 
< /// Find 2 parallel (m and n) and 1 reduction (k) dimension candidates that form
< /// a matmul subcomputation within `linalgOp`. These dimensions are such that:
< ///   1. The m dimension is involved in an outer-product along LHS
< ///      (i.e. it is a permutation on RES and LHS and does not appear in RHS).
< ///   2. The n dimension is involved in an outer-product along RHS
< ///      (i.e. it is a permutation on RES and RHS and does not appear in LHS).
< ///   3. The k dimension appears as a permutation on LHS and RHS.
< ///   4. m, n and k appear only once in any given indexing.
< /// This allows detecting that some matmul is embedded within `linalgOp` with
< /// some orthogonal heuristic.
< FailureOr<EmbeddedMatmulDimsCandidates>
< inferMatmulDims(linalg::LinalgOp linalgOp);
---
> class LinalgDependenceGraph;
90a51,98
> /// Helper function that creates a memref::DimOp or tensor::DimOp depending on
> /// the type of `source`.
> Value createOrFoldDimOp(OpBuilder &b, Location loc, Value source, int64_t dim);
> OpFoldResult createFoldedDimOp(OpBuilder &b, Location loc, Value source,
>                                int64_t dim);
> 
> /// Given an operation, retrieves the value of each dynamic dimension through
> /// constructing the necessary DimOp operators.
> SmallVector<Value, 4> getDynOperands(Location loc, Value val, OpBuilder &b);
> 
> /// Computes an upper bound for the result `value` of an index computation.
> /// Translates AffineMinOps and AffineApplyOps along the use-def chains of the
> /// index computation to affine constraints and projects out intermediate
> /// values. The method sets `boundMap` to an affine map that given
> /// `boundOperands` evaluates to an upper bound for the index computation.
> ///
> /// If constantRequired is true, only returns the constant bounds (potentially
> /// over-approximating) and fails when not possible.
> ///
> /// Example:
> /// ```
> /// %dim0 = dim %tensor, %c0
> /// %dim1 = dim %tensor, %c1
> /// %0 = affine.min affine.map<(d0) -> (40, d0)> (%dim0)
> /// %1 = affine.apply affine.map<(d0, d1) -> (d0 + d1)> (%0, %dim1)
> /// ```
> /// getUpperBoundForIndex(%1, boundMap, boundOperands)
> /// set the output parameters to:
> /// - boundMap = affine.map<(d0) -> (d0 + 40)>
> /// - boundOperands = [%dim1]
> void getUpperBoundForIndex(Value value, AffineMap &boundMap,
>                            SmallVectorImpl<Value> &boundOperands,
>                            bool constantRequired = false);
> 
> /// Returns a constant upper bound for the result `value` of an index
> /// computation. Calls `getUpperBoundForIndex` and returns a constant upper
> /// bound if the result of `boundMap` is a constant expression and failure
> /// otherwise.
> ///
> /// Example:
> /// ```
> /// %0 = affine.min affine.map<(d0) -> (40, d0)> (%d0)
> /// %1 = affine.apply affine.map<(d0) -> (d0 + 2)> (%0)
> /// ```
> /// getConstantUpperBoundForIndex(%1) returns 42
> /// (boundsMap = affine.map<() -> (42)>)
> FailureOr<int64_t> getConstantUpperBoundForIndex(Value value);
> 
93,96c101,104
< /// `source` size. The padding introduces trailing `pad` values until the
< /// target size is met. If `source` is defined by one or more LinalgOps that
< /// have been padded with the same value and sizes, return their padded result
< /// instead of creating a tensor::PadOp.
---
> /// `source` size. The padding introduces trailing `pad` values until the target
> /// size is met. If `source` is defined by one or more LinalgOps that have been
> /// padded with the same value and sizes, return their padded result instead of
> /// creating a tensor::PadOp.
113,114c121,122
< /// Returns a GenericOp that transposes `inputTensor` into `outputTensor`
< /// using `transposeVector` to permute the `inputTensor` dimensions.
---
> /// Returns a GenericOp that tansposes `inputTensor` into `outputTensor` using
> /// `transposeVector` to permute the `inputTensor` dimensions.
124,125c132,133
< /// Get the reassociation maps to fold the result of a extract_slice (or
< /// source of a insert_slice) operation with given offsets, and sizes to its
---
> /// Get the reassociation maps to fold the result of a extract_slice (or source
> /// of a insert_slice) operation with given offsets, and sizes to its
127,129c135,137
< /// and offset is 0. Strictly speaking the offset 0 is not required in
< /// general, but non-zero offsets are not handled by SPIR-V backend at this
< /// point (and potentially cannot be handled).
---
> /// and offset is 0. Strictly speaking the offset 0 is not required in general,
> /// but non-zero offsets are not handled by SPIR-V backend at this point (and
> /// potentially cannot be handled).
135c143
< std::optional<TypedAttr> getNeutralElement(Operation *op);
---
> std::optional<Attribute> getNeutralElement(Operation *op);
148,150c156,170
< /// Computes tile offsets, given a list of loop `ivs` and `tileSizes`. In case
< /// a tile size is zero (i.e., no tiling), the corresponding offset is also
< /// zero.
---
> /// Checks whether the specific `producer` is the last write to exactly the
> /// whole `consumedView`. This checks structural dominance, that the dependence
> /// is a RAW without any interleaved write to any piece of `consumedView`.
> bool isProducerLastWriteOfView(const LinalgDependenceGraph &graph,
>                                LinalgOp consumer, Value consumedView,
>                                LinalgOp producer);
> 
> /// Checks whether fusing the specific `producer` of the `consumedView` is
> /// feasible. This checks `producer` is the last write of `consumedView` and
> /// that no interleaved dependence would be violated (RAW, WAR or WAW).
> bool isFusableInto(const LinalgDependenceGraph &graph, LinalgOp consumer,
>                    Value consumedView, LinalgOp producer);
> 
> /// Computes tile offsets, given a list of loop `ivs` and `tileSizes`. In case a
> /// tile size is zero (i.e., no tiling), the corresponding offset is also zero.
164,165c184,185
< /// operation `op` is applied to the given `operands`. Note that `operands`
< /// are not necessarily the actual operands of `op`.
---
> /// operation `op` is applied to the given `operands`. Note that `operands` are
> /// not necessarily the actual operands of `op`.
169,173c189,192
< /// they were originally extracted from with `extract_slice` before being
< /// passed as `operands` to the given structured operation `op` or its clone.
< /// Note that `operands` are not necessarily the actual operands of `op`, the
< /// operation serves only as metadata container for operand types and
< /// positions.
---
> /// they were originally extracted from with `extract_slice` before being passed
> /// as `operands` to the given structured operation `op` or its clone. Note that
> /// `operands` are not necessarily the actual operands of `op`, the operation
> /// serves only as metadata container for operand types and positions.
186,187c205,206
< /// is being tiled with the given loop bounds `lbs` and `ubs` and the tile
< /// sizes `tileSizes`.
---
> /// is being tiled with the given loop bounds `lbs` and `ubs` and the tile sizes
> /// `tileSizes`.
218,219c237,238
< /// controls whether to omit the partial/boundary tile condition check in
< /// cases where we statically know that it is unnecessary.
---
> /// controls whether to omit the partial/boundary tile condition check in cases
> /// where we statically know that it is unnecessary.
231,232c250,251
< /// omit the partial/boundary tile condition check in cases where we
< /// statically know that it is unnecessary.
---
> /// omit the partial/boundary tile condition check in cases where we statically
> /// know that it is unnecessary.
251a271,277
> using FusableOpDependencesTy = llvm::MapVector<
>     Operation *,
>     SmallVector<LinalgDependenceGraph::LinalgDependenceGraphElem, 1>>;
> FusableOpDependencesTy
> findAllFusableDependences(ArrayRef<LinalgOp> ops,
>                           const LinalgDependenceGraph &dependenceGraph);
> 
253,255c279,280
< /// When operating on tensors, `fusedProducer` may feed into a `tensor.cast`
< /// op before the consumer Linalg op, until enough canonicalizations have
< /// applied.
---
> /// When operating on tensors, `fusedProducer` may feed into a `tensor.cast` op
> /// before the consumer Linalg op, until enough canonicalizations have applied.
260a286,293
> /// Fuses producer into consumer if the producer is structurally feasible and
> /// the fusion would not violate dependencies.
> /// Implements the fusion part of the "tileAndFuse on buffers" transformation
> /// and thus requires the `consumerOpOperand` to be a `subview` op (generally
> /// obtained by applying the tiling transformation).
> FailureOr<FusionInfo> fuseProducerOfBuffer(OpBuilder &b,
>                                            OpOperand &consumerOpOperand,
>                                            const LinalgDependenceGraph &graph);
268d300
< 
286,287c318
<   /// relationship between number of processors and number of iterations of
<   /// the
---
>   /// relationship between number of processors and number of iterations of the
294,295c325
<   /// scf.parallel(%iv)= (%lb + %procId * %step) to (%ub) step (%step *
<   /// %nprocs)
---
>   /// scf.parallel(%iv)= (%lb + %procId * %step) to (%ub) step (%step * %nprocs)
299,302c329,330
<   /// more than or equal to the number of iterations of the distributed loop.
<   /// In
<   /// such cases, a simple in-bounds check is enough (instead of materializing
<   /// a
---
>   /// more than or equal to the number of iterations of the distributed loop. In
>   /// such cases, a simple in-bounds check is enough (instead of materializing a
317,318c345
<   ///  equal to the number of iterations of the distributed loop. In such
<   ///  cases,
---
>   ///  equal to the number of iterations of the distributed loop. In such cases,
345,350c372,377
<   /// Callback function that returns the Values for processor ID (`procId`),
<   /// and number of processors (`nprocs`) used to execute the parallel loops.
<   /// The number of `{procId, nprocs}` pairs returned must be equal to the
<   /// number of `parallelLoopRanges` passed into the callback. The
<   /// `parallelLoopRanges` are ranges of the outer parallel loops of the
<   /// operation that do have non-zero tile sizes specified.
---
>   /// Callback function that returns the Values for processor ID (`procId`), and
>   /// number of processors (`nprocs`) used to execute the parallel loops. The
>   /// number of `{procId, nprocs}` pairs returned must be equal to the number of
>   /// `parallelLoopRanges` passed into the callback. The `parallelLoopRanges`
>   /// are ranges of the outer parallel loops of the operation that
>   /// do have non-zero tile sizes specified.
354,355c381
< /// Update the `lb`, `ub` and `step` to get per processor `lb`, `ub` and
< /// `step`.
---
> /// Update the `lb`, `ub` and `step` to get per processor `lb`, `ub` and `step`.
369,370c395,396
<   /// Tile the root operation using the given `tileSizes` and
<   /// `tileInterchange`, and `tileDistribution`.
---
>   /// Tile the root operation using the given `tileSizes` and `tileInterchange`,
>   /// and `tileDistribution`.
376,377c402,403
<   /// Fuse the producer of `consumerOpOperand` into the tile loop nest.
<   /// Returns the fused producer or fails if fusion is not possible.
---
>   /// Fuse the producer of `consumerOpOperand` into the tile loop nest. Returns
>   /// the fused producer or fails if fusion is not possible.
433,434c459,460
<   /// Matches the given linalg op if its body is performing binary operation
<   /// on int or float scalar values and returns the binary op kind.
---
>   /// Matches the given linalg op if its body is performing binary operation on
>   /// int or float scalar values and returns the binary op kind.
452,455c478,480
< /// `loopRanges` and loop type described by the `iteratorTypes`.
< /// `bodyBuilderFn` is used to generate the body of the innermost loop. It is
< /// passed a range of loop induction variables and a range of operand values
< /// to use.
---
> /// `loopRanges` and loop type described by the `iteratorTypes`. `bodyBuilderFn`
> /// is used to generate the body of the innermost loop. It is passed a range
> /// of loop induction variables and a range of operand values to use.
470c495,497
<   auto elidedAttrs = llvm::to_vector(op.getAttributeNames());
---
>   llvm::StringSet<> elidedAttrs;
>   elidedAttrs.insert(op.getAttributeNames().begin(),
>                      op.getAttributeNames().end());
472,473c499,506
<     elidedAttrs.push_back(LinalgDialect::kMemoizedIndexingMapsAttrName);
<   return getPrunedAttributeList(op, elidedAttrs);
---
>     elidedAttrs.insert(LinalgDialect::kMemoizedIndexingMapsAttrName);
>   SmallVector<NamedAttribute> attrs;
>   for (auto attr : op->getAttrs()) {
>     if (elidedAttrs.count(attr.getName()))
>       continue;
>     attrs.push_back(attr);
>   }
>   return attrs;
--- include/mlir/Dialect/Linalg/CMakeLists.txt
--- include/mlir/Dialect/Linalg/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Transforms/Hoisting.h include/mlir/Dialect/Linalg/Transforms/Hoisting.h
13d12
< class RewriterBase;
17,19d15
< namespace scf {
< class ForOp;
< } // namespace scf
35,37c31
< ///
< /// WARNING: This hoisting does not model parallelism and is generally incorrect
< /// when used on distributed loops with memref semantics!
---
> // TODO: generalize on a per-need basis.
40,140c34,35
< /// Greedily hoist redundant subset extract/insert operations on tensors outside
< /// of `forOp`. The logic follows:
< ///   1. Look for a write walking back from the `forOp` yield.
< ///   2. Check the uses of the matching block argument and look for a matching
< ///      read (i.e. extract_slice of transfer_read) with matching indices.
< ///   3. In the case of a transfer_write, we can bypass other non-conflicting
< ///      operations and find more hoisting opportunities.
< ///   4. Hoist the read/write pair and update the tensor SSA links.
< ///
< /// Return the unmodified `forOp` if no hoisting occured.
< /// Return a new scf::ForOp if hoisting on tensors occured.
< ///
< /// After this transformation the returned scf::ForOp may have unused arguments
< /// that can be removed by application of canonicalization patterns.
< ///
< /// Example:
< /// ========
< /// IR Resembling:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0)->(tensor<10xf32>) {
< ///  %1 = scf.for %j = %l to %u step %s iter_args(%a6 = %a0)->(tensor<10xf32>) {
< ///   %e = tensor.extract_slice %a6[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///   %r = vector.transfer_read %e[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   %w = vector.transfer_write %u, %e[%c0] : vector<4xf32>, tensor<?xf32>
< ///   %st = tensor.insert_slice %w into %a6[%i][%sz][1]
< ///     : tensor<?xf32> into tensor<10xf32>
< ///   scf.yield %st: tensor<10xf32>
< ///  }
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// Progressively hoists to:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %1:2 = scf.for %j = %l to %u step %s iter_args(%a6 = a0, %a7 = %e)
< ///     -> (tensor<10xf32>, tensor<?xf32>) {
< ///   %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   %w = vector.transfer_write %u, %a7[%c0] : vector<4xf32>, tensor<?xf32>
< ///   scf.yield %a6, %w: tensor<10xf32>, tensor<?xf32>
< ///  }
< ///  %st = tensor.insert_slice %1#1 into %1#0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// and
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///  %1:3 = scf.for %j = %l to %u step %s iter_args(%a6 = a0, %a7 = %e, %a7 = r)
< ///     -> (tensor<10xf32>, tensor<?xf32>, vector<4xf32>) {
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   scf.yield %a6, %a7, %u: tensor<10xf32>, tensor<?xf32>, vector<4xf32>
< ///  }
< ///  %w = vector.transfer_write %1#2, %1#1[%c0] : vector<4xf32>, tensor<?xf32>
< ///  %st = tensor.insert_slice %w into %1#0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// It can then canonicalize to:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///  %1 = scf.for %j = %l to %u step %s iter_args(%a7 = r)
< ///     -> (tensor<10xf32>, tensor<?xf32>, vector<4xf32>) {
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   scf.yield %u: vector<4xf32>
< ///  }
< ///  %w = vector.transfer_write %1, %e[%c0] : vector<4xf32>, tensor<?xf32>
< ///  %st = tensor.insert_slice %w into %a0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< // TODO: This should be further generalized along a few different axes:
< //   - Other loops than scf.ForOp that operate on tensors (both sequential and
< //     parallel loops).
< //   - Other subset extract/insert pairs than tensor.extract/insert_slice and
< //     vector.transfer_read/write.
< //   - More general areSubsetDisjoint analysis/interface to work across all
< //     subset op types and allow bypassing non-WAW-conflicting operations in
< //     more cases.
< scf::ForOp hoistRedundantSubsetExtractInsert(RewriterBase &rewriter,
<                                              scf::ForOp forOp);
< 
< /// Call into `hoistRedundantSubsetInsertExtract` without a RewriterBase.
< // TODO: obsolete and should be retired
---
> /// Same behavior as `hoistRedundantVectorTransfers` but works on tensors
> /// instead of buffers.
Only in include/mlir/Dialect/Linalg/Transforms: HoistPadding.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/Transforms/Transforms.h include/mlir/Dialect/Linalg/Transforms/Transforms.h
24d23
< #include "mlir/Support/LogicalResult.h"
30c29,31
< namespace linalg {
---
> namespace bufferization {
> class BufferizeTypeConverter;
> } // namespace bufferization
32c33
< class LinalgOp;
---
> class FrozenRewritePatternSet;
34,36c35
< //===----------------------------------------------------------------------===//
< // Utils.
< //===----------------------------------------------------------------------===//
---
> namespace linalg {
38,39c37,39
< /// Return vector::CombiningKind for the given op.
< std::optional<vector::CombiningKind> getCombinerOpKind(Operation *combinerOp);
---
> struct LinalgElementwiseFusionOptions;
> struct LinalgFusionOptions;
> struct LinalgTilingOptions;
42c42
< // Structs that configure the behavior of various transformations.
---
> // Transformations exposed as function calls.
43a44
> using LinalgLoops = SmallVector<Operation *, 4>;
45,46c46,47
< using TileSizeComputationFunction =
<     std::function<SmallVector<Value, 4>(OpBuilder &, Operation *)>;
---
> void populatePadTensorTilingPatterns(RewritePatternSet &patterns,
>                                      const LinalgTilingOptions &options);
48,52c49,56
< struct LinalgTilingOptions {
<   /// Computation function that returns the tile sizes for each operation.
<   /// Delayed construction of constant tile sizes should occur to interoperate
<   /// with folding.
<   TileSizeComputationFunction tileSizeComputationFunction = nullptr;
---
> /// Populate patterns for splitting a `LinalgOp` with multiple statements within
> /// its payload into multiple `GenericOp` that have a single statement.
> /// The option `removeDeadArgsAndResults` adds patterns to remove dead arguments
> /// and results from the generated decomposed ops. This is default `true` since
> /// the core decomposition patterns relies on these clean up patterns. It is set
> /// to false only for testing purposes.
> void populateDecomposeLinalgOpsPattern(RewritePatternSet &patterns,
>                                        bool removeDeadArgsAndResults = true);
54,69c58,62
<   LinalgTilingOptions &
<   setTileSizeComputationFunction(TileSizeComputationFunction fun) {
<     tileSizeComputationFunction = std::move(fun);
<     return *this;
<   }
<   /// Set the `tileSizeComputationFunction` to return the values `ts`. The
<   /// values must not fold away when tiling. Otherwise, use a more robust
<   /// `tileSizeComputationFunction`.
<   LinalgTilingOptions &setTileSizes(const SmallVector<Value, 4> &ts) {
<     tileSizeComputationFunction = [=](OpBuilder &, Operation *) { return ts; };
<     return *this;
<   }
<   /// Convenience function to set the `tileSizeComputationFunction` to a
<   /// function that computes tile sizes at the point they are needed. Allows
<   /// proper interaction with folding.
<   LinalgTilingOptions &setTileSizes(ArrayRef<int64_t> ts);
---
> /// Populate patterns for vectorizing low-D convolution ops. This is a step in
> /// progressive lowering for convolution ops, it assume high-D convolution ops
> /// were decomposed previously.
> void populateConvolutionVectorizationPatterns(RewritePatternSet &patterns,
>                                               PatternBenefit benefit = 1);
71,73c64,66
<   /// Tile all dynamic dimensions by 1. I.e., scalarize those dimensions.
<   /// Note: `scalarizeDynamicDims` and `setTileSizes` cannot be used together.
<   LinalgTilingOptions &scalarizeDynamicDims();
---
> /// Populate patterns that convert `ElementwiseMappable` ops to linalg
> /// parallel loops.
> void populateElementwiseToLinalgConversionPatterns(RewritePatternSet &patterns);
75,76c68,69
<   /// The interchange vector to reorder the tiled loops.
<   SmallVector<unsigned, 4> interchangeVector = {};
---
> /// Populate patterns that are only useful in the context of sparse tensors.
> void populateSparseTensorRewriting(RewritePatternSet &patterns);
78,81c71,74
<   LinalgTilingOptions &setInterchange(ArrayRef<unsigned> interchange) {
<     interchangeVector.assign(interchange.begin(), interchange.end());
<     return *this;
<   }
---
> /// Function type which is used to control when to stop fusion. It is expected
> /// that OpOperand is not modified in the callback. The OpOperand is not marked
> /// as const to allow callers to use non-const methods.
> using ControlFusionFn = std::function<bool(OpOperand *fusedOperand)>;
83,84c76
<   /// The type of tile loops to generate.
<   LinalgTilingLoopType loopType = LinalgTilingLoopType::Loops;
---
> /// Patterns for fusing linalg operation on tensors.
86,89c78,82
<   LinalgTilingOptions &setLoopType(LinalgTilingLoopType lt) {
<     loopType = lt;
<     return *this;
<   }
---
> /// Pattern to fuse `linalg.generic` -> `linalg.generic` operations
> /// when both operations are fusable elementwise operations.
> void populateElementwiseOpsFusionPatterns(
>     RewritePatternSet &patterns,
>     const ControlFusionFn &controlElementwiseOpFusion);
91,93c84,85
<   /// When specified, specifies distribution of generated tile loops to
<   /// processors.
<   std::optional<LinalgLoopDistributionOptions> distribution;
---
> /// Patterns to bubble up or down data layout ops across other operations.
> void populateDataLayoutPropagationPatterns(RewritePatternSet &patterns);
95,99c87,89
<   LinalgTilingOptions &
<   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
<     distribution = std::move(distributionOptions);
<     return *this;
<   }
---
> /// Pattern to remove dead operands and results of `linalg.generic` operations.
> /// This is effectively DCE for a linalg op.
> void populateEraseUnusedOperandsAndResultsPatterns(RewritePatternSet &patterns);
101,102c91,93
<   /// Specification markers of how to distribute the `linalg.tiled_loop`.
<   SmallVector<StringRef, 2> distributionTypes = {};
---
> /// Patterns to promote inputs to outputs and remove unused inputs of
> /// `linalg.generic` ops.
> void populateEraseUnnecessaryInputsPatterns(RewritePatternSet &patterns);
104,107c95,99
<   LinalgTilingOptions &setDistributionTypes(ArrayRef<StringRef> types) {
<     distributionTypes.assign(types.begin(), types.end());
<     return *this;
<   }
---
> /// Function type to control generic op dimension collapsing. It is expected
> /// to return an array of `ReassociationIndices` representing dimensions that
> /// should be merged.
> using GetCollapsableDimensionsFn =
>     std::function<SmallVector<ReassociationIndices>(linalg::GenericOp)>;
109,110c101,105
<   /// Peel the specified loops.
<   SmallVector<int64_t> peeledLoops;
---
> /// Pattern to collapse dimensions in a linalg.generic op. This will collapse
> /// tensor operands when needed and expand back the result tensors.
> void populateCollapseDimensions(
>     RewritePatternSet &patterns,
>     const GetCollapsableDimensionsFn &controlCollapseDimensions);
112,117c107,111
<   LinalgTilingOptions &setPeeledLoops(ArrayRef<int64_t> loops) {
<     peeledLoops.clear();
<     peeledLoops.append(loops.begin(), loops.end());
<     return *this;
<   }
< };
---
> /// Patterns to fold an expanding (collapsing) tensor_reshape operation with its
> /// producer (consumer) generic operation by expanding the dimensionality of the
> /// loop in the generic op.
> void populateFoldReshapeOpsByExpansionPatterns(
>     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
119,136c113,116
< struct LinalgTilingAndFusionOptions {
<   /// Tile sizes used to tile the root operation.
<   SmallVector<int64_t> tileSizes;
<   LinalgTilingAndFusionOptions &setTileSizes(ArrayRef<int64_t> ts) {
<     tileSizes.assign(ts.begin(), ts.end());
<     return *this;
<   }
<   /// Tile interchange used to permute the tile loops.
<   SmallVector<int64_t> tileInterchange;
<   /// When specified, specifies distribution of generated tile loops to
<   /// processors.
<   std::optional<LinalgLoopDistributionOptions> tileDistribution;
<   LinalgTilingAndFusionOptions &
<   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
<     tileDistribution = std::move(distributionOptions);
<     return *this;
<   }
< };
---
> /// Patterns to fold an expanding tensor.expand_shape operation with its
> /// producer generic operation by collapsing the dimensions of the generic op.
> void populateFoldReshapeOpsByCollapsingPatterns(
>     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
138,172c118,120
< struct LinalgPaddingOptions {
<   /// A padding value for every operand.
<   SmallVector<Attribute> paddingValues;
<   LinalgPaddingOptions &setPaddingValues(ArrayRef<Attribute> pv) {
<     paddingValues.assign(pv.begin(), pv.end());
<     return *this;
<   }
<   /// A list of iterator dimensions to pad.
<   SmallVector<int64_t> paddingDimensions;
<   LinalgPaddingOptions &setPaddingDimensions(ArrayRef<int64_t> pd) {
<     paddingDimensions.assign(pd.begin(), pd.end());
<     return *this;
<   }
<   /// A flag for every operand to mark the PadOp as nofold which enables
<   /// packing for statically shaped operands.
<   SmallVector<bool> packPaddings;
<   LinalgPaddingOptions &setPackPaddings(ArrayRef<bool> pp) {
<     packPaddings.assign(pp.begin(), pp.end());
<     return *this;
<   }
<   /// A number of loops to hoist the PadOp out for every operand.
<   SmallVector<int64_t> hoistPaddings;
<   LinalgPaddingOptions &setHoistPaddings(ArrayRef<int64_t> hp) {
<     hoistPaddings.assign(hp.begin(), hp.end());
<     return *this;
<   }
<   /// A permutation vector for every operand used to transpose the packed
<   /// PadOp results.
<   SmallVector<SmallVector<int64_t>> transposePaddings;
<   LinalgPaddingOptions &
<   setTransposePaddings(ArrayRef<SmallVector<int64_t>> tp) {
<     transposePaddings.assign(tp.begin(), tp.end());
<     return *this;
<   }
< };
---
> /// Patterns to constant fold Linalg operations.
> void populateConstantFoldLinalgOperations(RewritePatternSet &patterns,
>                                           const ControlFusionFn &controlFn);
174,181c122,125
< /// Callback function type used to perform the allocation for the promoted
< /// `subView`. In `boundingSubViewsize` a best attempt is made to find the
< /// smallest constant value for the size of the buffer needed for each
< /// dimension. If that is not possible, contains the dynamic size of the
< /// subview. The call back should return the buffer to use.
< using AllocBufferCallbackFn = std::function<std::optional<Value>(
<     OpBuilder &b, memref::SubViewOp subView,
<     ArrayRef<Value> boundingSubViewSize, DataLayout &layout)>;
---
> /// Pattern to fuse a `tensor.pad` operation with the producer of its source,
> /// if the producer is a `linalg` operation with all parallel iterator types.
> void populateFuseTensorPadWithProducerLinalgOpPatterns(
>     RewritePatternSet &patterns);
183,186c127,129
< /// Callback function type used to deallocate the buffers used to hold the
< /// promoted subview.
< using DeallocBufferCallbackFn =
<     std::function<LogicalResult(OpBuilder &b, Value buffer)>;
---
> /// Patterns to convert from one named op to another. These can be seen as
> /// canonicalizations of named ops into another named op.
> void populateLinalgNamedOpConversionPatterns(RewritePatternSet &patterns);
188,193c131,133
< /// Callback function type used to insert copy from original subview to
< /// subview of the promoted region for the read operands/subview of promoted
< /// region to original subview for the results. The copy has to happen from
< /// `src` to `dst`.
< using CopyCallbackFn =
<     std::function<LogicalResult(OpBuilder &b, Value src, Value dst)>;
---
> /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
> /// tensors via reassociative reshape ops.
> void populateFoldUnitExtentDimsViaReshapesPatterns(RewritePatternSet &patterns);
195,259c135,137
< struct LinalgPromotionOptions {
<   /// Indices of subViews to promote. If `std::nullopt`, try to promote all
<   /// operands.
<   std::optional<DenseSet<unsigned>> operandsToPromote;
<   LinalgPromotionOptions &setOperandsToPromote(ArrayRef<int64_t> operands) {
<     operandsToPromote = DenseSet<unsigned>();
<     operandsToPromote->insert(operands.begin(), operands.end());
<     return *this;
<   }
<   /// If ith element of `useFullTiles` is true the full view should be used
<   /// for the promoted buffer of the ith operand in `operandsToPromote`.
<   /// Otherwise the partial view will be used. The decision is defaulted to
<   /// `useFullTileBuffersDefault` when `useFullTileBuffers` is std::nullopt and
<   /// for operands missing from `useFullTileBuffers`.
<   std::optional<llvm::SmallBitVector> useFullTileBuffers;
<   LinalgPromotionOptions &setUseFullTileBuffers(ArrayRef<bool> useFullTiles) {
<     unsigned size = useFullTiles.size();
<     llvm::SmallBitVector tmp(size, false);
<     for (unsigned i = 0; i < size; ++i)
<       tmp[i] = useFullTiles[i];
<     useFullTileBuffers = tmp;
<     return *this;
<   }
<   /// If true all operands unspecified by `useFullTileBuffers` will use the
<   /// full view, otherwise the partial view.
<   bool useFullTileBuffersDefault = false;
<   LinalgPromotionOptions &setUseFullTileBuffersByDefault(bool use) {
<     useFullTileBuffersDefault = use;
<     return *this;
<   }
<   /// Alignment of promoted buffer. If `std::nullopt` do not specify alignment.
<   std::optional<unsigned> alignment;
<   LinalgPromotionOptions &setAlignment(unsigned align) {
<     alignment = align;
<     return *this;
<   }
<   /// Use alloca with the default allocation scheme.
<   bool useAlloca = false;
<   LinalgPromotionOptions &setUseAlloca(bool use) {
<     useAlloca = use;
<     return *this;
<   }
<   /// Callback function to do the allocation of the promoted buffer. If
<   /// std::nullopt, then the default allocation scheme of allocating a
<   /// memref<?xi8> buffer followed by a view operation is used.
<   std::optional<AllocBufferCallbackFn> allocationFn;
<   std::optional<DeallocBufferCallbackFn> deallocationFn;
<   LinalgPromotionOptions &
<   setAllocationDeallocationFns(AllocBufferCallbackFn const &allocFn,
<                                DeallocBufferCallbackFn const &deallocFn) {
<     allocationFn = allocFn;
<     deallocationFn = deallocFn;
<     return *this;
<   }
<   /// Callback function to do the copy of data to and from the promoted
<   /// subview. If std::nullopt then a memref.copy is used.
<   std::optional<CopyCallbackFn> copyInFn;
<   std::optional<CopyCallbackFn> copyOutFn;
<   LinalgPromotionOptions &setCopyInOutFns(CopyCallbackFn const &copyIn,
<                                           CopyCallbackFn const &copyOut) {
<     copyInFn = copyIn;
<     copyOutFn = copyOut;
<     return *this;
<   }
< };
---
> /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
> /// tensors via rank-reducing slices.
> void populateFoldUnitExtentDimsViaSlicesPatterns(RewritePatternSet &patterns);
261,271c139,140
< /// Split Reduction options.
< struct SplitReductionOptions {
<   // Ratio used to split the reduction dimension.  If the ratio is <= 1,
<   // nothing will be done.
<   int64_t ratio = 0;
<   // Index where the extra dimension is added to the intermediate tensor
<   // shape.
<   unsigned index = 0;
<   // If the inner dimension after splitting is parallel or reduction.
<   bool innerParallel = false;
< };
---
> /// A pattern that converts init operands to input operands.
> void populateMoveInitOperandsToInputPattern(RewritePatternSet &patterns);
273,277c142,143
< /// Function signature to control reduction splitting. This returns
< /// `SplitReductionOptions`.
< // TODO: don't use unsigned unless doing bit manipulation.
< using ControlSplitReductionFn =
<     std::function<SplitReductionOptions(LinalgOp op)>;
---
> /// Patterns that are used to inline constant operands into linalg generic ops.
> void populateInlineConstantOperandsPatterns(RewritePatternSet &patterns);
279,282c145,150
< //===----------------------------------------------------------------------===//
< // Preconditions that ensure the corresponding transformation succeeds and can
< // be applied as a rewrite pattern.
< //===----------------------------------------------------------------------===//
---
> /// Patterns that are used to bubble up extract slice op above linalg op.
> void populateBubbleUpExtractSliceOpPatterns(RewritePatternSet &patterns);
> 
> /// Adds patterns that waps tensor.extract_slice(linalg.fill(%cst, %init)) into
> /// linalg.fill(%cst, tensor.extract_slice(%init)).
> void populateSwapExtractSliceWithFillPatterns(RewritePatternSet &patterns);
289,333d156
< /// Promote memref.subviews feeding linalg-on-buffers operations.
< LogicalResult promoteSubviewsPrecondition(Operation *op,
<                                           LinalgPromotionOptions options);
< 
< /// Return success if the operation can be vectorized.
< LogicalResult
< vectorizeLinalgOpPrecondition(LinalgOp linalgOp,
<                               ArrayRef<int64_t> inputVectorSizes = {},
<                               bool vectorizeNDExtract = false);
< 
< //===----------------------------------------------------------------------===//
< // Transformations exposed as functional-style API calls.
< //===----------------------------------------------------------------------===//
< 
< using LinalgLoops = SmallVector<Operation *, 4>;
< 
< /// Materialize a buffer allocation for the given tensor.pad op and lower the
< /// op to linalg.fill/linalg.generic + memref.tensor_store. E.g.:
< ///
< /// %0 = tensor.pad low[%l] high[%h] %t ...
< ///
< /// is lowered to:
< ///
< /// %alloc = memref.alloc
< /// linalg.fill ... outs(%alloc)
< /// %subview = memref.subview %alloc [%l] [...] [1]
< /// memref.tensor_store %t, %subview
< /// %0 = bufferization.to_tensor %alloc restrict writable
< ///
< /// In addition to rewriting the IR as shown above, the result of the
< /// bufferization.to_tensor op is returned.
< Value bufferizeToAllocation(RewriterBase &rewriter, tensor::PadOp padOp,
<                             Attribute memorySpace = {});
< 
< /// Materialize a buffer allocation for the given tensor value. E.g.:
< ///
< /// %alloc = memref.alloc
< /// memref.tensor_store %value, %alloc
< /// %0 = bufferization.to_tensor %alloc restrict writable
< ///
< /// In case `value` is a tensor.pad result, the corresponding overload is used
< /// internally to produce a better bufferization.
< Value bufferizeToAllocation(RewriterBase &rewriter, Value value,
<                             Attribute memorySpace = {});
< 
337,449c160,161
< struct ElementwiseOpFusionResult {
<   Operation *fusedOp;
<   llvm::DenseMap<Value, Value> replacements;
< };
< FailureOr<ElementwiseOpFusionResult>
< fuseElementwiseOps(RewriterBase &rewriter, OpOperand *fusedOperand);
< 
< /// Try to peel and canonicalize loop `op` and return the new result.
< /// Also applies affine_min/max bounds simplification on the fly where relevant.
< // TODO: Add support for scf.parallel and affine.for loops.
< SmallVector<Value> peelLoop(RewriterBase &rewriter, Operation *op);
< 
< /// Peel 'loops' and applies affine_min/max bounds simplification on the fly
< /// where relevant.
< void peelLoops(RewriterBase &rewriter, ArrayRef<scf::ForOp> loops);
< 
< /// Pad the iterator dimensions `paddingDimensions` of all `opToPad` operands
< /// to a static bounding box. Use `paddingValues` and `packPaddings` to set
< /// padding value and nofold attribute of the created tensor::PadOps,
< /// respectively. Update `paddedOp` to the cloned operation with statically
< /// shaped `paddingDimensions` and return the extracted dynamically shaped
< /// results. If padding fails, return failure.
< FailureOr<SmallVector<Value>>
< rewriteAsPaddedOp(RewriterBase &rewriter, LinalgOp opToPad,
<                   ArrayRef<int64_t> paddingDimensions,
<                   ArrayRef<Attribute> paddingValues,
<                   ArrayRef<bool> packPaddings, LinalgOp &paddedOp);
< 
< namespace detail {
< 
< /// Helper struct to hold the results of building a packing loop nest.
< struct PackingResult {
<   SmallVector<OpFoldResult> offsets, sizes, strides;
<   SmallVector<Value> clonedLoopIvs, leadingPackedTensorIndexings;
<   GenericOp maybeTransposeOp;
<   tensor::PadOp hoistedPadOp;
< };
< 
< /// Build the packing loop nest required to hoist `opToHoist` above
< /// `outermostEnclosingForOp`.
< /// The loop nest is built just before `outermostEnclosingForOp`.
< FailureOr<PackingResult>
< buildPackingLoopNest(RewriterBase &rewriter, tensor::PadOp opToHoist,
<                      scf::ForOp outermostEnclosingForOp,
<                      ArrayRef<int64_t> transposeVector);
< 
< } // namespace detail
< 
< /// Mechanically hoist padding operations on tensors by `numLoops` into a new,
< /// generally larger tensor. This achieves packing of multiple padding ops into
< /// a larger tensor. On success, `opToHoist` is replaced by the cloned version
< /// in the packing loop so the caller can continue reasoning about the padding
< /// operation. If `transposeVector` is non-empty, hoist padding introduces a
< /// GenericOp to transpose the padded tensor before inserting it into the packed
< /// tensor. A `transposeVector` can change the storage order of the padded
< /// tensor but does not change the order of the pack or compute loops.
< ///
< /// TODO: In the future, we should consider rewriting as a tensor.pack after
< /// hoisting since this abstraction is now available.
< ///
< /// Example in pseudo-mlir:
< /// =======================
< ///
< /// If hoistPaddingOnTensors is called with `nLoops` = 2 on the following IR.
< /// ```
< ///    scf.for (%i, %j, %k)
< ///      %st0 = tensor.extract_slice f(%i, %k) : ... to tensor<?x?xf32>
< ///      %0 = tensor.pad %st0 low[0, 0] high[...] {
< ///      ^bb0( ... ):
< ///        linalg.yield %pad
< ///      } : tensor<?x?xf32> to tensor<4x8xf32>
< ///      compute(%0)
< /// ```
< ///
< /// IR resembling the following is produced:
< ///
< /// ```
< ///    scf.for (%i) {
< ///      %packed_init = tensor.empty range(%j) : tensor<?x4x8xf32>
< ///      %packed = scf.for (%k) iter_args(%p : %packed_init) {
< ///        %st0 = tensor.extract_slice f(%i, %k) : ... to tensor<?x?xf32>
< ///        %0 = tensor.pad %st0 low[0, 0] high[...] {
< ///        ^bb0( ... ):
< ///          linalg.yield %pad
< ///        } : tensor<?x?xf32> to tensor<4x8xf32>
< ///        %1 = tensor.insert_slice %0 ...
< ///            : tensor<4x8xf32> to tensor<?x4x8xf32>
< ///        scf.yield %1: tensor<?x4x8xf32>
< ///      } -> tensor<?x4x8xf32>
< ///      scf.for (%j, %k) {
< ///        %st0 = tensor.extract_slice %packed [%k, 0, 0][1, 4, 8][1, 1, 1] :
< ///                 tensor<?x4x8xf32> to tensor<4x8xf32>
< ///        compute(%st0)
< ///      }
< ///    }
< /// ```
< FailureOr<Value>
< hoistPaddingOnTensors(RewriterBase &rewriter, tensor::PadOp opToHoist,
<                       int64_t numLoops, ArrayRef<int64_t> transposeVector,
<                       tensor::PadOp &hoistedOp,
<                       SmallVectorImpl<GenericOp> &transposeOps);
< /// Calls into `hoistPaddingOnTensors` with a local IRRewriter.
< FailureOr<Value>
< hoistPaddingOnTensors(tensor::PadOp opToHoist, int64_t numLoops,
<                       ArrayRef<int64_t> transposeVector,
<                       tensor::PadOp &hoistedOp,
<                       SmallVectorImpl<GenericOp> &transposeOps);
< 
< /// Apply padding and hoisting to `linalgOp` according to the configuration
< /// specified in `options`.
< FailureOr<LinalgOp> padAndHoistLinalgOp(RewriterBase &rewriter,
<                                         LinalgOp linalgOp,
<                                         LinalgPaddingOptions options);
---
> FailureOr<Operation *> fuseElementwiseOps(RewriterBase &rewriter,
>                                           OpOperand *fusedOperand);
505a218,223
> /// Try to peel anad canonicalize loop `op` and return the new result.
> // TODO: Add support for scf.parallel and affine.for loops.
> SmallVector<Value> peelLoop(RewriterBase &rewriter, Operation *op);
> /// Peel and canonicalize 'loops'.
> void peelLoops(RewriterBase &rewriter, ArrayRef<scf::ForOp> loops);
> 
526a245,331
> /// Callback function type used to perform the allocation for the promoted
> /// `subView`. In `boundingSubViewsize` a best attempt is made to find the
> /// smallest constant value for the size of the buffer needed for each
> /// dimension. If that is not possible, contains the dynamic size of the
> /// subview. The call back should return the buffer to use.
> using AllocBufferCallbackFn = std::function<std::optional<Value>(
>     OpBuilder &b, memref::SubViewOp subView,
>     ArrayRef<Value> boundingSubViewSize, DataLayout &layout)>;
> 
> /// Callback function type used to deallocate the buffers used to hold the
> /// promoted subview.
> using DeallocBufferCallbackFn =
>     std::function<LogicalResult(OpBuilder &b, Value buffer)>;
> 
> /// Callback function type used to insert copy from original subview to
> /// subview of the promoted region for the read operands/subview of promoted
> /// region to original subview for the results. The copy has to happen from
> /// `src` to `dst`.
> using CopyCallbackFn =
>     std::function<LogicalResult(OpBuilder &b, Value src, Value dst)>;
> 
> struct LinalgPromotionOptions {
>   /// Indices of subViews to promote. If `std::nullopt`, try to promote all
>   /// operands.
>   std::optional<DenseSet<unsigned>> operandsToPromote;
>   LinalgPromotionOptions &setOperandsToPromote(ArrayRef<int64_t> operands) {
>     operandsToPromote = DenseSet<unsigned>();
>     operandsToPromote->insert(operands.begin(), operands.end());
>     return *this;
>   }
>   /// If ith element of `useFullTiles` is true the full view should be used
>   /// for the promoted buffer of the ith operand in `operandsToPromote`.
>   /// Otherwise the partial view will be used. The decision is defaulted to
>   /// `useFullTileBuffersDefault` when `useFullTileBuffers` is None and for
>   /// operands missing from `useFullTileBuffers`.
>   std::optional<llvm::SmallBitVector> useFullTileBuffers;
>   LinalgPromotionOptions &setUseFullTileBuffers(ArrayRef<bool> useFullTiles) {
>     unsigned size = useFullTiles.size();
>     llvm::SmallBitVector tmp(size, false);
>     for (unsigned i = 0; i < size; ++i)
>       tmp[i] = useFullTiles[i];
>     useFullTileBuffers = tmp;
>     return *this;
>   }
>   /// If true all operands unspecified by `useFullTileBuffers` will use the
>   /// full view, otherwise the partial view.
>   bool useFullTileBuffersDefault = false;
>   LinalgPromotionOptions &setUseFullTileBuffersByDefault(bool use) {
>     useFullTileBuffersDefault = use;
>     return *this;
>   }
>   /// Alignment of promoted buffer. If `std::nullopt` do not specify alignment.
>   std::optional<unsigned> alignment;
>   LinalgPromotionOptions &setAlignment(unsigned align) {
>     alignment = align;
>     return *this;
>   }
>   /// Use alloca with the default allocation scheme.
>   bool useAlloca = false;
>   LinalgPromotionOptions &setUseAlloca(bool use) {
>     useAlloca = use;
>     return *this;
>   }
>   /// Callback function to do the allocation of the promoted buffer. If
>   /// std::nullopt, then the default allocation scheme of allocating a
>   /// memref<?xi8> buffer followed by a view operation is used.
>   std::optional<AllocBufferCallbackFn> allocationFn;
>   std::optional<DeallocBufferCallbackFn> deallocationFn;
>   LinalgPromotionOptions &
>   setAllocationDeallocationFns(AllocBufferCallbackFn const &allocFn,
>                                DeallocBufferCallbackFn const &deallocFn) {
>     allocationFn = allocFn;
>     deallocationFn = deallocFn;
>     return *this;
>   }
>   /// Callback function to do the copy of data to and from the promoted
>   /// subview. If std::nullopt then a memref.copy is used.
>   std::optional<CopyCallbackFn> copyInFn;
>   std::optional<CopyCallbackFn> copyOutFn;
>   LinalgPromotionOptions &setCopyInOutFns(CopyCallbackFn const &copyIn,
>                                           CopyCallbackFn const &copyOut) {
>     copyInFn = copyIn;
>     copyOutFn = copyOut;
>     return *this;
>   }
> };
> 
553,578d357
< /// Allocate the subview in the GPU workgroup memory.
< std::optional<Value> allocateWorkgroupMemory(OpBuilder &builder,
<                                              memref::SubViewOp subview,
<                                              ArrayRef<Value> sizeBounds,
<                                              DataLayout &);
< 
< /// In case of GPU group memory there is no need to deallocate.
< LogicalResult deallocateWorkgroupMemory(OpBuilder &, Value /*buffer*/);
< 
< /// Create Memref copy operations and add gpu barrier guards before and after
< /// the copy operation to ensure data integrity.
< LogicalResult copyToWorkgroupMemory(OpBuilder &b, Value src, Value dst);
< 
< /// Allocate the subview in the GPU private memory.
< std::optional<Value> allocateGPUPrivateMemory(OpBuilder &builder,
<                                               memref::SubViewOp subview,
<                                               ArrayRef<Value> sizeBounds,
<                                               DataLayout &);
< 
< /// Normal copy to between src and dst.
< LogicalResult copyToGPUPrivateMemory(OpBuilder &b, Value src, Value dst);
< 
< /// In case of GPU private memory there is no need to deallocate since the
< /// memory is freed when going outside of the scope.
< LogicalResult deallocateGPUPrivateMemory(OpBuilder &, Value /*buffer*/);
< 
592,598d370
< /// Vectorize a `padOp` with (1) static result type, (2) constant padding value
< /// and (3) all-zero lowPad to
< ///   `transfer_write_in_bounds(transfer_read_masked(pad_source, pad_value))`.
< FailureOr<vector::TransferWriteOp>
< maskedVectorize(RewriterBase &rewriter, tensor::PadOp padOp,
<                 ArrayRef<int64_t> inputVectorSizes);
< 
600c372
< FailureOr<LinalgLoops> linalgOpToLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToLoops(PatternRewriter &rewriter,
604c376
< FailureOr<LinalgLoops> linalgOpToParallelLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToParallelLoops(PatternRewriter &rewriter,
608c380
< FailureOr<LinalgLoops> linalgOpToAffineLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToAffineLoops(PatternRewriter &rewriter,
610a383,403
> //===----------------------------------------------------------------------===//
> // Preconditions that ensure the corresponding transformation succeeds and can
> // be applied as a rewrite pattern.
> //===----------------------------------------------------------------------===//
> /// Promote memref.subviews feeding linalg-on-buffers operations.
> LogicalResult promoteSubviewsPrecondition(Operation *op,
>                                           LinalgPromotionOptions options);
> 
> /// Return success if the operation can be vectorized.
> LogicalResult
> vectorizeLinalgOpPrecondition(LinalgOp linalgOp,
>                               ArrayRef<int64_t> inputVectorSizes = {},
>                               bool vectorizeNDExtract = false);
> 
> //===----------------------------------------------------------------------===//
> // Transformations exposed as rewrite patterns.
> //===----------------------------------------------------------------------===//
> 
> using TileSizeComputationFunction =
>     std::function<SmallVector<Value, 4>(OpBuilder &, Operation *)>;
> 
679c472
< /// Rewrite a TilingInterface `op` to a tiled `scf.forall`, applying
---
> /// Rewrite a TilingInterface `op` to a tiled `scf.foreach_thread`, applying
682c475
< /// resulting `scf.forall`.
---
> /// resulting `scf.foreach_thread`.
687c480
< struct ForallTilingResult {
---
> struct ForeachThreadTilingResult {
691,694c484,487
< FailureOr<ForallTilingResult> tileToForallOp(RewriterBase &builder,
<                                              TilingInterface op,
<                                              ArrayRef<OpFoldResult> numThreads,
<                                              std::optional<ArrayAttr> mapping);
---
> FailureOr<ForeachThreadTilingResult>
> tileToForeachThreadOp(RewriterBase &builder, TilingInterface op,
>                       ArrayRef<OpFoldResult> numThreads,
>                       std::optional<ArrayAttr> mapping);
696c489
< /// Same as `tileToForallOp`, but calculate the number of threads
---
> /// Same as `tileToForeachThreadOp`, but calculate the number of threads
698,701c491,494
< FailureOr<ForallTilingResult>
< tileToForallOpUsingTileSizes(RewriterBase &builder, TilingInterface op,
<                              ArrayRef<OpFoldResult> tileSizes,
<                              std::optional<ArrayAttr> mapping);
---
> FailureOr<ForeachThreadTilingResult>
> tileToForeachThreadOpUsingTileSizes(RewriterBase &builder, TilingInterface op,
>                                     ArrayRef<OpFoldResult> tileSizes,
>                                     std::optional<ArrayAttr> mapping);
704c497
< struct ForallReductionTilingResult {
---
> struct ForeachThreadReductionTilingResult {
711,712c504,505
<   /// The `scf.forall` operation that iterate over the tiles.
<   scf::ForallOp loops;
---
>   /// The `scf.foreach_thread` operation that iterate over the tiles.
>   scf::ForeachThreadOp loops;
728c521
< /// %1 = scf.forall (%iv) in (%c4) shared_outs(%arg0 = %0)
---
> /// %1 = scf.foreach_thread (%iv) in (%c4) shared_outs(%arg0 = %0)
739,743c532,535
< FailureOr<ForallReductionTilingResult>
< tileReductionUsingForall(RewriterBase &b, PartialReductionOpInterface op,
<                          ArrayRef<OpFoldResult> numThreads,
<                          ArrayRef<OpFoldResult> tileSizes = {},
<                          std::optional<ArrayAttr> mapping = std::nullopt);
---
> FailureOr<ForeachThreadReductionTilingResult> tileReductionUsingForeachThread(
>     RewriterBase &b, PartialReductionOpInterface op,
>     ArrayRef<OpFoldResult> numThreads, ArrayRef<OpFoldResult> tileSizes = {},
>     std::optional<ArrayAttr> mapping = std::nullopt);
801,845c593,626
< /// Apply transformation to split the single linalg op reduction into a
< /// parallel and reduction dimension. Then create a new linalg.generic op
< /// doing the rest of the reduction. Return the new linalg op with an extra
< /// parallel dimension or failure if the transformation didn't happen.
< ///
< /// Example:
< /// ```
< ///  %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
< ///                                        affine_map<(d0) -> ()>],
< ///       iterator_types = ["reduction"]}
< ///  ins(%in : tensor<32xf32>)
< ///  outs(%out : tensor<f32>) {
< ///  ^bb0(%arg1: f32, %arg2: f32):
< ///    %y = arith.addf %arg1, %arg2 : f32
< ///    linalg.yield %y : f32
< ///  } -> tensor<f32>
< /// ```
< /// To:
< /// ```
< ///  %cst = arith.constant 0.000000e+00 : f32
< ///  %0 = tensor.expand_shape %in [[0, 1]] : tensor<32xf32> into
< ///  tensor<4x8xf32> %1 = tensor.empty [4] : tensor<4xf32> %2 = linalg.fill
< ///  ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32> %3 =
< ///  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
< ///                                        affine_map<(d0, d1) -> (d0)>],
< ///    iterator_types = ["parallel", "reduction"]}
< ///    ins(%0 : tensor<4x8xf32>) outs(%2 : tensor<4xf32>) {
< ///    ^bb0(%arg3: f32, %arg5: f32):
< ///    %5 = arith.addf %arg3, %arg4 : f32
< ///    linalg.yield %5 : f32
< ///  } -> tensor<4xf32>
< /// %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
< ///                                       affine_map<(d0) -> ()>],
< ///   iterator_types = ["reduction"]}
< ///   ins(%3 : tensor<4xf32>) outs(%out : tensor<f32>) {
< ///   ^bb0(%arg3: f32, %arg4: f32):
< ///   %5 = arith.addf %arg3, %arg4 : f32
< ///   linalg.yield %5 : f32
< /// } -> tensor<f32>
< /// ```
< struct SplitReductionResult {
<   Operation *initOrAlloc;
<   FillOp fillOp;
<   LinalgOp splitLinalgOp;
<   LinalgOp resultCombiningLinalgOp;
---
> struct LinalgPaddingOptions {
>   /// A padding value for every operand.
>   SmallVector<Attribute> paddingValues;
>   LinalgPaddingOptions &setPaddingValues(ArrayRef<Attribute> pv) {
>     paddingValues.assign(pv.begin(), pv.end());
>     return *this;
>   }
>   /// A list of iterator dimensions to pad.
>   SmallVector<int64_t> paddingDimensions;
>   LinalgPaddingOptions &setPaddingDimensions(ArrayRef<int64_t> pd) {
>     paddingDimensions.assign(pd.begin(), pd.end());
>     return *this;
>   }
>   /// A flag for every operand to mark the PadOp as nofold which enables
>   /// packing for statically shaped operands.
>   SmallVector<bool> packPaddings;
>   LinalgPaddingOptions &setPackPaddings(ArrayRef<bool> pp) {
>     packPaddings.assign(pp.begin(), pp.end());
>     return *this;
>   }
>   /// A number of loops to hoist the PadOp out for every operand.
>   SmallVector<int64_t> hoistPaddings;
>   LinalgPaddingOptions &setHoistPaddings(ArrayRef<int64_t> hp) {
>     hoistPaddings.assign(hp.begin(), hp.end());
>     return *this;
>   }
>   /// A permutation vector for every operand used to transpose the packed
>   /// PadOp results.
>   SmallVector<SmallVector<int64_t>> transposePaddings;
>   LinalgPaddingOptions &
>   setTransposePaddings(ArrayRef<SmallVector<int64_t>> tp) {
>     transposePaddings.assign(tp.begin(), tp.end());
>     return *this;
>   }
847,850d627
< FailureOr<SplitReductionResult>
< splitReduction(RewriterBase &b, LinalgOp op,
<                const ControlSplitReductionFn &controlSplitReductionFn,
<                bool useAlloc = false);
852,902c629,646
< /// Scaling-based implementation of the split reduction transformation.
< /// Instead of introducing an ExpandShapeOp, this rewrites a reduction
< /// dimension `k` into `k * scale + kk`.
< ///
< /// Example:
< /// ```
< ///  %0 = linalg.matmul ins(%A, %B: tensor<16x256xf32>, tensor<256x32xf32>)
< ///    outs(%C: tensor<16x32xf32>) -> tensor<16x32xf32>
< /// ```
< ///
< /// Is transformed to:
< ///
< /// ```
< ///  #map0 = affine_map<(d0, d1, d2, d3) -> (d0, d2 * 4 + d3)>
< ///  #map1 = affine_map<(d0, d1, d2, d3) -> (d2 * 4 + d3, d1)>
< ///  #map2 = affine_map<(d0, d1, d2, d3) -> (d2, d3)>
< ///  #map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
< ///  #map4 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
< ///  #map5 = affine_map<(d0, d1, d2) -> (d0, d1)>
< ///  %0 = tensor.empty [16, 32, 64] : tensor<16x32x64xf32>
< ///  %cst = arith.constant 0.000000e+00 : f32
< ///  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<16x32x64xf32>) ->
< ///     tensor<16x32x64xf32>
< ///  %2 = tensor.empty [64, 4] : tensor<64x4xi1>
< ///
< ///  %3 = linalg.generic {indexing_maps = [#map0, #map1, #map2, #map3],
< ///    iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
< ///    ins(%A, %B, %2 : tensor<16x256xf32>, tensor<256x32xf32>,
< ///    tensor<64x4xi1>)
< ///   outs(%1 : tensor<16x32x64xf32>) {
< ///      ^bb0(%arg3: f32, %arg4: f32, %arg5: i1, %arg6: f32):
< ///        %5 = arith.mulf %arg3, %arg4 : f32
< ///        %6 = arith.addf %arg6, %5 : f32
< ///        linalg.yield %6 : f32
< ///  } -> tensor<16x32x64xf32>
< ///
< ///  %4 = linalg.generic {indexing_maps = [#map4, #map5],
< ///    iterator_types = ["parallel", "parallel", "reduction"]}
< //     ins(%3 : tensor<16x32x64xf32>)
< ///    outs(%C : tensor<16x32xf32>) {
< ///      ^bb0(%arg3: f32, %arg4: f32):
< ///        %5 = arith.addf %arg3, %arg4 : f32
< ///        linalg.yield %5 : f32
< ///  } -> tensor<16x32xf32>
< ///
< ///  return %4 : tensor<16x32xf32>
< /// ```
< FailureOr<SplitReductionResult>
< splitReductionByScaling(RewriterBase &b, LinalgOp op,
<                         const ControlSplitReductionFn &controlSplitReductionFn,
<                         bool useAlloc = false);
---
> struct LinalgTilingAndFusionOptions {
>   /// Tile sizes used to tile the root operation.
>   SmallVector<int64_t> tileSizes;
>   LinalgTilingAndFusionOptions &setTileSizes(ArrayRef<int64_t> ts) {
>     tileSizes.assign(ts.begin(), ts.end());
>     return *this;
>   }
>   /// Tile interchange used to permute the tile loops.
>   SmallVector<int64_t> tileInterchange;
>   /// When specified, specifies distribution of generated tile loops to
>   /// processors.
>   std::optional<LinalgLoopDistributionOptions> tileDistribution;
>   LinalgTilingAndFusionOptions &
>   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
>     tileDistribution = std::move(distributionOptions);
>     return *this;
>   }
> };
904,908c648,652
< /// Collapses dimensions of linalg.generic operation. It also collapses inputs
< /// before the op and expands outputs after the op.
< FailureOr<SmallVector<Value>> collapseGenericOpIterationDims(
<     GenericOp genericOp, ArrayRef<ReassociationIndices> foldedIterationDims,
<     RewriterBase &rewriter);
---
> struct LinalgTilingOptions {
>   /// Computation function that returns the tile sizes for each operation.
>   /// Delayed construction of constant tile sizes should occur to interoperate
>   /// with folding.
>   TileSizeComputationFunction tileSizeComputationFunction = nullptr;
910,1027c654,669
< struct LowerPackResult {
<   tensor::PadOp padOp;
<   tensor::ExpandShapeOp expandShapeOp;
<   linalg::TransposeOp transposeOp;
< };
< 
< /// Rewrite pack as pad + reshape + transpose.
< FailureOr<LowerPackResult> lowerPack(RewriterBase &rewriter,
<                                      tensor::PackOp packOp);
< 
< struct LowerUnPackOpResult {
<   tensor::EmptyOp emptyOp;
<   linalg::TransposeOp transposeOp;
<   tensor::CollapseShapeOp collapseShapeOp;
<   tensor::ExtractSliceOp extractSliceOp;
< };
< 
< /// Rewrite pack as empty + transpose + reshape + extract_slice.
< FailureOr<LowerUnPackOpResult> lowerUnPack(RewriterBase &rewriter,
<                                            tensor::UnPackOp unPackOp);
< 
< /// Struct to hold the result of a `pack` call.
< struct PackResult {
<   SmallVector<tensor::PackOp> packOps;
<   linalg::LinalgOp packedLinalgOp;
<   SmallVector<tensor::UnPackOp> unPackOps;
< };
< /// Implement packing of a single LinalgOp by `packedSizes`.
< /// There must be one packedSizes entry per `linalgOp` iterator.
< /// Return the packed Linalg op on success, failure otherwise.
< FailureOr<PackResult> pack(RewriterBase &rewriter, linalg::LinalgOp linalgOp,
<                            ArrayRef<OpFoldResult> packedSizes);
< 
< /// Struct to hold the result of a `packTranspose` call.
< struct PackTransposeResult {
<   tensor::PackOp transposedPackOp;
<   linalg::LinalgOp transposedLinalgOp;
<   tensor::UnPackOp transposedUnPackOp;
< };
< /// Transpose a single PackOp -> LinalgOp -> UnPackOp chain and return the
< /// transposed PackOp -> LinalgOp -> UnPackOp chain after replacements.
< /// Return failure if either:
< ///   1. the `packOp` does not have the `linalgOp` as its unique use.
< ///   2. the `maybeUnPackOp`, if specified must be a consumer of the result tied
< ///      to the unique `packOp` use.
< ///   3. `outerPerm` (resp. `innerPerm`) must be valid permutations of
< ///      `packOp.getOuterDimsPerm` (resp. `packOp.getInnerDimsPerm`) or empty.
< FailureOr<PackTransposeResult>
< packTranspose(RewriterBase &rewriter, tensor::PackOp packOp,
<               linalg::LinalgOp linalgOp, tensor::UnPackOp maybeUnPackOp,
<               ArrayRef<int64_t> outerPerm, ArrayRef<int64_t> innerPerm);
< 
< /// Rewrite tensor.from_elements to linalg.generic.
< FailureOr<Operation *>
< rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                  tensor::FromElementsOp fromElementsOp);
< 
< /// Rewrite tensor.generate to linalg.generic.
< FailureOr<Operation *>
< rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                  tensor::GenerateOp generateOp);
< 
< /// Rewrite tensor.pad to linalg.generic + tensor.insert_slice.
< FailureOr<Operation *> rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                                         tensor::PadOp padOp);
< 
< /// Convert linalg.conv_2d_nhwc_hwcf into linalg.generic (for img2col packing)
< /// and linalg.matmul.
< ///
< /// A convolution operation can be written as a matrix-matrix multiplication by
< /// unfolding the cross-correlation between input and filter and explicitly copy
< /// overlapped sliding window inputs.
< ///
< /// Consider 2D input X with single channel input and output and 2x2 filter W:
< /// [x(0, 0)  , x(0, 1)  , ...,   x(0, n)  ]
< /// [x(1, 0)  , x(1, 1)  , ...,   x(1, n)  ]
< /// [.        ,  .       ,.   ,      .     ]            [w(0, 0), w(0, 1)]
< /// [.        ,  .       , .  ,      .     ]    (conv)  [w(1, 0), w(1, 1)]
< /// [.        ,  .       ,   .,      .     ]
< /// [x(n-1, 0), x(n-1, 1), ..., x(n-1, n-1)]
< ///
< /// The packed input data (img2col) is a matrix with |rows| = output spatial
< /// size, |columns| = filter spatial size. To compute the output Y(i, j) we need
< /// to calculate the dot product between filter window at input X(x, y)) and the
< /// filter which will look like the following where r.h.s is the img2col matrix
< /// and l.h.s is the flattened filter:
< ///
< /// [x(0,0), x(0,1), x(1,0), x(1,1)]
< /// [x(0,1), x(1,1), x(0,2), x(1,2)] (matmul) [w(0,0), w(0,1), w(1,0), w(1,1)]
< /// [x(0,1), x(1,1), x(0,2), x(1,2)]
< /// [   .  ,    .  ,    .  ,    .  ]
< ///
< /// In general for 2D case with (N, H, W, C) input and (Kh, Kw, C, D) filter
< /// and output (N, Ho, Wo, D) the convolution is the following matrix-matrix
< /// multiplication (Ho x Wo, Kh x Kw x C) * (Kh x Kw x C, D) for each input in
< /// the N input. For the case where N > 1 its a batched matrix-matrix
< /// multiplication.
< ///
< /// On success, return both the operation that produces the img2col tensor and
< /// the final operation of the sequence that replaces the original convolution.
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter, linalg::Conv2DNhwcHwcfOp convOp);
< 
< /// Similar to rewriteInIm2Col with linalg::Conv2DNhwcHwcfOp except there is no
< /// reduction among the input channels so each convolution can be a
< /// matrix-vector product and by transposing both input filter so channels are
< /// outer most the computation is a batched matrix-vector product.
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter,
<                 linalg::DepthwiseConv2DNhwcHwcOp convOp);
< 
< /// Similar to rewriteInIm2Col with linalg::Conv2DNhwcHwcfOp except because the
< /// channels are to the left of the image shape dimensions, the position of the
< /// contraction dimension in the resulting matmul is reversed. This swaps the
< /// LHS and RHS of the matmul when compared with nhwc (i.e. (D, C x Kh x Kw) *
< /// (C x Kh x Kw, Ho x Wo))
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter, linalg::Conv2DNchwFchwOp convOp);
---
>   LinalgTilingOptions &
>   setTileSizeComputationFunction(TileSizeComputationFunction fun) {
>     tileSizeComputationFunction = std::move(fun);
>     return *this;
>   }
>   /// Set the `tileSizeComputationFunction` to return the values `ts`. The
>   /// values must not fold away when tiling. Otherwise, use a more robust
>   /// `tileSizeComputationFunction`.
>   LinalgTilingOptions &setTileSizes(const SmallVector<Value, 4> &ts) {
>     tileSizeComputationFunction = [=](OpBuilder &, Operation *) { return ts; };
>     return *this;
>   }
>   /// Convenience function to set the `tileSizeComputationFunction` to a
>   /// function that computes tile sizes at the point they are needed. Allows
>   /// proper interaction with folding.
>   LinalgTilingOptions &setTileSizes(ArrayRef<int64_t> ts);
1029,1033c671,723
< //===----------------------------------------------------------------------===//
< // Rewrite patterns wrapping transformations.
< // TODO: every single such pattern should be a close to noop wrapper around a
< // functional-stye API call.
< //===----------------------------------------------------------------------===//
---
>   /// Tile all dynamic dimensions by 1. I.e., scalarize those dimensions.
>   /// Note: `scalarizeDynamicDims` and `setTileSizes` cannot be used together.
>   LinalgTilingOptions &scalarizeDynamicDims();
> 
>   /// The interchange vector to reorder the tiled loops.
>   SmallVector<unsigned, 4> interchangeVector = {};
> 
>   LinalgTilingOptions &setInterchange(ArrayRef<unsigned> interchange) {
>     interchangeVector.assign(interchange.begin(), interchange.end());
>     return *this;
>   }
> 
>   /// The type of tile loops to generate.
>   LinalgTilingLoopType loopType = LinalgTilingLoopType::Loops;
> 
>   LinalgTilingOptions &setLoopType(LinalgTilingLoopType lt) {
>     loopType = lt;
>     return *this;
>   }
> 
>   /// When specified, specifies distribution of generated tile loops to
>   /// processors.
>   std::optional<LinalgLoopDistributionOptions> distribution;
> 
>   LinalgTilingOptions &
>   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
>     distribution = std::move(distributionOptions);
>     return *this;
>   }
> 
>   /// Specification markers of how to distribute the `linalg.tiled_loop`.
>   SmallVector<StringRef, 2> distributionTypes = {};
> 
>   LinalgTilingOptions &setDistributionTypes(ArrayRef<StringRef> types) {
>     distributionTypes.assign(types.begin(), types.end());
>     return *this;
>   }
> 
>   /// Peel the specified loops.
>   SmallVector<int64_t> peeledLoops;
> 
>   LinalgTilingOptions &setPeeledLoops(ArrayRef<int64_t> loops) {
>     peeledLoops.clear();
>     peeledLoops.append(loops.begin(), loops.end());
>     return *this;
>   }
> };
> 
> /// Canonicalization patterns relevant to apply after tiling patterns. These
> /// are applied automatically by the tiling pass but need to be applied
> /// manually when tiling is called programmatically.
> RewritePatternSet getLinalgTilingCanonicalizationPatterns(MLIRContext *ctx);
> void populateLinalgTilingCanonicalizationPatterns(RewritePatternSet &patterns);
1044a735,739
>   /// `matchAndRewrite` implementation that returns the significant
>   /// transformed pieces of IR.
>   FailureOr<LinalgOp> returningMatchAndRewrite(LinalgOp op,
>                                                PatternRewriter &rewriter) const;
> 
1046c741,743
<                                 PatternRewriter &rewriter) const override;
---
>                                 PatternRewriter &rewriter) const override {
>     return returningMatchAndRewrite(op, rewriter);
>   }
1092,1104d788
< struct DownscaleConv2DOp final : public OpRewritePattern<Conv2DOp> {
<   DownscaleConv2DOp(MLIRContext *context, PatternBenefit benefit = 1)
<       : OpRewritePattern<Conv2DOp>(context, benefit) {}
< 
<   FailureOr<Conv1DOp> returningMatchAndRewrite(Conv2DOp convOp,
<                                                PatternRewriter &rewriter) const;
< 
<   LogicalResult matchAndRewrite(Conv2DOp convOp,
<                                 PatternRewriter &rewriter) const override {
<     return returningMatchAndRewrite(convOp, rewriter);
<   }
< };
< 
1137a822,846
> /// Return vector::CombiningKind for the given op.
> std::optional<vector::CombiningKind> getCombinerOpKind(Operation *combinerOp);
> 
> //===----------------------------------------------------------------------===//
> // Transformations exposed as rewrite patterns.
> //===----------------------------------------------------------------------===//
> 
> /// Linalg generalization patterns
> 
> /// Populates `patterns` with patterns to convert spec-generated named ops to
> /// linalg.generic ops.
> void populateLinalgNamedOpsGeneralizationPatterns(RewritePatternSet &patterns);
> 
> /// Linalg decompose convolutions patterns
> 
> /// Populates patterns to decompose high-D convolution ops into low-D ones.
> /// This is a step in progressive lowering for convolution ops, afterwards we
> /// can vectorize the low-D convolution ops.
> void populateDecomposeConvolutionPatterns(RewritePatternSet &patterns,
>                                           PatternBenefit benefit = 1);
> 
> //===----------------------------------------------------------------------===//
> // Op-specific patterns.
> //===----------------------------------------------------------------------===//
> 
1146a856,867
> /// Pad the iterator dimensions `paddingDimensions` of all `opToPad` operands
> /// to a static bounding box. Use `paddingValues` and `packPaddings` to set
> /// padding value and nofold attribute of the created tensor::PadOps,
> /// respectively. Update `paddedOp` to the cloned operation with statically
> /// shaped `paddingDimensions` and return the extracted dynamically shaped
> /// results. If padding fails, return failure.
> FailureOr<SmallVector<Value>>
> rewriteAsPaddedOp(OpBuilder &b, LinalgOp opToPad,
>                   ArrayRef<int64_t> paddingDimensions,
>                   ArrayRef<Attribute> paddingValues,
>                   ArrayRef<bool> packPaddings, LinalgOp &paddedOp);
> 
1148c869
<     std::function<LogicalResult(RewriterBase &, tensor::PadOp, Value)>;
---
>     std::function<LogicalResult(PatternRewriter &, tensor::PadOp, Value)>;
1164c885
<   Value createFillOrGenerateOp(RewriterBase &rewriter, tensor::PadOp padOp,
---
>   Value createFillOrGenerateOp(PatternRewriter &rewriter, tensor::PadOp padOp,
1188a910,921
> /// Populates `patterns` with patterns that vectorize tensor.pad.
> /// These patterns are meant to apply in a complementary fashion. Benefits
> /// are used to encode a certain ordering of pattern application. To avoid
> /// scattering magic constants throughout the code base, the patterns must be
> /// added with this function. `baseBenefit` can be used to offset the benefit
> /// of all tensor::PadOp vectorization patterns by a certain value.
> void populatePadOpVectorizationPatterns(RewritePatternSet &patterns,
>                                         PatternBenefit baseBenefit = 1);
> 
> void populateExtractOpVectorizationPatterns(RewritePatternSet &patterns,
>                                             PatternBenefit baseBenefit = 1);
> 
1274,1425c1007,1017
< //===----------------------------------------------------------------------===//
< // Populate functions.
< //===----------------------------------------------------------------------===//
< 
< /// Canonicalization patterns relevant to apply after tiling patterns. These
< /// are applied automatically by the tiling pass but need to be applied
< /// manually when tiling is called programmatically.
< RewritePatternSet getLinalgTilingCanonicalizationPatterns(MLIRContext *ctx);
< void populateLinalgTilingCanonicalizationPatterns(RewritePatternSet &patterns);
< 
< /// Linalg generalization patterns
< 
< /// Populates `patterns` with patterns to convert spec-generated named ops to
< /// linalg.generic ops.
< void populateLinalgNamedOpsGeneralizationPatterns(RewritePatternSet &patterns);
< 
< /// Linalg decompose convolutions patterns
< 
< /// Populates patterns to decompose high-D convolution ops into low-D ones.
< /// This is a step in progressive lowering for convolution ops, afterwards we
< /// can vectorize the low-D convolution ops.
< void populateDecomposeConvolutionPatterns(RewritePatternSet &patterns,
<                                           PatternBenefit benefit = 1);
< 
< /// Populates patterns to transform linalg.conv_2d_xxx operations into
< /// linalg.generic (for img2col packing) and linalg.matmul.
< /// \see rewriteInIm2Col for more details.
< void populateConvertConv2DToImg2ColPatterns(RewritePatternSet &patterns);
< 
< void populatePadTensorTilingPatterns(RewritePatternSet &patterns,
<                                      const LinalgTilingOptions &options);
< 
< /// Populates `patterns` with patterns that vectorize tensor.pad.
< /// These patterns are meant to apply in a complementary fashion. Benefits
< /// are used to encode a certain ordering of pattern application. To avoid
< /// scattering magic constants throughout the code base, the patterns must be
< /// added with this function. `baseBenefit` can be used to offset the benefit
< /// of all tensor::PadOp vectorization patterns by a certain value.
< void populatePadOpVectorizationPatterns(RewritePatternSet &patterns,
<                                         PatternBenefit baseBenefit = 1);
< 
< void populateExtractOpVectorizationPatterns(RewritePatternSet &patterns,
<                                             PatternBenefit baseBenefit = 1);
< 
< /// Populate patterns for splitting a `LinalgOp` with multiple statements within
< /// its payload into multiple `GenericOp` that have a single statement.
< /// The option `removeDeadArgsAndResults` adds patterns to remove dead arguments
< /// and results from the generated decomposed ops. This is default `true` since
< /// the core decomposition patterns relies on these clean up patterns. It is set
< /// to false only for testing purposes.
< void populateDecomposeLinalgOpsPattern(RewritePatternSet &patterns,
<                                        bool removeDeadArgsAndResults = true);
< 
< /// Populate patterns that convert non-destination-style ops to destination
< /// style ops.
< void populateConvertToDestinationStylePatterns(RewritePatternSet &patterns);
< 
< /// Populate patterns for vectorizing low-D convolution ops. This is a step in
< /// progressive lowering for convolution ops, it assume high-D convolution ops
< /// were decomposed previously.
< void populateConvolutionVectorizationPatterns(RewritePatternSet &patterns,
<                                               PatternBenefit benefit = 1);
< 
< /// Populate patterns that convert `ElementwiseMappable` ops to linalg
< /// parallel loops.
< void populateElementwiseToLinalgConversionPatterns(RewritePatternSet &patterns);
< 
< /// Populate patterns that are only useful in the context of sparse tensors.
< void populateSparseTensorRewriting(RewritePatternSet &patterns);
< 
< /// Function type which is used to control when to stop fusion. It is expected
< /// that OpOperand is not modified in the callback. The OpOperand is not marked
< /// as const to allow callers to use non-const methods.
< using ControlFusionFn = std::function<bool(OpOperand *fusedOperand)>;
< 
< /// Patterns for fusing linalg operation on tensors.
< 
< /// Pattern to fuse `linalg.generic` -> `linalg.generic` operations
< /// when both operations are fusable elementwise operations.
< void populateElementwiseOpsFusionPatterns(
<     RewritePatternSet &patterns,
<     const ControlFusionFn &controlElementwiseOpFusion);
< 
< /// Function type which is used to control propagation of tensor.pack/unpack
< /// ops.
< using ControlPropagationFn = std::function<bool(Operation *op)>;
< 
< /// Patterns to bubble up or down data layout ops across other operations.
< void populateDataLayoutPropagationPatterns(
<     RewritePatternSet &patterns,
<     const ControlPropagationFn &controlPackUnPackPropagation);
< 
< /// Pattern to remove dead operands and results of `linalg.generic` operations.
< /// This is effectively DCE for a linalg op.
< void populateEraseUnusedOperandsAndResultsPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to promote inputs to outputs and remove unused inputs of
< /// `linalg.generic` ops.
< void populateEraseUnnecessaryInputsPatterns(RewritePatternSet &patterns);
< 
< /// Function type to control generic op dimension collapsing. It is expected
< /// to return an array of `ReassociationIndices` representing dimensions that
< /// should be merged.
< using GetCollapsableDimensionsFn =
<     std::function<SmallVector<ReassociationIndices>(linalg::GenericOp)>;
< 
< /// Pattern to collapse dimensions in a linalg.generic op. This will collapse
< /// tensor operands when needed and expand back the result tensors.
< void populateCollapseDimensions(
<     RewritePatternSet &patterns,
<     const GetCollapsableDimensionsFn &controlCollapseDimensions);
< 
< /// Patterns to fold an expanding (collapsing) tensor_reshape operation with its
< /// producer (consumer) generic operation by expanding the dimensionality of the
< /// loop in the generic op.
< void populateFoldReshapeOpsByExpansionPatterns(
<     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
< 
< /// Patterns to fold an expanding tensor.expand_shape operation with its
< /// producer generic operation by collapsing the dimensions of the generic op.
< void populateFoldReshapeOpsByCollapsingPatterns(
<     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
< 
< /// Patterns to constant fold Linalg operations.
< void populateConstantFoldLinalgOperations(RewritePatternSet &patterns,
<                                           const ControlFusionFn &controlFn);
< 
< /// Pattern to fuse a `tensor.pad` operation with the producer of its source,
< /// if the producer is a `linalg` operation with all parallel iterator types.
< void populateFuseTensorPadWithProducerLinalgOpPatterns(
<     RewritePatternSet &patterns);
< 
< /// Patterns to convert from one named op to another. These can be seen as
< /// canonicalizations of named ops into another named op.
< void populateLinalgNamedOpConversionPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
< /// tensors via reassociative reshape ops.
< void populateFoldUnitExtentDimsViaReshapesPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
< /// tensors via rank-reducing slices.
< void populateFoldUnitExtentDimsViaSlicesPatterns(RewritePatternSet &patterns);
< 
< /// A pattern that converts init operands to input operands.
< void populateMoveInitOperandsToInputPattern(RewritePatternSet &patterns);
< 
< /// Patterns that are used to inline constant operands into linalg generic ops.
< void populateInlineConstantOperandsPatterns(RewritePatternSet &patterns);
< 
< /// Patterns that are used to bubble up extract slice op above linalg op.
< void populateBubbleUpExtractSliceOpPatterns(RewritePatternSet &patterns);
---
> /// Split Reduction options.
> struct SplitReductionOptions {
>   // Ratio used to split the reduction dimension.  If the ratio is <= 1,
>   // nothing will be done.
>   int64_t ratio = 0;
>   // Index where the extra dimension is added to the intermediate tensor
>   // shape.
>   unsigned index = 0;
>   // If the inner dimension after splitting is parallel or reduction.
>   bool innerParallel = false;
> };
1427,1429c1019,1023
< /// Adds patterns that waps tensor.extract_slice(linalg.fill(%cst, %init)) into
< /// linalg.fill(%cst, tensor.extract_slice(%init)).
< void populateSwapExtractSliceWithFillPatterns(RewritePatternSet &patterns);
---
> /// Function signature to control reduction splitting. This returns
> /// `SplitReductionOptions`.
> // TODO: don't use unsigned unless doing bit manipulation.
> using ControlSplitReductionFn =
>     std::function<SplitReductionOptions(LinalgOp op)>;
1435a1030,1138
> 
> /// Apply transformation to split the single linalg op reduction into a
> /// parallel and reduction dimension. Then create a new linalg.generic op
> /// doing the rest of the reduction. Return the new linalg op with an extra
> /// parallel dimension or failure if the transformation didn't happen.
> ///
> /// Example:
> /// ```
> ///  %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
> ///                                        affine_map<(d0) -> ()>],
> ///       iterator_types = ["reduction"]}
> ///  ins(%in : tensor<32xf32>)
> ///  outs(%out : tensor<f32>) {
> ///  ^bb0(%arg1: f32, %arg2: f32):
> ///    %y = arith.addf %arg1, %arg2 : f32
> ///    linalg.yield %y : f32
> ///  } -> tensor<f32>
> /// ```
> /// To:
> /// ```
> ///  %cst = arith.constant 0.000000e+00 : f32
> ///  %0 = tensor.expand_shape %in [[0, 1]] : tensor<32xf32> into
> ///  tensor<4x8xf32> %1 = tensor.empty [4] : tensor<4xf32> %2 = linalg.fill
> ///  ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32> %3 =
> ///  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
> ///                                        affine_map<(d0, d1) -> (d0)>],
> ///    iterator_types = ["parallel", "reduction"]}
> ///    ins(%0 : tensor<4x8xf32>) outs(%2 : tensor<4xf32>) {
> ///    ^bb0(%arg3: f32, %arg5: f32):
> ///    %5 = arith.addf %arg3, %arg4 : f32
> ///    linalg.yield %5 : f32
> ///  } -> tensor<4xf32>
> /// %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
> ///                                       affine_map<(d0) -> ()>],
> ///   iterator_types = ["reduction"]}
> ///   ins(%3 : tensor<4xf32>) outs(%out : tensor<f32>) {
> ///   ^bb0(%arg3: f32, %arg4: f32):
> ///   %5 = arith.addf %arg3, %arg4 : f32
> ///   linalg.yield %5 : f32
> /// } -> tensor<f32>
> /// ```
> struct SplitReductionResult {
>   Operation *initOrAlloc;
>   FillOp fillOp;
>   LinalgOp splitLinalgOp;
>   LinalgOp resultCombiningLinalgOp;
> };
> FailureOr<SplitReductionResult>
> splitReduction(PatternRewriter &b, LinalgOp op,
>                const ControlSplitReductionFn &controlSplitReductionFn,
>                bool useAlloc = false);
> 
> /// Scaling-based implementation of the split reduction transformation.
> /// Instead of introducing an ExpandShapeOp, this rewrites a reduction
> /// dimension `k` into `k * scale + kk`.
> ///
> /// Example:
> /// ```
> ///  %0 = linalg.matmul ins(%A, %B: tensor<16x256xf32>, tensor<256x32xf32>)
> ///    outs(%C: tensor<16x32xf32>) -> tensor<16x32xf32>
> /// ```
> ///
> /// Is transformed to:
> ///
> /// ```
> ///  #map0 = affine_map<(d0, d1, d2, d3) -> (d0, d2 * 4 + d3)>
> ///  #map1 = affine_map<(d0, d1, d2, d3) -> (d2 * 4 + d3, d1)>
> ///  #map2 = affine_map<(d0, d1, d2, d3) -> (d2, d3)>
> ///  #map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
> ///  #map4 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
> ///  #map5 = affine_map<(d0, d1, d2) -> (d0, d1)>
> ///  %0 = tensor.empty [16, 32, 64] : tensor<16x32x64xf32>
> ///  %cst = arith.constant 0.000000e+00 : f32
> ///  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<16x32x64xf32>) ->
> ///     tensor<16x32x64xf32>
> ///  %2 = tensor.empty [64, 4] : tensor<64x4xi1>
> ///
> ///  %3 = linalg.generic {indexing_maps = [#map0, #map1, #map2, #map3],
> ///    iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
> ///    ins(%A, %B, %2 : tensor<16x256xf32>, tensor<256x32xf32>,
> ///    tensor<64x4xi1>)
> ///   outs(%1 : tensor<16x32x64xf32>) {
> ///      ^bb0(%arg3: f32, %arg4: f32, %arg5: i1, %arg6: f32):
> ///        %5 = arith.mulf %arg3, %arg4 : f32
> ///        %6 = arith.addf %arg6, %5 : f32
> ///        linalg.yield %6 : f32
> ///  } -> tensor<16x32x64xf32>
> ///
> ///  %4 = linalg.generic {indexing_maps = [#map4, #map5],
> ///    iterator_types = ["parallel", "parallel", "reduction"]}
> //     ins(%3 : tensor<16x32x64xf32>)
> ///    outs(%C : tensor<16x32xf32>) {
> ///      ^bb0(%arg3: f32, %arg4: f32):
> ///        %5 = arith.addf %arg3, %arg4 : f32
> ///        linalg.yield %5 : f32
> ///  } -> tensor<16x32xf32>
> ///
> ///  return %4 : tensor<16x32xf32>
> /// ```
> FailureOr<SplitReductionResult>
> splitReductionByScaling(PatternRewriter &b, LinalgOp op,
>                         const ControlSplitReductionFn &controlSplitReductionFn,
>                         bool useAlloc = false);
> 
> /// Collapses dimensions of linalg.generic operation. It also collapses inputs
> /// before the op and expands outputs after the op.
> FailureOr<SmallVector<Value>> collapseGenericOpIterationDims(
>     GenericOp genericOp, ArrayRef<ReassociationIndices> foldedIterationDims,
>     RewriterBase &rewriter);
--- include/mlir/Dialect/Linalg/Transforms/HoistPadding.h
--- include/mlir/Dialect/Linalg/Transforms/Hoisting.h
13d12
< class RewriterBase;
17,19d15
< namespace scf {
< class ForOp;
< } // namespace scf
35,37c31
< ///
< /// WARNING: This hoisting does not model parallelism and is generally incorrect
< /// when used on distributed loops with memref semantics!
---
> // TODO: generalize on a per-need basis.
40,140c34,35
< /// Greedily hoist redundant subset extract/insert operations on tensors outside
< /// of `forOp`. The logic follows:
< ///   1. Look for a write walking back from the `forOp` yield.
< ///   2. Check the uses of the matching block argument and look for a matching
< ///      read (i.e. extract_slice of transfer_read) with matching indices.
< ///   3. In the case of a transfer_write, we can bypass other non-conflicting
< ///      operations and find more hoisting opportunities.
< ///   4. Hoist the read/write pair and update the tensor SSA links.
< ///
< /// Return the unmodified `forOp` if no hoisting occured.
< /// Return a new scf::ForOp if hoisting on tensors occured.
< ///
< /// After this transformation the returned scf::ForOp may have unused arguments
< /// that can be removed by application of canonicalization patterns.
< ///
< /// Example:
< /// ========
< /// IR Resembling:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0)->(tensor<10xf32>) {
< ///  %1 = scf.for %j = %l to %u step %s iter_args(%a6 = %a0)->(tensor<10xf32>) {
< ///   %e = tensor.extract_slice %a6[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///   %r = vector.transfer_read %e[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   %w = vector.transfer_write %u, %e[%c0] : vector<4xf32>, tensor<?xf32>
< ///   %st = tensor.insert_slice %w into %a6[%i][%sz][1]
< ///     : tensor<?xf32> into tensor<10xf32>
< ///   scf.yield %st: tensor<10xf32>
< ///  }
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// Progressively hoists to:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %1:2 = scf.for %j = %l to %u step %s iter_args(%a6 = a0, %a7 = %e)
< ///     -> (tensor<10xf32>, tensor<?xf32>) {
< ///   %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   %w = vector.transfer_write %u, %a7[%c0] : vector<4xf32>, tensor<?xf32>
< ///   scf.yield %a6, %w: tensor<10xf32>, tensor<?xf32>
< ///  }
< ///  %st = tensor.insert_slice %1#1 into %1#0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// and
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///  %1:3 = scf.for %j = %l to %u step %s iter_args(%a6 = a0, %a7 = %e, %a7 = r)
< ///     -> (tensor<10xf32>, tensor<?xf32>, vector<4xf32>) {
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   scf.yield %a6, %a7, %u: tensor<10xf32>, tensor<?xf32>, vector<4xf32>
< ///  }
< ///  %w = vector.transfer_write %1#2, %1#1[%c0] : vector<4xf32>, tensor<?xf32>
< ///  %st = tensor.insert_slice %w into %1#0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< /// It can then canonicalize to:
< ///
< /// ```
< /// %0 = scf.for %i = %l to %u step %s iter_args(%a0 = %t0) -> (tensor<10xf32>){
< ///  %e = tensor.extract_slice %a0[%i][%sz][1]: tensor<10xf32> to tensor<?xf32>
< ///  %r = vector.transfer_read %a7[%c0], %cst: tensor<?xf32>, vector<4xf32>
< ///  %1 = scf.for %j = %l to %u step %s iter_args(%a7 = r)
< ///     -> (tensor<10xf32>, tensor<?xf32>, vector<4xf32>) {
< ///   %u = "some_use"(%r) : (vector<4xf32>) -> vector<4xf32>
< ///   scf.yield %u: vector<4xf32>
< ///  }
< ///  %w = vector.transfer_write %1, %e[%c0] : vector<4xf32>, tensor<?xf32>
< ///  %st = tensor.insert_slice %w into %a0[%i][%sz][1]
< ///    : tensor<?xf32> into tensor<10xf32>
< ///  scf.yield %1: tensor<10xf32>
< /// }
< /// ```
< ///
< // TODO: This should be further generalized along a few different axes:
< //   - Other loops than scf.ForOp that operate on tensors (both sequential and
< //     parallel loops).
< //   - Other subset extract/insert pairs than tensor.extract/insert_slice and
< //     vector.transfer_read/write.
< //   - More general areSubsetDisjoint analysis/interface to work across all
< //     subset op types and allow bypassing non-WAW-conflicting operations in
< //     more cases.
< scf::ForOp hoistRedundantSubsetExtractInsert(RewriterBase &rewriter,
<                                              scf::ForOp forOp);
< 
< /// Call into `hoistRedundantSubsetInsertExtract` without a RewriterBase.
< // TODO: obsolete and should be retired
---
> /// Same behavior as `hoistRedundantVectorTransfers` but works on tensors
> /// instead of buffers.
--- include/mlir/Dialect/Linalg/Transforms/TilingInterfaceImpl.h
--- include/mlir/Dialect/Linalg/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/Linalg/Transforms/Transforms.h
24d23
< #include "mlir/Support/LogicalResult.h"
30c29,31
< namespace linalg {
---
> namespace bufferization {
> class BufferizeTypeConverter;
> } // namespace bufferization
32c33
< class LinalgOp;
---
> class FrozenRewritePatternSet;
34,36c35
< //===----------------------------------------------------------------------===//
< // Utils.
< //===----------------------------------------------------------------------===//
---
> namespace linalg {
38,39c37,39
< /// Return vector::CombiningKind for the given op.
< std::optional<vector::CombiningKind> getCombinerOpKind(Operation *combinerOp);
---
> struct LinalgElementwiseFusionOptions;
> struct LinalgFusionOptions;
> struct LinalgTilingOptions;
42c42
< // Structs that configure the behavior of various transformations.
---
> // Transformations exposed as function calls.
43a44
> using LinalgLoops = SmallVector<Operation *, 4>;
45,46c46,47
< using TileSizeComputationFunction =
<     std::function<SmallVector<Value, 4>(OpBuilder &, Operation *)>;
---
> void populatePadTensorTilingPatterns(RewritePatternSet &patterns,
>                                      const LinalgTilingOptions &options);
48,52c49,56
< struct LinalgTilingOptions {
<   /// Computation function that returns the tile sizes for each operation.
<   /// Delayed construction of constant tile sizes should occur to interoperate
<   /// with folding.
<   TileSizeComputationFunction tileSizeComputationFunction = nullptr;
---
> /// Populate patterns for splitting a `LinalgOp` with multiple statements within
> /// its payload into multiple `GenericOp` that have a single statement.
> /// The option `removeDeadArgsAndResults` adds patterns to remove dead arguments
> /// and results from the generated decomposed ops. This is default `true` since
> /// the core decomposition patterns relies on these clean up patterns. It is set
> /// to false only for testing purposes.
> void populateDecomposeLinalgOpsPattern(RewritePatternSet &patterns,
>                                        bool removeDeadArgsAndResults = true);
54,69c58,62
<   LinalgTilingOptions &
<   setTileSizeComputationFunction(TileSizeComputationFunction fun) {
<     tileSizeComputationFunction = std::move(fun);
<     return *this;
<   }
<   /// Set the `tileSizeComputationFunction` to return the values `ts`. The
<   /// values must not fold away when tiling. Otherwise, use a more robust
<   /// `tileSizeComputationFunction`.
<   LinalgTilingOptions &setTileSizes(const SmallVector<Value, 4> &ts) {
<     tileSizeComputationFunction = [=](OpBuilder &, Operation *) { return ts; };
<     return *this;
<   }
<   /// Convenience function to set the `tileSizeComputationFunction` to a
<   /// function that computes tile sizes at the point they are needed. Allows
<   /// proper interaction with folding.
<   LinalgTilingOptions &setTileSizes(ArrayRef<int64_t> ts);
---
> /// Populate patterns for vectorizing low-D convolution ops. This is a step in
> /// progressive lowering for convolution ops, it assume high-D convolution ops
> /// were decomposed previously.
> void populateConvolutionVectorizationPatterns(RewritePatternSet &patterns,
>                                               PatternBenefit benefit = 1);
71,73c64,66
<   /// Tile all dynamic dimensions by 1. I.e., scalarize those dimensions.
<   /// Note: `scalarizeDynamicDims` and `setTileSizes` cannot be used together.
<   LinalgTilingOptions &scalarizeDynamicDims();
---
> /// Populate patterns that convert `ElementwiseMappable` ops to linalg
> /// parallel loops.
> void populateElementwiseToLinalgConversionPatterns(RewritePatternSet &patterns);
75,76c68,69
<   /// The interchange vector to reorder the tiled loops.
<   SmallVector<unsigned, 4> interchangeVector = {};
---
> /// Populate patterns that are only useful in the context of sparse tensors.
> void populateSparseTensorRewriting(RewritePatternSet &patterns);
78,81c71,74
<   LinalgTilingOptions &setInterchange(ArrayRef<unsigned> interchange) {
<     interchangeVector.assign(interchange.begin(), interchange.end());
<     return *this;
<   }
---
> /// Function type which is used to control when to stop fusion. It is expected
> /// that OpOperand is not modified in the callback. The OpOperand is not marked
> /// as const to allow callers to use non-const methods.
> using ControlFusionFn = std::function<bool(OpOperand *fusedOperand)>;
83,84c76
<   /// The type of tile loops to generate.
<   LinalgTilingLoopType loopType = LinalgTilingLoopType::Loops;
---
> /// Patterns for fusing linalg operation on tensors.
86,89c78,82
<   LinalgTilingOptions &setLoopType(LinalgTilingLoopType lt) {
<     loopType = lt;
<     return *this;
<   }
---
> /// Pattern to fuse `linalg.generic` -> `linalg.generic` operations
> /// when both operations are fusable elementwise operations.
> void populateElementwiseOpsFusionPatterns(
>     RewritePatternSet &patterns,
>     const ControlFusionFn &controlElementwiseOpFusion);
91,93c84,85
<   /// When specified, specifies distribution of generated tile loops to
<   /// processors.
<   std::optional<LinalgLoopDistributionOptions> distribution;
---
> /// Patterns to bubble up or down data layout ops across other operations.
> void populateDataLayoutPropagationPatterns(RewritePatternSet &patterns);
95,99c87,89
<   LinalgTilingOptions &
<   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
<     distribution = std::move(distributionOptions);
<     return *this;
<   }
---
> /// Pattern to remove dead operands and results of `linalg.generic` operations.
> /// This is effectively DCE for a linalg op.
> void populateEraseUnusedOperandsAndResultsPatterns(RewritePatternSet &patterns);
101,102c91,93
<   /// Specification markers of how to distribute the `linalg.tiled_loop`.
<   SmallVector<StringRef, 2> distributionTypes = {};
---
> /// Patterns to promote inputs to outputs and remove unused inputs of
> /// `linalg.generic` ops.
> void populateEraseUnnecessaryInputsPatterns(RewritePatternSet &patterns);
104,107c95,99
<   LinalgTilingOptions &setDistributionTypes(ArrayRef<StringRef> types) {
<     distributionTypes.assign(types.begin(), types.end());
<     return *this;
<   }
---
> /// Function type to control generic op dimension collapsing. It is expected
> /// to return an array of `ReassociationIndices` representing dimensions that
> /// should be merged.
> using GetCollapsableDimensionsFn =
>     std::function<SmallVector<ReassociationIndices>(linalg::GenericOp)>;
109,110c101,105
<   /// Peel the specified loops.
<   SmallVector<int64_t> peeledLoops;
---
> /// Pattern to collapse dimensions in a linalg.generic op. This will collapse
> /// tensor operands when needed and expand back the result tensors.
> void populateCollapseDimensions(
>     RewritePatternSet &patterns,
>     const GetCollapsableDimensionsFn &controlCollapseDimensions);
112,117c107,111
<   LinalgTilingOptions &setPeeledLoops(ArrayRef<int64_t> loops) {
<     peeledLoops.clear();
<     peeledLoops.append(loops.begin(), loops.end());
<     return *this;
<   }
< };
---
> /// Patterns to fold an expanding (collapsing) tensor_reshape operation with its
> /// producer (consumer) generic operation by expanding the dimensionality of the
> /// loop in the generic op.
> void populateFoldReshapeOpsByExpansionPatterns(
>     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
119,136c113,116
< struct LinalgTilingAndFusionOptions {
<   /// Tile sizes used to tile the root operation.
<   SmallVector<int64_t> tileSizes;
<   LinalgTilingAndFusionOptions &setTileSizes(ArrayRef<int64_t> ts) {
<     tileSizes.assign(ts.begin(), ts.end());
<     return *this;
<   }
<   /// Tile interchange used to permute the tile loops.
<   SmallVector<int64_t> tileInterchange;
<   /// When specified, specifies distribution of generated tile loops to
<   /// processors.
<   std::optional<LinalgLoopDistributionOptions> tileDistribution;
<   LinalgTilingAndFusionOptions &
<   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
<     tileDistribution = std::move(distributionOptions);
<     return *this;
<   }
< };
---
> /// Patterns to fold an expanding tensor.expand_shape operation with its
> /// producer generic operation by collapsing the dimensions of the generic op.
> void populateFoldReshapeOpsByCollapsingPatterns(
>     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
138,172c118,120
< struct LinalgPaddingOptions {
<   /// A padding value for every operand.
<   SmallVector<Attribute> paddingValues;
<   LinalgPaddingOptions &setPaddingValues(ArrayRef<Attribute> pv) {
<     paddingValues.assign(pv.begin(), pv.end());
<     return *this;
<   }
<   /// A list of iterator dimensions to pad.
<   SmallVector<int64_t> paddingDimensions;
<   LinalgPaddingOptions &setPaddingDimensions(ArrayRef<int64_t> pd) {
<     paddingDimensions.assign(pd.begin(), pd.end());
<     return *this;
<   }
<   /// A flag for every operand to mark the PadOp as nofold which enables
<   /// packing for statically shaped operands.
<   SmallVector<bool> packPaddings;
<   LinalgPaddingOptions &setPackPaddings(ArrayRef<bool> pp) {
<     packPaddings.assign(pp.begin(), pp.end());
<     return *this;
<   }
<   /// A number of loops to hoist the PadOp out for every operand.
<   SmallVector<int64_t> hoistPaddings;
<   LinalgPaddingOptions &setHoistPaddings(ArrayRef<int64_t> hp) {
<     hoistPaddings.assign(hp.begin(), hp.end());
<     return *this;
<   }
<   /// A permutation vector for every operand used to transpose the packed
<   /// PadOp results.
<   SmallVector<SmallVector<int64_t>> transposePaddings;
<   LinalgPaddingOptions &
<   setTransposePaddings(ArrayRef<SmallVector<int64_t>> tp) {
<     transposePaddings.assign(tp.begin(), tp.end());
<     return *this;
<   }
< };
---
> /// Patterns to constant fold Linalg operations.
> void populateConstantFoldLinalgOperations(RewritePatternSet &patterns,
>                                           const ControlFusionFn &controlFn);
174,181c122,125
< /// Callback function type used to perform the allocation for the promoted
< /// `subView`. In `boundingSubViewsize` a best attempt is made to find the
< /// smallest constant value for the size of the buffer needed for each
< /// dimension. If that is not possible, contains the dynamic size of the
< /// subview. The call back should return the buffer to use.
< using AllocBufferCallbackFn = std::function<std::optional<Value>(
<     OpBuilder &b, memref::SubViewOp subView,
<     ArrayRef<Value> boundingSubViewSize, DataLayout &layout)>;
---
> /// Pattern to fuse a `tensor.pad` operation with the producer of its source,
> /// if the producer is a `linalg` operation with all parallel iterator types.
> void populateFuseTensorPadWithProducerLinalgOpPatterns(
>     RewritePatternSet &patterns);
183,186c127,129
< /// Callback function type used to deallocate the buffers used to hold the
< /// promoted subview.
< using DeallocBufferCallbackFn =
<     std::function<LogicalResult(OpBuilder &b, Value buffer)>;
---
> /// Patterns to convert from one named op to another. These can be seen as
> /// canonicalizations of named ops into another named op.
> void populateLinalgNamedOpConversionPatterns(RewritePatternSet &patterns);
188,193c131,133
< /// Callback function type used to insert copy from original subview to
< /// subview of the promoted region for the read operands/subview of promoted
< /// region to original subview for the results. The copy has to happen from
< /// `src` to `dst`.
< using CopyCallbackFn =
<     std::function<LogicalResult(OpBuilder &b, Value src, Value dst)>;
---
> /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
> /// tensors via reassociative reshape ops.
> void populateFoldUnitExtentDimsViaReshapesPatterns(RewritePatternSet &patterns);
195,259c135,137
< struct LinalgPromotionOptions {
<   /// Indices of subViews to promote. If `std::nullopt`, try to promote all
<   /// operands.
<   std::optional<DenseSet<unsigned>> operandsToPromote;
<   LinalgPromotionOptions &setOperandsToPromote(ArrayRef<int64_t> operands) {
<     operandsToPromote = DenseSet<unsigned>();
<     operandsToPromote->insert(operands.begin(), operands.end());
<     return *this;
<   }
<   /// If ith element of `useFullTiles` is true the full view should be used
<   /// for the promoted buffer of the ith operand in `operandsToPromote`.
<   /// Otherwise the partial view will be used. The decision is defaulted to
<   /// `useFullTileBuffersDefault` when `useFullTileBuffers` is std::nullopt and
<   /// for operands missing from `useFullTileBuffers`.
<   std::optional<llvm::SmallBitVector> useFullTileBuffers;
<   LinalgPromotionOptions &setUseFullTileBuffers(ArrayRef<bool> useFullTiles) {
<     unsigned size = useFullTiles.size();
<     llvm::SmallBitVector tmp(size, false);
<     for (unsigned i = 0; i < size; ++i)
<       tmp[i] = useFullTiles[i];
<     useFullTileBuffers = tmp;
<     return *this;
<   }
<   /// If true all operands unspecified by `useFullTileBuffers` will use the
<   /// full view, otherwise the partial view.
<   bool useFullTileBuffersDefault = false;
<   LinalgPromotionOptions &setUseFullTileBuffersByDefault(bool use) {
<     useFullTileBuffersDefault = use;
<     return *this;
<   }
<   /// Alignment of promoted buffer. If `std::nullopt` do not specify alignment.
<   std::optional<unsigned> alignment;
<   LinalgPromotionOptions &setAlignment(unsigned align) {
<     alignment = align;
<     return *this;
<   }
<   /// Use alloca with the default allocation scheme.
<   bool useAlloca = false;
<   LinalgPromotionOptions &setUseAlloca(bool use) {
<     useAlloca = use;
<     return *this;
<   }
<   /// Callback function to do the allocation of the promoted buffer. If
<   /// std::nullopt, then the default allocation scheme of allocating a
<   /// memref<?xi8> buffer followed by a view operation is used.
<   std::optional<AllocBufferCallbackFn> allocationFn;
<   std::optional<DeallocBufferCallbackFn> deallocationFn;
<   LinalgPromotionOptions &
<   setAllocationDeallocationFns(AllocBufferCallbackFn const &allocFn,
<                                DeallocBufferCallbackFn const &deallocFn) {
<     allocationFn = allocFn;
<     deallocationFn = deallocFn;
<     return *this;
<   }
<   /// Callback function to do the copy of data to and from the promoted
<   /// subview. If std::nullopt then a memref.copy is used.
<   std::optional<CopyCallbackFn> copyInFn;
<   std::optional<CopyCallbackFn> copyOutFn;
<   LinalgPromotionOptions &setCopyInOutFns(CopyCallbackFn const &copyIn,
<                                           CopyCallbackFn const &copyOut) {
<     copyInFn = copyIn;
<     copyOutFn = copyOut;
<     return *this;
<   }
< };
---
> /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
> /// tensors via rank-reducing slices.
> void populateFoldUnitExtentDimsViaSlicesPatterns(RewritePatternSet &patterns);
261,271c139,140
< /// Split Reduction options.
< struct SplitReductionOptions {
<   // Ratio used to split the reduction dimension.  If the ratio is <= 1,
<   // nothing will be done.
<   int64_t ratio = 0;
<   // Index where the extra dimension is added to the intermediate tensor
<   // shape.
<   unsigned index = 0;
<   // If the inner dimension after splitting is parallel or reduction.
<   bool innerParallel = false;
< };
---
> /// A pattern that converts init operands to input operands.
> void populateMoveInitOperandsToInputPattern(RewritePatternSet &patterns);
273,277c142,143
< /// Function signature to control reduction splitting. This returns
< /// `SplitReductionOptions`.
< // TODO: don't use unsigned unless doing bit manipulation.
< using ControlSplitReductionFn =
<     std::function<SplitReductionOptions(LinalgOp op)>;
---
> /// Patterns that are used to inline constant operands into linalg generic ops.
> void populateInlineConstantOperandsPatterns(RewritePatternSet &patterns);
279,282c145,150
< //===----------------------------------------------------------------------===//
< // Preconditions that ensure the corresponding transformation succeeds and can
< // be applied as a rewrite pattern.
< //===----------------------------------------------------------------------===//
---
> /// Patterns that are used to bubble up extract slice op above linalg op.
> void populateBubbleUpExtractSliceOpPatterns(RewritePatternSet &patterns);
> 
> /// Adds patterns that waps tensor.extract_slice(linalg.fill(%cst, %init)) into
> /// linalg.fill(%cst, tensor.extract_slice(%init)).
> void populateSwapExtractSliceWithFillPatterns(RewritePatternSet &patterns);
289,333d156
< /// Promote memref.subviews feeding linalg-on-buffers operations.
< LogicalResult promoteSubviewsPrecondition(Operation *op,
<                                           LinalgPromotionOptions options);
< 
< /// Return success if the operation can be vectorized.
< LogicalResult
< vectorizeLinalgOpPrecondition(LinalgOp linalgOp,
<                               ArrayRef<int64_t> inputVectorSizes = {},
<                               bool vectorizeNDExtract = false);
< 
< //===----------------------------------------------------------------------===//
< // Transformations exposed as functional-style API calls.
< //===----------------------------------------------------------------------===//
< 
< using LinalgLoops = SmallVector<Operation *, 4>;
< 
< /// Materialize a buffer allocation for the given tensor.pad op and lower the
< /// op to linalg.fill/linalg.generic + memref.tensor_store. E.g.:
< ///
< /// %0 = tensor.pad low[%l] high[%h] %t ...
< ///
< /// is lowered to:
< ///
< /// %alloc = memref.alloc
< /// linalg.fill ... outs(%alloc)
< /// %subview = memref.subview %alloc [%l] [...] [1]
< /// memref.tensor_store %t, %subview
< /// %0 = bufferization.to_tensor %alloc restrict writable
< ///
< /// In addition to rewriting the IR as shown above, the result of the
< /// bufferization.to_tensor op is returned.
< Value bufferizeToAllocation(RewriterBase &rewriter, tensor::PadOp padOp,
<                             Attribute memorySpace = {});
< 
< /// Materialize a buffer allocation for the given tensor value. E.g.:
< ///
< /// %alloc = memref.alloc
< /// memref.tensor_store %value, %alloc
< /// %0 = bufferization.to_tensor %alloc restrict writable
< ///
< /// In case `value` is a tensor.pad result, the corresponding overload is used
< /// internally to produce a better bufferization.
< Value bufferizeToAllocation(RewriterBase &rewriter, Value value,
<                             Attribute memorySpace = {});
< 
337,449c160,161
< struct ElementwiseOpFusionResult {
<   Operation *fusedOp;
<   llvm::DenseMap<Value, Value> replacements;
< };
< FailureOr<ElementwiseOpFusionResult>
< fuseElementwiseOps(RewriterBase &rewriter, OpOperand *fusedOperand);
< 
< /// Try to peel and canonicalize loop `op` and return the new result.
< /// Also applies affine_min/max bounds simplification on the fly where relevant.
< // TODO: Add support for scf.parallel and affine.for loops.
< SmallVector<Value> peelLoop(RewriterBase &rewriter, Operation *op);
< 
< /// Peel 'loops' and applies affine_min/max bounds simplification on the fly
< /// where relevant.
< void peelLoops(RewriterBase &rewriter, ArrayRef<scf::ForOp> loops);
< 
< /// Pad the iterator dimensions `paddingDimensions` of all `opToPad` operands
< /// to a static bounding box. Use `paddingValues` and `packPaddings` to set
< /// padding value and nofold attribute of the created tensor::PadOps,
< /// respectively. Update `paddedOp` to the cloned operation with statically
< /// shaped `paddingDimensions` and return the extracted dynamically shaped
< /// results. If padding fails, return failure.
< FailureOr<SmallVector<Value>>
< rewriteAsPaddedOp(RewriterBase &rewriter, LinalgOp opToPad,
<                   ArrayRef<int64_t> paddingDimensions,
<                   ArrayRef<Attribute> paddingValues,
<                   ArrayRef<bool> packPaddings, LinalgOp &paddedOp);
< 
< namespace detail {
< 
< /// Helper struct to hold the results of building a packing loop nest.
< struct PackingResult {
<   SmallVector<OpFoldResult> offsets, sizes, strides;
<   SmallVector<Value> clonedLoopIvs, leadingPackedTensorIndexings;
<   GenericOp maybeTransposeOp;
<   tensor::PadOp hoistedPadOp;
< };
< 
< /// Build the packing loop nest required to hoist `opToHoist` above
< /// `outermostEnclosingForOp`.
< /// The loop nest is built just before `outermostEnclosingForOp`.
< FailureOr<PackingResult>
< buildPackingLoopNest(RewriterBase &rewriter, tensor::PadOp opToHoist,
<                      scf::ForOp outermostEnclosingForOp,
<                      ArrayRef<int64_t> transposeVector);
< 
< } // namespace detail
< 
< /// Mechanically hoist padding operations on tensors by `numLoops` into a new,
< /// generally larger tensor. This achieves packing of multiple padding ops into
< /// a larger tensor. On success, `opToHoist` is replaced by the cloned version
< /// in the packing loop so the caller can continue reasoning about the padding
< /// operation. If `transposeVector` is non-empty, hoist padding introduces a
< /// GenericOp to transpose the padded tensor before inserting it into the packed
< /// tensor. A `transposeVector` can change the storage order of the padded
< /// tensor but does not change the order of the pack or compute loops.
< ///
< /// TODO: In the future, we should consider rewriting as a tensor.pack after
< /// hoisting since this abstraction is now available.
< ///
< /// Example in pseudo-mlir:
< /// =======================
< ///
< /// If hoistPaddingOnTensors is called with `nLoops` = 2 on the following IR.
< /// ```
< ///    scf.for (%i, %j, %k)
< ///      %st0 = tensor.extract_slice f(%i, %k) : ... to tensor<?x?xf32>
< ///      %0 = tensor.pad %st0 low[0, 0] high[...] {
< ///      ^bb0( ... ):
< ///        linalg.yield %pad
< ///      } : tensor<?x?xf32> to tensor<4x8xf32>
< ///      compute(%0)
< /// ```
< ///
< /// IR resembling the following is produced:
< ///
< /// ```
< ///    scf.for (%i) {
< ///      %packed_init = tensor.empty range(%j) : tensor<?x4x8xf32>
< ///      %packed = scf.for (%k) iter_args(%p : %packed_init) {
< ///        %st0 = tensor.extract_slice f(%i, %k) : ... to tensor<?x?xf32>
< ///        %0 = tensor.pad %st0 low[0, 0] high[...] {
< ///        ^bb0( ... ):
< ///          linalg.yield %pad
< ///        } : tensor<?x?xf32> to tensor<4x8xf32>
< ///        %1 = tensor.insert_slice %0 ...
< ///            : tensor<4x8xf32> to tensor<?x4x8xf32>
< ///        scf.yield %1: tensor<?x4x8xf32>
< ///      } -> tensor<?x4x8xf32>
< ///      scf.for (%j, %k) {
< ///        %st0 = tensor.extract_slice %packed [%k, 0, 0][1, 4, 8][1, 1, 1] :
< ///                 tensor<?x4x8xf32> to tensor<4x8xf32>
< ///        compute(%st0)
< ///      }
< ///    }
< /// ```
< FailureOr<Value>
< hoistPaddingOnTensors(RewriterBase &rewriter, tensor::PadOp opToHoist,
<                       int64_t numLoops, ArrayRef<int64_t> transposeVector,
<                       tensor::PadOp &hoistedOp,
<                       SmallVectorImpl<GenericOp> &transposeOps);
< /// Calls into `hoistPaddingOnTensors` with a local IRRewriter.
< FailureOr<Value>
< hoistPaddingOnTensors(tensor::PadOp opToHoist, int64_t numLoops,
<                       ArrayRef<int64_t> transposeVector,
<                       tensor::PadOp &hoistedOp,
<                       SmallVectorImpl<GenericOp> &transposeOps);
< 
< /// Apply padding and hoisting to `linalgOp` according to the configuration
< /// specified in `options`.
< FailureOr<LinalgOp> padAndHoistLinalgOp(RewriterBase &rewriter,
<                                         LinalgOp linalgOp,
<                                         LinalgPaddingOptions options);
---
> FailureOr<Operation *> fuseElementwiseOps(RewriterBase &rewriter,
>                                           OpOperand *fusedOperand);
505a218,223
> /// Try to peel anad canonicalize loop `op` and return the new result.
> // TODO: Add support for scf.parallel and affine.for loops.
> SmallVector<Value> peelLoop(RewriterBase &rewriter, Operation *op);
> /// Peel and canonicalize 'loops'.
> void peelLoops(RewriterBase &rewriter, ArrayRef<scf::ForOp> loops);
> 
526a245,331
> /// Callback function type used to perform the allocation for the promoted
> /// `subView`. In `boundingSubViewsize` a best attempt is made to find the
> /// smallest constant value for the size of the buffer needed for each
> /// dimension. If that is not possible, contains the dynamic size of the
> /// subview. The call back should return the buffer to use.
> using AllocBufferCallbackFn = std::function<std::optional<Value>(
>     OpBuilder &b, memref::SubViewOp subView,
>     ArrayRef<Value> boundingSubViewSize, DataLayout &layout)>;
> 
> /// Callback function type used to deallocate the buffers used to hold the
> /// promoted subview.
> using DeallocBufferCallbackFn =
>     std::function<LogicalResult(OpBuilder &b, Value buffer)>;
> 
> /// Callback function type used to insert copy from original subview to
> /// subview of the promoted region for the read operands/subview of promoted
> /// region to original subview for the results. The copy has to happen from
> /// `src` to `dst`.
> using CopyCallbackFn =
>     std::function<LogicalResult(OpBuilder &b, Value src, Value dst)>;
> 
> struct LinalgPromotionOptions {
>   /// Indices of subViews to promote. If `std::nullopt`, try to promote all
>   /// operands.
>   std::optional<DenseSet<unsigned>> operandsToPromote;
>   LinalgPromotionOptions &setOperandsToPromote(ArrayRef<int64_t> operands) {
>     operandsToPromote = DenseSet<unsigned>();
>     operandsToPromote->insert(operands.begin(), operands.end());
>     return *this;
>   }
>   /// If ith element of `useFullTiles` is true the full view should be used
>   /// for the promoted buffer of the ith operand in `operandsToPromote`.
>   /// Otherwise the partial view will be used. The decision is defaulted to
>   /// `useFullTileBuffersDefault` when `useFullTileBuffers` is None and for
>   /// operands missing from `useFullTileBuffers`.
>   std::optional<llvm::SmallBitVector> useFullTileBuffers;
>   LinalgPromotionOptions &setUseFullTileBuffers(ArrayRef<bool> useFullTiles) {
>     unsigned size = useFullTiles.size();
>     llvm::SmallBitVector tmp(size, false);
>     for (unsigned i = 0; i < size; ++i)
>       tmp[i] = useFullTiles[i];
>     useFullTileBuffers = tmp;
>     return *this;
>   }
>   /// If true all operands unspecified by `useFullTileBuffers` will use the
>   /// full view, otherwise the partial view.
>   bool useFullTileBuffersDefault = false;
>   LinalgPromotionOptions &setUseFullTileBuffersByDefault(bool use) {
>     useFullTileBuffersDefault = use;
>     return *this;
>   }
>   /// Alignment of promoted buffer. If `std::nullopt` do not specify alignment.
>   std::optional<unsigned> alignment;
>   LinalgPromotionOptions &setAlignment(unsigned align) {
>     alignment = align;
>     return *this;
>   }
>   /// Use alloca with the default allocation scheme.
>   bool useAlloca = false;
>   LinalgPromotionOptions &setUseAlloca(bool use) {
>     useAlloca = use;
>     return *this;
>   }
>   /// Callback function to do the allocation of the promoted buffer. If
>   /// std::nullopt, then the default allocation scheme of allocating a
>   /// memref<?xi8> buffer followed by a view operation is used.
>   std::optional<AllocBufferCallbackFn> allocationFn;
>   std::optional<DeallocBufferCallbackFn> deallocationFn;
>   LinalgPromotionOptions &
>   setAllocationDeallocationFns(AllocBufferCallbackFn const &allocFn,
>                                DeallocBufferCallbackFn const &deallocFn) {
>     allocationFn = allocFn;
>     deallocationFn = deallocFn;
>     return *this;
>   }
>   /// Callback function to do the copy of data to and from the promoted
>   /// subview. If std::nullopt then a memref.copy is used.
>   std::optional<CopyCallbackFn> copyInFn;
>   std::optional<CopyCallbackFn> copyOutFn;
>   LinalgPromotionOptions &setCopyInOutFns(CopyCallbackFn const &copyIn,
>                                           CopyCallbackFn const &copyOut) {
>     copyInFn = copyIn;
>     copyOutFn = copyOut;
>     return *this;
>   }
> };
> 
553,578d357
< /// Allocate the subview in the GPU workgroup memory.
< std::optional<Value> allocateWorkgroupMemory(OpBuilder &builder,
<                                              memref::SubViewOp subview,
<                                              ArrayRef<Value> sizeBounds,
<                                              DataLayout &);
< 
< /// In case of GPU group memory there is no need to deallocate.
< LogicalResult deallocateWorkgroupMemory(OpBuilder &, Value /*buffer*/);
< 
< /// Create Memref copy operations and add gpu barrier guards before and after
< /// the copy operation to ensure data integrity.
< LogicalResult copyToWorkgroupMemory(OpBuilder &b, Value src, Value dst);
< 
< /// Allocate the subview in the GPU private memory.
< std::optional<Value> allocateGPUPrivateMemory(OpBuilder &builder,
<                                               memref::SubViewOp subview,
<                                               ArrayRef<Value> sizeBounds,
<                                               DataLayout &);
< 
< /// Normal copy to between src and dst.
< LogicalResult copyToGPUPrivateMemory(OpBuilder &b, Value src, Value dst);
< 
< /// In case of GPU private memory there is no need to deallocate since the
< /// memory is freed when going outside of the scope.
< LogicalResult deallocateGPUPrivateMemory(OpBuilder &, Value /*buffer*/);
< 
592,598d370
< /// Vectorize a `padOp` with (1) static result type, (2) constant padding value
< /// and (3) all-zero lowPad to
< ///   `transfer_write_in_bounds(transfer_read_masked(pad_source, pad_value))`.
< FailureOr<vector::TransferWriteOp>
< maskedVectorize(RewriterBase &rewriter, tensor::PadOp padOp,
<                 ArrayRef<int64_t> inputVectorSizes);
< 
600c372
< FailureOr<LinalgLoops> linalgOpToLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToLoops(PatternRewriter &rewriter,
604c376
< FailureOr<LinalgLoops> linalgOpToParallelLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToParallelLoops(PatternRewriter &rewriter,
608c380
< FailureOr<LinalgLoops> linalgOpToAffineLoops(RewriterBase &rewriter,
---
> FailureOr<LinalgLoops> linalgOpToAffineLoops(PatternRewriter &rewriter,
610a383,403
> //===----------------------------------------------------------------------===//
> // Preconditions that ensure the corresponding transformation succeeds and can
> // be applied as a rewrite pattern.
> //===----------------------------------------------------------------------===//
> /// Promote memref.subviews feeding linalg-on-buffers operations.
> LogicalResult promoteSubviewsPrecondition(Operation *op,
>                                           LinalgPromotionOptions options);
> 
> /// Return success if the operation can be vectorized.
> LogicalResult
> vectorizeLinalgOpPrecondition(LinalgOp linalgOp,
>                               ArrayRef<int64_t> inputVectorSizes = {},
>                               bool vectorizeNDExtract = false);
> 
> //===----------------------------------------------------------------------===//
> // Transformations exposed as rewrite patterns.
> //===----------------------------------------------------------------------===//
> 
> using TileSizeComputationFunction =
>     std::function<SmallVector<Value, 4>(OpBuilder &, Operation *)>;
> 
679c472
< /// Rewrite a TilingInterface `op` to a tiled `scf.forall`, applying
---
> /// Rewrite a TilingInterface `op` to a tiled `scf.foreach_thread`, applying
682c475
< /// resulting `scf.forall`.
---
> /// resulting `scf.foreach_thread`.
687c480
< struct ForallTilingResult {
---
> struct ForeachThreadTilingResult {
691,694c484,487
< FailureOr<ForallTilingResult> tileToForallOp(RewriterBase &builder,
<                                              TilingInterface op,
<                                              ArrayRef<OpFoldResult> numThreads,
<                                              std::optional<ArrayAttr> mapping);
---
> FailureOr<ForeachThreadTilingResult>
> tileToForeachThreadOp(RewriterBase &builder, TilingInterface op,
>                       ArrayRef<OpFoldResult> numThreads,
>                       std::optional<ArrayAttr> mapping);
696c489
< /// Same as `tileToForallOp`, but calculate the number of threads
---
> /// Same as `tileToForeachThreadOp`, but calculate the number of threads
698,701c491,494
< FailureOr<ForallTilingResult>
< tileToForallOpUsingTileSizes(RewriterBase &builder, TilingInterface op,
<                              ArrayRef<OpFoldResult> tileSizes,
<                              std::optional<ArrayAttr> mapping);
---
> FailureOr<ForeachThreadTilingResult>
> tileToForeachThreadOpUsingTileSizes(RewriterBase &builder, TilingInterface op,
>                                     ArrayRef<OpFoldResult> tileSizes,
>                                     std::optional<ArrayAttr> mapping);
704c497
< struct ForallReductionTilingResult {
---
> struct ForeachThreadReductionTilingResult {
711,712c504,505
<   /// The `scf.forall` operation that iterate over the tiles.
<   scf::ForallOp loops;
---
>   /// The `scf.foreach_thread` operation that iterate over the tiles.
>   scf::ForeachThreadOp loops;
728c521
< /// %1 = scf.forall (%iv) in (%c4) shared_outs(%arg0 = %0)
---
> /// %1 = scf.foreach_thread (%iv) in (%c4) shared_outs(%arg0 = %0)
739,743c532,535
< FailureOr<ForallReductionTilingResult>
< tileReductionUsingForall(RewriterBase &b, PartialReductionOpInterface op,
<                          ArrayRef<OpFoldResult> numThreads,
<                          ArrayRef<OpFoldResult> tileSizes = {},
<                          std::optional<ArrayAttr> mapping = std::nullopt);
---
> FailureOr<ForeachThreadReductionTilingResult> tileReductionUsingForeachThread(
>     RewriterBase &b, PartialReductionOpInterface op,
>     ArrayRef<OpFoldResult> numThreads, ArrayRef<OpFoldResult> tileSizes = {},
>     std::optional<ArrayAttr> mapping = std::nullopt);
801,845c593,626
< /// Apply transformation to split the single linalg op reduction into a
< /// parallel and reduction dimension. Then create a new linalg.generic op
< /// doing the rest of the reduction. Return the new linalg op with an extra
< /// parallel dimension or failure if the transformation didn't happen.
< ///
< /// Example:
< /// ```
< ///  %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
< ///                                        affine_map<(d0) -> ()>],
< ///       iterator_types = ["reduction"]}
< ///  ins(%in : tensor<32xf32>)
< ///  outs(%out : tensor<f32>) {
< ///  ^bb0(%arg1: f32, %arg2: f32):
< ///    %y = arith.addf %arg1, %arg2 : f32
< ///    linalg.yield %y : f32
< ///  } -> tensor<f32>
< /// ```
< /// To:
< /// ```
< ///  %cst = arith.constant 0.000000e+00 : f32
< ///  %0 = tensor.expand_shape %in [[0, 1]] : tensor<32xf32> into
< ///  tensor<4x8xf32> %1 = tensor.empty [4] : tensor<4xf32> %2 = linalg.fill
< ///  ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32> %3 =
< ///  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
< ///                                        affine_map<(d0, d1) -> (d0)>],
< ///    iterator_types = ["parallel", "reduction"]}
< ///    ins(%0 : tensor<4x8xf32>) outs(%2 : tensor<4xf32>) {
< ///    ^bb0(%arg3: f32, %arg5: f32):
< ///    %5 = arith.addf %arg3, %arg4 : f32
< ///    linalg.yield %5 : f32
< ///  } -> tensor<4xf32>
< /// %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
< ///                                       affine_map<(d0) -> ()>],
< ///   iterator_types = ["reduction"]}
< ///   ins(%3 : tensor<4xf32>) outs(%out : tensor<f32>) {
< ///   ^bb0(%arg3: f32, %arg4: f32):
< ///   %5 = arith.addf %arg3, %arg4 : f32
< ///   linalg.yield %5 : f32
< /// } -> tensor<f32>
< /// ```
< struct SplitReductionResult {
<   Operation *initOrAlloc;
<   FillOp fillOp;
<   LinalgOp splitLinalgOp;
<   LinalgOp resultCombiningLinalgOp;
---
> struct LinalgPaddingOptions {
>   /// A padding value for every operand.
>   SmallVector<Attribute> paddingValues;
>   LinalgPaddingOptions &setPaddingValues(ArrayRef<Attribute> pv) {
>     paddingValues.assign(pv.begin(), pv.end());
>     return *this;
>   }
>   /// A list of iterator dimensions to pad.
>   SmallVector<int64_t> paddingDimensions;
>   LinalgPaddingOptions &setPaddingDimensions(ArrayRef<int64_t> pd) {
>     paddingDimensions.assign(pd.begin(), pd.end());
>     return *this;
>   }
>   /// A flag for every operand to mark the PadOp as nofold which enables
>   /// packing for statically shaped operands.
>   SmallVector<bool> packPaddings;
>   LinalgPaddingOptions &setPackPaddings(ArrayRef<bool> pp) {
>     packPaddings.assign(pp.begin(), pp.end());
>     return *this;
>   }
>   /// A number of loops to hoist the PadOp out for every operand.
>   SmallVector<int64_t> hoistPaddings;
>   LinalgPaddingOptions &setHoistPaddings(ArrayRef<int64_t> hp) {
>     hoistPaddings.assign(hp.begin(), hp.end());
>     return *this;
>   }
>   /// A permutation vector for every operand used to transpose the packed
>   /// PadOp results.
>   SmallVector<SmallVector<int64_t>> transposePaddings;
>   LinalgPaddingOptions &
>   setTransposePaddings(ArrayRef<SmallVector<int64_t>> tp) {
>     transposePaddings.assign(tp.begin(), tp.end());
>     return *this;
>   }
847,850d627
< FailureOr<SplitReductionResult>
< splitReduction(RewriterBase &b, LinalgOp op,
<                const ControlSplitReductionFn &controlSplitReductionFn,
<                bool useAlloc = false);
852,902c629,646
< /// Scaling-based implementation of the split reduction transformation.
< /// Instead of introducing an ExpandShapeOp, this rewrites a reduction
< /// dimension `k` into `k * scale + kk`.
< ///
< /// Example:
< /// ```
< ///  %0 = linalg.matmul ins(%A, %B: tensor<16x256xf32>, tensor<256x32xf32>)
< ///    outs(%C: tensor<16x32xf32>) -> tensor<16x32xf32>
< /// ```
< ///
< /// Is transformed to:
< ///
< /// ```
< ///  #map0 = affine_map<(d0, d1, d2, d3) -> (d0, d2 * 4 + d3)>
< ///  #map1 = affine_map<(d0, d1, d2, d3) -> (d2 * 4 + d3, d1)>
< ///  #map2 = affine_map<(d0, d1, d2, d3) -> (d2, d3)>
< ///  #map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
< ///  #map4 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
< ///  #map5 = affine_map<(d0, d1, d2) -> (d0, d1)>
< ///  %0 = tensor.empty [16, 32, 64] : tensor<16x32x64xf32>
< ///  %cst = arith.constant 0.000000e+00 : f32
< ///  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<16x32x64xf32>) ->
< ///     tensor<16x32x64xf32>
< ///  %2 = tensor.empty [64, 4] : tensor<64x4xi1>
< ///
< ///  %3 = linalg.generic {indexing_maps = [#map0, #map1, #map2, #map3],
< ///    iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
< ///    ins(%A, %B, %2 : tensor<16x256xf32>, tensor<256x32xf32>,
< ///    tensor<64x4xi1>)
< ///   outs(%1 : tensor<16x32x64xf32>) {
< ///      ^bb0(%arg3: f32, %arg4: f32, %arg5: i1, %arg6: f32):
< ///        %5 = arith.mulf %arg3, %arg4 : f32
< ///        %6 = arith.addf %arg6, %5 : f32
< ///        linalg.yield %6 : f32
< ///  } -> tensor<16x32x64xf32>
< ///
< ///  %4 = linalg.generic {indexing_maps = [#map4, #map5],
< ///    iterator_types = ["parallel", "parallel", "reduction"]}
< //     ins(%3 : tensor<16x32x64xf32>)
< ///    outs(%C : tensor<16x32xf32>) {
< ///      ^bb0(%arg3: f32, %arg4: f32):
< ///        %5 = arith.addf %arg3, %arg4 : f32
< ///        linalg.yield %5 : f32
< ///  } -> tensor<16x32xf32>
< ///
< ///  return %4 : tensor<16x32xf32>
< /// ```
< FailureOr<SplitReductionResult>
< splitReductionByScaling(RewriterBase &b, LinalgOp op,
<                         const ControlSplitReductionFn &controlSplitReductionFn,
<                         bool useAlloc = false);
---
> struct LinalgTilingAndFusionOptions {
>   /// Tile sizes used to tile the root operation.
>   SmallVector<int64_t> tileSizes;
>   LinalgTilingAndFusionOptions &setTileSizes(ArrayRef<int64_t> ts) {
>     tileSizes.assign(ts.begin(), ts.end());
>     return *this;
>   }
>   /// Tile interchange used to permute the tile loops.
>   SmallVector<int64_t> tileInterchange;
>   /// When specified, specifies distribution of generated tile loops to
>   /// processors.
>   std::optional<LinalgLoopDistributionOptions> tileDistribution;
>   LinalgTilingAndFusionOptions &
>   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
>     tileDistribution = std::move(distributionOptions);
>     return *this;
>   }
> };
904,908c648,652
< /// Collapses dimensions of linalg.generic operation. It also collapses inputs
< /// before the op and expands outputs after the op.
< FailureOr<SmallVector<Value>> collapseGenericOpIterationDims(
<     GenericOp genericOp, ArrayRef<ReassociationIndices> foldedIterationDims,
<     RewriterBase &rewriter);
---
> struct LinalgTilingOptions {
>   /// Computation function that returns the tile sizes for each operation.
>   /// Delayed construction of constant tile sizes should occur to interoperate
>   /// with folding.
>   TileSizeComputationFunction tileSizeComputationFunction = nullptr;
910,1027c654,669
< struct LowerPackResult {
<   tensor::PadOp padOp;
<   tensor::ExpandShapeOp expandShapeOp;
<   linalg::TransposeOp transposeOp;
< };
< 
< /// Rewrite pack as pad + reshape + transpose.
< FailureOr<LowerPackResult> lowerPack(RewriterBase &rewriter,
<                                      tensor::PackOp packOp);
< 
< struct LowerUnPackOpResult {
<   tensor::EmptyOp emptyOp;
<   linalg::TransposeOp transposeOp;
<   tensor::CollapseShapeOp collapseShapeOp;
<   tensor::ExtractSliceOp extractSliceOp;
< };
< 
< /// Rewrite pack as empty + transpose + reshape + extract_slice.
< FailureOr<LowerUnPackOpResult> lowerUnPack(RewriterBase &rewriter,
<                                            tensor::UnPackOp unPackOp);
< 
< /// Struct to hold the result of a `pack` call.
< struct PackResult {
<   SmallVector<tensor::PackOp> packOps;
<   linalg::LinalgOp packedLinalgOp;
<   SmallVector<tensor::UnPackOp> unPackOps;
< };
< /// Implement packing of a single LinalgOp by `packedSizes`.
< /// There must be one packedSizes entry per `linalgOp` iterator.
< /// Return the packed Linalg op on success, failure otherwise.
< FailureOr<PackResult> pack(RewriterBase &rewriter, linalg::LinalgOp linalgOp,
<                            ArrayRef<OpFoldResult> packedSizes);
< 
< /// Struct to hold the result of a `packTranspose` call.
< struct PackTransposeResult {
<   tensor::PackOp transposedPackOp;
<   linalg::LinalgOp transposedLinalgOp;
<   tensor::UnPackOp transposedUnPackOp;
< };
< /// Transpose a single PackOp -> LinalgOp -> UnPackOp chain and return the
< /// transposed PackOp -> LinalgOp -> UnPackOp chain after replacements.
< /// Return failure if either:
< ///   1. the `packOp` does not have the `linalgOp` as its unique use.
< ///   2. the `maybeUnPackOp`, if specified must be a consumer of the result tied
< ///      to the unique `packOp` use.
< ///   3. `outerPerm` (resp. `innerPerm`) must be valid permutations of
< ///      `packOp.getOuterDimsPerm` (resp. `packOp.getInnerDimsPerm`) or empty.
< FailureOr<PackTransposeResult>
< packTranspose(RewriterBase &rewriter, tensor::PackOp packOp,
<               linalg::LinalgOp linalgOp, tensor::UnPackOp maybeUnPackOp,
<               ArrayRef<int64_t> outerPerm, ArrayRef<int64_t> innerPerm);
< 
< /// Rewrite tensor.from_elements to linalg.generic.
< FailureOr<Operation *>
< rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                  tensor::FromElementsOp fromElementsOp);
< 
< /// Rewrite tensor.generate to linalg.generic.
< FailureOr<Operation *>
< rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                  tensor::GenerateOp generateOp);
< 
< /// Rewrite tensor.pad to linalg.generic + tensor.insert_slice.
< FailureOr<Operation *> rewriteInDestinationPassingStyle(RewriterBase &rewriter,
<                                                         tensor::PadOp padOp);
< 
< /// Convert linalg.conv_2d_nhwc_hwcf into linalg.generic (for img2col packing)
< /// and linalg.matmul.
< ///
< /// A convolution operation can be written as a matrix-matrix multiplication by
< /// unfolding the cross-correlation between input and filter and explicitly copy
< /// overlapped sliding window inputs.
< ///
< /// Consider 2D input X with single channel input and output and 2x2 filter W:
< /// [x(0, 0)  , x(0, 1)  , ...,   x(0, n)  ]
< /// [x(1, 0)  , x(1, 1)  , ...,   x(1, n)  ]
< /// [.        ,  .       ,.   ,      .     ]            [w(0, 0), w(0, 1)]
< /// [.        ,  .       , .  ,      .     ]    (conv)  [w(1, 0), w(1, 1)]
< /// [.        ,  .       ,   .,      .     ]
< /// [x(n-1, 0), x(n-1, 1), ..., x(n-1, n-1)]
< ///
< /// The packed input data (img2col) is a matrix with |rows| = output spatial
< /// size, |columns| = filter spatial size. To compute the output Y(i, j) we need
< /// to calculate the dot product between filter window at input X(x, y)) and the
< /// filter which will look like the following where r.h.s is the img2col matrix
< /// and l.h.s is the flattened filter:
< ///
< /// [x(0,0), x(0,1), x(1,0), x(1,1)]
< /// [x(0,1), x(1,1), x(0,2), x(1,2)] (matmul) [w(0,0), w(0,1), w(1,0), w(1,1)]
< /// [x(0,1), x(1,1), x(0,2), x(1,2)]
< /// [   .  ,    .  ,    .  ,    .  ]
< ///
< /// In general for 2D case with (N, H, W, C) input and (Kh, Kw, C, D) filter
< /// and output (N, Ho, Wo, D) the convolution is the following matrix-matrix
< /// multiplication (Ho x Wo, Kh x Kw x C) * (Kh x Kw x C, D) for each input in
< /// the N input. For the case where N > 1 its a batched matrix-matrix
< /// multiplication.
< ///
< /// On success, return both the operation that produces the img2col tensor and
< /// the final operation of the sequence that replaces the original convolution.
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter, linalg::Conv2DNhwcHwcfOp convOp);
< 
< /// Similar to rewriteInIm2Col with linalg::Conv2DNhwcHwcfOp except there is no
< /// reduction among the input channels so each convolution can be a
< /// matrix-vector product and by transposing both input filter so channels are
< /// outer most the computation is a batched matrix-vector product.
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter,
<                 linalg::DepthwiseConv2DNhwcHwcOp convOp);
< 
< /// Similar to rewriteInIm2Col with linalg::Conv2DNhwcHwcfOp except because the
< /// channels are to the left of the image shape dimensions, the position of the
< /// contraction dimension in the resulting matmul is reversed. This swaps the
< /// LHS and RHS of the matmul when compared with nhwc (i.e. (D, C x Kh x Kw) *
< /// (C x Kh x Kw, Ho x Wo))
< FailureOr<std::pair<Operation *, Operation *>>
< rewriteInIm2Col(RewriterBase &rewriter, linalg::Conv2DNchwFchwOp convOp);
---
>   LinalgTilingOptions &
>   setTileSizeComputationFunction(TileSizeComputationFunction fun) {
>     tileSizeComputationFunction = std::move(fun);
>     return *this;
>   }
>   /// Set the `tileSizeComputationFunction` to return the values `ts`. The
>   /// values must not fold away when tiling. Otherwise, use a more robust
>   /// `tileSizeComputationFunction`.
>   LinalgTilingOptions &setTileSizes(const SmallVector<Value, 4> &ts) {
>     tileSizeComputationFunction = [=](OpBuilder &, Operation *) { return ts; };
>     return *this;
>   }
>   /// Convenience function to set the `tileSizeComputationFunction` to a
>   /// function that computes tile sizes at the point they are needed. Allows
>   /// proper interaction with folding.
>   LinalgTilingOptions &setTileSizes(ArrayRef<int64_t> ts);
1029,1033c671,723
< //===----------------------------------------------------------------------===//
< // Rewrite patterns wrapping transformations.
< // TODO: every single such pattern should be a close to noop wrapper around a
< // functional-stye API call.
< //===----------------------------------------------------------------------===//
---
>   /// Tile all dynamic dimensions by 1. I.e., scalarize those dimensions.
>   /// Note: `scalarizeDynamicDims` and `setTileSizes` cannot be used together.
>   LinalgTilingOptions &scalarizeDynamicDims();
> 
>   /// The interchange vector to reorder the tiled loops.
>   SmallVector<unsigned, 4> interchangeVector = {};
> 
>   LinalgTilingOptions &setInterchange(ArrayRef<unsigned> interchange) {
>     interchangeVector.assign(interchange.begin(), interchange.end());
>     return *this;
>   }
> 
>   /// The type of tile loops to generate.
>   LinalgTilingLoopType loopType = LinalgTilingLoopType::Loops;
> 
>   LinalgTilingOptions &setLoopType(LinalgTilingLoopType lt) {
>     loopType = lt;
>     return *this;
>   }
> 
>   /// When specified, specifies distribution of generated tile loops to
>   /// processors.
>   std::optional<LinalgLoopDistributionOptions> distribution;
> 
>   LinalgTilingOptions &
>   setDistributionOptions(LinalgLoopDistributionOptions distributionOptions) {
>     distribution = std::move(distributionOptions);
>     return *this;
>   }
> 
>   /// Specification markers of how to distribute the `linalg.tiled_loop`.
>   SmallVector<StringRef, 2> distributionTypes = {};
> 
>   LinalgTilingOptions &setDistributionTypes(ArrayRef<StringRef> types) {
>     distributionTypes.assign(types.begin(), types.end());
>     return *this;
>   }
> 
>   /// Peel the specified loops.
>   SmallVector<int64_t> peeledLoops;
> 
>   LinalgTilingOptions &setPeeledLoops(ArrayRef<int64_t> loops) {
>     peeledLoops.clear();
>     peeledLoops.append(loops.begin(), loops.end());
>     return *this;
>   }
> };
> 
> /// Canonicalization patterns relevant to apply after tiling patterns. These
> /// are applied automatically by the tiling pass but need to be applied
> /// manually when tiling is called programmatically.
> RewritePatternSet getLinalgTilingCanonicalizationPatterns(MLIRContext *ctx);
> void populateLinalgTilingCanonicalizationPatterns(RewritePatternSet &patterns);
1044a735,739
>   /// `matchAndRewrite` implementation that returns the significant
>   /// transformed pieces of IR.
>   FailureOr<LinalgOp> returningMatchAndRewrite(LinalgOp op,
>                                                PatternRewriter &rewriter) const;
> 
1046c741,743
<                                 PatternRewriter &rewriter) const override;
---
>                                 PatternRewriter &rewriter) const override {
>     return returningMatchAndRewrite(op, rewriter);
>   }
1092,1104d788
< struct DownscaleConv2DOp final : public OpRewritePattern<Conv2DOp> {
<   DownscaleConv2DOp(MLIRContext *context, PatternBenefit benefit = 1)
<       : OpRewritePattern<Conv2DOp>(context, benefit) {}
< 
<   FailureOr<Conv1DOp> returningMatchAndRewrite(Conv2DOp convOp,
<                                                PatternRewriter &rewriter) const;
< 
<   LogicalResult matchAndRewrite(Conv2DOp convOp,
<                                 PatternRewriter &rewriter) const override {
<     return returningMatchAndRewrite(convOp, rewriter);
<   }
< };
< 
1137a822,846
> /// Return vector::CombiningKind for the given op.
> std::optional<vector::CombiningKind> getCombinerOpKind(Operation *combinerOp);
> 
> //===----------------------------------------------------------------------===//
> // Transformations exposed as rewrite patterns.
> //===----------------------------------------------------------------------===//
> 
> /// Linalg generalization patterns
> 
> /// Populates `patterns` with patterns to convert spec-generated named ops to
> /// linalg.generic ops.
> void populateLinalgNamedOpsGeneralizationPatterns(RewritePatternSet &patterns);
> 
> /// Linalg decompose convolutions patterns
> 
> /// Populates patterns to decompose high-D convolution ops into low-D ones.
> /// This is a step in progressive lowering for convolution ops, afterwards we
> /// can vectorize the low-D convolution ops.
> void populateDecomposeConvolutionPatterns(RewritePatternSet &patterns,
>                                           PatternBenefit benefit = 1);
> 
> //===----------------------------------------------------------------------===//
> // Op-specific patterns.
> //===----------------------------------------------------------------------===//
> 
1146a856,867
> /// Pad the iterator dimensions `paddingDimensions` of all `opToPad` operands
> /// to a static bounding box. Use `paddingValues` and `packPaddings` to set
> /// padding value and nofold attribute of the created tensor::PadOps,
> /// respectively. Update `paddedOp` to the cloned operation with statically
> /// shaped `paddingDimensions` and return the extracted dynamically shaped
> /// results. If padding fails, return failure.
> FailureOr<SmallVector<Value>>
> rewriteAsPaddedOp(OpBuilder &b, LinalgOp opToPad,
>                   ArrayRef<int64_t> paddingDimensions,
>                   ArrayRef<Attribute> paddingValues,
>                   ArrayRef<bool> packPaddings, LinalgOp &paddedOp);
> 
1148c869
<     std::function<LogicalResult(RewriterBase &, tensor::PadOp, Value)>;
---
>     std::function<LogicalResult(PatternRewriter &, tensor::PadOp, Value)>;
1164c885
<   Value createFillOrGenerateOp(RewriterBase &rewriter, tensor::PadOp padOp,
---
>   Value createFillOrGenerateOp(PatternRewriter &rewriter, tensor::PadOp padOp,
1188a910,921
> /// Populates `patterns` with patterns that vectorize tensor.pad.
> /// These patterns are meant to apply in a complementary fashion. Benefits
> /// are used to encode a certain ordering of pattern application. To avoid
> /// scattering magic constants throughout the code base, the patterns must be
> /// added with this function. `baseBenefit` can be used to offset the benefit
> /// of all tensor::PadOp vectorization patterns by a certain value.
> void populatePadOpVectorizationPatterns(RewritePatternSet &patterns,
>                                         PatternBenefit baseBenefit = 1);
> 
> void populateExtractOpVectorizationPatterns(RewritePatternSet &patterns,
>                                             PatternBenefit baseBenefit = 1);
> 
1274,1425c1007,1017
< //===----------------------------------------------------------------------===//
< // Populate functions.
< //===----------------------------------------------------------------------===//
< 
< /// Canonicalization patterns relevant to apply after tiling patterns. These
< /// are applied automatically by the tiling pass but need to be applied
< /// manually when tiling is called programmatically.
< RewritePatternSet getLinalgTilingCanonicalizationPatterns(MLIRContext *ctx);
< void populateLinalgTilingCanonicalizationPatterns(RewritePatternSet &patterns);
< 
< /// Linalg generalization patterns
< 
< /// Populates `patterns` with patterns to convert spec-generated named ops to
< /// linalg.generic ops.
< void populateLinalgNamedOpsGeneralizationPatterns(RewritePatternSet &patterns);
< 
< /// Linalg decompose convolutions patterns
< 
< /// Populates patterns to decompose high-D convolution ops into low-D ones.
< /// This is a step in progressive lowering for convolution ops, afterwards we
< /// can vectorize the low-D convolution ops.
< void populateDecomposeConvolutionPatterns(RewritePatternSet &patterns,
<                                           PatternBenefit benefit = 1);
< 
< /// Populates patterns to transform linalg.conv_2d_xxx operations into
< /// linalg.generic (for img2col packing) and linalg.matmul.
< /// \see rewriteInIm2Col for more details.
< void populateConvertConv2DToImg2ColPatterns(RewritePatternSet &patterns);
< 
< void populatePadTensorTilingPatterns(RewritePatternSet &patterns,
<                                      const LinalgTilingOptions &options);
< 
< /// Populates `patterns` with patterns that vectorize tensor.pad.
< /// These patterns are meant to apply in a complementary fashion. Benefits
< /// are used to encode a certain ordering of pattern application. To avoid
< /// scattering magic constants throughout the code base, the patterns must be
< /// added with this function. `baseBenefit` can be used to offset the benefit
< /// of all tensor::PadOp vectorization patterns by a certain value.
< void populatePadOpVectorizationPatterns(RewritePatternSet &patterns,
<                                         PatternBenefit baseBenefit = 1);
< 
< void populateExtractOpVectorizationPatterns(RewritePatternSet &patterns,
<                                             PatternBenefit baseBenefit = 1);
< 
< /// Populate patterns for splitting a `LinalgOp` with multiple statements within
< /// its payload into multiple `GenericOp` that have a single statement.
< /// The option `removeDeadArgsAndResults` adds patterns to remove dead arguments
< /// and results from the generated decomposed ops. This is default `true` since
< /// the core decomposition patterns relies on these clean up patterns. It is set
< /// to false only for testing purposes.
< void populateDecomposeLinalgOpsPattern(RewritePatternSet &patterns,
<                                        bool removeDeadArgsAndResults = true);
< 
< /// Populate patterns that convert non-destination-style ops to destination
< /// style ops.
< void populateConvertToDestinationStylePatterns(RewritePatternSet &patterns);
< 
< /// Populate patterns for vectorizing low-D convolution ops. This is a step in
< /// progressive lowering for convolution ops, it assume high-D convolution ops
< /// were decomposed previously.
< void populateConvolutionVectorizationPatterns(RewritePatternSet &patterns,
<                                               PatternBenefit benefit = 1);
< 
< /// Populate patterns that convert `ElementwiseMappable` ops to linalg
< /// parallel loops.
< void populateElementwiseToLinalgConversionPatterns(RewritePatternSet &patterns);
< 
< /// Populate patterns that are only useful in the context of sparse tensors.
< void populateSparseTensorRewriting(RewritePatternSet &patterns);
< 
< /// Function type which is used to control when to stop fusion. It is expected
< /// that OpOperand is not modified in the callback. The OpOperand is not marked
< /// as const to allow callers to use non-const methods.
< using ControlFusionFn = std::function<bool(OpOperand *fusedOperand)>;
< 
< /// Patterns for fusing linalg operation on tensors.
< 
< /// Pattern to fuse `linalg.generic` -> `linalg.generic` operations
< /// when both operations are fusable elementwise operations.
< void populateElementwiseOpsFusionPatterns(
<     RewritePatternSet &patterns,
<     const ControlFusionFn &controlElementwiseOpFusion);
< 
< /// Function type which is used to control propagation of tensor.pack/unpack
< /// ops.
< using ControlPropagationFn = std::function<bool(Operation *op)>;
< 
< /// Patterns to bubble up or down data layout ops across other operations.
< void populateDataLayoutPropagationPatterns(
<     RewritePatternSet &patterns,
<     const ControlPropagationFn &controlPackUnPackPropagation);
< 
< /// Pattern to remove dead operands and results of `linalg.generic` operations.
< /// This is effectively DCE for a linalg op.
< void populateEraseUnusedOperandsAndResultsPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to promote inputs to outputs and remove unused inputs of
< /// `linalg.generic` ops.
< void populateEraseUnnecessaryInputsPatterns(RewritePatternSet &patterns);
< 
< /// Function type to control generic op dimension collapsing. It is expected
< /// to return an array of `ReassociationIndices` representing dimensions that
< /// should be merged.
< using GetCollapsableDimensionsFn =
<     std::function<SmallVector<ReassociationIndices>(linalg::GenericOp)>;
< 
< /// Pattern to collapse dimensions in a linalg.generic op. This will collapse
< /// tensor operands when needed and expand back the result tensors.
< void populateCollapseDimensions(
<     RewritePatternSet &patterns,
<     const GetCollapsableDimensionsFn &controlCollapseDimensions);
< 
< /// Patterns to fold an expanding (collapsing) tensor_reshape operation with its
< /// producer (consumer) generic operation by expanding the dimensionality of the
< /// loop in the generic op.
< void populateFoldReshapeOpsByExpansionPatterns(
<     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
< 
< /// Patterns to fold an expanding tensor.expand_shape operation with its
< /// producer generic operation by collapsing the dimensions of the generic op.
< void populateFoldReshapeOpsByCollapsingPatterns(
<     RewritePatternSet &patterns, const ControlFusionFn &controlFoldingReshapes);
< 
< /// Patterns to constant fold Linalg operations.
< void populateConstantFoldLinalgOperations(RewritePatternSet &patterns,
<                                           const ControlFusionFn &controlFn);
< 
< /// Pattern to fuse a `tensor.pad` operation with the producer of its source,
< /// if the producer is a `linalg` operation with all parallel iterator types.
< void populateFuseTensorPadWithProducerLinalgOpPatterns(
<     RewritePatternSet &patterns);
< 
< /// Patterns to convert from one named op to another. These can be seen as
< /// canonicalizations of named ops into another named op.
< void populateLinalgNamedOpConversionPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
< /// tensors via reassociative reshape ops.
< void populateFoldUnitExtentDimsViaReshapesPatterns(RewritePatternSet &patterns);
< 
< /// Patterns to fold unit-extent dimensions in operands/results of linalg ops on
< /// tensors via rank-reducing slices.
< void populateFoldUnitExtentDimsViaSlicesPatterns(RewritePatternSet &patterns);
< 
< /// A pattern that converts init operands to input operands.
< void populateMoveInitOperandsToInputPattern(RewritePatternSet &patterns);
< 
< /// Patterns that are used to inline constant operands into linalg generic ops.
< void populateInlineConstantOperandsPatterns(RewritePatternSet &patterns);
< 
< /// Patterns that are used to bubble up extract slice op above linalg op.
< void populateBubbleUpExtractSliceOpPatterns(RewritePatternSet &patterns);
---
> /// Split Reduction options.
> struct SplitReductionOptions {
>   // Ratio used to split the reduction dimension.  If the ratio is <= 1,
>   // nothing will be done.
>   int64_t ratio = 0;
>   // Index where the extra dimension is added to the intermediate tensor
>   // shape.
>   unsigned index = 0;
>   // If the inner dimension after splitting is parallel or reduction.
>   bool innerParallel = false;
> };
1427,1429c1019,1023
< /// Adds patterns that waps tensor.extract_slice(linalg.fill(%cst, %init)) into
< /// linalg.fill(%cst, tensor.extract_slice(%init)).
< void populateSwapExtractSliceWithFillPatterns(RewritePatternSet &patterns);
---
> /// Function signature to control reduction splitting. This returns
> /// `SplitReductionOptions`.
> // TODO: don't use unsigned unless doing bit manipulation.
> using ControlSplitReductionFn =
>     std::function<SplitReductionOptions(LinalgOp op)>;
1435a1030,1138
> 
> /// Apply transformation to split the single linalg op reduction into a
> /// parallel and reduction dimension. Then create a new linalg.generic op
> /// doing the rest of the reduction. Return the new linalg op with an extra
> /// parallel dimension or failure if the transformation didn't happen.
> ///
> /// Example:
> /// ```
> ///  %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
> ///                                        affine_map<(d0) -> ()>],
> ///       iterator_types = ["reduction"]}
> ///  ins(%in : tensor<32xf32>)
> ///  outs(%out : tensor<f32>) {
> ///  ^bb0(%arg1: f32, %arg2: f32):
> ///    %y = arith.addf %arg1, %arg2 : f32
> ///    linalg.yield %y : f32
> ///  } -> tensor<f32>
> /// ```
> /// To:
> /// ```
> ///  %cst = arith.constant 0.000000e+00 : f32
> ///  %0 = tensor.expand_shape %in [[0, 1]] : tensor<32xf32> into
> ///  tensor<4x8xf32> %1 = tensor.empty [4] : tensor<4xf32> %2 = linalg.fill
> ///  ins(%cst : f32) outs(%1 : tensor<4xf32>) -> tensor<4xf32> %3 =
> ///  linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
> ///                                        affine_map<(d0, d1) -> (d0)>],
> ///    iterator_types = ["parallel", "reduction"]}
> ///    ins(%0 : tensor<4x8xf32>) outs(%2 : tensor<4xf32>) {
> ///    ^bb0(%arg3: f32, %arg5: f32):
> ///    %5 = arith.addf %arg3, %arg4 : f32
> ///    linalg.yield %5 : f32
> ///  } -> tensor<4xf32>
> /// %r = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>,
> ///                                       affine_map<(d0) -> ()>],
> ///   iterator_types = ["reduction"]}
> ///   ins(%3 : tensor<4xf32>) outs(%out : tensor<f32>) {
> ///   ^bb0(%arg3: f32, %arg4: f32):
> ///   %5 = arith.addf %arg3, %arg4 : f32
> ///   linalg.yield %5 : f32
> /// } -> tensor<f32>
> /// ```
> struct SplitReductionResult {
>   Operation *initOrAlloc;
>   FillOp fillOp;
>   LinalgOp splitLinalgOp;
>   LinalgOp resultCombiningLinalgOp;
> };
> FailureOr<SplitReductionResult>
> splitReduction(PatternRewriter &b, LinalgOp op,
>                const ControlSplitReductionFn &controlSplitReductionFn,
>                bool useAlloc = false);
> 
> /// Scaling-based implementation of the split reduction transformation.
> /// Instead of introducing an ExpandShapeOp, this rewrites a reduction
> /// dimension `k` into `k * scale + kk`.
> ///
> /// Example:
> /// ```
> ///  %0 = linalg.matmul ins(%A, %B: tensor<16x256xf32>, tensor<256x32xf32>)
> ///    outs(%C: tensor<16x32xf32>) -> tensor<16x32xf32>
> /// ```
> ///
> /// Is transformed to:
> ///
> /// ```
> ///  #map0 = affine_map<(d0, d1, d2, d3) -> (d0, d2 * 4 + d3)>
> ///  #map1 = affine_map<(d0, d1, d2, d3) -> (d2 * 4 + d3, d1)>
> ///  #map2 = affine_map<(d0, d1, d2, d3) -> (d2, d3)>
> ///  #map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
> ///  #map4 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
> ///  #map5 = affine_map<(d0, d1, d2) -> (d0, d1)>
> ///  %0 = tensor.empty [16, 32, 64] : tensor<16x32x64xf32>
> ///  %cst = arith.constant 0.000000e+00 : f32
> ///  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<16x32x64xf32>) ->
> ///     tensor<16x32x64xf32>
> ///  %2 = tensor.empty [64, 4] : tensor<64x4xi1>
> ///
> ///  %3 = linalg.generic {indexing_maps = [#map0, #map1, #map2, #map3],
> ///    iterator_types = ["parallel", "parallel", "parallel", "reduction"]}
> ///    ins(%A, %B, %2 : tensor<16x256xf32>, tensor<256x32xf32>,
> ///    tensor<64x4xi1>)
> ///   outs(%1 : tensor<16x32x64xf32>) {
> ///      ^bb0(%arg3: f32, %arg4: f32, %arg5: i1, %arg6: f32):
> ///        %5 = arith.mulf %arg3, %arg4 : f32
> ///        %6 = arith.addf %arg6, %5 : f32
> ///        linalg.yield %6 : f32
> ///  } -> tensor<16x32x64xf32>
> ///
> ///  %4 = linalg.generic {indexing_maps = [#map4, #map5],
> ///    iterator_types = ["parallel", "parallel", "reduction"]}
> //     ins(%3 : tensor<16x32x64xf32>)
> ///    outs(%C : tensor<16x32xf32>) {
> ///      ^bb0(%arg3: f32, %arg4: f32):
> ///        %5 = arith.addf %arg3, %arg4 : f32
> ///        linalg.yield %5 : f32
> ///  } -> tensor<16x32xf32>
> ///
> ///  return %4 : tensor<16x32xf32>
> /// ```
> FailureOr<SplitReductionResult>
> splitReductionByScaling(PatternRewriter &b, LinalgOp op,
>                         const ControlSplitReductionFn &controlSplitReductionFn,
>                         bool useAlloc = false);
> 
> /// Collapses dimensions of linalg.generic operation. It also collapses inputs
> /// before the op and expands outputs after the op.
> FailureOr<SmallVector<Value>> collapseGenericOpIterationDims(
>     GenericOp genericOp, ArrayRef<ReassociationIndices> foldedIterationDims,
>     RewriterBase &rewriter);
--- include/mlir/Dialect/Linalg/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR/CMakeLists.txt include/mlir/Dialect/Linalg/IR/CMakeLists.txt
5a6,7
>   # Note: workaround for https://github.com/ROCmSoftwarePlatform/llvm-project-private/issues/524
>   set(DUMMY_FILE ${CMAKE_CURRENT_BINARY_DIR}/dummy)
19a22,27
>   add_custom_command(
>     OUTPUT ${DUMMY_FILE}
>     COMMAND touch ${DUMMY_FILE}
>     DEPENDS
>     ${GEN_ODS_FILE} ${GEN_CPP_FILE}
>     )
25,26c33,34
<     ${GEN_ODS_FILE} ${GEN_CPP_FILE})
<   list(APPEND LLVM_TARGET_DEPENDS ${GEN_ODS_FILE})
---
>     ${GEN_ODS_FILE} ${GEN_CPP_FILE} ${DUMMY_FILE})
>   list(APPEND LLVM_TARGET_DEPENDS ${GEN_ODS_FILE} ${DUMMY_FILE})
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR/LinalgBase.td include/mlir/Dialect/Linalg/IR/LinalgBase.td
40c40
<     "affine::AffineDialect",
---
>     "AffineDialect",
48a49
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR/LinalgInterfaces.h include/mlir/Dialect/Linalg/IR/LinalgInterfaces.h
45,69d44
< /// Result of matching a Linalg generic against the predicates of it being a
< /// convolution.
< enum class MatchConvolutionResult;
< 
< /// Positions of a Linalg op loops that correspond to different kinds of a
< /// convolution dimension.
< struct ConvolutionDimensions {
<   SmallVector<unsigned, 2> batch;
<   SmallVector<unsigned, 2> outputImage;
<   SmallVector<unsigned, 2> outputChannel;
<   SmallVector<unsigned, 2> filterLoop;
<   SmallVector<unsigned, 2> inputChannel;
<   SmallVector<unsigned, 2> depth;
< };
< 
< /// Checks whether `op` conforms to ConvolutionOpInterface and populates
< /// `dimensions` with indexes of the different kinds of dimensions when present.
< MatchConvolutionResult
< isConvolutionInterfaceImpl(Operation *op,
<                            ConvolutionDimensions *dimensions = nullptr);
< 
< /// Returns the error message corresponding to the convolution checking return
< /// code.
< StringRef getMatchConvolutionMessage(MatchConvolutionResult res);
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR/LinalgInterfaces.td include/mlir/Dialect/Linalg/IR/LinalgInterfaces.td
624,648d623
<     InterfaceMethod<
<       /*desc=*/[{
<         Given a dimension of the iteration space of a Linalg operation, finds
<         all the operands in the operation that are defined on such dimension.
<         Returns all the operand values found and their dimension positions in
<         `operandDimPairs`.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"mapIterationSpaceDimToAllOperandDims",
<       /*args=*/(ins "unsigned":$dimPos,
<                     "mlir::SmallVectorImpl<std::pair<Value, unsigned>>&":$operandDimPairs),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         for (auto [i, idxMap] : llvm::enumerate($_op.getIndexingMapsArray())) {
<           if (idxMap.isProjectedPermutation()) {
<             if (auto mayOperandDim = idxMap.getResultPosition(
<                 getAffineDimExpr(dimPos, idxMap.getContext()))) {
<               operandDimPairs.push_back({$_op->getOperand(i), *mayOperandDim});
<             }
<           }
<         }
< 
<         return;
<       }]
<     >,
800,802d774
< 
<     /// Return the index in the indexingMaps vector that corresponds to this `opOperand`
<     int64_t getIndexingMapIndex(OpOperand *opOperand);
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/IR: ValueBoundsOpInterfaceImpl.h
--- include/mlir/Dialect/Linalg/IR/LinalgOps.td
--- include/mlir/Dialect/Linalg/IR/Linalg.h
--- include/mlir/Dialect/Linalg/IR/LinalgInterfaces.td
624,648d623
<     InterfaceMethod<
<       /*desc=*/[{
<         Given a dimension of the iteration space of a Linalg operation, finds
<         all the operands in the operation that are defined on such dimension.
<         Returns all the operand values found and their dimension positions in
<         `operandDimPairs`.
<       }],
<       /*retTy=*/"void",
<       /*methodName=*/"mapIterationSpaceDimToAllOperandDims",
<       /*args=*/(ins "unsigned":$dimPos,
<                     "mlir::SmallVectorImpl<std::pair<Value, unsigned>>&":$operandDimPairs),
<       /*methodBody=*/"",
<       /*defaultImplementation=*/[{
<         for (auto [i, idxMap] : llvm::enumerate($_op.getIndexingMapsArray())) {
<           if (idxMap.isProjectedPermutation()) {
<             if (auto mayOperandDim = idxMap.getResultPosition(
<                 getAffineDimExpr(dimPos, idxMap.getContext()))) {
<               operandDimPairs.push_back({$_op->getOperand(i), *mayOperandDim});
<             }
<           }
<         }
< 
<         return;
<       }]
<     >,
800,802d774
< 
<     /// Return the index in the indexingMaps vector that corresponds to this `opOperand`
<     int64_t getIndexingMapIndex(OpOperand *opOperand);
--- include/mlir/Dialect/Linalg/IR/CMakeLists.txt
5a6,7
>   # Note: workaround for https://github.com/ROCmSoftwarePlatform/llvm-project-private/issues/524
>   set(DUMMY_FILE ${CMAKE_CURRENT_BINARY_DIR}/dummy)
19a22,27
>   add_custom_command(
>     OUTPUT ${DUMMY_FILE}
>     COMMAND touch ${DUMMY_FILE}
>     DEPENDS
>     ${GEN_ODS_FILE} ${GEN_CPP_FILE}
>     )
25,26c33,34
<     ${GEN_ODS_FILE} ${GEN_CPP_FILE})
<   list(APPEND LLVM_TARGET_DEPENDS ${GEN_ODS_FILE})
---
>     ${GEN_ODS_FILE} ${GEN_CPP_FILE} ${DUMMY_FILE})
>   list(APPEND LLVM_TARGET_DEPENDS ${GEN_ODS_FILE} ${DUMMY_FILE})
--- include/mlir/Dialect/Linalg/IR/LinalgEnums.td
--- include/mlir/Dialect/Linalg/IR/LinalgStructuredOps.td
--- include/mlir/Dialect/Linalg/IR/LinalgNamedStructuredOps.yaml
--- include/mlir/Dialect/Linalg/IR/LinalgBase.td
40c40
<     "affine::AffineDialect",
---
>     "AffineDialect",
48a49
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Linalg/IR/LinalgInterfaces.h
45,69d44
< /// Result of matching a Linalg generic against the predicates of it being a
< /// convolution.
< enum class MatchConvolutionResult;
< 
< /// Positions of a Linalg op loops that correspond to different kinds of a
< /// convolution dimension.
< struct ConvolutionDimensions {
<   SmallVector<unsigned, 2> batch;
<   SmallVector<unsigned, 2> outputImage;
<   SmallVector<unsigned, 2> outputChannel;
<   SmallVector<unsigned, 2> filterLoop;
<   SmallVector<unsigned, 2> inputChannel;
<   SmallVector<unsigned, 2> depth;
< };
< 
< /// Checks whether `op` conforms to ConvolutionOpInterface and populates
< /// `dimensions` with indexes of the different kinds of dimensions when present.
< MatchConvolutionResult
< isConvolutionInterfaceImpl(Operation *op,
<                            ConvolutionDimensions *dimensions = nullptr);
< 
< /// Returns the error message corresponding to the convolution checking return
< /// code.
< StringRef getMatchConvolutionMessage(MatchConvolutionResult res);
< 
--- include/mlir/Dialect/Linalg/IR/LinalgDoc.td
--- include/mlir/Dialect/Linalg/Analysis
--- include/mlir/Dialect/Linalg/Analysis/DependenceAnalysis.h
--- include/mlir/Dialect/Linalg/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps/CMakeLists.txt include/mlir/Dialect/Linalg/TransformOps/CMakeLists.txt
1,5d0
< set(LLVM_TARGET_DEFINITIONS LinalgMatchOps.td)
< mlir_tablegen(LinalgMatchOps.h.inc -gen-op-decls)
< mlir_tablegen(LinalgMatchOps.cpp.inc -gen-op-defs)
< add_public_tablegen_target(MLIRLinalgMatchOpsIncGen)
< 
9,11d3
< add_public_tablegen_target(MLIRLinalgTransformOpsIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS LinalgTransformEnums.td)
14c6
< add_public_tablegen_target(MLIRLinalgTransformEnumsIncGen)
---
> add_public_tablegen_target(MLIRLinalgTransformOpsIncGen)
16d7
< add_mlir_doc(LinalgMatchOps LinalgStructuredMatchOps Dialects/ -gen-op-doc)
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps: DialectExtension.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps: LinalgMatchOps.h
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps: LinalgMatchOps.td
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps: LinalgTransformEnums.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h
12d11
< #include "mlir/Dialect/Func/IR/FuncOps.h"
14,15d12
< #include "mlir/Dialect/Transform/IR/TransformAttrs.h"
< #include "mlir/Dialect/Transform/IR/TransformDialect.h"
17d13
< #include "mlir/Dialect/Utils/StructuredOpsUtils.h"
24d19
< 
30,36d24
< namespace tensor {
< class InsertSliceOp;
< class PackOp;
< class PadOp;
< class UnPackOp;
< } // namespace tensor
< 
44a33,41
> //===----------------------------------------------------------------------===//
> // Linalg Transform Operations
> //===----------------------------------------------------------------------===//
> 
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOpsEnums.h.inc"
> 
> #define GET_OP_CLASSES
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h.inc"
> 
50,51c47,48
< /// Implementation of tiling operations using `scf.forall`.
< DiagnosedSilenceableFailure tileToForallOpImpl(
---
> /// Implementation of tiling operations using `scf.foreach_thread`.
> DiagnosedSilenceableFailure tileToForeachThreadOpImpl(
57d53
< 
59,65d54
< } // namespace mlir
< 
< //===----------------------------------------------------------------------===//
< // Linalg Transform Operations
< //===----------------------------------------------------------------------===//
< 
< #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOpsEnums.h.inc"
67,68c56,59
< #define GET_OP_CLASSES
< #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h.inc"
---
> namespace linalg {
> void registerTransformDialectExtension(DialectRegistry &registry);
> } // namespace linalg
> } // namespace mlir
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.td include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.td
12,13d11
< include "mlir/Dialect/Linalg/TransformOps/LinalgTransformEnums.td"
< include "mlir/Dialect/Transform/IR/TransformAttrs.td"
14a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
19a19
> include "mlir/IR/EnumAttr.td"
31,82d30
< // BufferizeToAllocationOp
< //===----------------------------------------------------------------------===//
< 
< def BufferizeToAllocationOp : Op<Transform_Dialect,
<     "structured.bufferize_to_allocation",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     This transform materializes an allocation for the targeted tensor value. It
<     replaces all original uses of the target with the newly allocated buffer,
<     wrapped in a `bufferization.to_tensor` op. It returns a handle to the result
<     of the `to_tensor` op.
< 
<     Example:
<     ```
<     %0 = "some_op"() : () -> (tensor<10xf32>)
<     "some_use"(%0) : (tensor<10xf32>) -> ()
<     ```
< 
<     Is rewritten to:
<     ```
<     %0 = "some_op"() : () -> (tensor<10xf32>)
<     %1 = memref.alloc() : memref<10xf32>
<     memref.tensor_store %0, %1 : memref<10xf32>
<     %2 = bufferization.to_tensor %1 restrict writable : memref<10xf32>
<     "some_use"(%2) : (tensor<10xf32>) -> ()
<     ```
< 
<     This transform has optimized lowerings for certain targets that are results
<     of non-DPS ops. For such targets, not only a buffer allocation is emitted
<     but also the defining op is bufferized. This is to avoid a second
<     allocation for the missing destination of the non-DPS op (when subsequently
<     running a bufferization pass/transform). Currently supported ops with
<     optimized lowerings:
<     - tensor.pad
< 
<     An optional memory space attribute can be specified for the materialized
<     buffer allocation.
< 
<     #### Return modes
< 
<     This operation consumes the `target` handle and produces the `transformed`
<     handle. It always succeeds.
<   }];
< 
<   let arguments = (ins Transform_AnyValue:$target,
<                        OptionalAttr<AnyAttr>:$memory_space);
<   let results = (outs Transform_AnyValue:$transformed);
<   let assemblyFormat = "$target attr-dict";
< }
< 
< //===----------------------------------------------------------------------===//
87,90c35,36
<     [FunctionalStyleTransformOpTrait, 
<      MemoryEffectsOpInterface,
<      TransformOpInterface, 
<      TransformEachOpTrait]> {
---
>     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
>      TransformOpInterface, TransformEachOpTrait]> {
146,147c92
<       [DeclareOpInterfaceMethods<TransformOpInterface>,
<        DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
>       [DeclareOpInterfaceMethods<TransformOpInterface>]> {
183,185c128,135
<   let arguments = (ins PDL_Operation:$producer_op,
<                        PDL_Operation:$containing_op);
<   let results = (outs PDL_Operation:$fused_op);
---
>   let arguments = (ins Arg<PDL_Operation, "",
>                            [TransformMappingRead,
>                             TransformMappingFree]>:$producer_op,
>                        Arg<PDL_Operation, "",
>                            [TransformMappingRead]>:$containing_op);
>   let results = (outs Res<PDL_Operation, "",
>                           [TransformMappingAlloc,
>                            TransformMappingWrite]>:$fused_op);
269,306c219
< // LowerPackOp
< //===----------------------------------------------------------------------===//
< def LowerPackOp : Op<Transform_Dialect, "structured.lower_pack", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          TransformEachOpTrait,
<                          TransformOpInterface]> {
<   let description = [{
<     Rewrite a tensor.pack into tensor.pad + tensor.expand_shape + linalg.transpose.
< 
<     #### Return modes
< 
<     This operation ignores non-pack ops and drops them in the return.
<     This operation produces a silenceableFailure if the rewrite fails for any
<     reason.
<     If all the operations referred to by the `target` are rewritten, the
<     transform succeeds.
<     Return handles to the newly produced pad, expand_shape and transpose ops.
<   }];
< 
<   let arguments = (ins Transform_ConcreteOpType<"tensor.pack">:$target);
<   let results = (outs Transform_ConcreteOpType<"tensor.pad">:$pad_op,
<                       Transform_ConcreteOpType<"tensor.expand_shape">:$expand_shape_op,
<                       Transform_ConcreteOpType<"linalg.transpose">:$transpose_op);
<   let assemblyFormat = [{
<     $target attr-dict `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::PackOp target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // LowerUnPackOp
---
> // MatchOp
308,325d220
< def LowerUnPackOp : Op<Transform_Dialect, "structured.lower_unpack", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          TransformEachOpTrait,
<                          TransformOpInterface]> {
<   let description = [{
<     Lower a tensor.unpack into empty + linalg.transpose + tensor.collapse_shape + 
<     tensor.extract_slice.
< 
<     #### Return modes
< 
<     This operation ignores non-unpack ops and drops them in the return.
<     This operation produces a silenceableFailure if the rewrite fails for any
<     reason.
<     If all the operations referred to by the `target` are rewritten, the
<     transform succeeds.
<     Return handles to the newly produced empty, transpose, collapse_shape and extract_slice ops.
<   }];
327,341c222,227
<   let arguments = (ins Transform_ConcreteOpType<"tensor.unpack">:$target);
<   let results = (outs Transform_ConcreteOpType<"tensor.empty">:$empty_op,
<                       Transform_ConcreteOpType<"linalg.transpose">:$transpose_op,
<                       Transform_ConcreteOpType<"tensor.collapse_shape">:$collapse_shape_op,
<                       Transform_ConcreteOpType<"tensor.extract_slice">:$extract_slice_op);
<   let assemblyFormat = [{ 
<     $target attr-dict `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::UnPackOp target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
<         ::mlir::transform::TransformState &state);
<   }];
---
> def MatchInterfaceEnum : I32EnumAttr<"MatchInterfaceEnum", "An interface to match",
>     [
>       I32EnumAttrCase<"LinalgOp", 0>,
>       I32EnumAttrCase<"TilingInterface", 1>
>     ]>{
>   let cppNamespace = "mlir::transform";
344,347d229
< //===----------------------------------------------------------------------===//
< // MatchOp
< //===----------------------------------------------------------------------===//
< 
381c263
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>   let arguments = (ins PDL_Operation:$target,
387c269
<   let results = (outs TransformHandleTypeInterface:$results);
---
>   let results = (outs PDL_Operation:$results);
390,391c272
<     OpBuilder<(ins "Value":$target, "ArrayRef<StringRef>":$opNames)>,
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$target, "ArrayRef<StringRef>":$opNames)>
---
>     OpBuilder<(ins "Value":$target, "ArrayRef<StringRef>":$opNames)>
400d280
<     `:` functional-type($target, results)
486,489d365
< //===----------------------------------------------------------------------===//
< // PackOp
< //===----------------------------------------------------------------------===//
< 
491,492c367,368
<                 DeclareOpInterfaceMethods<TransformOpInterface>,
<                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
>                 TransformOpInterface,
>                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,]> {
565,673d440
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedPackedSizes)>
<   ];
< 
<   let extraClassDeclaration = [{
<     ::llvm::SmallVector<::mlir::OpFoldResult> getMixedPackedSizes();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // PackGreedilyOp
< //===----------------------------------------------------------------------===//
< def PackGreedilyOp : Op<Transform_Dialect, "structured.pack_greedily", [
<                         DeclareOpInterfaceMethods<TransformOpInterface>,
<                         DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     Target a Linalg op and rewrite it into packed LinalgOp form by trying to
<     infer whether a known suboperation is embedded
< 
<     Different packing strategies are applied in order, when one applies 
<     successfully, the transform returns:
<       1. Matmul packing: Try to infer a matmul operation embedded in the target op.
<          Specifically, this looks for 2 parallel dimensions that participate in
<          an outer-product and 1 reduction dimension.
<          These dimensions are referred as (m, n, k) to match canonical matmul
<          terminology.
<          
<          The packed sizes for (m, n, k) are specified by `matmul_packed_sizes`
<          and the optional `matmul_padded_sizes_next_multiple_of`.
<          When an entry `matmul_packed_sizes[i]` is non-0, the corresponding 
<          dimension is packed by `matmul_packed_sizes[i]`.
<          Otherwise, the dimension is merely padded to the next multiple of
<          `matmul_padded_sizes_next_multiple_of[i]`.
< 
<          `matmul_padded_sizes_next_multiple_of` is optional and is expected to
<          either be empty or of size `3`, matching the size of `matmul_packed_sizes`.
<          For each individual element of `matmul_packed_sizes` and 
<          `matmul_padded_sizes_next_multiple_of`, only one of them is allowed to
<          be non-zero.
<          
<          The ordering of the packed dimensions (mm, nn, kk) is specified by the
<          `matmul_inner_dims_order` attribute.
< 
<     Packing occurs as follows:
<       1. Find the dimensions to pack according to the strategy.
<       2. The target is converted to linalg.generic form.
<       3. An interchange transform is applied to isolate the dimensions to pack as
<          the most minor indexing dimensions of the linalg.generic. The most minor
<          dimensions are themselves ordered according to `inner_dims_order`.
<       4. An elementwise traversal of `matmul_packed_sizes` and
<          `matmul_padded_sizes_next_multiple_of` is performed and for each 
<          dimension `d`, either pack to `matmul_packed_sizes[d]` or pad to the
<          `matmul_padded_sizes_next_multiple_of[d]`.
<       5. Packing/padding is performed by the amounts determined in step 4. and
<          following `inner_dims_order`.
< 
<     By normalizing the most minor dimensions to `inner_dims_order`, the transform
<     guarantees that packing immediately generates inner dimensions in a desirable
<     layout.
< 
<     Outer dimension layout permutations are not controlled by this transform op
<     at the moment and can be obtained by composing with the pack_transpose
<     transformation.
< 
<     #### Return modes
< 
<     This operation ignores non-Linalg ops and drops them in the return.
<     It returns the list of packed Linalg ops or the original op when all available
<     packing strategies failed to apply.
<   }];
< 
<   // TODO: Transform_ConcreteOpType<linalg::LinalgOp> needs interface.
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                    Variadic<PDL_Operation>:$matmul_packed_sizes,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [DenseArrayCount<3>]>:$static_matmul_packed_sizes,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [Attr<
<                                     Or<[DenseArrayCount<0>.predicate, 
<                                         DenseArrayCount<3>.predicate]>,
<                                         "with 0 or 3 elements"
<                                       >]>
<                                  :$matmul_padded_sizes_next_multiple_of,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [DenseArrayCount<3>]>:$matmul_inner_dims_order);
<   let results = (outs Transform_ConcreteOpType<"linalg.generic">:$packed_op);
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedMatmulPackedSizes,
<                    "ArrayRef<int64_t>":$matmulPaddededSizesNextMultipleOf,
<                    CArg<"ArrayRef<int64_t>", "{}">:$matmulDimsInnerDimsOrder)>
<   ];
< 
<   let assemblyFormat = [{
<     $target
<     oilist(
<       `matmul_packed_sizes` `=` custom<DynamicIndexList>($matmul_packed_sizes,
<                                                          $static_matmul_packed_sizes)
<       (`matmul_padded_sizes_next_multiple_of` `=` 
<         $matmul_padded_sizes_next_multiple_of^)?
<       `matmul_inner_dims_order` `=` $matmul_inner_dims_order
<     )
<     attr-dict
<     `:` functional-type($target, results)
<   }];
<   let hasVerifier = 1;
< 
675,723c442,444
<     /// Returns the list of tile sizes, which may be static (Attribute) or
<     /// dynamic (Value).
<     SmallVector<OpFoldResult> getMixedMatmulPackedSizes();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // PackTransposeOp
< //===----------------------------------------------------------------------===//
< def PackTransposeOp : Op<Transform_Dialect, "structured.pack_transpose", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let description = [{
<     Apply a transposition to a single `tensor.pack` (resp. `tensor.unpack`) and 
<     update the `linalg.generic` op that consumes (resp. produces) the operation.
< 
<     This transform allows composing a simple `structured.pack` with additional
<     transpositions to e.g. match the data format required by a specific library
<     call or ISA instruction.
< 
<     The transpose spec must specify at least one of `outer_perm` or `inner_perm`
<     attributes, which will act upon the `outer_dims_perm` or `inner_dims_pos` of
<     the specified `tensor.pack` or `tensor.unpack` op.
< 
<     If the `target` of this op is a `tensor.pack` then a new `tensor.empty` will
<     be created along with transposed versions of the `tensor.pack` and the 
<     consuming `linalg.generic`, which is expected to be the sole consumer.
< 
<     If the `target` of this op is a `tensor.unpack` then the whole pack / compute
<     / unpack chain will be transposed and transposed clones of `tensor.pack`,
<     the consuming `linalg.generic` and the tail `tensor.pack` will be created.
< 
<     #### Return modes
< 
<     This operation targets a single `tensor.pack` / `tensor.unpack` op and a
<     single matching `linalg.generic` that consumes / produces the op. Otherwise,
<     it produces a silenceableFailure.
< 
<     This operation may produce a silenceableFailure if the transpose spec is
<     ill-formed (i.e. `outer_perm` or `inner_perm` are not permutations of the
<     proper rank) or if the tranposition of all involved operations fails for any
<     reason.
< 
<     This operation returns 3 handles, one to the transformed LinalgOp, one to
<     the transformed `tensor.pack` and one to the transformed `tensor.unpack`.
<     The last handle for `tensor.unpack` is empty if `target_pack_or_unpack_op` 
<     was not itself a `tensor.unpack`.
<   }];
---
>     ::mlir::DiagnosedSilenceableFailure apply(
>       transform::TransformResults &transformResults,
>       transform::TransformState &state);
725,738c446
<   let arguments = (ins TransformHandleTypeInterface:$target_pack_or_un_pack_op,
<                        TransformHandleTypeInterface:$target_linalg_op,
<                        DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$outer_perm,
<                        DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$inner_perm);
<   let results = (outs TransformHandleTypeInterface:$packed_op,
<                       TransformHandleTypeInterface:$pack_op,
<                       TransformHandleTypeInterface:$un_pack_op);
<   let assemblyFormat = [{
<     $target_pack_or_un_pack_op
<     `with_compute_op` `(` $target_linalg_op `)`
<     (`outer_perm` `=` $outer_perm^ )?
<     (`inner_perm` `=` $inner_perm^ )?
<     attr-dict
<     `:` functional-type(operands, results)
---
>     ::llvm::SmallVector<::mlir::OpFoldResult> getMixedPackedSizes();
740,741d447
< 
<   let hasVerifier = 1;
770a477
>          DefaultValuedAttr<I64ArrayAttr, "{}">:$hoist_paddings,
788,881d494
< // HoistPadOp
< //===----------------------------------------------------------------------===//
< 
< def HoistPadBuildPackingLoopNestOp :
<     Op<Transform_Dialect,
<        "structured.hoist_pad.build_packing_loop_nest",
<     [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let description = [{
<     Helper transform used to hoist a tensor.pad target operation. This operation
<     creates the packing loop nest required by the hoist_pad operation and makes
<     that functionality available independently.
< 
<     TODO: In the future, we should consider rewriting as a tensor.pack after
<     hoisting since this abstraction is now available.
< 
<     #### Return modes
< 
<     This operation ignores non-tensor.pad ops and drops them in the result.
<     If any non-tensor.pad is passed, the transform emits a silenceable failure.
< 
<     The return handle points to only the subset of successfully created packing
<     loop nests, which can be empty.
<   }];
< 
<   // Also allow any !pdl.operation for simpler composition. Non-tensor.pad ops
<   // will be dropped from the results.
<   let arguments =
<     (ins TransformHandleTypeInterface:$target,
<          TransformHandleTypeInterface:$loop,
<          DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
<   let results = (outs TransformHandleTypeInterface:$packing_loop);
< 
<   let assemblyFormat = [{
<     $target
<     `above` $loop
<     (`,` `transpose` `by` $transpose^)?
<     attr-dict
<     `:` functional-type(operands, results)
<   }];
<   let hasVerifier = 1;
< }
< 
< def HoistPadOp : Op<Transform_Dialect, "structured.hoist_pad",
<     [FunctionalStyleTransformOpTrait,
<      MemoryEffectsOpInterface,
<      TransformOpInterface,
<      TransformEachOpTrait]> {
<   let description = [{
<     Hoist the tensor.pad target operation by at most the given number of loops.
<     Optionally apply the transpose attribute to the inner dimensions.
< 
<     TODO: In the future, we should consider rewriting as a tensor.pack after 
<     hoisting since this abstraction is now available.
<     TODO: Maybe also return the linalg.generic transpose created at some point.
< 
<     #### Return modes
< 
<     This operation ignores non-tensor.pad ops and drops them in the result.
<     If any non-tensor.pad is passed, the transform emits a silenceable failure.
< 
<     If all the operations referred to by the `target` handle padproperly, the
<     transform succeeds. Otherwise the transform silently fails.
< 
<     The return handle points to only the subset of successfully hoisted 
<     tensor.pad operations, which can be empty.
<   }];
< 
<   // Also allow any !pdl.operation for simpler composition. Non-tensor.pad ops
<   // will be dropped from the results.
<   let arguments =
<     (ins TransformHandleTypeInterface:$target,
<          I64Attr:$num_loops,
<          DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = [{
<     $target 
<     `by` $num_loops `loops` 
<     (`,` `transpose` `by` $transpose^)? 
<     attr-dict
<     `:` functional-type(operands, results)
<   }];
<   let hasVerifier = 1;
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::PadOp,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
885d497
< 
912d523
<                        OptionalAttr<DeviceMappingArrayAttr>:$mapping,
996,1042d606
< // RewriteInDestinationPassingStyleOp.
< //===----------------------------------------------------------------------===//
< 
< def RewriteInDestinationPassingStyleOp : Op<
<     Transform_Dialect, "structured.rewrite_in_destination_passing_style",
<     [FunctionalStyleTransformOpTrait, 
<      MemoryEffectsOpInterface,
<      TransformOpInterface, 
<      TransformEachOpTrait]> {
<   let description = [{
<     Rewrite a supported tensor operation that is not in destination-passing style
<     into a form that is in destination-passing style.
<     Currently supported operations are:
<       - tensor.pad
<       - tensor.generate
<       - tensor.from_elements
<     This dichotomy hints at a future interface, for now the implementation just 
<     switches between different implementation.
< 
<     #### Return modes
< 
<     This operation ignores non-unsupported ops and drops them from the return.
<     If all the operations referred to by the `target` PDLOperation generalize
<     properly, the transform succeeds. Otherwise the transform silently fails.
<     The return handle points to a subset of successfully produced operations:
<       - tensor.pad case, the returned handle points to the tensor.insert_slice.
<       - tensor.generate case, the returned handle points to the linalg.generic.
<       - tensor.from_elements case, the returned handle points to the last 
<         tensor.insert.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
<   let assemblyFormat = [{
<     $target attr-dict
<     `:` functional-type($target, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
1327c891
<   // TODO: support mixed static-dynamic (see TileToForallOp).
---
>   // TODO: support mixed static-dynamic (see TileToForeachThreadOp).
1355c919
< // TileReductionUsingForallOp
---
> // TileReductionUsingForeachThreadOp
1358,1359c922,923
< def TileReductionUsingForallOp :
<   Op<Transform_Dialect, "structured.tile_reduction_using_forall",
---
> def TileReductionUsingForeachThreadOp :
>   Op<Transform_Dialect, "structured.tile_reduction_using_foreach_thread",
1363c927
<     Tile a PartialReductionOpInterface op to a tiled `scf.forall` doing
---
>     Tile a PartialReductionOpInterface op to a tiled `scf.foreach_thread` doing
1368c932
<     `scf.forall` loops with the number threads given by `num_threads`.
---
>     `scf.foreach_thread` loops with the number threads given by `num_threads`.
1374c938
<     distributed on the threads of the `scf.foralls` loop.
---
>     distributed on the threads of the `scf.foreach_threads` loop.
1379c943
<       - the parent forall op,
---
>       - the parent foreach_thread op,
1404c968
<       %2 = scf.forall (%arg2) in (%c5) shared_outs(%arg3 = %1) -> (tensor<?x5xf32>) {
---
>       %2 = scf.foreach_thread (%arg2) in (%c5) shared_outs(%arg3 = %1) -> (tensor<?x5xf32>) {
1416c980
<         scf.forall.in_parallel {
---
>         scf.foreach_thread.perform_concurrently {
1428c992
<   // TODO: support mixed static-dynamic (see TileToForallOp).
---
>   // TODO: support mixed static-dynamic (see TileToForeachThreadOp).
1433c997
<   let results = (outs PDL_Operation:$forall_op,
---
>   let results = (outs PDL_Operation:$foreach_thread_op,
1542c1106
< // TileToForallOp
---
> // TileToForeachThreadOp
1545,1546c1109,1110
< def TileToForallOp :
<     Op<Transform_Dialect, "structured.tile_to_forall_op",
---
> def TileToForeachThreadOp :
>     Op<Transform_Dialect, "structured.tile_to_foreach_thread_op",
1551c1115
<     Tile a TilingInterface op to a tiled `scf.forall`.
---
>     Tile a TilingInterface op to a tiled `scf.foreach_thread`.
1568c1132
<     resulting `scf.forall`.
---
>     resulting `scf.foreach_thread`.
1587c1151
<       - the new scf.forall op,
---
>       - the new scf.foreach_thread op,
1594c1158
<     %3:2 = transform.structured.tile_to_forall_op %0 num_threads [10, 20]
---
>     %3:2 = transform.structured.tile_to_foreach_thread_op %0 num_threads [10, 20]
1602c1166
<     %3:2 = transform.structured.tile_to_forall_op %0 tile_sizes [0, %sz, 20]
---
>     %3:2 = transform.structured.tile_to_foreach_thread_op %0 tile_sizes [0, %sz, 20]
1614c1178
<   let results = (outs PDL_Operation:$forall_op,
---
>   let results = (outs PDL_Operation:$foreach_thread_op,
1713,1718d1276
<   
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedTileSizes,
<                    CArg<"ArrayRef<int64_t>", "{}">:$interchange)>
<   ];
1805,1810d1362
<     Note: The input vector sizes must be bigger than or equal to their
<     counterpart iteration space sizes.
< 
<     Typically this operator should be applied to linalg operations that have
<     already be tiled to the appropriate sizes.
< 
1821d1372
<                        UnitAttr:$vectorize_nd_extract,
1839,2043d1389
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // HoistRedundantVectorTransfersOp
< //===----------------------------------------------------------------------===//
< 
< def HoistRedundantVectorTransfersOp :
<   Op<Transform_Dialect, "structured.hoist_redundant_vector_transfers",
<     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<      TransformEachOpTrait, TransformOpInterface]> {
<   let description = [{
<     Hoist vector.transfer_read / vector.transfer_write pairs out of immediately
<     enclosing scf::ForOp iteratively, if the following conditions are true:
<        1. The 2 ops access the same memref with the same indices.
<        2. All operands are invariant under the enclosing scf::ForOp.
<        3. No uses of the memref either dominate the transfer_read or are
<        dominated by the transfer_write (i.e. no aliasing between the write and
<        the read across the loop)
< 
<     WARNING: This hoisting does not model parallelism and is generally incorrect
<     when used on distributed loops with memref semantics!
<     TODO: obsolete and should be retired.
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns a handle to the transformed
<     function op.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>,
<   ];
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::func::FuncOp target,
<          ::mlir::transform::ApplyToEachResultList &results,
<          ::mlir::transform::TransformState &state);
<    }];
< }
< 
< //===----------------------------------------------------------------------===//
< // ConvertConv2DToImg2ColOp
< //===----------------------------------------------------------------------===//
< 
< def ConvertConv2DToImg2ColOp : Op<Transform_Dialect,
<     "structured.convert_conv2d_to_img2col",
<     [FunctionalStyleTransformOpTrait,
<      MemoryEffectsOpInterface,
<      TransformOpInterface,
<      TransformEachOpTrait]> {
<   let description = [{
<     Convert linalg.conv_2d_xxx into linalg.generic (for img2col packing)
<     and linalg.matmul.
< 
<     A convolution operation can be written as a matrix-matrix multiplication by
<     unfolding the cross-correlation between input and filter and explicitly copy
<     overlapped sliding window inputs.
< 
<     Consider 2D input X with single channel input and output and 2x2 filter W:
<     ```
<     [x(0, 0)  , x(0, 1)  , ...,   x(0, n)  ]
<     [x(1, 0)  , x(1, 1)  , ...,   x(1, n)  ]
<     [.        ,  .       ,.   ,      .     ]            [w(0, 0), w(0, 1)]
<     [.        ,  .       , .  ,      .     ]    (conv)  [w(1, 0), w(1, 1)]
<     [.        ,  .       ,   .,      .     ]
<     [x(n-1, 0), x(n-1, 1), ..., x(n-1, n-1)]
<     ```
< 
<     The packed input data (img2col) is a matrix with |rows| = output spatial
<     size, |columns| = filter spatial size. To compute the output Y(i, j) we need
<     to calculate the dot product between filter window at input X(x, y)) and the
<     filter which will look like the following where r.h.s is the img2col matrix
<     and l.h.s is the flattned filter:
<     ```
<     [x(0,0), x(0,1), x(1,0), x(1,1)]
<     [x(0,1), x(1,1), x(0,2), x(1,2)] (matmul) [w(0,0), w(0,1), w(1,0), w(1,1)]
<     [x(0,1), x(1,1), x(0,2), x(1,2)]
<     [   .  ,    .  ,    .  ,    .  ]
<     ```
< 
<     In general for 2D case with (N, H, W, C) input and (Kh, Kw, C, D) filter
<     and output (N, Ho, Wo, D) the convolution is the following matrix-matrix
<     multiplication (Ho x Wo, Kh x Kw x C) * (Kh x Kw x C, D) for each input in
<     the N input. For the case where N > 1 its a batched matrxi-matrix
<     multplication.
< 
<     Returns two handles:
<     - One on the operation that produces the img2col tensor.
<     - One on the final operation of the sequence that replaces the original
<       convolution.
< 
<     #### Return modes:
< 
<     Returns a definite failure if target is not isolated from above.
<     Returns a silenceable failure if the pattern application failed.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$img2col_tensor,
<                       TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat =
<     "$target attr-dict `:` functional-type($target, results)";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>
<   ];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::linalg::LinalgOp target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // HoistRedundantTensorSubsetsOp
< //===----------------------------------------------------------------------===//
< 
< def HoistRedundantTensorSubsetsOp :
<   Op<Transform_Dialect, "structured.hoist_redundant_tensor_subsets",
<     [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      TransformEachOpTrait,
<      TransformOpInterface]> {
<   let description = [{
<     Hoists supported tensor subset extract/insert operation pairs out of 
<     immediately enclosing loop iteratively, if the following conditions
<     are true:
<        1. The 2 ops access the same tensor subset.
<        2. All operands are invariant under the enclosing loop.
<     
<     The supported subset extract/insert operation pairs currently comprise:
<        - tensor.extract_slice / tensor.insert_slice
<        - vector.transfer_read / vector.transfer_write on tensors
<     
<     Only scf.for loops are currently supported.
< 
<     When applied to:
<        1. an scf.for loop, hoist out of this loop only.
<        2. a non-loop op, apply hoisting to all the contained loop ops.
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns nothing.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs);
< 
<   let assemblyFormat = [{
<     $target 
<     attr-dict 
<     `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // InsertSliceToCopyOp
< //===----------------------------------------------------------------------===//
< 
< def InsertSliceToCopyOp :
<   Op<Transform_Dialect, "structured.insert_slice_to_copy",
<     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<      TransformEachOpTrait, TransformOpInterface]> {
<   let description = [{
<     Targeted rewrite of an tensor.insert_slice to linalg.copy.
<     This is useful to materialize copies explicitly before bufferization and 
<     transform them, avoiding the need to rediscover them after bufferization.
< 
<     If the insert_slice source is already a linalg.copy, only return the source
<     op (i.e. do not create an additional linalg.copy op).
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns a handle to the relevant 
<     linalg.copy op.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>,
<   ];
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
--- include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h
12d11
< #include "mlir/Dialect/Func/IR/FuncOps.h"
14,15d12
< #include "mlir/Dialect/Transform/IR/TransformAttrs.h"
< #include "mlir/Dialect/Transform/IR/TransformDialect.h"
17d13
< #include "mlir/Dialect/Utils/StructuredOpsUtils.h"
24d19
< 
30,36d24
< namespace tensor {
< class InsertSliceOp;
< class PackOp;
< class PadOp;
< class UnPackOp;
< } // namespace tensor
< 
44a33,41
> //===----------------------------------------------------------------------===//
> // Linalg Transform Operations
> //===----------------------------------------------------------------------===//
> 
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOpsEnums.h.inc"
> 
> #define GET_OP_CLASSES
> #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h.inc"
> 
50,51c47,48
< /// Implementation of tiling operations using `scf.forall`.
< DiagnosedSilenceableFailure tileToForallOpImpl(
---
> /// Implementation of tiling operations using `scf.foreach_thread`.
> DiagnosedSilenceableFailure tileToForeachThreadOpImpl(
57d53
< 
59,65d54
< } // namespace mlir
< 
< //===----------------------------------------------------------------------===//
< // Linalg Transform Operations
< //===----------------------------------------------------------------------===//
< 
< #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOpsEnums.h.inc"
67,68c56,59
< #define GET_OP_CLASSES
< #include "mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.h.inc"
---
> namespace linalg {
> void registerTransformDialectExtension(DialectRegistry &registry);
> } // namespace linalg
> } // namespace mlir
--- include/mlir/Dialect/Linalg/TransformOps/LinalgTransformOps.td
12,13d11
< include "mlir/Dialect/Linalg/TransformOps/LinalgTransformEnums.td"
< include "mlir/Dialect/Transform/IR/TransformAttrs.td"
14a13
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
19a19
> include "mlir/IR/EnumAttr.td"
31,82d30
< // BufferizeToAllocationOp
< //===----------------------------------------------------------------------===//
< 
< def BufferizeToAllocationOp : Op<Transform_Dialect,
<     "structured.bufferize_to_allocation",
<     [DeclareOpInterfaceMethods<TransformOpInterface>,
<      DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     This transform materializes an allocation for the targeted tensor value. It
<     replaces all original uses of the target with the newly allocated buffer,
<     wrapped in a `bufferization.to_tensor` op. It returns a handle to the result
<     of the `to_tensor` op.
< 
<     Example:
<     ```
<     %0 = "some_op"() : () -> (tensor<10xf32>)
<     "some_use"(%0) : (tensor<10xf32>) -> ()
<     ```
< 
<     Is rewritten to:
<     ```
<     %0 = "some_op"() : () -> (tensor<10xf32>)
<     %1 = memref.alloc() : memref<10xf32>
<     memref.tensor_store %0, %1 : memref<10xf32>
<     %2 = bufferization.to_tensor %1 restrict writable : memref<10xf32>
<     "some_use"(%2) : (tensor<10xf32>) -> ()
<     ```
< 
<     This transform has optimized lowerings for certain targets that are results
<     of non-DPS ops. For such targets, not only a buffer allocation is emitted
<     but also the defining op is bufferized. This is to avoid a second
<     allocation for the missing destination of the non-DPS op (when subsequently
<     running a bufferization pass/transform). Currently supported ops with
<     optimized lowerings:
<     - tensor.pad
< 
<     An optional memory space attribute can be specified for the materialized
<     buffer allocation.
< 
<     #### Return modes
< 
<     This operation consumes the `target` handle and produces the `transformed`
<     handle. It always succeeds.
<   }];
< 
<   let arguments = (ins Transform_AnyValue:$target,
<                        OptionalAttr<AnyAttr>:$memory_space);
<   let results = (outs Transform_AnyValue:$transformed);
<   let assemblyFormat = "$target attr-dict";
< }
< 
< //===----------------------------------------------------------------------===//
87,90c35,36
<     [FunctionalStyleTransformOpTrait, 
<      MemoryEffectsOpInterface,
<      TransformOpInterface, 
<      TransformEachOpTrait]> {
---
>     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
>      TransformOpInterface, TransformEachOpTrait]> {
146,147c92
<       [DeclareOpInterfaceMethods<TransformOpInterface>,
<        DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
>       [DeclareOpInterfaceMethods<TransformOpInterface>]> {
183,185c128,135
<   let arguments = (ins PDL_Operation:$producer_op,
<                        PDL_Operation:$containing_op);
<   let results = (outs PDL_Operation:$fused_op);
---
>   let arguments = (ins Arg<PDL_Operation, "",
>                            [TransformMappingRead,
>                             TransformMappingFree]>:$producer_op,
>                        Arg<PDL_Operation, "",
>                            [TransformMappingRead]>:$containing_op);
>   let results = (outs Res<PDL_Operation, "",
>                           [TransformMappingAlloc,
>                            TransformMappingWrite]>:$fused_op);
269,306c219
< // LowerPackOp
< //===----------------------------------------------------------------------===//
< def LowerPackOp : Op<Transform_Dialect, "structured.lower_pack", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          TransformEachOpTrait,
<                          TransformOpInterface]> {
<   let description = [{
<     Rewrite a tensor.pack into tensor.pad + tensor.expand_shape + linalg.transpose.
< 
<     #### Return modes
< 
<     This operation ignores non-pack ops and drops them in the return.
<     This operation produces a silenceableFailure if the rewrite fails for any
<     reason.
<     If all the operations referred to by the `target` are rewritten, the
<     transform succeeds.
<     Return handles to the newly produced pad, expand_shape and transpose ops.
<   }];
< 
<   let arguments = (ins Transform_ConcreteOpType<"tensor.pack">:$target);
<   let results = (outs Transform_ConcreteOpType<"tensor.pad">:$pad_op,
<                       Transform_ConcreteOpType<"tensor.expand_shape">:$expand_shape_op,
<                       Transform_ConcreteOpType<"linalg.transpose">:$transpose_op);
<   let assemblyFormat = [{
<     $target attr-dict `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::PackOp target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // LowerUnPackOp
---
> // MatchOp
308,325d220
< def LowerUnPackOp : Op<Transform_Dialect, "structured.lower_unpack", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          TransformEachOpTrait,
<                          TransformOpInterface]> {
<   let description = [{
<     Lower a tensor.unpack into empty + linalg.transpose + tensor.collapse_shape + 
<     tensor.extract_slice.
< 
<     #### Return modes
< 
<     This operation ignores non-unpack ops and drops them in the return.
<     This operation produces a silenceableFailure if the rewrite fails for any
<     reason.
<     If all the operations referred to by the `target` are rewritten, the
<     transform succeeds.
<     Return handles to the newly produced empty, transpose, collapse_shape and extract_slice ops.
<   }];
327,341c222,227
<   let arguments = (ins Transform_ConcreteOpType<"tensor.unpack">:$target);
<   let results = (outs Transform_ConcreteOpType<"tensor.empty">:$empty_op,
<                       Transform_ConcreteOpType<"linalg.transpose">:$transpose_op,
<                       Transform_ConcreteOpType<"tensor.collapse_shape">:$collapse_shape_op,
<                       Transform_ConcreteOpType<"tensor.extract_slice">:$extract_slice_op);
<   let assemblyFormat = [{ 
<     $target attr-dict `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::UnPackOp target,
<         ::mlir::transform::ApplyToEachResultList &transformResults,
<         ::mlir::transform::TransformState &state);
<   }];
---
> def MatchInterfaceEnum : I32EnumAttr<"MatchInterfaceEnum", "An interface to match",
>     [
>       I32EnumAttrCase<"LinalgOp", 0>,
>       I32EnumAttrCase<"TilingInterface", 1>
>     ]>{
>   let cppNamespace = "mlir::transform";
344,347d229
< //===----------------------------------------------------------------------===//
< // MatchOp
< //===----------------------------------------------------------------------===//
< 
381c263
<   let arguments = (ins TransformHandleTypeInterface:$target,
---
>   let arguments = (ins PDL_Operation:$target,
387c269
<   let results = (outs TransformHandleTypeInterface:$results);
---
>   let results = (outs PDL_Operation:$results);
390,391c272
<     OpBuilder<(ins "Value":$target, "ArrayRef<StringRef>":$opNames)>,
<     OpBuilder<(ins "TypeRange":$resultTypes, "Value":$target, "ArrayRef<StringRef>":$opNames)>
---
>     OpBuilder<(ins "Value":$target, "ArrayRef<StringRef>":$opNames)>
400d280
<     `:` functional-type($target, results)
486,489d365
< //===----------------------------------------------------------------------===//
< // PackOp
< //===----------------------------------------------------------------------===//
< 
491,492c367,368
<                 DeclareOpInterfaceMethods<TransformOpInterface>,
<                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
---
>                 TransformOpInterface,
>                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,]> {
565,673d440
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedPackedSizes)>
<   ];
< 
<   let extraClassDeclaration = [{
<     ::llvm::SmallVector<::mlir::OpFoldResult> getMixedPackedSizes();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // PackGreedilyOp
< //===----------------------------------------------------------------------===//
< def PackGreedilyOp : Op<Transform_Dialect, "structured.pack_greedily", [
<                         DeclareOpInterfaceMethods<TransformOpInterface>,
<                         DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     Target a Linalg op and rewrite it into packed LinalgOp form by trying to
<     infer whether a known suboperation is embedded
< 
<     Different packing strategies are applied in order, when one applies 
<     successfully, the transform returns:
<       1. Matmul packing: Try to infer a matmul operation embedded in the target op.
<          Specifically, this looks for 2 parallel dimensions that participate in
<          an outer-product and 1 reduction dimension.
<          These dimensions are referred as (m, n, k) to match canonical matmul
<          terminology.
<          
<          The packed sizes for (m, n, k) are specified by `matmul_packed_sizes`
<          and the optional `matmul_padded_sizes_next_multiple_of`.
<          When an entry `matmul_packed_sizes[i]` is non-0, the corresponding 
<          dimension is packed by `matmul_packed_sizes[i]`.
<          Otherwise, the dimension is merely padded to the next multiple of
<          `matmul_padded_sizes_next_multiple_of[i]`.
< 
<          `matmul_padded_sizes_next_multiple_of` is optional and is expected to
<          either be empty or of size `3`, matching the size of `matmul_packed_sizes`.
<          For each individual element of `matmul_packed_sizes` and 
<          `matmul_padded_sizes_next_multiple_of`, only one of them is allowed to
<          be non-zero.
<          
<          The ordering of the packed dimensions (mm, nn, kk) is specified by the
<          `matmul_inner_dims_order` attribute.
< 
<     Packing occurs as follows:
<       1. Find the dimensions to pack according to the strategy.
<       2. The target is converted to linalg.generic form.
<       3. An interchange transform is applied to isolate the dimensions to pack as
<          the most minor indexing dimensions of the linalg.generic. The most minor
<          dimensions are themselves ordered according to `inner_dims_order`.
<       4. An elementwise traversal of `matmul_packed_sizes` and
<          `matmul_padded_sizes_next_multiple_of` is performed and for each 
<          dimension `d`, either pack to `matmul_packed_sizes[d]` or pad to the
<          `matmul_padded_sizes_next_multiple_of[d]`.
<       5. Packing/padding is performed by the amounts determined in step 4. and
<          following `inner_dims_order`.
< 
<     By normalizing the most minor dimensions to `inner_dims_order`, the transform
<     guarantees that packing immediately generates inner dimensions in a desirable
<     layout.
< 
<     Outer dimension layout permutations are not controlled by this transform op
<     at the moment and can be obtained by composing with the pack_transpose
<     transformation.
< 
<     #### Return modes
< 
<     This operation ignores non-Linalg ops and drops them in the return.
<     It returns the list of packed Linalg ops or the original op when all available
<     packing strategies failed to apply.
<   }];
< 
<   // TODO: Transform_ConcreteOpType<linalg::LinalgOp> needs interface.
<   let arguments = (ins TransformHandleTypeInterface:$target,
<                    Variadic<PDL_Operation>:$matmul_packed_sizes,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [DenseArrayCount<3>]>:$static_matmul_packed_sizes,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [Attr<
<                                     Or<[DenseArrayCount<0>.predicate, 
<                                         DenseArrayCount<3>.predicate]>,
<                                         "with 0 or 3 elements"
<                                       >]>
<                                  :$matmul_padded_sizes_next_multiple_of,
<                    ConfinedAttr<DefaultValuedAttr<DenseI64ArrayAttr, "{}">,
<                                  [DenseArrayCount<3>]>:$matmul_inner_dims_order);
<   let results = (outs Transform_ConcreteOpType<"linalg.generic">:$packed_op);
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedMatmulPackedSizes,
<                    "ArrayRef<int64_t>":$matmulPaddededSizesNextMultipleOf,
<                    CArg<"ArrayRef<int64_t>", "{}">:$matmulDimsInnerDimsOrder)>
<   ];
< 
<   let assemblyFormat = [{
<     $target
<     oilist(
<       `matmul_packed_sizes` `=` custom<DynamicIndexList>($matmul_packed_sizes,
<                                                          $static_matmul_packed_sizes)
<       (`matmul_padded_sizes_next_multiple_of` `=` 
<         $matmul_padded_sizes_next_multiple_of^)?
<       `matmul_inner_dims_order` `=` $matmul_inner_dims_order
<     )
<     attr-dict
<     `:` functional-type($target, results)
<   }];
<   let hasVerifier = 1;
< 
675,723c442,444
<     /// Returns the list of tile sizes, which may be static (Attribute) or
<     /// dynamic (Value).
<     SmallVector<OpFoldResult> getMixedMatmulPackedSizes();
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // PackTransposeOp
< //===----------------------------------------------------------------------===//
< def PackTransposeOp : Op<Transform_Dialect, "structured.pack_transpose", [
<                          FunctionalStyleTransformOpTrait,
<                          MemoryEffectsOpInterface,
<                          DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let description = [{
<     Apply a transposition to a single `tensor.pack` (resp. `tensor.unpack`) and 
<     update the `linalg.generic` op that consumes (resp. produces) the operation.
< 
<     This transform allows composing a simple `structured.pack` with additional
<     transpositions to e.g. match the data format required by a specific library
<     call or ISA instruction.
< 
<     The transpose spec must specify at least one of `outer_perm` or `inner_perm`
<     attributes, which will act upon the `outer_dims_perm` or `inner_dims_pos` of
<     the specified `tensor.pack` or `tensor.unpack` op.
< 
<     If the `target` of this op is a `tensor.pack` then a new `tensor.empty` will
<     be created along with transposed versions of the `tensor.pack` and the 
<     consuming `linalg.generic`, which is expected to be the sole consumer.
< 
<     If the `target` of this op is a `tensor.unpack` then the whole pack / compute
<     / unpack chain will be transposed and transposed clones of `tensor.pack`,
<     the consuming `linalg.generic` and the tail `tensor.pack` will be created.
< 
<     #### Return modes
< 
<     This operation targets a single `tensor.pack` / `tensor.unpack` op and a
<     single matching `linalg.generic` that consumes / produces the op. Otherwise,
<     it produces a silenceableFailure.
< 
<     This operation may produce a silenceableFailure if the transpose spec is
<     ill-formed (i.e. `outer_perm` or `inner_perm` are not permutations of the
<     proper rank) or if the tranposition of all involved operations fails for any
<     reason.
< 
<     This operation returns 3 handles, one to the transformed LinalgOp, one to
<     the transformed `tensor.pack` and one to the transformed `tensor.unpack`.
<     The last handle for `tensor.unpack` is empty if `target_pack_or_unpack_op` 
<     was not itself a `tensor.unpack`.
<   }];
---
>     ::mlir::DiagnosedSilenceableFailure apply(
>       transform::TransformResults &transformResults,
>       transform::TransformState &state);
725,738c446
<   let arguments = (ins TransformHandleTypeInterface:$target_pack_or_un_pack_op,
<                        TransformHandleTypeInterface:$target_linalg_op,
<                        DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$outer_perm,
<                        DefaultValuedOptionalAttr<DenseI64ArrayAttr, "{}">:$inner_perm);
<   let results = (outs TransformHandleTypeInterface:$packed_op,
<                       TransformHandleTypeInterface:$pack_op,
<                       TransformHandleTypeInterface:$un_pack_op);
<   let assemblyFormat = [{
<     $target_pack_or_un_pack_op
<     `with_compute_op` `(` $target_linalg_op `)`
<     (`outer_perm` `=` $outer_perm^ )?
<     (`inner_perm` `=` $inner_perm^ )?
<     attr-dict
<     `:` functional-type(operands, results)
---
>     ::llvm::SmallVector<::mlir::OpFoldResult> getMixedPackedSizes();
740,741d447
< 
<   let hasVerifier = 1;
770a477
>          DefaultValuedAttr<I64ArrayAttr, "{}">:$hoist_paddings,
788,881d494
< // HoistPadOp
< //===----------------------------------------------------------------------===//
< 
< def HoistPadBuildPackingLoopNestOp :
<     Op<Transform_Dialect,
<        "structured.hoist_pad.build_packing_loop_nest",
<     [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      DeclareOpInterfaceMethods<TransformOpInterface>]> {
<   let description = [{
<     Helper transform used to hoist a tensor.pad target operation. This operation
<     creates the packing loop nest required by the hoist_pad operation and makes
<     that functionality available independently.
< 
<     TODO: In the future, we should consider rewriting as a tensor.pack after
<     hoisting since this abstraction is now available.
< 
<     #### Return modes
< 
<     This operation ignores non-tensor.pad ops and drops them in the result.
<     If any non-tensor.pad is passed, the transform emits a silenceable failure.
< 
<     The return handle points to only the subset of successfully created packing
<     loop nests, which can be empty.
<   }];
< 
<   // Also allow any !pdl.operation for simpler composition. Non-tensor.pad ops
<   // will be dropped from the results.
<   let arguments =
<     (ins TransformHandleTypeInterface:$target,
<          TransformHandleTypeInterface:$loop,
<          DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
<   let results = (outs TransformHandleTypeInterface:$packing_loop);
< 
<   let assemblyFormat = [{
<     $target
<     `above` $loop
<     (`,` `transpose` `by` $transpose^)?
<     attr-dict
<     `:` functional-type(operands, results)
<   }];
<   let hasVerifier = 1;
< }
< 
< def HoistPadOp : Op<Transform_Dialect, "structured.hoist_pad",
<     [FunctionalStyleTransformOpTrait,
<      MemoryEffectsOpInterface,
<      TransformOpInterface,
<      TransformEachOpTrait]> {
<   let description = [{
<     Hoist the tensor.pad target operation by at most the given number of loops.
<     Optionally apply the transpose attribute to the inner dimensions.
< 
<     TODO: In the future, we should consider rewriting as a tensor.pack after 
<     hoisting since this abstraction is now available.
<     TODO: Maybe also return the linalg.generic transpose created at some point.
< 
<     #### Return modes
< 
<     This operation ignores non-tensor.pad ops and drops them in the result.
<     If any non-tensor.pad is passed, the transform emits a silenceable failure.
< 
<     If all the operations referred to by the `target` handle padproperly, the
<     transform succeeds. Otherwise the transform silently fails.
< 
<     The return handle points to only the subset of successfully hoisted 
<     tensor.pad operations, which can be empty.
<   }];
< 
<   // Also allow any !pdl.operation for simpler composition. Non-tensor.pad ops
<   // will be dropped from the results.
<   let arguments =
<     (ins TransformHandleTypeInterface:$target,
<          I64Attr:$num_loops,
<          DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$transpose);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = [{
<     $target 
<     `by` $num_loops `loops` 
<     (`,` `transpose` `by` $transpose^)? 
<     attr-dict
<     `:` functional-type(operands, results)
<   }];
<   let hasVerifier = 1;
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::tensor::PadOp,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
885d497
< 
912d523
<                        OptionalAttr<DeviceMappingArrayAttr>:$mapping,
996,1042d606
< // RewriteInDestinationPassingStyleOp.
< //===----------------------------------------------------------------------===//
< 
< def RewriteInDestinationPassingStyleOp : Op<
<     Transform_Dialect, "structured.rewrite_in_destination_passing_style",
<     [FunctionalStyleTransformOpTrait, 
<      MemoryEffectsOpInterface,
<      TransformOpInterface, 
<      TransformEachOpTrait]> {
<   let description = [{
<     Rewrite a supported tensor operation that is not in destination-passing style
<     into a form that is in destination-passing style.
<     Currently supported operations are:
<       - tensor.pad
<       - tensor.generate
<       - tensor.from_elements
<     This dichotomy hints at a future interface, for now the implementation just 
<     switches between different implementation.
< 
<     #### Return modes
< 
<     This operation ignores non-unsupported ops and drops them from the return.
<     If all the operations referred to by the `target` PDLOperation generalize
<     properly, the transform succeeds. Otherwise the transform silently fails.
<     The return handle points to a subset of successfully produced operations:
<       - tensor.pad case, the returned handle points to the tensor.insert_slice.
<       - tensor.generate case, the returned handle points to the linalg.generic.
<       - tensor.from_elements case, the returned handle points to the last 
<         tensor.insert.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
<   let assemblyFormat = [{
<     $target attr-dict
<     `:` functional-type($target, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
1327c891
<   // TODO: support mixed static-dynamic (see TileToForallOp).
---
>   // TODO: support mixed static-dynamic (see TileToForeachThreadOp).
1355c919
< // TileReductionUsingForallOp
---
> // TileReductionUsingForeachThreadOp
1358,1359c922,923
< def TileReductionUsingForallOp :
<   Op<Transform_Dialect, "structured.tile_reduction_using_forall",
---
> def TileReductionUsingForeachThreadOp :
>   Op<Transform_Dialect, "structured.tile_reduction_using_foreach_thread",
1363c927
<     Tile a PartialReductionOpInterface op to a tiled `scf.forall` doing
---
>     Tile a PartialReductionOpInterface op to a tiled `scf.foreach_thread` doing
1368c932
<     `scf.forall` loops with the number threads given by `num_threads`.
---
>     `scf.foreach_thread` loops with the number threads given by `num_threads`.
1374c938
<     distributed on the threads of the `scf.foralls` loop.
---
>     distributed on the threads of the `scf.foreach_threads` loop.
1379c943
<       - the parent forall op,
---
>       - the parent foreach_thread op,
1404c968
<       %2 = scf.forall (%arg2) in (%c5) shared_outs(%arg3 = %1) -> (tensor<?x5xf32>) {
---
>       %2 = scf.foreach_thread (%arg2) in (%c5) shared_outs(%arg3 = %1) -> (tensor<?x5xf32>) {
1416c980
<         scf.forall.in_parallel {
---
>         scf.foreach_thread.perform_concurrently {
1428c992
<   // TODO: support mixed static-dynamic (see TileToForallOp).
---
>   // TODO: support mixed static-dynamic (see TileToForeachThreadOp).
1433c997
<   let results = (outs PDL_Operation:$forall_op,
---
>   let results = (outs PDL_Operation:$foreach_thread_op,
1542c1106
< // TileToForallOp
---
> // TileToForeachThreadOp
1545,1546c1109,1110
< def TileToForallOp :
<     Op<Transform_Dialect, "structured.tile_to_forall_op",
---
> def TileToForeachThreadOp :
>     Op<Transform_Dialect, "structured.tile_to_foreach_thread_op",
1551c1115
<     Tile a TilingInterface op to a tiled `scf.forall`.
---
>     Tile a TilingInterface op to a tiled `scf.foreach_thread`.
1568c1132
<     resulting `scf.forall`.
---
>     resulting `scf.foreach_thread`.
1587c1151
<       - the new scf.forall op,
---
>       - the new scf.foreach_thread op,
1594c1158
<     %3:2 = transform.structured.tile_to_forall_op %0 num_threads [10, 20]
---
>     %3:2 = transform.structured.tile_to_foreach_thread_op %0 num_threads [10, 20]
1602c1166
<     %3:2 = transform.structured.tile_to_forall_op %0 tile_sizes [0, %sz, 20]
---
>     %3:2 = transform.structured.tile_to_foreach_thread_op %0 tile_sizes [0, %sz, 20]
1614c1178
<   let results = (outs PDL_Operation:$forall_op,
---
>   let results = (outs PDL_Operation:$foreach_thread_op,
1713,1718d1276
<   
<   let builders = [
<     OpBuilder<(ins "Value":$target,
<                    "ArrayRef<OpFoldResult>":$mixedTileSizes,
<                    CArg<"ArrayRef<int64_t>", "{}">:$interchange)>
<   ];
1805,1810d1362
<     Note: The input vector sizes must be bigger than or equal to their
<     counterpart iteration space sizes.
< 
<     Typically this operator should be applied to linalg operations that have
<     already be tiled to the appropriate sizes.
< 
1821d1372
<                        UnitAttr:$vectorize_nd_extract,
1839,2043d1389
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // HoistRedundantVectorTransfersOp
< //===----------------------------------------------------------------------===//
< 
< def HoistRedundantVectorTransfersOp :
<   Op<Transform_Dialect, "structured.hoist_redundant_vector_transfers",
<     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<      TransformEachOpTrait, TransformOpInterface]> {
<   let description = [{
<     Hoist vector.transfer_read / vector.transfer_write pairs out of immediately
<     enclosing scf::ForOp iteratively, if the following conditions are true:
<        1. The 2 ops access the same memref with the same indices.
<        2. All operands are invariant under the enclosing scf::ForOp.
<        3. No uses of the memref either dominate the transfer_read or are
<        dominated by the transfer_write (i.e. no aliasing between the write and
<        the read across the loop)
< 
<     WARNING: This hoisting does not model parallelism and is generally incorrect
<     when used on distributed loops with memref semantics!
<     TODO: obsolete and should be retired.
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns a handle to the transformed
<     function op.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>,
<   ];
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::func::FuncOp target,
<          ::mlir::transform::ApplyToEachResultList &results,
<          ::mlir::transform::TransformState &state);
<    }];
< }
< 
< //===----------------------------------------------------------------------===//
< // ConvertConv2DToImg2ColOp
< //===----------------------------------------------------------------------===//
< 
< def ConvertConv2DToImg2ColOp : Op<Transform_Dialect,
<     "structured.convert_conv2d_to_img2col",
<     [FunctionalStyleTransformOpTrait,
<      MemoryEffectsOpInterface,
<      TransformOpInterface,
<      TransformEachOpTrait]> {
<   let description = [{
<     Convert linalg.conv_2d_xxx into linalg.generic (for img2col packing)
<     and linalg.matmul.
< 
<     A convolution operation can be written as a matrix-matrix multiplication by
<     unfolding the cross-correlation between input and filter and explicitly copy
<     overlapped sliding window inputs.
< 
<     Consider 2D input X with single channel input and output and 2x2 filter W:
<     ```
<     [x(0, 0)  , x(0, 1)  , ...,   x(0, n)  ]
<     [x(1, 0)  , x(1, 1)  , ...,   x(1, n)  ]
<     [.        ,  .       ,.   ,      .     ]            [w(0, 0), w(0, 1)]
<     [.        ,  .       , .  ,      .     ]    (conv)  [w(1, 0), w(1, 1)]
<     [.        ,  .       ,   .,      .     ]
<     [x(n-1, 0), x(n-1, 1), ..., x(n-1, n-1)]
<     ```
< 
<     The packed input data (img2col) is a matrix with |rows| = output spatial
<     size, |columns| = filter spatial size. To compute the output Y(i, j) we need
<     to calculate the dot product between filter window at input X(x, y)) and the
<     filter which will look like the following where r.h.s is the img2col matrix
<     and l.h.s is the flattned filter:
<     ```
<     [x(0,0), x(0,1), x(1,0), x(1,1)]
<     [x(0,1), x(1,1), x(0,2), x(1,2)] (matmul) [w(0,0), w(0,1), w(1,0), w(1,1)]
<     [x(0,1), x(1,1), x(0,2), x(1,2)]
<     [   .  ,    .  ,    .  ,    .  ]
<     ```
< 
<     In general for 2D case with (N, H, W, C) input and (Kh, Kw, C, D) filter
<     and output (N, Ho, Wo, D) the convolution is the following matrix-matrix
<     multiplication (Ho x Wo, Kh x Kw x C) * (Kh x Kw x C, D) for each input in
<     the N input. For the case where N > 1 its a batched matrxi-matrix
<     multplication.
< 
<     Returns two handles:
<     - One on the operation that produces the img2col tensor.
<     - One on the final operation of the sequence that replaces the original
<       convolution.
< 
<     #### Return modes:
< 
<     Returns a definite failure if target is not isolated from above.
<     Returns a silenceable failure if the pattern application failed.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$img2col_tensor,
<                       TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat =
<     "$target attr-dict `:` functional-type($target, results)";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>
<   ];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::linalg::LinalgOp target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // HoistRedundantTensorSubsetsOp
< //===----------------------------------------------------------------------===//
< 
< def HoistRedundantTensorSubsetsOp :
<   Op<Transform_Dialect, "structured.hoist_redundant_tensor_subsets",
<     [DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,
<      TransformEachOpTrait,
<      TransformOpInterface]> {
<   let description = [{
<     Hoists supported tensor subset extract/insert operation pairs out of 
<     immediately enclosing loop iteratively, if the following conditions
<     are true:
<        1. The 2 ops access the same tensor subset.
<        2. All operands are invariant under the enclosing loop.
<     
<     The supported subset extract/insert operation pairs currently comprise:
<        - tensor.extract_slice / tensor.insert_slice
<        - vector.transfer_read / vector.transfer_write on tensors
<     
<     Only scf.for loops are currently supported.
< 
<     When applied to:
<        1. an scf.for loop, hoist out of this loop only.
<        2. a non-loop op, apply hoisting to all the contained loop ops.
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns nothing.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs);
< 
<   let assemblyFormat = [{
<     $target 
<     attr-dict 
<     `:` functional-type(operands, results)
<   }];
< 
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
<   }];
< }
< 
< //===----------------------------------------------------------------------===//
< // InsertSliceToCopyOp
< //===----------------------------------------------------------------------===//
< 
< def InsertSliceToCopyOp :
<   Op<Transform_Dialect, "structured.insert_slice_to_copy",
<     [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<      TransformEachOpTrait, TransformOpInterface]> {
<   let description = [{
<     Targeted rewrite of an tensor.insert_slice to linalg.copy.
<     This is useful to materialize copies explicitly before bufferization and 
<     transform them, avoiding the need to rediscover them after bufferization.
< 
<     If the insert_slice source is already a linalg.copy, only return the source
<     op (i.e. do not create an additional linalg.copy op).
< 
<     #### Return modes:
< 
<     The operation always succeeds and returns a handle to the relevant 
<     linalg.copy op.
<   }];
< 
<   let arguments = (ins TransformHandleTypeInterface:$target);
<   let results = (outs TransformHandleTypeInterface:$transformed);
< 
<   let assemblyFormat = "$target attr-dict `:` functional-type(operands, results) ";
< 
<   let builders = [
<     OpBuilder<(ins "Value":$target)>,
<   ];
<   let extraClassDeclaration = [{
<     ::mlir::DiagnosedSilenceableFailure applyToOne(
<         ::mlir::Operation *target,
<         ::mlir::transform::ApplyToEachResultList &results,
<         ::mlir::transform::TransformState &state);
--- include/mlir/Dialect/Linalg/TransformOps/CMakeLists.txt
1,5d0
< set(LLVM_TARGET_DEFINITIONS LinalgMatchOps.td)
< mlir_tablegen(LinalgMatchOps.h.inc -gen-op-decls)
< mlir_tablegen(LinalgMatchOps.cpp.inc -gen-op-defs)
< add_public_tablegen_target(MLIRLinalgMatchOpsIncGen)
< 
9,11d3
< add_public_tablegen_target(MLIRLinalgTransformOpsIncGen)
< 
< set(LLVM_TARGET_DEFINITIONS LinalgTransformEnums.td)
14c6
< add_public_tablegen_target(MLIRLinalgTransformEnumsIncGen)
---
> add_public_tablegen_target(MLIRLinalgTransformOpsIncGen)
16d7
< add_mlir_doc(LinalgMatchOps LinalgStructuredMatchOps Dialects/ -gen-op-doc)
--- include/mlir/Dialect/Shape
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape/Analysis and include/mlir/Dialect/Shape/Analysis
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape/IR and include/mlir/Dialect/Shape/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape/Transforms and include/mlir/Dialect/Shape/Transforms
--- include/mlir/Dialect/Shape/CMakeLists.txt
--- include/mlir/Dialect/Shape/Transforms
--- include/mlir/Dialect/Shape/Transforms/Passes.h
--- include/mlir/Dialect/Shape/Transforms/Passes.td
--- include/mlir/Dialect/Shape/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/Shape/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Shape/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape/IR/ShapeBase.td include/mlir/Dialect/Shape/IR/ShapeBase.td
43a44
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Shape/IR/ShapeOps.td include/mlir/Dialect/Shape/IR/ShapeOps.td
1152,1163d1151
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
--- include/mlir/Dialect/Shape/IR/ShapeOps.td
1152,1163d1151
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
--- include/mlir/Dialect/Shape/IR/CMakeLists.txt
--- include/mlir/Dialect/Shape/IR/Shape.h
--- include/mlir/Dialect/Shape/IR/ShapeBase.td
43a44
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Shape/Analysis
--- include/mlir/Dialect/Shape/Analysis/ShapeMappingAnalysis.h
--- include/mlir/Dialect/CommonFolders.h
27d26
< /// Uses `resultType` for the type of the returned attribute.
33d31
<                                        Type resultType,
36c34
<   if (!resultType || !operands[0] || !operands[1])
---
>   if (!operands[0] || !operands[1])
50c48
<     return AttrElementT::get(resultType, *calRes);
---
>     return AttrElementT::get(lhs.getType(), *calRes);
67,70c65,67
<     return DenseElementsAttr::get(cast<ShapedType>(resultType), *elementResult);
<   }
< 
<   if (operands[0].isa<ElementsAttr>() && operands[1].isa<ElementsAttr>()) {
---
>     return DenseElementsAttr::get(lhs.getType(), *elementResult);
>   } else if (operands[0].isa<ElementsAttr>() &&
>              operands[1].isa<ElementsAttr>()) {
89c86
<     return DenseElementsAttr::get(cast<ShapedType>(resultType), elementResults);
---
>     return DenseElementsAttr::get(lhs.getType(), elementResults);
94,135d90
< /// Performs constant folding `calculate` with element-wise behavior on the two
< /// attributes in `operands` and returns the result if possible.
< /// Uses the operand element type for the element type of the returned
< /// attribute.
< template <class AttrElementT,
<           class ElementValueT = typename AttrElementT::ValueType,
<           class CalculationT = function_ref<
<               std::optional<ElementValueT>(ElementValueT, ElementValueT)>>
< Attribute constFoldBinaryOpConditional(ArrayRef<Attribute> operands,
<                                        const CalculationT &calculate) {
<   assert(operands.size() == 2 && "binary op takes two operands");
<   auto getResultType = [](Attribute attr) -> Type {
<     if (auto typed = attr.dyn_cast_or_null<TypedAttr>())
<       return typed.getType();
<     return {};
<   };
< 
<   Type lhsType = getResultType(operands[0]);
<   Type rhsType = getResultType(operands[1]);
<   if (!lhsType || !rhsType)
<     return {};
<   if (lhsType != rhsType)
<     return {};
< 
<   return constFoldBinaryOpConditional<AttrElementT, ElementValueT,
<                                       CalculationT>(operands, lhsType,
<                                                     calculate);
< }
< 
< template <class AttrElementT,
<           class ElementValueT = typename AttrElementT::ValueType,
<           class CalculationT =
<               function_ref<ElementValueT(ElementValueT, ElementValueT)>>
< Attribute constFoldBinaryOp(ArrayRef<Attribute> operands, Type resultType,
<                             const CalculationT &calculate) {
<   return constFoldBinaryOpConditional<AttrElementT>(
<       operands, resultType,
<       [&](ElementValueT a, ElementValueT b) -> std::optional<ElementValueT> {
<         return calculate(a, b);
<       });
< }
< 
192c147
<     return DenseElementsAttr::get(op.getShapedType(), elementResults);
---
>     return DenseElementsAttr::get(op.getType(), elementResults);
236c191
<     return DenseElementsAttr::get(cast<ShapedType>(resType), elementResult);
---
>     return DenseElementsAttr::get(resType, elementResult);
253c208
<     return DenseElementsAttr::get(cast<ShapedType>(resType), elementResults);
---
>     return DenseElementsAttr::get(resType, elementResults);
--- include/mlir/Dialect/Math
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Math/IR and include/mlir/Dialect/Math/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Math/Transforms and include/mlir/Dialect/Math/Transforms
--- include/mlir/Dialect/Math/CMakeLists.txt
--- include/mlir/Dialect/Math/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Math/Transforms/Passes.h include/mlir/Dialect/Math/Transforms/Passes.h
17d16
< void populateExpandTanPattern(RewritePatternSet &patterns);
19,25c18
< void populateExpandFmaFPattern(RewritePatternSet &patterns);
< void populateExpandFloorFPattern(RewritePatternSet &patterns);
< void populateExpandCeilFPattern(RewritePatternSet &patterns);
< void populateExpandExp2FPattern(RewritePatternSet &patterns);
< void populateExpandPowFPattern(RewritePatternSet &patterns);
< void populateExpandRoundFPattern(RewritePatternSet &patterns);
< void populateExpandRoundEvenPattern(RewritePatternSet &patterns);
---
> 
--- include/mlir/Dialect/Math/Transforms/Passes.h
17d16
< void populateExpandTanPattern(RewritePatternSet &patterns);
19,25c18
< void populateExpandFmaFPattern(RewritePatternSet &patterns);
< void populateExpandFloorFPattern(RewritePatternSet &patterns);
< void populateExpandCeilFPattern(RewritePatternSet &patterns);
< void populateExpandExp2FPattern(RewritePatternSet &patterns);
< void populateExpandPowFPattern(RewritePatternSet &patterns);
< void populateExpandRoundFPattern(RewritePatternSet &patterns);
< void populateExpandRoundEvenPattern(RewritePatternSet &patterns);
---
> 
--- include/mlir/Dialect/Math/Transforms/Approximation.h
--- include/mlir/Dialect/Math/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Math/IR/MathBase.td include/mlir/Dialect/Math/IR/MathBase.td
33,35c33
<   let dependentDialects = [
<     "::mlir::arith::ArithDialect"
<   ];
---
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Math/IR/CMakeLists.txt
--- include/mlir/Dialect/Math/IR/Math.h
--- include/mlir/Dialect/Math/IR/MathOps.td
--- include/mlir/Dialect/Math/IR/MathBase.td
33,35c33
<   let dependentDialects = [
<     "::mlir::arith::ArithDialect"
<   ];
---
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Complex
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Complex/IR and include/mlir/Dialect/Complex/IR
--- include/mlir/Dialect/Complex/CMakeLists.txt
--- include/mlir/Dialect/Complex/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Complex/IR/ComplexBase.td include/mlir/Dialect/Complex/IR/ComplexBase.td
24a25
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Complex/IR/Complex.h
--- include/mlir/Dialect/Complex/IR/ComplexOps.td
--- include/mlir/Dialect/Complex/IR/ComplexAttributes.td
--- include/mlir/Dialect/Complex/IR/CMakeLists.txt
--- include/mlir/Dialect/Complex/IR/ComplexBase.td
24a25
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Bufferization
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR and include/mlir/Dialect/Bufferization/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/TransformOps and include/mlir/Dialect/Bufferization/TransformOps
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms and include/mlir/Dialect/Bufferization/Transforms
--- include/mlir/Dialect/Bufferization/CMakeLists.txt
--- include/mlir/Dialect/Bufferization/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/BufferUtils.h include/mlir/Dialect/Bufferization/Transforms/BufferUtils.h
128,129c128
<                                          uint64_t alignment,
<                                          Attribute memorySpace = {});
---
>                                          uint64_t alignment);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/BufferViewFlowAnalysis.h include/mlir/Dialect/Bufferization/Transforms/BufferViewFlowAnalysis.h
58c58
<   void remove(const SetVector<Value> &aliasValues);
---
>   void remove(const SmallPtrSetImpl<Value> &aliasValues);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/FuncBufferizableOpInterfaceImpl.h include/mlir/Dialect/Bufferization/Transforms/FuncBufferizableOpInterfaceImpl.h
18a19,22
> namespace func {
> class FuncOp;
> } // namespace func
> 
23a28,29
> using func::FuncOp;
> 
47c53,56
<   DenseMap<FunctionOpInterface, IndexMapping> equivalentFuncArgs;
---
>   DenseMap<FuncOp, IndexMapping> equivalentFuncArgs;
> 
>   /// A mapping of ReturnOp OpOperand indices to aliasing FuncOp BBArg indices.
>   DenseMap<FuncOp, IndexToIndexListMapping> aliasingFuncArgs;
50c59
<   DenseMap<FunctionOpInterface, IndexToIndexListMapping> aliasingReturnVals;
---
>   DenseMap<FuncOp, IndexToIndexListMapping> aliasingReturnVals;
53c62
<   DenseMap<FunctionOpInterface, BbArgIndexSet> readBbArgs;
---
>   DenseMap<FuncOp, BbArgIndexSet> readBbArgs;
56c65
<   DenseMap<FunctionOpInterface, BbArgIndexSet> writtenBbArgs;
---
>   DenseMap<FuncOp, BbArgIndexSet> writtenBbArgs;
60c69
<   DenseMap<FunctionOpInterface, FuncOpAnalysisState> analyzedFuncOps;
---
>   DenseMap<FuncOp, FuncOpAnalysisState> analyzedFuncOps;
64c73
<   void startFunctionAnalysis(FunctionOpInterface funcOp);
---
>   void startFunctionAnalysis(FuncOp funcOp);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/OneShotAnalysis.h include/mlir/Dialect/Bufferization/Transforms/OneShotAnalysis.h
14d13
< #include <string>
17,18d15
< class DominanceInfo;
< 
21a19
> class BufferizationAliasInfo;
37a36,112
> };
> 
> /// The BufferizationAliasInfo class maintains a list of buffer aliases and
> /// equivalence classes to support bufferization.
> class BufferizationAliasInfo {
> public:
>   explicit BufferizationAliasInfo(Operation *rootOp);
> 
>   // BufferizationAliasInfo should be passed as a reference.
>   BufferizationAliasInfo(const BufferizationAliasInfo &) = delete;
> 
>   /// Add a new entry for `v` in the `aliasInfo` and `equivalentInfo`. In the
>   /// beginning the alias and equivalence sets only contain `v` itself.
>   void createAliasInfoEntry(Value v);
> 
>   /// Insert an info entry for `newValue` and merge its alias set with that of
>   /// `alias`.
>   void insertNewBufferAlias(Value newValue, Value alias);
> 
>   /// Insert an info entry for `newValue` and merge its alias set with that of
>   /// `alias`. Additionally, merge their equivalence classes.
>   void insertNewBufferEquivalence(Value newValue, Value alias);
> 
>   /// Set the inPlace bufferization spec to true.
>   /// Merge result's and operand's aliasing sets and iterate to a fixed point.
>   void bufferizeInPlace(OpOperand &operand, AnalysisState &state);
> 
>   /// Set the inPlace bufferization spec to false.
>   void bufferizeOutOfPlace(OpOperand &operand);
> 
>   /// Return true if `v1` and `v2` may bufferize to aliasing buffers.
>   bool areAliasingBufferizedValues(Value v1, Value v2) const {
>     return aliasInfo.isEquivalent(v1, v2);
>   }
> 
>   /// Return true if `v1` and `v2` bufferize to equivalent buffers.
>   bool areEquivalentBufferizedValues(Value v1, Value v2) const {
>     return equivalentInfo.isEquivalent(v1, v2);
>   }
> 
>   /// Union the alias sets of `v1` and `v2`.
>   void unionAliasSets(Value v1, Value v2) { aliasInfo.unionSets(v1, v2); }
> 
>   /// Union the equivalence classes of `v1` and `v2`.
>   void unionEquivalenceClasses(Value v1, Value v2) {
>     equivalentInfo.unionSets(v1, v2);
>   }
> 
>   /// Apply `fun` to all the members of the equivalence class of `v`.
>   void applyOnEquivalenceClass(Value v, function_ref<void(Value)> fun) const;
> 
>   /// Apply `fun` to all aliases of `v`.
>   void applyOnAliases(Value v, function_ref<void(Value)> fun) const;
> 
>   /// Mark a value as in-place bufferized.
>   void markInPlace(OpOperand &o) { inplaceBufferized.insert(&o); }
> 
>   /// Return `true` if a value was marked as in-place bufferized.
>   bool isInPlace(OpOperand &opOperand) const;
> 
>   int64_t getStatNumTensorOutOfPlace() const { return statNumTensorOutOfPlace; }
>   int64_t getStatNumTensorInPlace() const { return statNumTensorInPlace; }
> 
> private:
>   /// llvm::EquivalenceClasses wants comparable elements. This comparator uses
>   /// uses pointer comparison on the defining op. This is a poor man's
>   /// comparison but it's not like UnionFind needs ordering anyway.
>   struct ValueComparator {
>     bool operator()(const Value &lhs, const Value &rhs) const {
>       return lhs.getImpl() < rhs.getImpl();
>     }
>   };
> 
>   using EquivalenceClassRangeType = llvm::iterator_range<
>       llvm::EquivalenceClasses<Value, ValueComparator>::member_iterator>;
>   /// Check that aliasInfo for `v` exists and return a reference to it.
>   EquivalenceClassRangeType getAliases(Value v) const;
39,41c114,134
<   /// Specify the functions that should not be analyzed. copyBeforeWrite will be
<   /// set to true when bufferizing them.
<   llvm::ArrayRef<std::string> noAnalysisFuncFilter;
---
>   /// Set of all OpResults that were decided to bufferize in-place.
>   llvm::DenseSet<OpOperand *> inplaceBufferized;
> 
>   /// Auxiliary structure to store all the values a given value may alias with.
>   /// Alias information is "may be" conservative: In the presence of branches, a
>   /// value may alias with one of multiple other values. The concrete aliasing
>   /// value may not even be known at compile time. All such values are
>   /// considered to be aliases.
>   llvm::EquivalenceClasses<Value, ValueComparator> aliasInfo;
> 
>   /// Auxiliary structure to store all the equivalent buffer classes. Equivalent
>   /// buffer information is "must be" conservative: Only if two values are
>   /// guaranteed to be equivalent at runtime, they said to be equivalent. It is
>   /// possible that, in the presence of branches, it cannot be determined
>   /// statically if two values are equivalent. In that case, the values are
>   /// considered to be not equivalent.
>   llvm::EquivalenceClasses<Value, ValueComparator> equivalentInfo;
> 
>   // Bufferization statistics.
>   int64_t statNumTensorOutOfPlace = 0;
>   int64_t statNumTensorInPlace = 0;
45,48c138,139
< /// sets, equivalence sets, in-place OpOperands and other things.
< ///
< /// Note: Modifying the IR generally invalidates the result of the analysis.
< /// Adding new operations is safe if they are analyzed subsequently.
---
> /// (via BufferizationAliasInfo) to decide if tensor OpOperands should bufferize
> /// in-place.
68,69c159,160
<   /// Analyze the given op and its nested ops.
<   LogicalResult analyzeOp(Operation *op, const DominanceInfo &domInfo);
---
>   /// Return a reference to the BufferizationAliasInfo.
>   BufferizationAliasInfo &getAliasInfo() { return aliasInfo; }
71,78c162,163
<   /// Analyze a single op (without nested ops).
<   LogicalResult analyzeSingleOp(Operation *op, const DominanceInfo &domInfo);
< 
<   /// Apply `fun` to all the members of the equivalence class of `v`.
<   void applyOnEquivalenceClass(Value v, function_ref<void(Value)> fun) const;
< 
<   /// Apply `fun` to all aliases of `v`.
<   void applyOnAliases(Value v, function_ref<void(Value)> fun) const;
---
>   /// Return `true` if the given OpResult has been decided to bufferize inplace.
>   bool isInPlace(OpOperand &opOperand) const override;
86,91c171,172
<   /// Mark the given OpOperand as in-place and merge the results' and operand's
<   /// aliasing sets.
<   void bufferizeInPlace(OpOperand &operand);
< 
<   /// Mark the given OpOperand as out-of-place.
<   void bufferizeOutOfPlace(OpOperand &operand);
---
>   /// Return `true` if the given tensor has undefined contents.
>   bool hasUndefinedContents(OpOperand *opOperand) const override;
93,95c174,176
<   /// Add a new entry for `v` in the `aliasInfo` and `equivalentInfo`. In the
<   /// beginning the alias and equivalence sets only contain `v` itself.
<   void createAliasInfoEntry(Value v);
---
>   /// Return true if the given tensor (or an aliasing tensor) is yielded from
>   /// the containing block. Also include all aliasing tensors in the same block.
>   bool isTensorYielded(Value tensor) const override;
105,117d185
<   int64_t getStatNumTensorOutOfPlace() const { return statNumTensorOutOfPlace; }
<   int64_t getStatNumTensorInPlace() const { return statNumTensorInPlace; }
< 
<   /// Return `true` if the given tensor has undefined contents.
<   bool hasUndefinedContents(OpOperand *opOperand) const override;
< 
<   /// Return `true` if the given OpResult has been decided to bufferize inplace.
<   bool isInPlace(OpOperand &opOperand) const override;
< 
<   /// Return true if the given tensor (or an aliasing tensor) is yielded from
<   /// the containing block. Also include all aliasing tensors in the same block.
<   bool isTensorYielded(Value tensor) const override;
< 
125,137d192
<   /// Find the definitions of the given tensor value or retrieve them from the
<   /// cache.
<   const SetVector<Value> &findDefinitionsCached(Value value);
< 
<   /// Reset cached data structures.
<   void resetCache();
< 
<   /// Union the alias sets of `v1` and `v2`.
<   void unionAliasSets(Value v1, Value v2);
< 
<   /// Union the equivalence classes of `v1` and `v2`.
<   void unionEquivalenceClasses(Value v1, Value v2);
< 
222,259c277,279
<   /// llvm::EquivalenceClasses wants comparable elements. This comparator uses
<   /// pointer comparison on the defining op. This is a poor man's comparison
<   /// but it's not like UnionFind needs ordering anyway.
<   struct ValueComparator {
<     bool operator()(const Value &lhs, const Value &rhs) const {
<       return lhs.getImpl() < rhs.getImpl();
<     }
<   };
< 
<   using EquivalenceClassRangeType = llvm::iterator_range<
<       llvm::EquivalenceClasses<Value, ValueComparator>::member_iterator>;
<   /// Check that aliasInfo for `v` exists and return a reference to it.
<   EquivalenceClassRangeType getAliases(Value v) const;
< 
<   /// Cache definitions of tensor values.
<   DenseMap<Value, SetVector<Value>> cachedDefinitions;
< 
<   /// Set of all OpResults that were decided to bufferize in-place.
<   llvm::DenseSet<OpOperand *> inplaceBufferized;
< 
<   /// Auxiliary structure to store all the values a given value may alias with.
<   /// Alias information is "may be" conservative: In the presence of branches, a
<   /// value may alias with one of multiple other values. The concrete aliasing
<   /// value may not even be known at compile time. All such values are
<   /// considered to be aliases.
<   llvm::EquivalenceClasses<Value, ValueComparator> aliasInfo;
< 
<   /// Auxiliary structure to store all the equivalent buffer classes. Equivalent
<   /// buffer information is "must be" conservative: Only if two values are
<   /// guaranteed to be equivalent at runtime, they said to be equivalent. It is
<   /// possible that, in the presence of branches, it cannot be determined
<   /// statically if two values are equivalent. In that case, the values are
<   /// considered to be not equivalent.
<   llvm::EquivalenceClasses<Value, ValueComparator> equivalentInfo;
< 
<   // Bufferization statistics.
<   int64_t statNumTensorOutOfPlace = 0;
<   int64_t statNumTensorInPlace = 0;
---
>   /// `aliasInfo` keeps track of aliasing and equivalent values. Only internal
>   /// functions and `runOneShotBufferize` may access this object.
>   BufferizationAliasInfo aliasInfo;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/OneShotModuleBufferize.h include/mlir/Dialect/Bufferization/Transforms/OneShotModuleBufferize.h
30,35c30,31
< /// inserted except two cases:
< /// - `options.copyBeforeWrite` is set, in which case buffers are copied before
< ///   every write.
< /// - `options.copyBeforeWrite` is not set and `options.noAnalysisFuncFilter`
< ///   is not empty. The FuncOps it contains were not analyzed. Buffer copies
< ///   will be inserted only to these FuncOps.
---
> /// inserted unless `options.copyBeforeWrite` is set, in which case buffers are
> /// copied before every write.
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/Passes.h include/mlir/Dialect/Bufferization/Transforms/Passes.h
5d4
< #include "mlir/IR/FunctionInterfaces.h"
12c11
< }
---
> } // namespace func
44c43
<   llvm::function_ref<bool(FunctionOpInterface)> filterFn = [](FunctionOpInterface func) {
---
>   llvm::function_ref<bool(func::FuncOp *)> filterFn = [](func::FuncOp *func) {
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/Passes.td include/mlir/Dialect/Bufferization/Transforms/Passes.td
300,302d299
<     ListOption<"noAnalysisFuncFilter", "no-analysis-func-filter", "std::string",
<                "Skip analysis of functions with these symbol names."
<                "Set copyBeforeWrite to true when bufferizing them.">,
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/Transforms/Transforms.h include/mlir/Dialect/Bufferization/Transforms/Transforms.h
19d18
< class OneShotAnalysisState;
40c39
<                                     OneShotAnalysisState &state,
---
>                                     bufferization::AnalysisState &state,
48c47
<     RewriterBase &rewriter, Operation *op, OneShotAnalysisState &state);
---
>     RewriterBase &rewriter, Operation *op, bufferization::AnalysisState &state);
--- include/mlir/Dialect/Bufferization/Transforms/Passes.h
5d4
< #include "mlir/IR/FunctionInterfaces.h"
12c11
< }
---
> } // namespace func
44c43
<   llvm::function_ref<bool(FunctionOpInterface)> filterFn = [](FunctionOpInterface func) {
---
>   llvm::function_ref<bool(func::FuncOp *)> filterFn = [](func::FuncOp *func) {
--- include/mlir/Dialect/Bufferization/Transforms/Passes.td
300,302d299
<     ListOption<"noAnalysisFuncFilter", "no-analysis-func-filter", "std::string",
<                "Skip analysis of functions with these symbol names."
<                "Set copyBeforeWrite to true when bufferizing them.">,
--- include/mlir/Dialect/Bufferization/Transforms/BufferUtils.h
128,129c128
<                                          uint64_t alignment,
<                                          Attribute memorySpace = {});
---
>                                          uint64_t alignment);
--- include/mlir/Dialect/Bufferization/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Bufferization/Transforms/OneShotAnalysis.h
14d13
< #include <string>
17,18d15
< class DominanceInfo;
< 
21a19
> class BufferizationAliasInfo;
37a36,112
> };
> 
> /// The BufferizationAliasInfo class maintains a list of buffer aliases and
> /// equivalence classes to support bufferization.
> class BufferizationAliasInfo {
> public:
>   explicit BufferizationAliasInfo(Operation *rootOp);
> 
>   // BufferizationAliasInfo should be passed as a reference.
>   BufferizationAliasInfo(const BufferizationAliasInfo &) = delete;
> 
>   /// Add a new entry for `v` in the `aliasInfo` and `equivalentInfo`. In the
>   /// beginning the alias and equivalence sets only contain `v` itself.
>   void createAliasInfoEntry(Value v);
> 
>   /// Insert an info entry for `newValue` and merge its alias set with that of
>   /// `alias`.
>   void insertNewBufferAlias(Value newValue, Value alias);
> 
>   /// Insert an info entry for `newValue` and merge its alias set with that of
>   /// `alias`. Additionally, merge their equivalence classes.
>   void insertNewBufferEquivalence(Value newValue, Value alias);
> 
>   /// Set the inPlace bufferization spec to true.
>   /// Merge result's and operand's aliasing sets and iterate to a fixed point.
>   void bufferizeInPlace(OpOperand &operand, AnalysisState &state);
> 
>   /// Set the inPlace bufferization spec to false.
>   void bufferizeOutOfPlace(OpOperand &operand);
> 
>   /// Return true if `v1` and `v2` may bufferize to aliasing buffers.
>   bool areAliasingBufferizedValues(Value v1, Value v2) const {
>     return aliasInfo.isEquivalent(v1, v2);
>   }
> 
>   /// Return true if `v1` and `v2` bufferize to equivalent buffers.
>   bool areEquivalentBufferizedValues(Value v1, Value v2) const {
>     return equivalentInfo.isEquivalent(v1, v2);
>   }
> 
>   /// Union the alias sets of `v1` and `v2`.
>   void unionAliasSets(Value v1, Value v2) { aliasInfo.unionSets(v1, v2); }
> 
>   /// Union the equivalence classes of `v1` and `v2`.
>   void unionEquivalenceClasses(Value v1, Value v2) {
>     equivalentInfo.unionSets(v1, v2);
>   }
> 
>   /// Apply `fun` to all the members of the equivalence class of `v`.
>   void applyOnEquivalenceClass(Value v, function_ref<void(Value)> fun) const;
> 
>   /// Apply `fun` to all aliases of `v`.
>   void applyOnAliases(Value v, function_ref<void(Value)> fun) const;
> 
>   /// Mark a value as in-place bufferized.
>   void markInPlace(OpOperand &o) { inplaceBufferized.insert(&o); }
> 
>   /// Return `true` if a value was marked as in-place bufferized.
>   bool isInPlace(OpOperand &opOperand) const;
> 
>   int64_t getStatNumTensorOutOfPlace() const { return statNumTensorOutOfPlace; }
>   int64_t getStatNumTensorInPlace() const { return statNumTensorInPlace; }
> 
> private:
>   /// llvm::EquivalenceClasses wants comparable elements. This comparator uses
>   /// uses pointer comparison on the defining op. This is a poor man's
>   /// comparison but it's not like UnionFind needs ordering anyway.
>   struct ValueComparator {
>     bool operator()(const Value &lhs, const Value &rhs) const {
>       return lhs.getImpl() < rhs.getImpl();
>     }
>   };
> 
>   using EquivalenceClassRangeType = llvm::iterator_range<
>       llvm::EquivalenceClasses<Value, ValueComparator>::member_iterator>;
>   /// Check that aliasInfo for `v` exists and return a reference to it.
>   EquivalenceClassRangeType getAliases(Value v) const;
39,41c114,134
<   /// Specify the functions that should not be analyzed. copyBeforeWrite will be
<   /// set to true when bufferizing them.
<   llvm::ArrayRef<std::string> noAnalysisFuncFilter;
---
>   /// Set of all OpResults that were decided to bufferize in-place.
>   llvm::DenseSet<OpOperand *> inplaceBufferized;
> 
>   /// Auxiliary structure to store all the values a given value may alias with.
>   /// Alias information is "may be" conservative: In the presence of branches, a
>   /// value may alias with one of multiple other values. The concrete aliasing
>   /// value may not even be known at compile time. All such values are
>   /// considered to be aliases.
>   llvm::EquivalenceClasses<Value, ValueComparator> aliasInfo;
> 
>   /// Auxiliary structure to store all the equivalent buffer classes. Equivalent
>   /// buffer information is "must be" conservative: Only if two values are
>   /// guaranteed to be equivalent at runtime, they said to be equivalent. It is
>   /// possible that, in the presence of branches, it cannot be determined
>   /// statically if two values are equivalent. In that case, the values are
>   /// considered to be not equivalent.
>   llvm::EquivalenceClasses<Value, ValueComparator> equivalentInfo;
> 
>   // Bufferization statistics.
>   int64_t statNumTensorOutOfPlace = 0;
>   int64_t statNumTensorInPlace = 0;
45,48c138,139
< /// sets, equivalence sets, in-place OpOperands and other things.
< ///
< /// Note: Modifying the IR generally invalidates the result of the analysis.
< /// Adding new operations is safe if they are analyzed subsequently.
---
> /// (via BufferizationAliasInfo) to decide if tensor OpOperands should bufferize
> /// in-place.
68,69c159,160
<   /// Analyze the given op and its nested ops.
<   LogicalResult analyzeOp(Operation *op, const DominanceInfo &domInfo);
---
>   /// Return a reference to the BufferizationAliasInfo.
>   BufferizationAliasInfo &getAliasInfo() { return aliasInfo; }
71,78c162,163
<   /// Analyze a single op (without nested ops).
<   LogicalResult analyzeSingleOp(Operation *op, const DominanceInfo &domInfo);
< 
<   /// Apply `fun` to all the members of the equivalence class of `v`.
<   void applyOnEquivalenceClass(Value v, function_ref<void(Value)> fun) const;
< 
<   /// Apply `fun` to all aliases of `v`.
<   void applyOnAliases(Value v, function_ref<void(Value)> fun) const;
---
>   /// Return `true` if the given OpResult has been decided to bufferize inplace.
>   bool isInPlace(OpOperand &opOperand) const override;
86,91c171,172
<   /// Mark the given OpOperand as in-place and merge the results' and operand's
<   /// aliasing sets.
<   void bufferizeInPlace(OpOperand &operand);
< 
<   /// Mark the given OpOperand as out-of-place.
<   void bufferizeOutOfPlace(OpOperand &operand);
---
>   /// Return `true` if the given tensor has undefined contents.
>   bool hasUndefinedContents(OpOperand *opOperand) const override;
93,95c174,176
<   /// Add a new entry for `v` in the `aliasInfo` and `equivalentInfo`. In the
<   /// beginning the alias and equivalence sets only contain `v` itself.
<   void createAliasInfoEntry(Value v);
---
>   /// Return true if the given tensor (or an aliasing tensor) is yielded from
>   /// the containing block. Also include all aliasing tensors in the same block.
>   bool isTensorYielded(Value tensor) const override;
105,117d185
<   int64_t getStatNumTensorOutOfPlace() const { return statNumTensorOutOfPlace; }
<   int64_t getStatNumTensorInPlace() const { return statNumTensorInPlace; }
< 
<   /// Return `true` if the given tensor has undefined contents.
<   bool hasUndefinedContents(OpOperand *opOperand) const override;
< 
<   /// Return `true` if the given OpResult has been decided to bufferize inplace.
<   bool isInPlace(OpOperand &opOperand) const override;
< 
<   /// Return true if the given tensor (or an aliasing tensor) is yielded from
<   /// the containing block. Also include all aliasing tensors in the same block.
<   bool isTensorYielded(Value tensor) const override;
< 
125,137d192
<   /// Find the definitions of the given tensor value or retrieve them from the
<   /// cache.
<   const SetVector<Value> &findDefinitionsCached(Value value);
< 
<   /// Reset cached data structures.
<   void resetCache();
< 
<   /// Union the alias sets of `v1` and `v2`.
<   void unionAliasSets(Value v1, Value v2);
< 
<   /// Union the equivalence classes of `v1` and `v2`.
<   void unionEquivalenceClasses(Value v1, Value v2);
< 
222,259c277,279
<   /// llvm::EquivalenceClasses wants comparable elements. This comparator uses
<   /// pointer comparison on the defining op. This is a poor man's comparison
<   /// but it's not like UnionFind needs ordering anyway.
<   struct ValueComparator {
<     bool operator()(const Value &lhs, const Value &rhs) const {
<       return lhs.getImpl() < rhs.getImpl();
<     }
<   };
< 
<   using EquivalenceClassRangeType = llvm::iterator_range<
<       llvm::EquivalenceClasses<Value, ValueComparator>::member_iterator>;
<   /// Check that aliasInfo for `v` exists and return a reference to it.
<   EquivalenceClassRangeType getAliases(Value v) const;
< 
<   /// Cache definitions of tensor values.
<   DenseMap<Value, SetVector<Value>> cachedDefinitions;
< 
<   /// Set of all OpResults that were decided to bufferize in-place.
<   llvm::DenseSet<OpOperand *> inplaceBufferized;
< 
<   /// Auxiliary structure to store all the values a given value may alias with.
<   /// Alias information is "may be" conservative: In the presence of branches, a
<   /// value may alias with one of multiple other values. The concrete aliasing
<   /// value may not even be known at compile time. All such values are
<   /// considered to be aliases.
<   llvm::EquivalenceClasses<Value, ValueComparator> aliasInfo;
< 
<   /// Auxiliary structure to store all the equivalent buffer classes. Equivalent
<   /// buffer information is "must be" conservative: Only if two values are
<   /// guaranteed to be equivalent at runtime, they said to be equivalent. It is
<   /// possible that, in the presence of branches, it cannot be determined
<   /// statically if two values are equivalent. In that case, the values are
<   /// considered to be not equivalent.
<   llvm::EquivalenceClasses<Value, ValueComparator> equivalentInfo;
< 
<   // Bufferization statistics.
<   int64_t statNumTensorOutOfPlace = 0;
<   int64_t statNumTensorInPlace = 0;
---
>   /// `aliasInfo` keeps track of aliasing and equivalent values. Only internal
>   /// functions and `runOneShotBufferize` may access this object.
>   BufferizationAliasInfo aliasInfo;
--- include/mlir/Dialect/Bufferization/Transforms/BufferViewFlowAnalysis.h
58c58
<   void remove(const SetVector<Value> &aliasValues);
---
>   void remove(const SmallPtrSetImpl<Value> &aliasValues);
--- include/mlir/Dialect/Bufferization/Transforms/Bufferize.h
--- include/mlir/Dialect/Bufferization/Transforms/OneShotModuleBufferize.h
30,35c30,31
< /// inserted except two cases:
< /// - `options.copyBeforeWrite` is set, in which case buffers are copied before
< ///   every write.
< /// - `options.copyBeforeWrite` is not set and `options.noAnalysisFuncFilter`
< ///   is not empty. The FuncOps it contains were not analyzed. Buffer copies
< ///   will be inserted only to these FuncOps.
---
> /// inserted unless `options.copyBeforeWrite` is set, in which case buffers are
> /// copied before every write.
--- include/mlir/Dialect/Bufferization/Transforms/Transforms.h
19d18
< class OneShotAnalysisState;
40c39
<                                     OneShotAnalysisState &state,
---
>                                     bufferization::AnalysisState &state,
48c47
<     RewriterBase &rewriter, Operation *op, OneShotAnalysisState &state);
---
>     RewriterBase &rewriter, Operation *op, bufferization::AnalysisState &state);
--- include/mlir/Dialect/Bufferization/Transforms/FuncBufferizableOpInterfaceImpl.h
18a19,22
> namespace func {
> class FuncOp;
> } // namespace func
> 
23a28,29
> using func::FuncOp;
> 
47c53,56
<   DenseMap<FunctionOpInterface, IndexMapping> equivalentFuncArgs;
---
>   DenseMap<FuncOp, IndexMapping> equivalentFuncArgs;
> 
>   /// A mapping of ReturnOp OpOperand indices to aliasing FuncOp BBArg indices.
>   DenseMap<FuncOp, IndexToIndexListMapping> aliasingFuncArgs;
50c59
<   DenseMap<FunctionOpInterface, IndexToIndexListMapping> aliasingReturnVals;
---
>   DenseMap<FuncOp, IndexToIndexListMapping> aliasingReturnVals;
53c62
<   DenseMap<FunctionOpInterface, BbArgIndexSet> readBbArgs;
---
>   DenseMap<FuncOp, BbArgIndexSet> readBbArgs;
56c65
<   DenseMap<FunctionOpInterface, BbArgIndexSet> writtenBbArgs;
---
>   DenseMap<FuncOp, BbArgIndexSet> writtenBbArgs;
60c69
<   DenseMap<FunctionOpInterface, FuncOpAnalysisState> analyzedFuncOps;
---
>   DenseMap<FuncOp, FuncOpAnalysisState> analyzedFuncOps;
64c73
<   void startFunctionAnalysis(FunctionOpInterface funcOp);
---
>   void startFunctionAnalysis(FuncOp funcOp);
--- include/mlir/Dialect/Bufferization/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h
22,24d21
< namespace func {
< class FuncOp;
< }
31,98d27
< /// Specifies a fine-grain relationship between buffers to enable more analysis.
< enum class BufferRelation {
<   Unknown,
<   // TODO: ResultContainsOperand,
<   // TODO: OperandContainsResult,
<   Equivalent
< };
< 
< /// A maybe aliasing OpOperand. If `isDefinite` is `true`, the OpOperand is
< /// guaranteed to alias at runtime.
< struct AliasingOpOperand {
<   AliasingOpOperand(OpOperand *opOperand, BufferRelation relation,
<                     bool isDefinite = true)
<       : opOperand(opOperand), relation(relation), isDefinite(isDefinite) {}
< 
<   OpOperand *opOperand;
<   BufferRelation relation;
<   bool isDefinite;
< };
< 
< /// A maybe aliasing OpResult. If `isDefinite` is `true`, the OpResult is
< /// guaranteed to alias at runtime.
< struct AliasingOpResult {
<   AliasingOpResult(OpResult opResult, BufferRelation relation,
<                    bool isDefinite = true)
<       : opResult(opResult), relation(relation), isDefinite(isDefinite) {}
< 
<   OpResult opResult;
<   BufferRelation relation;
<   bool isDefinite;
< };
< 
< template <typename T> class AliasList {
< public:
<   /// Create an empty list of aliases.
<   AliasList() = default;
< 
<   /// Create a list of aliases.
<   AliasList(std::initializer_list<T> elems) {
<     for (T alias : elems)
<       addAlias(alias);
<   }
< 
<   /// Create a list of aliases.
<   AliasList(SmallVector<T> &&aliases) : aliases(std::move(aliases)) {}
< 
<   ArrayRef<T> getAliases() const { return aliases; }
< 
<   size_t getNumAliases() const { return aliases.size(); }
< 
<   void addAlias(T alias) { aliases.push_back(alias); }
< 
<   auto begin() const { return aliases.begin(); }
<   auto end() const { return aliases.end(); }
< 
< private:
<   /// The list of aliases.
<   SmallVector<T> aliases;
< };
< 
< /// A list of possible aliasing OpOperands. This list models the runtime
< /// aliasing relationship for an OpResult.
< using AliasingOpOperandList = AliasList<AliasingOpOperand>;
< 
< /// A list of possible aliasing OpResults. This list models the runtime
< /// aliasing relationship for an OpOperand.
< using AliasingOpResultList = AliasList<AliasingOpResult>;
< 
256,260d184
<   /// Parameters: Value, memory space, func op, bufferization options
<   using FunctionArgTypeConverterFn =
<       std::function<BaseMemRefType(TensorType, Attribute memorySpace,
<                                    func::FuncOp, const BufferizationOptions &)>;
<   /// Tensor -> MemRef type converter.
324,325c248
<   /// This function controls buffer types on function signatures. Sets
<   /// `functionArgTypeConverterFn` and `inferFunctionResultLayout` accordingly.
---
>   /// This flag controls buffer types on function signatures.
337a261,262
>   /// If `bufferizeFunctionBoundaries` is not set, this flag has no effect.
>   ///
341,356c266,267
<   void setFunctionBoundaryTypeConversion(LayoutMapOption layoutMapOption);
< 
<   /// Type converter from tensors to memrefs. This type converter is used to
<   /// determine bufferized function argument types. By default, a type
<   /// converter that returns a memref type with a fully dynamic layout map is
<   /// used.
<   ///
<   /// If `bufferizeFunctionBoundaries` is not set, this function isn't used.
<   FunctionArgTypeConverterFn functionArgTypeConverterFn = nullptr;
< 
<   /// If true, function result types are inferred from the body of the function.
<   /// Otherwise, function result type is determined by
<   /// `functionArgTypeConverterFn`.
<   ///
<   /// If `bufferizeFunctionBoundaries` is not set, this flag has no effect.
<   bool inferFunctionResultLayout = true;
---
>   LayoutMapOption functionBoundaryTypeConversion =
>       LayoutMapOption::InferLayoutMap;
391a303,310
> /// Specify fine-grain relationship between buffers to enable more analysis.
> enum class BufferRelation {
>   None,
>   // TODO: ResultContainsOperand,
>   // TODO: OperandContainsResult,
>   Equivalent
> };
> 
400,402c319,320
<   /// bufferized in place. Return all tensor OpOperand* if the op is not
<   /// bufferizable.
<   AliasingOpOperandList getAliasingOpOperands(OpResult result) const;
---
>   /// bufferized in place. Return an empty vector if the op is not bufferizable.
>   SmallVector<OpOperand *> getAliasingOpOperand(OpResult result) const;
405,407c323,324
<   /// bufferized in place. Return all tensor OpResults if the op is not
<   /// bufferizable.
<   AliasingOpResultList getAliasingOpResults(OpOperand &opOperand) const;
---
>   /// bufferized in place. Return an empty vector if the op is not bufferizable.
>   SmallVector<OpResult> getAliasingOpResult(OpOperand &opOperand) const;
417,421d333
<   /// Return true if the given `value` bufferizes to a memory write. Return
<   /// true if the value is a block argument. Return `true` if the defining op is
<   /// not bufferizable. Otherwise, consult the BufferizableOpInterface.
<   bool bufferizesToMemoryWrite(Value value) const;
< 
441,442c353
<   /// OpOperands), also return the last Value of that chain if
<   /// `alwaysIncludeLeaves` is set.
---
>   /// OpOperands), also return the last Value of that chain.
461,497c372,385
<   SetVector<Value> findValueInReverseUseDefChain(
<       Value value, llvm::function_ref<bool(Value)> condition,
<       bool followEquivalentOnly = false, bool alwaysIncludeLeaves = true) const;
< 
<   /// Find the values that may define the contents of the given value at
<   /// runtime. A block argument is always a definition. An OpResult is a
<   /// definition if it bufferizes to memory write. If it does not bufferize to
<   /// a memory write but has aliasing operands, we continue the lookup on these
<   /// values.
<   ///
<   /// Example: %r = tensor.insert %f into %t[%c0] : tensor<?xf32>
<   /// findDefinitions(%r) = {%r} because %r bufferizes to memory write.
<   ///
<   /// Example: %r = tensor.empty() : tensor<10xf32>
<   /// findDefinitions(%r) = {} because tensor.empty does not the define the
<   /// contents of its result (i.e., it does not bufferize to a memory write)
<   /// and it has no aliasing OpOperands.
<   ///
<   /// Example:
<   /// %a = arith.constant ... : tensor<10xf32>
<   /// %b1 = tensor.insert %f into %t : tensor<50xf32>
<   /// %b2 = tensor.extract_slice %b1[0][10][1] : tensor<50xf32> tensor<10xf32>
<   /// %r = arith.select %cond, %a, %b : tensor<10xf32>
<   /// findDefinitions(%r) = {%a, %b1}. %r and %b2 are skipped (lookup continues
<   /// in the operands) because their defining ops do not define the contents of
<   /// the tensor.
<   ///
<   /// Example:
<   /// %a = tensor.empty() : tensor<10xf32>
<   /// %b = arith.constant ... : tensor<10xf32>
<   /// %r = arith.select %cond, %a, %b : tensor<10xf32>
<   /// findDefinitions(%r) = {%b}. %a is excluded because it does not define the
<   /// contents of the tensor.
<   ///
<   /// Note: OpResults of unknown ops are handled conservatively and assumed to
<   /// be definitions.
<   SetVector<Value> findDefinitions(Value value) const;
---
>   SetVector<Value>
>   findValueInReverseUseDefChain(Value value,
>                                 llvm::function_ref<bool(Value)> condition,
>                                 bool followEquivalentOnly = false) const;
> 
>   /// Find the Values of the last preceding write of a given Value.
>   ///
>   /// Note: Unknown ops are handled conservatively and assumed to be writes.
>   /// Furthermore, BlockArguments are also assumed to be writes. There is no
>   /// analysis across block boundaries.
>   ///
>   /// Note: When reaching an end of the reverse SSA use-def chain, that value
>   /// is returned regardless of whether it is a memory write or not.
>   SetVector<Value> findLastPrecedingWrite(Value value) const;
651,655d538
< /// Assuming that the given region is repetitive, find the next enclosing
< /// repetitive region.
< Region *getNextEnclosingRepetitiveRegion(Region *region,
<                                          const BufferizationOptions &options);
< 
658,663d540
< /// BufferizableOpInterface::getAliasingOpOperands. Should not be called from
< /// other places.
< AliasingOpOperandList defaultGetAliasingOpOperands(OpResult opResult,
<                                                    const AnalysisState &state);
< 
< /// This is the default implementation of
671,676d547
< /// BufferizableOpInterface::resultBufferizesToMemoryWrite. Should not be called
< /// from other places.
< bool defaultResultBufferizesToMemoryWrite(OpResult opResult,
<                                           const AnalysisState &state);
< 
< /// This is the default implementation of
681,688d551
< 
< /// This is the default implementation of getAliasingOpOperands in case the
< /// defining op does not implement the BufferizableOpInterface.
< AliasingOpOperandList unknownGetAliasingOpOperands(OpResult opResult);
< 
< /// This is the default implementation of getAliasingOpResults in case the
< /// owner op does not implement the BufferizableOpInterface.
< AliasingOpResultList unknownGetAliasingOpResults(OpOperand &opOperand);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.td include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.td
95,138c95,103
<         /*desc=*/[{
<           Return `true` if the given OpResult bufferizes to a memory write.
<           This is the same property as `bufferizesToMemoryWrite`, but from The
<           perspective of OpResults.
< 
<           This method will never be called on OpResults that do not have a
<           tensor type.
< 
<           This method has a default implementation. By default, it returns
<           `true` if any of the following three cases applies.
< 
<           1. There is no corresponding aliasing OpOperand.
< 
<              Example: `tensor.generate ... : tensor<10xf32>`
<              The op fills a newly allocated buffer and bufferizes to a memory
<              write.
< 
<              Counter-example: bufferization.alloc_tensor
<              The op just allocates and does not specifiy the data of the tensor,
<              so resultBufferizesToMemoryWrite is overridden to return false.
< 
<           2. At least one aliasing OpOperand bufferizes to a memory write.
< 
<              Example: `tensor.insert %f into %t[...] : tensor<?xf32>`
<              The destination OpOperand bufferizes to a memory write, so the
<              result also bufferizes to a memory write.
< 
<           3. At least one aliasing OpOperand's value is defined inside the
<              defining op of the given OpResult and it is a memory write.
< 
<              According to this rule, an aliasing OpOperand value that is defined
<              inside this op and is bufferizing to a memory write makes the given
<              OpResult bufferize to a memory write.
< 
<              Example:
<              ```
<              %r = scf.if ... -> tensor<?xf32> {
<                %1 = tensor.insert %f into %t[...] : tensor<?xf32>
<                scf.yield %1 : tensor<?xf32>
<              } else { ... }
<              ```
<              The scf.if result bufferizes to a memory write because %1 (an
<              OpResult defined inside the scf.if op) bufferizes to a memory
<              write.
---
>           /*desc=*/[{
>             Return `true` if the given OpResult is a memory write. This is the
>             case if in the following cases:
> 
>             * The corresponding aliasing OpOperand bufferizes to a memory write.
>             * Or: There is no corresponding aliasing OpOperand.
> 
>             If the OpResult has multiple aliasing OpOperands, this method
>             returns `true` if at least one of them bufferizes to a memory write.
140,150c105,123
<         /*retType=*/"bool",
<         /*methodName=*/"resultBufferizesToMemoryWrite",
<         /*args=*/(ins "::mlir::OpResult":$opResult,
<                       "const ::mlir::bufferization::AnalysisState &":$state),
<         /*methodBody=*/"",
<         /*defaultImplementation=*/[{
<           assert(opResult.getDefiningOp() == $_op.getOperation() &&
<                  "invalid OpResult");
<           return ::mlir::bufferization::detail::defaultResultBufferizesToMemoryWrite(
<               opResult, state);
<         }]
---
>           /*retType=*/"bool",
>           /*methodName=*/"isMemoryWrite",
>           /*args=*/(ins "::mlir::OpResult":$opResult,
>                         "const ::mlir::bufferization::AnalysisState &":$state),
>           /*methodBody=*/"",
>           /*defaultImplementation=*/[{
>             auto bufferizableOp =
>                 cast<BufferizableOpInterface>($_op.getOperation());
>             SmallVector<OpOperand*> opOperands =
>               bufferizableOp.getAliasingOpOperand(opResult, state);
>             if (opOperands.empty())
>               return true;
>             return llvm::any_of(
>                 opOperands,
>                 [&](OpOperand *operand) {
>                   return bufferizableOp.bufferizesToMemoryWrite(*operand,
>                                                                 state);
>                 });
>           }]
158,161d130
< 
<           Note: Unranked tensor OpOperands always bufferize in-place. This could
<           be extended in the future. Unranked tensors are used with external
<           functions only.
169c138
<           return opOperand.get().getType().isa<::mlir::UnrankedTensorType>();
---
>           return false;
174c143
<           Return the OpResults that may alias with a given OpOperand when
---
>           Return the OpResult that aliases with a given OpOperand when
178,227c147,149
<           This method can return multiple OpResults, indicating that a given
<           OpOperand may at runtime alias with any (or multiple) of the returned
<           OpResults.
< 
<           Each alias is specified with a degree of certainty:
< 
<           * MAYBE (`isDefinite = false`): At runtime, buffer(opOperand) may
<             alias with the specified OpResult.
<           * DEFINITE (`isDefinite = true`, default): At runtime,
<             buffer(opOperand) is guaranteed to alias the buffer of the specified
<             OpResult. This is a stronger property than MAYBE and allows for more
<             precise analyses. DEFINITE properties should be used when possible.
< 
<           Furthermore, each alias is specified with a buffer relation:
< 
<           * `BufferRelation::Equivalent`: Both aliases are the exact same
<             buffer. I.e., same size, no offset, same strides.
<           * `BufferRelation::Unknown`: There is no further information apart
<             from the fact that both buffers alias.
< 
<           False positives are allowed in the list of OpResults, but they can
<           adversely affect the accuracy of the anlysis. On the contrary,
<           omitting potential aliases is incorrect.
< 
<           One possible (conservative) implementation of this interface method,
<           that is always safe, is to return all tensor OpResults with
<           BufferRelation::Unknown and MAYBE.
< 
<           Examples:
< 
<           ```
<           // aliasingOpResults(%t) = DEFINITE {Equivalent %r}
<           %r = tensor.insert_slice %f into %t : tensor<10xf32>
< 
<           // aliasingOpResults(%t) = DEFINITE {Unknown %r}
<           // Note: "Buffer is subset of buffer" relationship are not yet
<           // supported, so "Unknown" is the best we can do for now.
<           %r = tensor.extract_slice %t[0]][5][1]
<               : tensor<10xf32> to tensor<5xf32>
< 
<           // aliasingOpResults(%t1) = MAYBE {Equivalent %r}
<           // aliasingOpResults(%t2) = MAYBE {Equivalent %r}
<           %r = arith.select %c, %t1, %t2 : tensor<10xf32>
< 
<           // A hypothetical op that bufferizes to rolling a dice and based on
<           // the result to either return buffer(%t) or a newly allocated copy
<           // thereof.
<           // aliasingOpResults(%t) = MAYBE {Equivalent %r}
<           %r = "dummy.alias_or_copy(%t) : (tensor<10xf32>) -> (tensor<10xf32>)"
<           ```
---
>           Note: This method can return multiple OpResults, indicating that a
>           given OpOperand may at runtime alias with any (or multiple) of the
>           returned OpResults.
229,230c151,152
<         /*retType=*/"::mlir::bufferization::AliasingOpResultList",
<         /*methodName=*/"getAliasingOpResults",
---
>         /*retType=*/"::mlir::SmallVector<::mlir::OpResult>",
>         /*methodName=*/"getAliasingOpResult",
236,238c158
<           assert(opOperand.get().getType().isa<::mlir::TensorType>() &&
<                  "expected OpOperand with tensor type");
<           llvm_unreachable("getAliasingOpResults not implemented");
---
>           llvm_unreachable("getAliasingOpResult not implemented");
247c167
<           By default, this method is the inverse of `getAliasingOpResults`. Ops
---
>           By default, this method is the inverse of `getAliasingOpResult`. Ops
251,299c171,174
<           This method can return multiple OpOperands, indicating that a given
<           OpResult may at runtime alias with any (or multiple) of the returned
<           OpOperands.
< 
<           This property is specified with a degree of certainty:
< 
<           * MAYBE (`isDefinite = false`): At runtime, buffer(opResult) may
<             alias with the specified OpOperand.
<           * DEFINITE (`isDefinite = true`, default): At runtime,
<             buffer(opResult) is guaranteed to alias the buffer of the specified
<             OpOperand. This is a stronger property than MAYBE and allows for
<             more precise analyses. DEFINITE properties should be used when
<             possible.
< 
<           For each alias, a BufferRelation can be specified:
< 
<           * `BufferRelation::Equivalent`: Both aliases are the exact same
<             buffer. I.e., same size, no offset, same strides.
<           * `BufferRelation::Unknown`: There is no further information apart
<             from the fact that both buffers alias.
< 
<           False positives are allowed in the list of OpOperands, but they can
<           adversely affect the accuracy of the anlysis. On the contrary,
<           omitting potential aliases is incorrect.
< 
<           One possible (conservative) implementation of this interface method,
<           that is always safe, is to return all tensor OpOperands with
<           BufferRelation::Unknown and MAYBE.
< 
<           Note: If the returned list of OpOperands is empty, this op definitely
<           bufferizes to a new allocation. In that case `bufferizesToAllocation`
<           must return `true`.
< 
<           Examples:
< 
<           ```
<           // aliasingOpOperands(%r) = DEFINITE {Equivalent %t}
<           %r = tensor.insert_slice %f into %t : tensor<10xf32>
< 
<           // aliasingOpOperands(%r) = DEFINITE {Unknown %t}
<           %r = tensor.extract_slice %t[0]][5][1]
<               : tensor<10xf32> to tensor<5xf32>
< 
<           // aliasingOpOperands(%r) = DEFINITE {Equivalent %t1, Equivalent %t2}
<           %r = arith.select %c, %t1, %t2 : tensor<10xf32>
< 
<           // aliasingOpOperands(%r) = MAYBE {}
<           %r = tensor.empty() : tensor<10xf32>
<           ```
---
>           Note: This method can return multiple OpOperands, indicating that the
>           given OpResult may at runtime alias with any (or multiple) of the
>           returned OpOperands. This can be useful for branches and for ops such
>           as `arith.select`.
301,302c176,177
<         /*retType=*/"::mlir::bufferization::AliasingOpOperandList",
<         /*methodName=*/"getAliasingOpOperands",
---
>         /*retType=*/"::mlir::SmallVector<::mlir::OpOperand *>",
>         /*methodName=*/"getAliasingOpOperand",
307c182
<           assert(opResult.getType().isa<::mlir::TensorType>() &&
---
>           assert(opResult.getType().isa<TensorType>() &&
309,310c184,217
<           return ::mlir::bufferization::detail::defaultGetAliasingOpOperands(
<               opResult, state);
---
>           SmallVector<OpOperand *> result;
>           auto bufferizableOp =
>               cast<BufferizableOpInterface>($_op.getOperation());
>           for (OpOperand &opOperand : $_op.getOperation()->getOpOperands()) {
>             if (!opOperand.get().getType().isa<TensorType>())
>               continue;
>             SmallVector<OpResult> aliasingOpResults =
>                 bufferizableOp.getAliasingOpResult(opOperand, state);
>             if (llvm::is_contained(aliasingOpResults, opResult))
>               result.push_back(&opOperand);
>           }
>           return result;
>         }]
>       >,
>       InterfaceMethod<
>         /*desc=*/[{
>           Return the buffer relation between the given OpResult and its aliasing
>           OpOperands when bufferized in-place. Most OpOperands have an
>           "equivalence" relation. This method will never be called on OpResults
>           that do not have a tensor type. It will also never be called on
>           OpResults that do not have at least one aliasing OpOperand.
> 
>           TODO: Support other relations such as "OpOperand is included in
>           OpResult".
>         }],
>         /*retType=*/"::mlir::bufferization::BufferRelation",
>         /*methodName=*/"bufferRelation",
>         /*args=*/(ins "::mlir::OpResult":$opResult,
>                       "const ::mlir::bufferization::AnalysisState &":$state),
>         /*methodBody=*/"",
>         /*defaultImplementation=*/[{
>           // Does not have to be implemented for ops without tensor OpResults
>           // that have an aliasing OpOperand.
>           llvm_unreachable("bufferRelation not implemented");
334c241
<               ::llvm::cast<BufferizableOpInterface>($_op.getOperation());
---
>               cast<BufferizableOpInterface>($_op.getOperation());
349c256
<           remaining methods, in particular `getAliasingOpOperands`. I.e., a
---
>           remaining methods, in particular `getAliasingOpOperand`. I.e., a
351,353c258,259
< 
<           a) One of the buffers in getAliasingOpOperands(r).
<           b) Or: A newly allocated buffer (only if `bufferizesToAllocation`).
---
>           a) A buffer that aliases one of buffers in getAliasingOpOperand(r).
>           b) Or: A newly allocated buffer.
372c278
<           return ::mlir::failure();
---
>           return failure();
397c303
<           return value.isa<::mlir::OpResult>();
---
>           return value.isa<OpResult>();
437c343
<           return ::mlir::success();
---
>           return success();
458c364
<           return ::mlir::bufferization::detail::defaultGetBufferType(
---
>           return bufferization::detail::defaultGetBufferType(
481,482c387,388
<           return ::mlir::bufferization::detail::defaultIsRepetitiveRegion(
<               ::llvm::cast<BufferizableOpInterface>($_op.getOperation()), index);
---
>           return mlir::bufferization::detail::defaultIsRepetitiveRegion(
>               cast<BufferizableOpInterface>($_op.getOperation()), index);
490,492c396,397
<     ::mlir::LogicalResult resolveTensorOpOperandConflicts(
<         ::mlir::RewriterBase &rewriter,
<         const ::mlir::bufferization::AnalysisState &state);
---
>     LogicalResult resolveTensorOpOperandConflicts(
>         RewriterBase &rewriter, const AnalysisState &state);
500,502c405,406
<     bool bufferizesToAliasOnly(
<         ::mlir::OpOperand &opOperand,
<         const ::mlir::bufferization::AnalysisState &state) {
---
>     bool bufferizesToAliasOnly(OpOperand &opOperand,
>                                const AnalysisState &state) {
504c408
<           ::llvm::cast<::mlir::bufferization::BufferizableOpInterface>(getOperation());
---
>           cast<BufferizableOpInterface>(getOperation());
507,508c411
<           && bufferizableOp.getAliasingOpResults(opOperand, state)
<               .getNumAliases() != 0;
---
>           && !bufferizableOp.getAliasingOpResult(opOperand, state).empty();
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR/BufferizationBase.td include/mlir/Dialect/Bufferization/IR/BufferizationBase.td
29c29
<     "affine::AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
---
>     "AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
71a72
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR/BufferizationOps.td include/mlir/Dialect/Bufferization/IR/BufferizationOps.td
92,93c92
<     bool resultBufferizesToMemoryWrite(OpResult opResult,
<                                        const AnalysisState &state);
---
>     bool isMemoryWrite(OpResult opResult, const AnalysisState &state);
103c102
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
251c250
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
275,276c274,276
<     An operation that creates a tensor from a `memref`. The result value is a
<     tensor whose shape and element type match the memref operand.
---
>     Create a tensor from a `memref`, making an independent copy of the element
>     data. The result value is a tensor whose shape and element type match the
>     memref operand.
286c286
<     %t = bufferization.to_tensor %m : memref<4x?xf32, #layout, 0>
---
>     %12 = bufferization.to_tensor %10 : memref<4x?xf32, #layout, memspace0>
289,313c289,290
<     If the `writable` unit attribute is set, the produced tensor is considered
<     "writable" during bufferization. Otherwise, every OpOperand that bufferizes
<     to a write to the future buffer of the resulting tensor (or an alias
<     thereof) will bufferize out-of-place to prevent emitting any writes to
<     `memref` during bufferization.
< 
<     If the given memref does not alias with any other memref passed to another
<     `to_tensor` op, the `restrict` unit attribute can be set. Only such
<     operations are supported by One-Shot Bufferize. (Otherwise, potential memref
<     aliasing relationships would have to be captured in One-Shot Bufferize.)
< 
<     Example:
< 
<     ```
<     %t = bufferization.to_tensor %m restrict writable : memref<4xf32>
< 
<     // %t is writable, so the tensor.insert may bufferize in-place in the
<     // absence of other conflicts.
<     %r = tensor.insert %f into %t[%idx] : tensor<4xf32>
<     ```
< 
<     `to_tensor` ops are not bufferized. They are expected to fold away after
<     bufferization. If there are non-bufferizable ops in the IR and
<     `allowUnknownOps` is set, they may be part of the resulting IR and not fold
<     away. However, such IR is no longer bufferizable with One-Shot Bufferize.
---
>     If tensor load is used in the bufferization steps, mutating the source
>     buffer after loading leads to undefined behavior.
317,318c294
<                        "the reference to load from", [MemRead]>:$memref,
<                        UnitAttr:$restrict, UnitAttr:$writable);
---
>                        "the reference to load from", [MemRead]>:$memref);
333a310,317
>     // ToTensorOp conceptually loads a tensor from a memory location. The
>     // One-Shot analysis has no information about the memref that is loaded from
>     // by ToTensorOp. We have to assume that the loaded tensor may after
>     // bufferization potentially alias with any other bufferized tensor. Since
>     // ToTensorOp and ToMemrefOp have no aliasing OpOperand/OpResult pairs, this
>     // cannot be encoded directly in the analysis. However, declaring ToTensorOp
>     // results as not writable enforces a buffer copy and has the same effect.
> 
336c320,326
<       // to_tensor/to_memref pairs fold away after bufferization.
---
>       // to_tensor cannot be bufferized. However, other ops that are using
>       // to_tensor's result will eventually be bufferized. At that point, they
>       // will start using to_tensor's memref operand. Once all users of
>       // to_tensor are bufferized, the op will not have any users anymore and
>       // DCE away. In case of partial bufferization, to_memref(to_tensor(x))
>       // constructs may be left over. These are folded by the canonicalizer or
>       // FinalizingBufferize.
340c330,333
<     bool isWritable(Value value, const AnalysisState &state);
---
>     bool isWritable(Value value, const AnalysisState &state) const {
>       // It is unknown whether the memref operand is writable or not.
>       return false;
>     }
349,352c342
<   let assemblyFormat = [{
<     $memref (`restrict` $restrict^)? (`writable` $writable^)? attr-dict
<       `:` type($memref)
<   }];
---
>   let assemblyFormat = "$memref attr-dict `:` type($memref)";
374c364
<     An operation that returns the future buffer of a `tensor`.
---
>     Casts a tensor to a memref.
377,378c367,368
<     // Result type is memref<4x?xf32, #layout, 0>
<     %m = bufferization.to_memref %t : memref<4x?xf32, #layout, 0>
---
>     // Result type is memref<4x?xf32, #layout, 42>
>     %12 = bufferization.to_memref %10 : memref<4x?xf32, #layout, 42>
381,383c371,372
<     This operation is a specialized variant of the built-in
<     `unrealized_conversion_cast` and is used to make sure that the IR stays
<     valid at any point during the bufferization.
---
>     Note, that mutating the result of the `to_memref` operation leads to
>     undefined behavior.
385,386c374,376
<     IR that contains `to_memref` ops cannot be bufferized with One-Shot
<     Bufferize.
---
>     This operation is a specialized variant of the built-in
>     `unrealized_conversion_cast` and is intended for use in the context of
>     gradual bufferization.
421c411
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/IR/DstBufferizableOpInterfaceImpl.h include/mlir/Dialect/Bufferization/IR/DstBufferizableOpInterfaceImpl.h
39c39
<   AliasingOpResultList getAliasingOpResults(Operation *op, OpOperand &opOperand,
---
>   SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
44c44
<       return {{dstOp.getTiedOpResult(&opOperand), BufferRelation::Equivalent}};
---
>       return {dstOp.getTiedOpResult(&opOperand)};
45a46,52
>   }
> 
>   BufferRelation bufferRelation(Operation *op, OpResult opResult,
>                                 const AnalysisState &state) const {
>     assert(isa<DestinationStyleOpInterface>(op) &&
>            "expected that op implements DestinationStyleOpInterface");
>     return BufferRelation::Equivalent;
--- include/mlir/Dialect/Bufferization/IR/DstBufferizableOpInterfaceImpl.h
39c39
<   AliasingOpResultList getAliasingOpResults(Operation *op, OpOperand &opOperand,
---
>   SmallVector<OpResult> getAliasingOpResult(Operation *op, OpOperand &opOperand,
44c44
<       return {{dstOp.getTiedOpResult(&opOperand), BufferRelation::Equivalent}};
---
>       return {dstOp.getTiedOpResult(&opOperand)};
45a46,52
>   }
> 
>   BufferRelation bufferRelation(Operation *op, OpResult opResult,
>                                 const AnalysisState &state) const {
>     assert(isa<DestinationStyleOpInterface>(op) &&
>            "expected that op implements DestinationStyleOpInterface");
>     return BufferRelation::Equivalent;
--- include/mlir/Dialect/Bufferization/IR/AllocationOpInterface.h
--- include/mlir/Dialect/Bufferization/IR/Bufferization.h
--- include/mlir/Dialect/Bufferization/IR/CMakeLists.txt
--- include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h
22,24d21
< namespace func {
< class FuncOp;
< }
31,98d27
< /// Specifies a fine-grain relationship between buffers to enable more analysis.
< enum class BufferRelation {
<   Unknown,
<   // TODO: ResultContainsOperand,
<   // TODO: OperandContainsResult,
<   Equivalent
< };
< 
< /// A maybe aliasing OpOperand. If `isDefinite` is `true`, the OpOperand is
< /// guaranteed to alias at runtime.
< struct AliasingOpOperand {
<   AliasingOpOperand(OpOperand *opOperand, BufferRelation relation,
<                     bool isDefinite = true)
<       : opOperand(opOperand), relation(relation), isDefinite(isDefinite) {}
< 
<   OpOperand *opOperand;
<   BufferRelation relation;
<   bool isDefinite;
< };
< 
< /// A maybe aliasing OpResult. If `isDefinite` is `true`, the OpResult is
< /// guaranteed to alias at runtime.
< struct AliasingOpResult {
<   AliasingOpResult(OpResult opResult, BufferRelation relation,
<                    bool isDefinite = true)
<       : opResult(opResult), relation(relation), isDefinite(isDefinite) {}
< 
<   OpResult opResult;
<   BufferRelation relation;
<   bool isDefinite;
< };
< 
< template <typename T> class AliasList {
< public:
<   /// Create an empty list of aliases.
<   AliasList() = default;
< 
<   /// Create a list of aliases.
<   AliasList(std::initializer_list<T> elems) {
<     for (T alias : elems)
<       addAlias(alias);
<   }
< 
<   /// Create a list of aliases.
<   AliasList(SmallVector<T> &&aliases) : aliases(std::move(aliases)) {}
< 
<   ArrayRef<T> getAliases() const { return aliases; }
< 
<   size_t getNumAliases() const { return aliases.size(); }
< 
<   void addAlias(T alias) { aliases.push_back(alias); }
< 
<   auto begin() const { return aliases.begin(); }
<   auto end() const { return aliases.end(); }
< 
< private:
<   /// The list of aliases.
<   SmallVector<T> aliases;
< };
< 
< /// A list of possible aliasing OpOperands. This list models the runtime
< /// aliasing relationship for an OpResult.
< using AliasingOpOperandList = AliasList<AliasingOpOperand>;
< 
< /// A list of possible aliasing OpResults. This list models the runtime
< /// aliasing relationship for an OpOperand.
< using AliasingOpResultList = AliasList<AliasingOpResult>;
< 
256,260d184
<   /// Parameters: Value, memory space, func op, bufferization options
<   using FunctionArgTypeConverterFn =
<       std::function<BaseMemRefType(TensorType, Attribute memorySpace,
<                                    func::FuncOp, const BufferizationOptions &)>;
<   /// Tensor -> MemRef type converter.
324,325c248
<   /// This function controls buffer types on function signatures. Sets
<   /// `functionArgTypeConverterFn` and `inferFunctionResultLayout` accordingly.
---
>   /// This flag controls buffer types on function signatures.
337a261,262
>   /// If `bufferizeFunctionBoundaries` is not set, this flag has no effect.
>   ///
341,356c266,267
<   void setFunctionBoundaryTypeConversion(LayoutMapOption layoutMapOption);
< 
<   /// Type converter from tensors to memrefs. This type converter is used to
<   /// determine bufferized function argument types. By default, a type
<   /// converter that returns a memref type with a fully dynamic layout map is
<   /// used.
<   ///
<   /// If `bufferizeFunctionBoundaries` is not set, this function isn't used.
<   FunctionArgTypeConverterFn functionArgTypeConverterFn = nullptr;
< 
<   /// If true, function result types are inferred from the body of the function.
<   /// Otherwise, function result type is determined by
<   /// `functionArgTypeConverterFn`.
<   ///
<   /// If `bufferizeFunctionBoundaries` is not set, this flag has no effect.
<   bool inferFunctionResultLayout = true;
---
>   LayoutMapOption functionBoundaryTypeConversion =
>       LayoutMapOption::InferLayoutMap;
391a303,310
> /// Specify fine-grain relationship between buffers to enable more analysis.
> enum class BufferRelation {
>   None,
>   // TODO: ResultContainsOperand,
>   // TODO: OperandContainsResult,
>   Equivalent
> };
> 
400,402c319,320
<   /// bufferized in place. Return all tensor OpOperand* if the op is not
<   /// bufferizable.
<   AliasingOpOperandList getAliasingOpOperands(OpResult result) const;
---
>   /// bufferized in place. Return an empty vector if the op is not bufferizable.
>   SmallVector<OpOperand *> getAliasingOpOperand(OpResult result) const;
405,407c323,324
<   /// bufferized in place. Return all tensor OpResults if the op is not
<   /// bufferizable.
<   AliasingOpResultList getAliasingOpResults(OpOperand &opOperand) const;
---
>   /// bufferized in place. Return an empty vector if the op is not bufferizable.
>   SmallVector<OpResult> getAliasingOpResult(OpOperand &opOperand) const;
417,421d333
<   /// Return true if the given `value` bufferizes to a memory write. Return
<   /// true if the value is a block argument. Return `true` if the defining op is
<   /// not bufferizable. Otherwise, consult the BufferizableOpInterface.
<   bool bufferizesToMemoryWrite(Value value) const;
< 
441,442c353
<   /// OpOperands), also return the last Value of that chain if
<   /// `alwaysIncludeLeaves` is set.
---
>   /// OpOperands), also return the last Value of that chain.
461,497c372,385
<   SetVector<Value> findValueInReverseUseDefChain(
<       Value value, llvm::function_ref<bool(Value)> condition,
<       bool followEquivalentOnly = false, bool alwaysIncludeLeaves = true) const;
< 
<   /// Find the values that may define the contents of the given value at
<   /// runtime. A block argument is always a definition. An OpResult is a
<   /// definition if it bufferizes to memory write. If it does not bufferize to
<   /// a memory write but has aliasing operands, we continue the lookup on these
<   /// values.
<   ///
<   /// Example: %r = tensor.insert %f into %t[%c0] : tensor<?xf32>
<   /// findDefinitions(%r) = {%r} because %r bufferizes to memory write.
<   ///
<   /// Example: %r = tensor.empty() : tensor<10xf32>
<   /// findDefinitions(%r) = {} because tensor.empty does not the define the
<   /// contents of its result (i.e., it does not bufferize to a memory write)
<   /// and it has no aliasing OpOperands.
<   ///
<   /// Example:
<   /// %a = arith.constant ... : tensor<10xf32>
<   /// %b1 = tensor.insert %f into %t : tensor<50xf32>
<   /// %b2 = tensor.extract_slice %b1[0][10][1] : tensor<50xf32> tensor<10xf32>
<   /// %r = arith.select %cond, %a, %b : tensor<10xf32>
<   /// findDefinitions(%r) = {%a, %b1}. %r and %b2 are skipped (lookup continues
<   /// in the operands) because their defining ops do not define the contents of
<   /// the tensor.
<   ///
<   /// Example:
<   /// %a = tensor.empty() : tensor<10xf32>
<   /// %b = arith.constant ... : tensor<10xf32>
<   /// %r = arith.select %cond, %a, %b : tensor<10xf32>
<   /// findDefinitions(%r) = {%b}. %a is excluded because it does not define the
<   /// contents of the tensor.
<   ///
<   /// Note: OpResults of unknown ops are handled conservatively and assumed to
<   /// be definitions.
<   SetVector<Value> findDefinitions(Value value) const;
---
>   SetVector<Value>
>   findValueInReverseUseDefChain(Value value,
>                                 llvm::function_ref<bool(Value)> condition,
>                                 bool followEquivalentOnly = false) const;
> 
>   /// Find the Values of the last preceding write of a given Value.
>   ///
>   /// Note: Unknown ops are handled conservatively and assumed to be writes.
>   /// Furthermore, BlockArguments are also assumed to be writes. There is no
>   /// analysis across block boundaries.
>   ///
>   /// Note: When reaching an end of the reverse SSA use-def chain, that value
>   /// is returned regardless of whether it is a memory write or not.
>   SetVector<Value> findLastPrecedingWrite(Value value) const;
651,655d538
< /// Assuming that the given region is repetitive, find the next enclosing
< /// repetitive region.
< Region *getNextEnclosingRepetitiveRegion(Region *region,
<                                          const BufferizationOptions &options);
< 
658,663d540
< /// BufferizableOpInterface::getAliasingOpOperands. Should not be called from
< /// other places.
< AliasingOpOperandList defaultGetAliasingOpOperands(OpResult opResult,
<                                                    const AnalysisState &state);
< 
< /// This is the default implementation of
671,676d547
< /// BufferizableOpInterface::resultBufferizesToMemoryWrite. Should not be called
< /// from other places.
< bool defaultResultBufferizesToMemoryWrite(OpResult opResult,
<                                           const AnalysisState &state);
< 
< /// This is the default implementation of
681,688d551
< 
< /// This is the default implementation of getAliasingOpOperands in case the
< /// defining op does not implement the BufferizableOpInterface.
< AliasingOpOperandList unknownGetAliasingOpOperands(OpResult opResult);
< 
< /// This is the default implementation of getAliasingOpResults in case the
< /// owner op does not implement the BufferizableOpInterface.
< AliasingOpResultList unknownGetAliasingOpResults(OpOperand &opOperand);
--- include/mlir/Dialect/Bufferization/IR/BufferizationEnums.td
--- include/mlir/Dialect/Bufferization/IR/BufferizationOps.td
92,93c92
<     bool resultBufferizesToMemoryWrite(OpResult opResult,
<                                        const AnalysisState &state);
---
>     bool isMemoryWrite(OpResult opResult, const AnalysisState &state);
103c102
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
251c250
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
275,276c274,276
<     An operation that creates a tensor from a `memref`. The result value is a
<     tensor whose shape and element type match the memref operand.
---
>     Create a tensor from a `memref`, making an independent copy of the element
>     data. The result value is a tensor whose shape and element type match the
>     memref operand.
286c286
<     %t = bufferization.to_tensor %m : memref<4x?xf32, #layout, 0>
---
>     %12 = bufferization.to_tensor %10 : memref<4x?xf32, #layout, memspace0>
289,313c289,290
<     If the `writable` unit attribute is set, the produced tensor is considered
<     "writable" during bufferization. Otherwise, every OpOperand that bufferizes
<     to a write to the future buffer of the resulting tensor (or an alias
<     thereof) will bufferize out-of-place to prevent emitting any writes to
<     `memref` during bufferization.
< 
<     If the given memref does not alias with any other memref passed to another
<     `to_tensor` op, the `restrict` unit attribute can be set. Only such
<     operations are supported by One-Shot Bufferize. (Otherwise, potential memref
<     aliasing relationships would have to be captured in One-Shot Bufferize.)
< 
<     Example:
< 
<     ```
<     %t = bufferization.to_tensor %m restrict writable : memref<4xf32>
< 
<     // %t is writable, so the tensor.insert may bufferize in-place in the
<     // absence of other conflicts.
<     %r = tensor.insert %f into %t[%idx] : tensor<4xf32>
<     ```
< 
<     `to_tensor` ops are not bufferized. They are expected to fold away after
<     bufferization. If there are non-bufferizable ops in the IR and
<     `allowUnknownOps` is set, they may be part of the resulting IR and not fold
<     away. However, such IR is no longer bufferizable with One-Shot Bufferize.
---
>     If tensor load is used in the bufferization steps, mutating the source
>     buffer after loading leads to undefined behavior.
317,318c294
<                        "the reference to load from", [MemRead]>:$memref,
<                        UnitAttr:$restrict, UnitAttr:$writable);
---
>                        "the reference to load from", [MemRead]>:$memref);
333a310,317
>     // ToTensorOp conceptually loads a tensor from a memory location. The
>     // One-Shot analysis has no information about the memref that is loaded from
>     // by ToTensorOp. We have to assume that the loaded tensor may after
>     // bufferization potentially alias with any other bufferized tensor. Since
>     // ToTensorOp and ToMemrefOp have no aliasing OpOperand/OpResult pairs, this
>     // cannot be encoded directly in the analysis. However, declaring ToTensorOp
>     // results as not writable enforces a buffer copy and has the same effect.
> 
336c320,326
<       // to_tensor/to_memref pairs fold away after bufferization.
---
>       // to_tensor cannot be bufferized. However, other ops that are using
>       // to_tensor's result will eventually be bufferized. At that point, they
>       // will start using to_tensor's memref operand. Once all users of
>       // to_tensor are bufferized, the op will not have any users anymore and
>       // DCE away. In case of partial bufferization, to_memref(to_tensor(x))
>       // constructs may be left over. These are folded by the canonicalizer or
>       // FinalizingBufferize.
340c330,333
<     bool isWritable(Value value, const AnalysisState &state);
---
>     bool isWritable(Value value, const AnalysisState &state) const {
>       // It is unknown whether the memref operand is writable or not.
>       return false;
>     }
349,352c342
<   let assemblyFormat = [{
<     $memref (`restrict` $restrict^)? (`writable` $writable^)? attr-dict
<       `:` type($memref)
<   }];
---
>   let assemblyFormat = "$memref attr-dict `:` type($memref)";
374c364
<     An operation that returns the future buffer of a `tensor`.
---
>     Casts a tensor to a memref.
377,378c367,368
<     // Result type is memref<4x?xf32, #layout, 0>
<     %m = bufferization.to_memref %t : memref<4x?xf32, #layout, 0>
---
>     // Result type is memref<4x?xf32, #layout, 42>
>     %12 = bufferization.to_memref %10 : memref<4x?xf32, #layout, 42>
381,383c371,372
<     This operation is a specialized variant of the built-in
<     `unrealized_conversion_cast` and is used to make sure that the IR stays
<     valid at any point during the bufferization.
---
>     Note, that mutating the result of the `to_memref` operation leads to
>     undefined behavior.
385,386c374,376
<     IR that contains `to_memref` ops cannot be bufferized with One-Shot
<     Bufferize.
---
>     This operation is a specialized variant of the built-in
>     `unrealized_conversion_cast` and is intended for use in the context of
>     gradual bufferization.
421c411
<     AliasingOpResultList getAliasingOpResults(
---
>     SmallVector<OpResult> getAliasingOpResult(
--- include/mlir/Dialect/Bufferization/IR/BufferizationBase.td
29c29
<     "affine::AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
---
>     "AffineDialect", "memref::MemRefDialect", "tensor::TensorDialect"
71a72
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Bufferization/IR/BufferizableOpInterface.td
95,138c95,103
<         /*desc=*/[{
<           Return `true` if the given OpResult bufferizes to a memory write.
<           This is the same property as `bufferizesToMemoryWrite`, but from The
<           perspective of OpResults.
< 
<           This method will never be called on OpResults that do not have a
<           tensor type.
< 
<           This method has a default implementation. By default, it returns
<           `true` if any of the following three cases applies.
< 
<           1. There is no corresponding aliasing OpOperand.
< 
<              Example: `tensor.generate ... : tensor<10xf32>`
<              The op fills a newly allocated buffer and bufferizes to a memory
<              write.
< 
<              Counter-example: bufferization.alloc_tensor
<              The op just allocates and does not specifiy the data of the tensor,
<              so resultBufferizesToMemoryWrite is overridden to return false.
< 
<           2. At least one aliasing OpOperand bufferizes to a memory write.
< 
<              Example: `tensor.insert %f into %t[...] : tensor<?xf32>`
<              The destination OpOperand bufferizes to a memory write, so the
<              result also bufferizes to a memory write.
< 
<           3. At least one aliasing OpOperand's value is defined inside the
<              defining op of the given OpResult and it is a memory write.
< 
<              According to this rule, an aliasing OpOperand value that is defined
<              inside this op and is bufferizing to a memory write makes the given
<              OpResult bufferize to a memory write.
< 
<              Example:
<              ```
<              %r = scf.if ... -> tensor<?xf32> {
<                %1 = tensor.insert %f into %t[...] : tensor<?xf32>
<                scf.yield %1 : tensor<?xf32>
<              } else { ... }
<              ```
<              The scf.if result bufferizes to a memory write because %1 (an
<              OpResult defined inside the scf.if op) bufferizes to a memory
<              write.
---
>           /*desc=*/[{
>             Return `true` if the given OpResult is a memory write. This is the
>             case if in the following cases:
> 
>             * The corresponding aliasing OpOperand bufferizes to a memory write.
>             * Or: There is no corresponding aliasing OpOperand.
> 
>             If the OpResult has multiple aliasing OpOperands, this method
>             returns `true` if at least one of them bufferizes to a memory write.
140,150c105,123
<         /*retType=*/"bool",
<         /*methodName=*/"resultBufferizesToMemoryWrite",
<         /*args=*/(ins "::mlir::OpResult":$opResult,
<                       "const ::mlir::bufferization::AnalysisState &":$state),
<         /*methodBody=*/"",
<         /*defaultImplementation=*/[{
<           assert(opResult.getDefiningOp() == $_op.getOperation() &&
<                  "invalid OpResult");
<           return ::mlir::bufferization::detail::defaultResultBufferizesToMemoryWrite(
<               opResult, state);
<         }]
---
>           /*retType=*/"bool",
>           /*methodName=*/"isMemoryWrite",
>           /*args=*/(ins "::mlir::OpResult":$opResult,
>                         "const ::mlir::bufferization::AnalysisState &":$state),
>           /*methodBody=*/"",
>           /*defaultImplementation=*/[{
>             auto bufferizableOp =
>                 cast<BufferizableOpInterface>($_op.getOperation());
>             SmallVector<OpOperand*> opOperands =
>               bufferizableOp.getAliasingOpOperand(opResult, state);
>             if (opOperands.empty())
>               return true;
>             return llvm::any_of(
>                 opOperands,
>                 [&](OpOperand *operand) {
>                   return bufferizableOp.bufferizesToMemoryWrite(*operand,
>                                                                 state);
>                 });
>           }]
158,161d130
< 
<           Note: Unranked tensor OpOperands always bufferize in-place. This could
<           be extended in the future. Unranked tensors are used with external
<           functions only.
169c138
<           return opOperand.get().getType().isa<::mlir::UnrankedTensorType>();
---
>           return false;
174c143
<           Return the OpResults that may alias with a given OpOperand when
---
>           Return the OpResult that aliases with a given OpOperand when
178,227c147,149
<           This method can return multiple OpResults, indicating that a given
<           OpOperand may at runtime alias with any (or multiple) of the returned
<           OpResults.
< 
<           Each alias is specified with a degree of certainty:
< 
<           * MAYBE (`isDefinite = false`): At runtime, buffer(opOperand) may
<             alias with the specified OpResult.
<           * DEFINITE (`isDefinite = true`, default): At runtime,
<             buffer(opOperand) is guaranteed to alias the buffer of the specified
<             OpResult. This is a stronger property than MAYBE and allows for more
<             precise analyses. DEFINITE properties should be used when possible.
< 
<           Furthermore, each alias is specified with a buffer relation:
< 
<           * `BufferRelation::Equivalent`: Both aliases are the exact same
<             buffer. I.e., same size, no offset, same strides.
<           * `BufferRelation::Unknown`: There is no further information apart
<             from the fact that both buffers alias.
< 
<           False positives are allowed in the list of OpResults, but they can
<           adversely affect the accuracy of the anlysis. On the contrary,
<           omitting potential aliases is incorrect.
< 
<           One possible (conservative) implementation of this interface method,
<           that is always safe, is to return all tensor OpResults with
<           BufferRelation::Unknown and MAYBE.
< 
<           Examples:
< 
<           ```
<           // aliasingOpResults(%t) = DEFINITE {Equivalent %r}
<           %r = tensor.insert_slice %f into %t : tensor<10xf32>
< 
<           // aliasingOpResults(%t) = DEFINITE {Unknown %r}
<           // Note: "Buffer is subset of buffer" relationship are not yet
<           // supported, so "Unknown" is the best we can do for now.
<           %r = tensor.extract_slice %t[0]][5][1]
<               : tensor<10xf32> to tensor<5xf32>
< 
<           // aliasingOpResults(%t1) = MAYBE {Equivalent %r}
<           // aliasingOpResults(%t2) = MAYBE {Equivalent %r}
<           %r = arith.select %c, %t1, %t2 : tensor<10xf32>
< 
<           // A hypothetical op that bufferizes to rolling a dice and based on
<           // the result to either return buffer(%t) or a newly allocated copy
<           // thereof.
<           // aliasingOpResults(%t) = MAYBE {Equivalent %r}
<           %r = "dummy.alias_or_copy(%t) : (tensor<10xf32>) -> (tensor<10xf32>)"
<           ```
---
>           Note: This method can return multiple OpResults, indicating that a
>           given OpOperand may at runtime alias with any (or multiple) of the
>           returned OpResults.
229,230c151,152
<         /*retType=*/"::mlir::bufferization::AliasingOpResultList",
<         /*methodName=*/"getAliasingOpResults",
---
>         /*retType=*/"::mlir::SmallVector<::mlir::OpResult>",
>         /*methodName=*/"getAliasingOpResult",
236,238c158
<           assert(opOperand.get().getType().isa<::mlir::TensorType>() &&
<                  "expected OpOperand with tensor type");
<           llvm_unreachable("getAliasingOpResults not implemented");
---
>           llvm_unreachable("getAliasingOpResult not implemented");
247c167
<           By default, this method is the inverse of `getAliasingOpResults`. Ops
---
>           By default, this method is the inverse of `getAliasingOpResult`. Ops
251,299c171,174
<           This method can return multiple OpOperands, indicating that a given
<           OpResult may at runtime alias with any (or multiple) of the returned
<           OpOperands.
< 
<           This property is specified with a degree of certainty:
< 
<           * MAYBE (`isDefinite = false`): At runtime, buffer(opResult) may
<             alias with the specified OpOperand.
<           * DEFINITE (`isDefinite = true`, default): At runtime,
<             buffer(opResult) is guaranteed to alias the buffer of the specified
<             OpOperand. This is a stronger property than MAYBE and allows for
<             more precise analyses. DEFINITE properties should be used when
<             possible.
< 
<           For each alias, a BufferRelation can be specified:
< 
<           * `BufferRelation::Equivalent`: Both aliases are the exact same
<             buffer. I.e., same size, no offset, same strides.
<           * `BufferRelation::Unknown`: There is no further information apart
<             from the fact that both buffers alias.
< 
<           False positives are allowed in the list of OpOperands, but they can
<           adversely affect the accuracy of the anlysis. On the contrary,
<           omitting potential aliases is incorrect.
< 
<           One possible (conservative) implementation of this interface method,
<           that is always safe, is to return all tensor OpOperands with
<           BufferRelation::Unknown and MAYBE.
< 
<           Note: If the returned list of OpOperands is empty, this op definitely
<           bufferizes to a new allocation. In that case `bufferizesToAllocation`
<           must return `true`.
< 
<           Examples:
< 
<           ```
<           // aliasingOpOperands(%r) = DEFINITE {Equivalent %t}
<           %r = tensor.insert_slice %f into %t : tensor<10xf32>
< 
<           // aliasingOpOperands(%r) = DEFINITE {Unknown %t}
<           %r = tensor.extract_slice %t[0]][5][1]
<               : tensor<10xf32> to tensor<5xf32>
< 
<           // aliasingOpOperands(%r) = DEFINITE {Equivalent %t1, Equivalent %t2}
<           %r = arith.select %c, %t1, %t2 : tensor<10xf32>
< 
<           // aliasingOpOperands(%r) = MAYBE {}
<           %r = tensor.empty() : tensor<10xf32>
<           ```
---
>           Note: This method can return multiple OpOperands, indicating that the
>           given OpResult may at runtime alias with any (or multiple) of the
>           returned OpOperands. This can be useful for branches and for ops such
>           as `arith.select`.
301,302c176,177
<         /*retType=*/"::mlir::bufferization::AliasingOpOperandList",
<         /*methodName=*/"getAliasingOpOperands",
---
>         /*retType=*/"::mlir::SmallVector<::mlir::OpOperand *>",
>         /*methodName=*/"getAliasingOpOperand",
307c182
<           assert(opResult.getType().isa<::mlir::TensorType>() &&
---
>           assert(opResult.getType().isa<TensorType>() &&
309,310c184,217
<           return ::mlir::bufferization::detail::defaultGetAliasingOpOperands(
<               opResult, state);
---
>           SmallVector<OpOperand *> result;
>           auto bufferizableOp =
>               cast<BufferizableOpInterface>($_op.getOperation());
>           for (OpOperand &opOperand : $_op.getOperation()->getOpOperands()) {
>             if (!opOperand.get().getType().isa<TensorType>())
>               continue;
>             SmallVector<OpResult> aliasingOpResults =
>                 bufferizableOp.getAliasingOpResult(opOperand, state);
>             if (llvm::is_contained(aliasingOpResults, opResult))
>               result.push_back(&opOperand);
>           }
>           return result;
>         }]
>       >,
>       InterfaceMethod<
>         /*desc=*/[{
>           Return the buffer relation between the given OpResult and its aliasing
>           OpOperands when bufferized in-place. Most OpOperands have an
>           "equivalence" relation. This method will never be called on OpResults
>           that do not have a tensor type. It will also never be called on
>           OpResults that do not have at least one aliasing OpOperand.
> 
>           TODO: Support other relations such as "OpOperand is included in
>           OpResult".
>         }],
>         /*retType=*/"::mlir::bufferization::BufferRelation",
>         /*methodName=*/"bufferRelation",
>         /*args=*/(ins "::mlir::OpResult":$opResult,
>                       "const ::mlir::bufferization::AnalysisState &":$state),
>         /*methodBody=*/"",
>         /*defaultImplementation=*/[{
>           // Does not have to be implemented for ops without tensor OpResults
>           // that have an aliasing OpOperand.
>           llvm_unreachable("bufferRelation not implemented");
334c241
<               ::llvm::cast<BufferizableOpInterface>($_op.getOperation());
---
>               cast<BufferizableOpInterface>($_op.getOperation());
349c256
<           remaining methods, in particular `getAliasingOpOperands`. I.e., a
---
>           remaining methods, in particular `getAliasingOpOperand`. I.e., a
351,353c258,259
< 
<           a) One of the buffers in getAliasingOpOperands(r).
<           b) Or: A newly allocated buffer (only if `bufferizesToAllocation`).
---
>           a) A buffer that aliases one of buffers in getAliasingOpOperand(r).
>           b) Or: A newly allocated buffer.
372c278
<           return ::mlir::failure();
---
>           return failure();
397c303
<           return value.isa<::mlir::OpResult>();
---
>           return value.isa<OpResult>();
437c343
<           return ::mlir::success();
---
>           return success();
458c364
<           return ::mlir::bufferization::detail::defaultGetBufferType(
---
>           return bufferization::detail::defaultGetBufferType(
481,482c387,388
<           return ::mlir::bufferization::detail::defaultIsRepetitiveRegion(
<               ::llvm::cast<BufferizableOpInterface>($_op.getOperation()), index);
---
>           return mlir::bufferization::detail::defaultIsRepetitiveRegion(
>               cast<BufferizableOpInterface>($_op.getOperation()), index);
490,492c396,397
<     ::mlir::LogicalResult resolveTensorOpOperandConflicts(
<         ::mlir::RewriterBase &rewriter,
<         const ::mlir::bufferization::AnalysisState &state);
---
>     LogicalResult resolveTensorOpOperandConflicts(
>         RewriterBase &rewriter, const AnalysisState &state);
500,502c405,406
<     bool bufferizesToAliasOnly(
<         ::mlir::OpOperand &opOperand,
<         const ::mlir::bufferization::AnalysisState &state) {
---
>     bool bufferizesToAliasOnly(OpOperand &opOperand,
>                                const AnalysisState &state) {
504c408
<           ::llvm::cast<::mlir::bufferization::BufferizableOpInterface>(getOperation());
---
>           cast<BufferizableOpInterface>(getOperation());
507,508c411
<           && bufferizableOp.getAliasingOpResults(opOperand, state)
<               .getNumAliases() != 0;
---
>           && !bufferizableOp.getAliasingOpResult(opOperand, state).empty();
--- include/mlir/Dialect/Bufferization/IR/AllocationOpInterface.td
--- include/mlir/Dialect/Bufferization/TransformOps
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Bufferization/TransformOps/BufferizationTransformOps.td include/mlir/Dialect/Bufferization/TransformOps/BufferizationTransformOps.td
13a14
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
20,26d20
< def Transform_EmptyOp : Transform_ConcreteOpType<"tensor.empty">;
< def Transform_AllocTensorOp : Transform_ConcreteOpType<"bufferization.alloc_tensor">;
< 
< //===----------------------------------------------------------------------===//
< // OneShotBufferizeOp
< //===----------------------------------------------------------------------===//
< 
29,30c23,24
<         [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<          DeclareOpInterfaceMethods<TransformOpInterface>]> {
---
>         [DeclareOpInterfaceMethods<TransformOpInterface>,
>          DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
38,39c32,35
<     The targeted ops must be modules or functions. This is because there is
<     always a single, bufferized replacement op for such targets.
---
>     If `target_is_module` is set, `target` must be a module. In that case the
>     `target` handle can be reused by other transform ops. When bufferizing other
>     ops, the `target` handled is freed after bufferization and can no longer be
>     used.
47,51d42
< 
<     #### Return modes
< 
<     This operation consumes the `target` handle and produces the `transformed`
<     handle.
55c46
<       ins TransformHandleTypeInterface:$target,
---
>       ins PDL_Operation:$target,
60a52
>       DefaultValuedAttr<BoolAttr, "true">:$target_is_module,
64c56
<   let results = (outs TransformHandleTypeInterface:$transformed);
---
>   let results = (outs);
68c60
<     $target attr-dict `:` functional-type($target, results)
---
>     $target attr-dict
73c65
< // EliminateEmptyTensorsOp
---
> // EmptyTensorToAllocTensorOp
76,124d67
< def EliminateEmptyTensorsOp
<     : Op<Transform_Dialect, "bufferization.eliminate_empty_tensors",
<         [DeclareOpInterfaceMethods<TransformOpInterface>,
<          DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     Try to eliminate all `tensor.empty` ops within the targeted op by replacing
<     them with a destination tensor.
< 
<     `tensor.empty` ops cannot be bufferizes. They can either be converted to
<     `bufferization.alloc_tensor` or replaced with another tensor (via this
<     transform). `tensor.empty` does not specify the contents of the returned
<     tensor so their results can be replaced with arbitrary tensor values as long
<     as the dimensions match.
< 
<     This transform looks for `tensor.empty` ops where the SSA use-def chain of
<     the result ends in a supported "anchor op" (always following the aliasing
<     OpOperand/OpResult chain). Currently supported anchor ops are:
<     - `tensor.insert_slice`
<     - `bufferization.yield` (inside `bufferization.alloc_tensor`)
< 
<     Example:
< 
<     ```
<     %0 = tensor.empty() : tensor<5xf32>
<     %1 = linalg.fill ... outs(%0)
<     %2 = tensor.insert_slice %1 into %t[1][5][1]
<     ```
< 
<     Is rewritten with:
<     ```
<     %0 = tensor.extract_slice %t[1][5][1]
<     %1 = linalg.fill ... outs(%0)
<     %2 = tensor.insert_slice %1 into %t[1][5][1]
<     ```
< 
<     The above example can bufferize without an allocation (in the absence of
<     other conflicts) because there is no longer a `tensor.empty` op.
< 
<     See `-eliminate-empty-tensors` for more details.
< 
<     #### Return modes
< 
<     This transform reads the target handle and modifies the payload. It does
<     not produce any handle.
<   }];
< 
<   let arguments = (ins PDL_Operation:$target);
< 
<   let results = (outs);
126,131c69,70
<   let assemblyFormat = "$target attr-dict";
< }
< 
< //===----------------------------------------------------------------------===//
< // EmptyTensorToAllocTensorOp
< //===----------------------------------------------------------------------===//
---
> def Transform_EmptyOp : Transform_ConcreteOpType<"tensor.empty">;
> def Transform_AllocTensorOp : Transform_ConcreteOpType<"bufferization.alloc_tensor">;
--- include/mlir/Dialect/Bufferization/TransformOps/BufferizationTransformOps.td
13a14
> include "mlir/Dialect/Transform/IR/TransformEffects.td"
20,26d20
< def Transform_EmptyOp : Transform_ConcreteOpType<"tensor.empty">;
< def Transform_AllocTensorOp : Transform_ConcreteOpType<"bufferization.alloc_tensor">;
< 
< //===----------------------------------------------------------------------===//
< // OneShotBufferizeOp
< //===----------------------------------------------------------------------===//
< 
29,30c23,24
<         [FunctionalStyleTransformOpTrait, MemoryEffectsOpInterface,
<          DeclareOpInterfaceMethods<TransformOpInterface>]> {
---
>         [DeclareOpInterfaceMethods<TransformOpInterface>,
>          DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
38,39c32,35
<     The targeted ops must be modules or functions. This is because there is
<     always a single, bufferized replacement op for such targets.
---
>     If `target_is_module` is set, `target` must be a module. In that case the
>     `target` handle can be reused by other transform ops. When bufferizing other
>     ops, the `target` handled is freed after bufferization and can no longer be
>     used.
47,51d42
< 
<     #### Return modes
< 
<     This operation consumes the `target` handle and produces the `transformed`
<     handle.
55c46
<       ins TransformHandleTypeInterface:$target,
---
>       ins PDL_Operation:$target,
60a52
>       DefaultValuedAttr<BoolAttr, "true">:$target_is_module,
64c56
<   let results = (outs TransformHandleTypeInterface:$transformed);
---
>   let results = (outs);
68c60
<     $target attr-dict `:` functional-type($target, results)
---
>     $target attr-dict
73c65
< // EliminateEmptyTensorsOp
---
> // EmptyTensorToAllocTensorOp
76,124d67
< def EliminateEmptyTensorsOp
<     : Op<Transform_Dialect, "bufferization.eliminate_empty_tensors",
<         [DeclareOpInterfaceMethods<TransformOpInterface>,
<          DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {
<   let description = [{
<     Try to eliminate all `tensor.empty` ops within the targeted op by replacing
<     them with a destination tensor.
< 
<     `tensor.empty` ops cannot be bufferizes. They can either be converted to
<     `bufferization.alloc_tensor` or replaced with another tensor (via this
<     transform). `tensor.empty` does not specify the contents of the returned
<     tensor so their results can be replaced with arbitrary tensor values as long
<     as the dimensions match.
< 
<     This transform looks for `tensor.empty` ops where the SSA use-def chain of
<     the result ends in a supported "anchor op" (always following the aliasing
<     OpOperand/OpResult chain). Currently supported anchor ops are:
<     - `tensor.insert_slice`
<     - `bufferization.yield` (inside `bufferization.alloc_tensor`)
< 
<     Example:
< 
<     ```
<     %0 = tensor.empty() : tensor<5xf32>
<     %1 = linalg.fill ... outs(%0)
<     %2 = tensor.insert_slice %1 into %t[1][5][1]
<     ```
< 
<     Is rewritten with:
<     ```
<     %0 = tensor.extract_slice %t[1][5][1]
<     %1 = linalg.fill ... outs(%0)
<     %2 = tensor.insert_slice %1 into %t[1][5][1]
<     ```
< 
<     The above example can bufferize without an allocation (in the absence of
<     other conflicts) because there is no longer a `tensor.empty` op.
< 
<     See `-eliminate-empty-tensors` for more details.
< 
<     #### Return modes
< 
<     This transform reads the target handle and modifies the payload. It does
<     not produce any handle.
<   }];
< 
<   let arguments = (ins PDL_Operation:$target);
< 
<   let results = (outs);
126,131c69,70
<   let assemblyFormat = "$target attr-dict";
< }
< 
< //===----------------------------------------------------------------------===//
< // EmptyTensorToAllocTensorOp
< //===----------------------------------------------------------------------===//
---
> def Transform_EmptyOp : Transform_ConcreteOpType<"tensor.empty">;
> def Transform_AllocTensorOp : Transform_ConcreteOpType<"bufferization.alloc_tensor">;
--- include/mlir/Dialect/Bufferization/TransformOps/CMakeLists.txt
--- include/mlir/Dialect/Bufferization/TransformOps/BufferizationTransformOps.h
--- include/mlir/Dialect/SPIRV
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR and include/mlir/Dialect/SPIRV/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/Linking and include/mlir/Dialect/SPIRV/Linking
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/Transforms and include/mlir/Dialect/SPIRV/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/Utils and include/mlir/Dialect/SPIRV/Utils
--- include/mlir/Dialect/SPIRV/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/Utils/LayoutUtils.h include/mlir/Dialect/SPIRV/Utils/LayoutUtils.h
29c29
< /// According to the Vulkan spec "15.6.4. Offset and Stride Assignment":
---
> /// According to the Vulkan spec "14.5.4. Offset and Stride Assignment":
--- include/mlir/Dialect/SPIRV/Utils/LayoutUtils.h
29c29
< /// According to the Vulkan spec "15.6.4. Offset and Stride Assignment":
---
> /// According to the Vulkan spec "14.5.4. Offset and Stride Assignment":
--- include/mlir/Dialect/SPIRV/CMakeLists.txt
--- include/mlir/Dialect/SPIRV/Transforms
--- include/mlir/Dialect/SPIRV/Transforms/Passes.h
--- include/mlir/Dialect/SPIRV/Transforms/Passes.td
--- include/mlir/Dialect/SPIRV/Transforms/CMakeLists.txt
--- include/mlir/Dialect/SPIRV/Transforms/SPIRVConversion.h
--- include/mlir/Dialect/SPIRV/Transforms/SPIRVWebGPUTransforms.h
--- include/mlir/Dialect/SPIRV/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVBase.td include/mlir/Dialect/SPIRV/IR/SPIRVBase.td
50a51
>   let useFoldAPI = kEmitFoldAdaptorFolder;
402d402
< def SPV_INTEL_bfloat16_conversion                : I32EnumAttrCase<"SPV_INTEL_bfloat16_conversion", 4031>;
461c461
<       SPV_INTEL_bfloat16_conversion, SPV_NV_compute_shader_derivatives, SPV_NV_cooperative_matrix,
---
>       SPV_NV_compute_shader_derivatives, SPV_NV_cooperative_matrix,
1417,1422d1416
< def SPIRV_C_Bfloat16ConversionINTEL                         : I32EnumAttrCase<"Bfloat16ConversionINTEL", 6115> {
<   list<Availability> availability = [
<     Extension<[SPV_INTEL_bfloat16_conversion]>
<   ];
< }
< 
1514c1508
<       SPIRV_C_ShaderStereoViewNV, SPIRV_C_JointMatrixINTEL, SPIRV_C_Bfloat16ConversionINTEL
---
>       SPIRV_C_ShaderStereoViewNV, SPIRV_C_JointMatrixINTEL
4089d4082
< def SPIRV_Int16 : TypeAlias<I16, "Int16">;
4418,4420d4410
< def SPIRV_OC_OpConvertFToBF16INTEL        : I32EnumAttrCase<"OpConvertFToBF16INTEL", 6116>;
< def SPIRV_OC_OpConvertBF16ToFINTEL        : I32EnumAttrCase<"OpConvertBF16ToFINTEL", 6117>;
< 
4506,4508c4496
<       SPIRV_OC_OpTypejointMatrixWorkItemLengthINTEL,
< 
<       SPIRV_OC_OpConvertFToBF16INTEL, SPIRV_OC_OpConvertBF16ToFINTEL
---
>       SPIRV_OC_OpTypejointMatrixWorkItemLengthINTEL
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVControlFlowOps.td include/mlir/Dialect/SPIRV/IR/SPIRVControlFlowOps.td
239,242d238
<   let extraClassDeclaration = [{
<     Operation::operand_range getCallOperands();
<   }];
< 
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR: SPIRVIntelExtOps.td
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVOps.td include/mlir/Dialect/SPIRV/IR/SPIRVOps.td
34d33
< include "mlir/Dialect/SPIRV/IR/SPIRVIntelExtOps.td"
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SPIRV/IR/TargetAndABI.h include/mlir/Dialect/SPIRV/IR/TargetAndABI.h
124,125c124
< AddressingModel getAddressingModel(TargetEnvAttr targetAttr,
<                                    bool use64bitAddress);
---
> AddressingModel getAddressingModel(TargetEnvAttr targetAttr);
--- include/mlir/Dialect/SPIRV/IR/SPIRVOpTraits.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVMatrixOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVAvailability.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVCompositeOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVArithmeticOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVGroupOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVImageOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVControlFlowOps.td
239,242d238
<   let extraClassDeclaration = [{
<     Operation::operand_range getCallOperands();
<   }];
< 
--- include/mlir/Dialect/SPIRV/IR/SPIRVJointMatrixOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVAttributes.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVCastOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVNonUniformOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVBarrierOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVEnums.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVAttributes.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVBitOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVIntegerDotProductOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVMiscOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVOps.td
34d33
< include "mlir/Dialect/SPIRV/IR/SPIRVIntelExtOps.td"
--- include/mlir/Dialect/SPIRV/IR/SPIRVDialect.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVMemoryOps.td
--- include/mlir/Dialect/SPIRV/IR/CMakeLists.txt
--- include/mlir/Dialect/SPIRV/IR/SPIRVStructureOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVTypes.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVLogicalOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVBase.td
50a51
>   let useFoldAPI = kEmitFoldAdaptorFolder;
402d402
< def SPV_INTEL_bfloat16_conversion                : I32EnumAttrCase<"SPV_INTEL_bfloat16_conversion", 4031>;
461c461
<       SPV_INTEL_bfloat16_conversion, SPV_NV_compute_shader_derivatives, SPV_NV_cooperative_matrix,
---
>       SPV_NV_compute_shader_derivatives, SPV_NV_cooperative_matrix,
1417,1422d1416
< def SPIRV_C_Bfloat16ConversionINTEL                         : I32EnumAttrCase<"Bfloat16ConversionINTEL", 6115> {
<   list<Availability> availability = [
<     Extension<[SPV_INTEL_bfloat16_conversion]>
<   ];
< }
< 
1514c1508
<       SPIRV_C_ShaderStereoViewNV, SPIRV_C_JointMatrixINTEL, SPIRV_C_Bfloat16ConversionINTEL
---
>       SPIRV_C_ShaderStereoViewNV, SPIRV_C_JointMatrixINTEL
4089d4082
< def SPIRV_Int16 : TypeAlias<I16, "Int16">;
4418,4420d4410
< def SPIRV_OC_OpConvertFToBF16INTEL        : I32EnumAttrCase<"OpConvertFToBF16INTEL", 6116>;
< def SPIRV_OC_OpConvertBF16ToFINTEL        : I32EnumAttrCase<"OpConvertBF16ToFINTEL", 6117>;
< 
4506,4508c4496
<       SPIRV_OC_OpTypejointMatrixWorkItemLengthINTEL,
< 
<       SPIRV_OC_OpConvertFToBF16INTEL, SPIRV_OC_OpConvertBF16ToFINTEL
---
>       SPIRV_OC_OpTypejointMatrixWorkItemLengthINTEL
--- include/mlir/Dialect/SPIRV/IR/SPIRVAtomicOps.td
--- include/mlir/Dialect/SPIRV/IR/TargetAndABI.h
124,125c124
< AddressingModel getAddressingModel(TargetEnvAttr targetAttr,
<                                    bool use64bitAddress);
---
> AddressingModel getAddressingModel(TargetEnvAttr targetAttr);
--- include/mlir/Dialect/SPIRV/IR/SPIRVCLOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVOps.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVGLCanonicalization.h
--- include/mlir/Dialect/SPIRV/IR/SPIRVGLOps.td
--- include/mlir/Dialect/SPIRV/IR/SPIRVCooperativeMatrixOps.td
--- include/mlir/Dialect/SPIRV/IR/ParserUtils.h
--- include/mlir/Dialect/SPIRV/Linking
--- include/mlir/Dialect/SPIRV/Linking/ModuleCombiner.h
--- include/mlir/Dialect/ArmSVE
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/ArmSVE/ArmSVE.td include/mlir/Dialect/ArmSVE/ArmSVE.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/ArmSVE/CMakeLists.txt
--- include/mlir/Dialect/ArmSVE/ArmSVEDialect.h
--- include/mlir/Dialect/ArmSVE/ArmSVE.td
31a32
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/ArmSVE/Transforms.h
--- include/mlir/Dialect/Quant
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Quant/QuantOpsBase.td include/mlir/Dialect/Quant/QuantOpsBase.td
22a23
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Quant/QuantTypes.h
--- include/mlir/Dialect/Quant/QuantOpsBase.td
22a23
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/Quant/CMakeLists.txt
--- include/mlir/Dialect/Quant/QuantOps.td
--- include/mlir/Dialect/Quant/UniformSupport.h
--- include/mlir/Dialect/Quant/QuantOps.h
--- include/mlir/Dialect/Quant/FakeQuantSupport.h
--- include/mlir/Dialect/SparseTensor
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR and include/mlir/Dialect/SparseTensor/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Pipelines and include/mlir/Dialect/SparseTensor/Pipelines
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Transforms and include/mlir/Dialect/SparseTensor/Transforms
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Utils and include/mlir/Dialect/SparseTensor/Utils
--- include/mlir/Dialect/SparseTensor/Utils
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Utils/Merger.h include/mlir/Dialect/SparseTensor/Utils/Merger.h
16,17d15
< #include "mlir/Dialect/SparseTensor/Utils/MergerNewtypes.h"
< 
20d17
< #include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
28,75d24
< /// Tensor expression. Represents an MLIR expression in tensor index notation.
< struct TensorExp final {
<   enum class Kind;
< 
<   /// Child subexpressions for non-leaf expressions.
<   struct Children final {
<     ExprId e0;
<     ExprId e1;
<   };
< 
<   // The `x` parameter has different types depending on the value of the
<   // `k` parameter.  The correspondences are:
<   // * `kTensor`    -> `TensorId`
<   // * `kInvariant` -> `kInvalidId`
<   // * `kLoopVar`   -> `LoopId`
<   // * else         -> `ExprId`
<   //
<   // The `y`, `v`, and `op` parameters either must or must not be
<   // `kInvalidId`/`nullptr`, depending on the value of the `k` parameter;
<   // however, they have uniform C++ types regardless of the value of `k`.
<   TensorExp(Kind k, unsigned x, ExprId y, Value v, Operation *op);
< 
<   /// Tensor expression kind.
<   Kind kind;
< 
<   union {
<     /// `kTensor` expressions simply have a tensor identifier.
<     TensorId tensor;
< 
<     /// `kLoopVar` expressions simply have a loop identifier.
<     LoopId loop;
< 
<     /// All other expressions hold the `ExprId`s of their children.
<     Children children;
<   };
< 
<   /// Direct link to IR for an invariant or the destination value (to
<   /// infer destination type) of a cast operation During code generation,
<   /// this field may be used to cache "hoisted" loop invariant tensor loads.
<   Value val;
< 
<   /// Code blocks used by semirings. For the case of kUnary, kBinary, kReduce,
<   /// and kSelect, this holds the original operation with all regions. For
<   /// kBinaryBranch, this holds the YieldOp for the left or right half
<   /// to be merged into a nested scf loop.
<   Operation *op;
< };
< 
77,89c26
< ///
< /// The `kLoopVar` leaf kind is for representing `linalg::IndexOp`.
< /// That is, its argument is a `LoopId` identifying the loop-variable
< /// in question, and its value will be the current iteration's value
< /// of that loop-variable.  See the `LoopId` documentation for more details.
< //
< // TODO: Modify this definition so that the numeric values already encode
< // the `ExpArity` (while extending the notion of "arity" to include not
< // just the number of `ExprId` children the node has, but also whether the
< // node has a `Value` and/or `Operation*`).  Doing this will avoid needing
< // to enumerate all the kinds in `getExpArity` and in the `TensorExp` ctor,
< // and should help clean up a few other places as well.
< enum class TensorExp::Kind {
---
> enum Kind {
93c30
<   kLoopVar,
---
>   kIndex,
153,160c90,101
< //===----------------------------------------------------------------------===//
< /// Lattice point.  Each lattice point consists of a formal conjunction
< /// of `TensorLoopId`s, together with the identifier of the corresponding
< /// tensor expression.  The formal conjunction is represented as a set of
< /// `TensorLoopId`, where that set is implemented as a `BitVector`.
< struct LatPoint final {
<   /// Construct a lattice point with the empty set of `TensorLoopId`s.
<   LatPoint(unsigned size, ExprId e) : bits(size, false), exp(e) {}
---
> /// Children subexpressions of tensor operations.
> struct Children {
>   unsigned e0;
>   unsigned e1;
> };
> 
> /// Tensor expression. Represents a MLIR expression in tensor index notation.
> struct TensorExp {
>   TensorExp(Kind k, unsigned x, unsigned y, Value v, Operation *operation);
> 
>   /// Tensor expression kind.
>   Kind kind;
162,163c103,105
<   /// Construct a lattice point from the given set of `TensorLoopId`s.
<   LatPoint(const BitVector &bits, ExprId e) : bits(bits), exp(e) {}
---
>   union {
>     /// Expressions representing tensors simply have a tensor number.
>     unsigned tensor;
165c107,134
<   /// Conjunction of all `TensorLoopId`s involved in the tensor expression.
---
>     /// Indices hold the index number.
>     unsigned index;
> 
>     /// Tensor operations hold the indices of their children.
>     Children children;
>   };
> 
>   /// Direct link to IR for an invariant or the destination value (to
>   /// infer destination type) of a cast operation During code generation,
>   /// this field may be used to cache "hoisted" loop invariant tensor loads.
>   Value val;
> 
>   /// Code blocks used by semirings. For the case of kUnary, kBinary, kReduce,
>   /// and kSelect, this holds the original operation with all regions. For
>   /// kBinaryBranch, this holds the YieldOp for the left or right half
>   /// to be merged into a nested scf loop.
>   Operation *op;
> };
> 
> /// Lattice point. Each lattice point consists of a conjunction of tensor
> /// loop indices (encoded in a bitvector) and the index of the corresponding
> /// tensor expression.
> struct LatPoint {
>   LatPoint(unsigned n, unsigned e, unsigned b);
>   LatPoint(const BitVector &b, unsigned e);
> 
>   /// Conjunction of tensor loop indices as bitvector. This represents
>   /// all indices involved in the tensor expression
168c137
<   /// Simplified conjunction of `TensorLoopId` as bitvector.  This
---
>   /// Simplified conjunction of tensor loop indices as bitvector. This
173,174c142,143
<   /// Identifier of the tensor expression.
<   ExprId exp;
---
>   /// Index of the tensor expression.
>   unsigned exp;
177d145
< //===----------------------------------------------------------------------===//
192c160
<   /// sparse levels.  E.g., (d0, d1, d2) => (d0 + d1, d2), a naive
---
>   /// sparse dimensions. E.g., (d0, d1, d2) => (d0 + d1, d2), a naive
195,196c163,164
<   /// for (const auto c0 : coordinates[0]) {
<   ///   if (c0 == d0 + d1) {
---
>   /// for (coord : sparse_dim[0])
>   ///   if (coord == d0 + d1) {
203,259c171,175
<   /// The maxLvlRank specifies the max level rank of all inputs/output tensors.
<   /// It is used to pre-allocate sufficient memory for internal storage.
<   //
<   // TODO: we want to make the filter loop more efficient in the future,
<   // e.g., by avoiding scanning the full list of stored coordinates (keeping
<   // the last position in ordered list) or even apply binary search to find
<   // the coordinate.
<   //
<   // TODO: would be cleaner to understand/document if the first argument
<   // gave the number of input tensors, instead of the current number of
<   // input+output tensors.
<   Merger(unsigned numInputOutputTensors, unsigned numNativeLoops,
<          unsigned numFilterLoops, unsigned maxLvlRank);
< 
<   //
<   // Constructing valid tensor and loop identifiers.
<   //
< 
<   /// Safely converts the argument to a tensor identifier.
<   constexpr TensorId makeTensorId(unsigned t) const {
<     assert(isValidTensorId(t));
<     return t;
<   }
< 
<   /// Safely converts the argument to a loop identifier.
<   constexpr LoopId makeLoopId(unsigned i) const {
<     assert(isValidLoopId(i));
<     return i;
<   }
< 
<   /// Safely converts the arguments to a pair of (tensor,loop) identifiers.
<   constexpr TensorLoopId makeTensorLoopId(unsigned t, unsigned i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return numTensors * i + t;
<   }
< 
<   //
<   // Allocating new expressions, points, and sets.
<   //
< 
<   /// Constructs a new tensor expression, and returns its identifier.
<   ExprId addTensorExp(TensorId t);
<   /// Constructs a new loop-variable expression, and returns its identifier.
<   ExprId addLoopVarExp(LoopId i);
<   /// Constructs a new invariant expression, and returns its identifier.
<   ExprId addInvariantExp(Value v);
<   /// Constructs a new unary or binary expression, and returns its identifier.
<   ExprId addExp(TensorExp::Kind k, ExprId e0, ExprId e1 = detail::kInvalidId,
<                 Operation *op = nullptr);
<   /// Constructs a new sesquinary expression, and returns its identifier.
<   /// Currently no sesquinary `Kind` allows specifying the `op`, but we
<   /// allow it anyways because `mapSet` is designed to allow it.
<   ExprId addExp(TensorExp::Kind k, ExprId e, Value v, Operation *op = nullptr);
< 
<   /// Constructs a new iteration lattice point, and returns its identifier.
<   LatPointId addLat(TensorId t, LoopId i, ExprId e);
<   LatPointId addLat(const BitVector &bits, ExprId e);
---
>   /// TODO: we want to make the filter loop more efficient in the future, e.g.,
>   /// by avoiding scanning the full stored index sparse (keeping the last
>   /// position in ordered list) or even apply binary search to find the index.
>   ///
>   Merger(unsigned t, unsigned l, unsigned fl);
261,262c177,191
<   /// Constructs a new (initially empty) set, and returns its identifier.
<   LatSetId addSet();
---
>   /// Adds a tensor expression. Returns its index.
>   unsigned addExp(Kind k, unsigned e0, unsigned e1 = -1u, Value v = Value(),
>                   Operation *op = nullptr);
>   unsigned addExp(Kind k, unsigned e, Value v, Operation *op = nullptr) {
>     return addExp(k, e, -1u, v, op);
>   }
>   unsigned addExp(Kind k, Value v, Operation *op = nullptr) {
>     return addExp(k, -1u, -1u, v, op);
>   }
> 
>   /// Adds an iteration lattice point. Returns its index.
>   unsigned addLat(unsigned t, unsigned i, unsigned e);
> 
>   /// Adds a new, initially empty, set. Returns its index.
>   unsigned addSet();
265,287c194,215
<   /// of `LoopId` (effectively constructing a larger "intersection" of those
<   /// loops) with a newly constructed tensor (sub)expression of given kind.
<   /// Returns the identifier of the new lattice point.
<   LatPointId conjLat(TensorExp::Kind kind, LatPointId p0, LatPointId p1,
<                      Operation *op = nullptr);
< 
<   /// Conjunctive merge of two lattice sets: `(s0 /\_op s1)`.
<   /// Returns the identifier of the new set.
<   LatSetId conjSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                    Operation *op = nullptr);
< 
<   /// Disjunctive merge of two lattice sets: `(s0 /\_op s1, s0, s1)`.
<   /// Returns the identifier of the new set.
<   LatSetId disjSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                    Operation *op = nullptr);
< 
<   /// Disjunctive merge of two lattice sets with custom handling of the
<   /// overlap, left, and right regions.  Any region may be left missing
<   /// in the output.  Returns the identifier of the new set.
<   LatSetId combiSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                     Operation *orig, bool includeLeft, TensorExp::Kind ltrans,
<                     Operation *opleft, bool includeRight,
<                     TensorExp::Kind rtrans, Operation *opright);
---
>   /// of loop indices (effectively constructing a larger "intersection" of those
>   /// indices) with a newly constructed tensor (sub)expression of given kind.
>   /// Returns the index of the new lattice point.
>   unsigned conjLatPoint(Kind kind, unsigned p0, unsigned p1,
>                         Operation *op = nullptr);
> 
>   /// Conjunctive merge of two lattice sets L0 and L1 is conjunction of
>   /// cartesian product. Returns the index of the new set.
>   unsigned takeConj(Kind kind, unsigned s0, unsigned s1,
>                     Operation *op = nullptr);
> 
>   /// Disjunctive merge of two lattice sets L0 and L1 is (L0 /\_op L1, L0, L1).
>   /// Returns the index of the new set.
>   unsigned takeDisj(Kind kind, unsigned s0, unsigned s1,
>                     Operation *op = nullptr);
> 
>   /// Disjunctive merge of two lattice sets L0 and L1 with custom handling of
>   /// the overlap, left, and right regions. Any region may be left missing in
>   /// the output. Returns the index of the new set.
>   unsigned takeCombi(Kind kind, unsigned s0, unsigned s1, Operation *orig,
>                      bool includeLeft, Kind ltrans, Operation *opleft,
>                      bool includeRight, Kind rtrans, Operation *opright);
291,292c219,220
<   /// as new expression. Returns the identifier of the new set.
<   LatSetId mapSet(TensorExp::Kind kind, LatSetId s, Value v = Value(),
---
>   /// as new expression. Returns the index of the new set.
>   unsigned mapSet(Kind kind, unsigned s0, Value v = Value(),
298c226
<   LatSetId optimizeSet(LatSetId s);
---
>   unsigned optimizeSet(unsigned s0);
304,307c232
<   BitVector simplifyCond(LatSetId s, LatPointId p);
< 
<   /// Returns true if p0 > p1.
<   bool latGT(LatPointId p0, LatPointId p1) const;
---
>   BitVector simplifyCond(unsigned s0, unsigned p0);
309,310c234,235
<   /// Returns true if p0 and p1 only differ in dense.
<   bool onlyDenseDiff(LatPointId p0, LatPointId p1) const;
---
>   /// Returns true if Li > Lj.
>   bool latGT(unsigned i, unsigned j) const;
312,315c237,238
<   /// Gets the tensor-identifier of the `TensorLoopId`.
<   constexpr TensorId tensor(TensorLoopId b) const { return b % numTensors; }
<   /// Gets the loop-identifier of the `TensorLoopId`.
<   constexpr LoopId loop(TensorLoopId b) const { return b / numTensors; }
---
>   /// Returns true if Li and Lj only differ in dense.
>   bool onlyDenseDiff(unsigned i, unsigned j);
317,319c240,243
<   /// Get the total number of tensors (including the output-tensor and
<   /// synthetic-tensor).
<   constexpr unsigned getNumTensors() const { return numTensors; }
---
>   /// Bit translation (get tensor ID).
>   unsigned tensor(unsigned b) const { return b % numTensors; }
>   /// Bit translation (get loop index).
>   unsigned index(unsigned b) const { return b / numTensors; }
321,322c245,246
<   /// Get the total number of loops (native loops + filter loops).
<   constexpr unsigned getNumLoops() const { return numLoops; }
---
>   /// Get the number of total loops (native loops + filter loops).
>   unsigned getNumLoops() const { return numLoops; }
324c248
<   constexpr unsigned getNumNativeLoops() const { return numNativeLoops; }
---
>   unsigned getNumNativeLoops() const { return numNativeLoops; }
326,332c250,252
<   constexpr unsigned getNumFilterLoops() const {
<     return numLoops - numNativeLoops;
<   }
<   /// Get the identifier of the first filter-loop.
<   constexpr LoopId getStartingFilterLoopId() const {
<     return getNumNativeLoops();
<   }
---
>   unsigned getNumFilterLoops() const { return numLoops - numNativeLoops; }
>   /// Get the starting filter loop index.
>   unsigned getFilterLoopStartingIdx() const { return getNumNativeLoops(); }
334,336c254,256
<   /// Returns true if `b` is the `i`th loop of the output tensor.
<   constexpr bool isOutTensor(TensorLoopId b, LoopId i) const {
<     return b == makeTensorLoopId(outTensor, i);
---
>   /// Returns true if bit corresponds to index of output tensor.
>   bool isOutTensor(unsigned b, unsigned i) const {
>     return tensor(b) == outTensor && index(b) == i;
339,343c259,263
<   /// Get the output tensor's identifier.
<   constexpr TensorId getOutTensorID() const { return outTensor; }
<   /// Get the synthetic tensor's identifier (used for all invariant
<   /// tensor expressions).
<   constexpr TensorId getSynTensorID() const { return syntheticTensor; }
---
>   /// Gets tensor ID for the output tensor.
>   unsigned getOutTensorID() const { return outTensor; }
>   /// Gets tensor ID for the synthetic tensor (used for all invariant tensor
>   /// expressions).
>   unsigned getSynTensorID() const { return syntheticTensor; }
345,347c265,267
<   constexpr bool isFilterLoop(LoopId i) const {
<     assert(isValidLoopId(i));
<     return i >= numNativeLoops;
---
>   bool isFilterLoop(unsigned ldx) const {
>     assert(ldx < numLoops);
>     return ldx >= numNativeLoops;
350,357c270,271
<   /// Returns true if the expression is `(kTensor t)`.
<   bool expIsTensor(ExprId e, TensorId t) const {
<     const auto &expr = exp(e);
<     return expr.kind == TensorExp::Kind::kTensor && expr.tensor == t;
<   }
< 
<   /// Returns true if the expression contains the tensor as an operand.
<   bool expContainsTensor(ExprId e, TensorId t) const;
---
>   /// Returns true if the expression contains the `t` as an operand.
>   bool expContainsTensor(unsigned e, unsigned t) const;
363c277
<   bool hasNegateOnOut(ExprId e) const;
---
>   bool hasNegateOnOut(unsigned e) const;
369c283
<   bool isSingleCondition(TensorId t, ExprId e) const;
---
>   bool isSingleCondition(unsigned t, unsigned e) const;
371,372c285
<   /// Returns true if any `TensorLoopId` in the bitvector corresponds
<   /// to sparse level-type.
---
>   /// Returns true if any set bit corresponds to sparse dimension level type.
375,447c288,291
<   /// Returns true if bits contains a dependent index reduction condition on
<   /// sparse levels.
<   bool hasSparseIdxReduction(const BitVector &bits) const;
< 
<   /// Gets the level-type of the `t`th tensor on `i`th loop.
<   DimLevelType getDimLevelType(TensorId t, LoopId i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return lvlTypes[t][i];
<   }
< 
<   /// Gets the level-type of the TensorLoopId.
<   DimLevelType getDimLevelType(TensorLoopId b) const {
<     return getDimLevelType(tensor(b), loop(b));
<   }
< 
<   /// Gets the loop identifier for the `lvl`th level of the `t`th tensor.
<   std::optional<LoopId> getLoopId(TensorId t, Level lvl) const {
<     assert(isValidLevel(t, lvl));
<     return lvlToLoop[t][lvl];
<   }
< 
<   /// Gets the level number of the the `t`th tensor on `i`th loop.
<   std::optional<Level> getLvl(TensorId t, LoopId i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToLvl[t][i];
<   }
<   std::optional<Level> getLvl(TensorLoopId b) const {
<     return getLvl(tensor(b), loop(b));
<   }
< 
<   /// Sets the level number and level-type of the `t`th tensor on
<   /// `i`th loop.
<   void setLevelAndType(TensorId t, LoopId i, Level lvl, DimLevelType dlt) {
<     assert(isValidLevel(t, lvl) && isValidLoopId(i) && isValidDLT(dlt));
<     lvlTypes[t][i] = dlt;
<     loopToLvl[t][i] = lvl;
<     lvlToLoop[t][lvl] = i;
<     // TODO: Maybe we should favor a constant loop bound when there are multiple
<     // choices.
<     loopBounds[i] = std::make_pair(t, lvl);
<   }
< 
<   using ForeachTensorLoopIdCallback = function_ref<void(
<       TensorLoopId, TensorId, std::optional<Level>, DimLevelType, bool)>;
< 
<   /// Iterates over a set of `TensorLoopId`s, invoking the callback
<   /// for each `TensorLoopId` and passing it the corresponding tensor
<   /// identifier, level, and level-type, following with a boolean value
<   /// indicating whether it is a dependent index reduction loop condition.
<   void foreachTensorLoopId(LatPointId p,
<                            ForeachTensorLoopIdCallback callback) const {
<     // TODO: the default ought to be simple=true; but we'll need to make
<     // sure to update all the tests to make sure they do the right thing.
<     foreachTensorLoopId(p, /*simple=*/false, callback);
<   }
<   void foreachTensorLoopId(LatPointId p, bool simple,
<                            ForeachTensorLoopIdCallback callback) const {
<     const auto &point = lat(p);
<     const auto &bits = simple ? point.simple : point.bits;
<     for (const TensorLoopId b : bits.set_bits()) {
<       const TensorId t = tensor(b);
<       const auto optLvl = getLvl(b);
<       const auto lvlTp = getDimLevelType(b);
<       if (isLvlWithNonTrivialIdxExp(b)) {
<         // This must be an undefined level.
<         assert(!optLvl.has_value());
<         // Slice the tid along the dependent level to iterate current loop.
<         callback(b, t, getLoopDependentLevel(b), lvlTp,
<                  /*isIdxReduc=*/true);
<       } else {
<         callback(b, t, optLvl, lvlTp, /*isIdxReduc=*/false);
<       }
<     }
---
>   /// Gets the dimension level type of the `t`th tensor on `i`th loop.
>   DimLevelType getDimLevelType(unsigned t, unsigned i) const {
>     assert(t < numTensors && i < numLoops);
>     return dimTypes[t][i];
450,465c294,296
<   /// Sets whether the output tensor is sparse or not.
<   void setHasSparseOut(bool s) { hasSparseOut = s; }
< 
<   /// Establishes the two-way map that i <-> <t, lvl, dlt>.
<   void setLoopDependentTensorLevel(LoopId i, TensorId t, Level lvl,
<                                    DimLevelType dlt) {
<     assert(isValidLoopId(i) && isValidLevel(t, lvl));
<     assert(!loopToDependencies[i][t].has_value()); // must be the first def
<     loopToDependencies[i][t] = std::make_pair(lvl, dlt);
<     levelToDependentLoop[t][lvl].push_back(i);
<   }
< 
<   /// Whether the loop has dependent slice.
<   bool hasDependentLvl(LoopId i, TensorId t) {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToDependencies[i][t].has_value();
---
>   /// Gets the dimension level type of `b`.
>   DimLevelType getDimLevelType(unsigned b) const {
>     return getDimLevelType(tensor(b), index(b));
468,472c299,301
<   /// Returns the list of loop indices which appear in the non-trivial index
<   /// expression on t_l, e.g., A[i+j] => {i, j}
<   std::vector<LoopId> &getDependentLoops(TensorId t, Level lvl) {
<     assert(isValidLevel(t, lvl));
<     return levelToDependentLoop[t][lvl];
---
>   std::optional<unsigned> getLoopIdx(unsigned t, unsigned dim) const {
>     assert(t < numTensors && dim < numLoops);
>     return dimToLoopIdx[t][dim];
475,478c304,307
<   /// Returns the defining [tid, lvl] for the loop.
<   std::pair<TensorId, Level> getLoopDefiningLvl(LoopId i) const {
<     assert(isValidLoopId(i));
<     return loopBounds[i];
---
>   /// Gets the dimension number of the the `t`th tensor on `i`th loop.
>   std::optional<unsigned> getDimNum(unsigned t, unsigned i) const {
>     assert(t < numTensors && i < numLoops);
>     return loopIdxToDim[t][i];
481,487c310,312
<   /// Checks whether the TensorLoopId represents a tensor level contains
<   /// non-trivial index expression.
<   bool isLvlWithNonTrivialIdxExp(TensorLoopId b) const {
<     const TensorId t = tensor(b);
<     const LoopId i = loop(b);
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToDependencies[i][t].has_value();
---
>   /// Gets the dimension number of `b`.
>   std::optional<unsigned> getDimNum(unsigned b) const {
>     return getDimNum(tensor(b), index(b));
490,497c315,323
<   /// Checks whether the TensorLoopId represents a sparse tensor level contains
<   /// non-trivial index expression.
<   bool isSparseLvlWithNonTrivialIdxExp(TensorLoopId b) const {
<     if (isLvlWithNonTrivialIdxExp(b)) {
<       auto dlt = getLoopDependentLevelType(b);
<       return isCompressedDLT(dlt) || isSingletonDLT(dlt);
<     }
<     return false;
---
>   /// Sets the dimension and dimension level type of the `t`th tensor on `i`th
>   /// loop.
>   void setDimAndDimLevelType(unsigned t, unsigned i, unsigned dim,
>                              DimLevelType dlt) {
>     assert(isValidDLT(dlt));
>     dimTypes[t][i] = dlt;
>     loopIdxToDim[t][i] = dim;
>     assert(dim < numLoops);
>     dimToLoopIdx[t][dim] = i;
500,502c326,334
<   Level getLoopDependentLevel(TensorLoopId b) const {
<     assert(isLvlWithNonTrivialIdxExp(b));
<     return loopToDependencies[loop(b)][tensor(b)]->first;
---
>   // Iterates the bits of a lattice, for each set bit, converts it into the
>   // corresponding tensor dimension and invokes the callback.
>   void foreachTidDimPairInBits(
>       const BitVector &bits,
>       function_ref<void(unsigned b, unsigned tid, std::optional<unsigned> dim,
>                         DimLevelType dlt)>
>           cb) {
>     for (unsigned b : bits.set_bits())
>       cb(b, tensor(b), getDimNum(b), getDimLevelType(b));
505,508c337,338
<   DimLevelType getLoopDependentLevelType(TensorLoopId b) const {
<     assert(isLvlWithNonTrivialIdxExp(b));
<     return loopToDependencies[loop(b)][tensor(b)]->second;
<   }
---
>   // Has sparse output tensor setter.
>   void setHasSparseOut(bool s) { hasSparseOut = s; }
511,576c341,346
<   /// These methods return `const&` because the underlying objects must
<   /// not be mutated by client code.  The only exception is for mutating
<   /// the value associated with an expression, for which there are
<   /// dedicated methods below.
<   ///
<   /// NOTE: It is inadvisable to keep the reference alive for a long
<   /// time (e.g., as in `TensorExpr &te = merger.exp(e)`), since insertions
<   /// into the merger can cause data movement which will invalidate the
<   /// underlying memory address.  This isn't just a problem with the `&`
<   /// references, but also applies to the `ArrayRef`.  In particular,
<   /// using `for (LatPointId p : merger.set(s))` will run into the same
<   /// dangling-reference problems if the loop body inserts new sets.
<   const TensorExp &exp(ExprId e) const {
<     assert(isValidExprId(e));
<     return tensorExps[e];
<   }
<   const LatPoint &lat(LatPointId p) const {
<     assert(isValidLatPointId(p));
<     return latPoints[p];
<   }
<   ArrayRef<LatPointId> set(LatSetId s) const {
<     assert(isValidLatSetId(s));
<     return latSets[s];
<   }
< 
<   /// Checks whether the given expression has an associated value.
<   bool hasExprValue(ExprId e) const { return static_cast<bool>(exp(e).val); }
< 
<   /// Sets the expression to have the associated value.  Asserts that
<   /// the new value is defined, and that the expression does not already
<   /// have a value.  If you want to overwrite a previous associated value,
<   /// use `updateExprValue` instead.
<   void setExprValue(ExprId e, Value v) {
<     assert(isValidExprId(e));
<     assert(v && "Got an undefined value");
<     auto &val = tensorExps[e].val;
<     assert(!val && "Expression already has an associated value");
<     val = v;
<   }
< 
<   /// Clears the value associated with the expression.  Asserts that the
<   /// expression does indeed have an associated value before clearing it.
<   /// If you don't want to check for a previous associated value first,
<   /// then use `updateExprValue` instead.
<   void clearExprValue(ExprId e) {
<     assert(isValidExprId(e));
<     auto &val = tensorExps[e].val;
<     assert(val && "Expression does not have an associated value to clear");
<     val = Value();
<   }
< 
<   /// Unilaterally updates the expression to have the associated value.
<   /// That is, unlike `setExprValue` and `clearExprValue`, this method
<   /// does not perform any checks on whether the expression had a
<   /// previously associated value nor whether the new value is defined.
<   //
<   // TODO: The unilateral update semantics are required by the
<   // current implementation of `CodegenEnv::genLoopBoundary`; however,
<   // that implementation seems a bit dubious.  We would much rather have
<   // the semantics `{ clearExprValue(e); setExprValue(e, v); }` or
<   // `{ clearExprValue(e); if (v) setExprValue(e, v); }` since those
<   // provide better invariants.
<   void updateExprValue(ExprId e, Value v) {
<     assert(isValidExprId(e));
<     tensorExps[e].val = v;
<   }
---
>   /// Typically it is inadvisible to keep the reference around, as in
>   /// "TensorExpr &te = merger.exp(e))", since insertions into the merger
>   /// may cause data movement and invalidate the underlying memory address.
>   TensorExp &exp(unsigned e) { return tensorExps[e]; }
>   LatPoint &lat(unsigned l) { return latPoints[l]; }
>   SmallVector<unsigned> &set(unsigned s) { return latSets[s]; }
580,582c350,352
<   void dumpExp(ExprId e) const;
<   void dumpLat(LatPointId p) const;
<   void dumpSet(LatSetId s) const;
---
>   void dumpExp(unsigned e) const;
>   void dumpLat(unsigned p) const;
>   void dumpSet(unsigned s) const;
587,589c357,359
<   /// remaining tensor (sub)expression and the next loop in the iteration
<   /// graph.  Returns the identifier of the root set.
<   LatSetId buildLattices(ExprId e, LoopId i);
---
>   /// remaining tensor (sub)expression and the next loop index in the
>   /// iteration graph. Returns index of the root expression.
>   unsigned buildLattices(unsigned e, unsigned i);
592,593c362,363
<   /// On success, returns the identifier of the root expression.
<   std::optional<ExprId> buildTensorExpFromLinalg(linalg::GenericOp op);
---
>   /// Returns index of the root expression on success.
>   std::optional<unsigned> buildTensorExpFromLinalg(linalg::GenericOp op);
596,597c366,367
<   Value buildExp(RewriterBase &rewriter, Location loc, ExprId e, Value v0,
<                  Value v1) const;
---
>   Value buildExp(RewriterBase &rewriter, Location loc, unsigned e, Value v0,
>                  Value v1);
601,622c371,373
<   constexpr bool isValidTensorId(TensorId t) const { return t < numTensors; }
<   constexpr bool isValidLoopId(LoopId i) const {
<     return i != detail::kInvalidId && i < numLoops;
<   }
<   bool isValidLevel(TensorId t, Level lvl) const {
<     assert(levelToDependentLoop[t].size() == lvlToLoop[t].size());
<     return isValidTensorId(t) && lvl < lvlToLoop[t].size();
<   }
<   bool isValidExprId(ExprId e) const {
<     return e != detail::kInvalidId && e < tensorExps.size();
<   }
<   bool isValidLatPointId(LatPointId p) const {
<     return p != detail::kInvalidId && p < latPoints.size();
<   }
<   bool isValidLatSetId(LatSetId s) const {
<     return s != detail::kInvalidId && s < latSets.size();
<   }
<   bool maybeZero(ExprId e) const;
<   bool isInvariant(ExprId e) const {
<     return exp(e).kind == TensorExp::Kind::kInvariant;
<   }
<   Type inferType(ExprId e, Value src) const;
---
>   bool maybeZero(unsigned e) const;
>   bool isInvariant(unsigned e) const;
>   Type inferType(unsigned e, Value src);
625c376
<   std::optional<ExprId> buildTensorExp(linalg::GenericOp op, Value v);
---
>   std::optional<unsigned> buildTensorExp(linalg::GenericOp op, Value v);
628,629c379,380
<   const TensorId outTensor;
<   const TensorId syntheticTensor;
---
>   const unsigned outTensor;
>   const unsigned syntheticTensor;
635,663c386,392
<   // Below we use `std::vector` for things which have a priori fixed
<   // sizes, whereas we use `llvm::SmallVector` for things with variable
<   // size.  Do beware that these two classes differ in the semantics of
<   // `operator[]`: `SmallVector` performs OOB checks, whereas `std::vector`
<   // does not.
< 
<   // Map that converts pair<TensorId, LoopId> to the corresponding
<   // level-type.
<   std::vector<std::vector<DimLevelType>> lvlTypes;
< 
<   // Map that converts pair<TensorId, LoopId> to the corresponding
<   // level.
<   std::vector<std::vector<std::optional<Level>>> loopToLvl;
< 
<   // Map that converts pair<TensorId, Level> to the corresponding LoopId.
<   std::vector<std::vector<std::optional<LoopId>>> lvlToLoop;
< 
<   // Map from a loop to its dependencies if any.
<   // The dependencies of a loop is a set of (tensor, level) pairs.
<   // It is currently only set for non-trivial index expressions.
<   // E.g., A[i+j] => i and j will have dependencies {A0, dlt(A0)} to indicate
<   // that i and j are used in the non-trivial index expression on A0.
<   std::vector<std::vector<std::optional<std::pair<Level, DimLevelType>>>>
<       loopToDependencies;
< 
<   // The inverse map of ldxToDependencies from tensor level -> dependent loop
<   // E.g., A[i+j], we have A0 => {i, j}, to indicate that A0 uses both {i, j}
<   // to compute its indices.
<   std::vector<std::vector<std::vector<LoopId>>> levelToDependentLoop;
---
>   // Map that converts pair<tensor id, loop id> to the corresponding dimension
>   // level type.
>   std::vector<std::vector<DimLevelType>> dimTypes;
> 
>   // Map that converts pair<tensor id, loop id> to the corresponding
>   // dimension.
>   std::vector<std::vector<std::optional<unsigned>>> loopIdxToDim;
665,666c394,395
<   // Map from a loop to the [tid, lvl] pair that defines the loop boundary.
<   std::vector<std::pair<TensorId, Level>> loopBounds;
---
>   // Map that converts pair<tensor id, dim> to the corresponding loop id.
>   std::vector<std::vector<std::optional<unsigned>>> dimToLoopIdx;
670c399
<   llvm::SmallVector<SmallVector<LatPointId>> latSets;
---
>   llvm::SmallVector<SmallVector<unsigned>> latSets;
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Utils: MergerNewtypes.h
--- include/mlir/Dialect/SparseTensor/Utils/Merger.h
16,17d15
< #include "mlir/Dialect/SparseTensor/Utils/MergerNewtypes.h"
< 
20d17
< #include "mlir/Dialect/SparseTensor/IR/SparseTensor.h"
28,75d24
< /// Tensor expression. Represents an MLIR expression in tensor index notation.
< struct TensorExp final {
<   enum class Kind;
< 
<   /// Child subexpressions for non-leaf expressions.
<   struct Children final {
<     ExprId e0;
<     ExprId e1;
<   };
< 
<   // The `x` parameter has different types depending on the value of the
<   // `k` parameter.  The correspondences are:
<   // * `kTensor`    -> `TensorId`
<   // * `kInvariant` -> `kInvalidId`
<   // * `kLoopVar`   -> `LoopId`
<   // * else         -> `ExprId`
<   //
<   // The `y`, `v`, and `op` parameters either must or must not be
<   // `kInvalidId`/`nullptr`, depending on the value of the `k` parameter;
<   // however, they have uniform C++ types regardless of the value of `k`.
<   TensorExp(Kind k, unsigned x, ExprId y, Value v, Operation *op);
< 
<   /// Tensor expression kind.
<   Kind kind;
< 
<   union {
<     /// `kTensor` expressions simply have a tensor identifier.
<     TensorId tensor;
< 
<     /// `kLoopVar` expressions simply have a loop identifier.
<     LoopId loop;
< 
<     /// All other expressions hold the `ExprId`s of their children.
<     Children children;
<   };
< 
<   /// Direct link to IR for an invariant or the destination value (to
<   /// infer destination type) of a cast operation During code generation,
<   /// this field may be used to cache "hoisted" loop invariant tensor loads.
<   Value val;
< 
<   /// Code blocks used by semirings. For the case of kUnary, kBinary, kReduce,
<   /// and kSelect, this holds the original operation with all regions. For
<   /// kBinaryBranch, this holds the YieldOp for the left or right half
<   /// to be merged into a nested scf loop.
<   Operation *op;
< };
< 
77,89c26
< ///
< /// The `kLoopVar` leaf kind is for representing `linalg::IndexOp`.
< /// That is, its argument is a `LoopId` identifying the loop-variable
< /// in question, and its value will be the current iteration's value
< /// of that loop-variable.  See the `LoopId` documentation for more details.
< //
< // TODO: Modify this definition so that the numeric values already encode
< // the `ExpArity` (while extending the notion of "arity" to include not
< // just the number of `ExprId` children the node has, but also whether the
< // node has a `Value` and/or `Operation*`).  Doing this will avoid needing
< // to enumerate all the kinds in `getExpArity` and in the `TensorExp` ctor,
< // and should help clean up a few other places as well.
< enum class TensorExp::Kind {
---
> enum Kind {
93c30
<   kLoopVar,
---
>   kIndex,
153,160c90,101
< //===----------------------------------------------------------------------===//
< /// Lattice point.  Each lattice point consists of a formal conjunction
< /// of `TensorLoopId`s, together with the identifier of the corresponding
< /// tensor expression.  The formal conjunction is represented as a set of
< /// `TensorLoopId`, where that set is implemented as a `BitVector`.
< struct LatPoint final {
<   /// Construct a lattice point with the empty set of `TensorLoopId`s.
<   LatPoint(unsigned size, ExprId e) : bits(size, false), exp(e) {}
---
> /// Children subexpressions of tensor operations.
> struct Children {
>   unsigned e0;
>   unsigned e1;
> };
> 
> /// Tensor expression. Represents a MLIR expression in tensor index notation.
> struct TensorExp {
>   TensorExp(Kind k, unsigned x, unsigned y, Value v, Operation *operation);
> 
>   /// Tensor expression kind.
>   Kind kind;
162,163c103,105
<   /// Construct a lattice point from the given set of `TensorLoopId`s.
<   LatPoint(const BitVector &bits, ExprId e) : bits(bits), exp(e) {}
---
>   union {
>     /// Expressions representing tensors simply have a tensor number.
>     unsigned tensor;
165c107,134
<   /// Conjunction of all `TensorLoopId`s involved in the tensor expression.
---
>     /// Indices hold the index number.
>     unsigned index;
> 
>     /// Tensor operations hold the indices of their children.
>     Children children;
>   };
> 
>   /// Direct link to IR for an invariant or the destination value (to
>   /// infer destination type) of a cast operation During code generation,
>   /// this field may be used to cache "hoisted" loop invariant tensor loads.
>   Value val;
> 
>   /// Code blocks used by semirings. For the case of kUnary, kBinary, kReduce,
>   /// and kSelect, this holds the original operation with all regions. For
>   /// kBinaryBranch, this holds the YieldOp for the left or right half
>   /// to be merged into a nested scf loop.
>   Operation *op;
> };
> 
> /// Lattice point. Each lattice point consists of a conjunction of tensor
> /// loop indices (encoded in a bitvector) and the index of the corresponding
> /// tensor expression.
> struct LatPoint {
>   LatPoint(unsigned n, unsigned e, unsigned b);
>   LatPoint(const BitVector &b, unsigned e);
> 
>   /// Conjunction of tensor loop indices as bitvector. This represents
>   /// all indices involved in the tensor expression
168c137
<   /// Simplified conjunction of `TensorLoopId` as bitvector.  This
---
>   /// Simplified conjunction of tensor loop indices as bitvector. This
173,174c142,143
<   /// Identifier of the tensor expression.
<   ExprId exp;
---
>   /// Index of the tensor expression.
>   unsigned exp;
177d145
< //===----------------------------------------------------------------------===//
192c160
<   /// sparse levels.  E.g., (d0, d1, d2) => (d0 + d1, d2), a naive
---
>   /// sparse dimensions. E.g., (d0, d1, d2) => (d0 + d1, d2), a naive
195,196c163,164
<   /// for (const auto c0 : coordinates[0]) {
<   ///   if (c0 == d0 + d1) {
---
>   /// for (coord : sparse_dim[0])
>   ///   if (coord == d0 + d1) {
203,259c171,175
<   /// The maxLvlRank specifies the max level rank of all inputs/output tensors.
<   /// It is used to pre-allocate sufficient memory for internal storage.
<   //
<   // TODO: we want to make the filter loop more efficient in the future,
<   // e.g., by avoiding scanning the full list of stored coordinates (keeping
<   // the last position in ordered list) or even apply binary search to find
<   // the coordinate.
<   //
<   // TODO: would be cleaner to understand/document if the first argument
<   // gave the number of input tensors, instead of the current number of
<   // input+output tensors.
<   Merger(unsigned numInputOutputTensors, unsigned numNativeLoops,
<          unsigned numFilterLoops, unsigned maxLvlRank);
< 
<   //
<   // Constructing valid tensor and loop identifiers.
<   //
< 
<   /// Safely converts the argument to a tensor identifier.
<   constexpr TensorId makeTensorId(unsigned t) const {
<     assert(isValidTensorId(t));
<     return t;
<   }
< 
<   /// Safely converts the argument to a loop identifier.
<   constexpr LoopId makeLoopId(unsigned i) const {
<     assert(isValidLoopId(i));
<     return i;
<   }
< 
<   /// Safely converts the arguments to a pair of (tensor,loop) identifiers.
<   constexpr TensorLoopId makeTensorLoopId(unsigned t, unsigned i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return numTensors * i + t;
<   }
< 
<   //
<   // Allocating new expressions, points, and sets.
<   //
< 
<   /// Constructs a new tensor expression, and returns its identifier.
<   ExprId addTensorExp(TensorId t);
<   /// Constructs a new loop-variable expression, and returns its identifier.
<   ExprId addLoopVarExp(LoopId i);
<   /// Constructs a new invariant expression, and returns its identifier.
<   ExprId addInvariantExp(Value v);
<   /// Constructs a new unary or binary expression, and returns its identifier.
<   ExprId addExp(TensorExp::Kind k, ExprId e0, ExprId e1 = detail::kInvalidId,
<                 Operation *op = nullptr);
<   /// Constructs a new sesquinary expression, and returns its identifier.
<   /// Currently no sesquinary `Kind` allows specifying the `op`, but we
<   /// allow it anyways because `mapSet` is designed to allow it.
<   ExprId addExp(TensorExp::Kind k, ExprId e, Value v, Operation *op = nullptr);
< 
<   /// Constructs a new iteration lattice point, and returns its identifier.
<   LatPointId addLat(TensorId t, LoopId i, ExprId e);
<   LatPointId addLat(const BitVector &bits, ExprId e);
---
>   /// TODO: we want to make the filter loop more efficient in the future, e.g.,
>   /// by avoiding scanning the full stored index sparse (keeping the last
>   /// position in ordered list) or even apply binary search to find the index.
>   ///
>   Merger(unsigned t, unsigned l, unsigned fl);
261,262c177,191
<   /// Constructs a new (initially empty) set, and returns its identifier.
<   LatSetId addSet();
---
>   /// Adds a tensor expression. Returns its index.
>   unsigned addExp(Kind k, unsigned e0, unsigned e1 = -1u, Value v = Value(),
>                   Operation *op = nullptr);
>   unsigned addExp(Kind k, unsigned e, Value v, Operation *op = nullptr) {
>     return addExp(k, e, -1u, v, op);
>   }
>   unsigned addExp(Kind k, Value v, Operation *op = nullptr) {
>     return addExp(k, -1u, -1u, v, op);
>   }
> 
>   /// Adds an iteration lattice point. Returns its index.
>   unsigned addLat(unsigned t, unsigned i, unsigned e);
> 
>   /// Adds a new, initially empty, set. Returns its index.
>   unsigned addSet();
265,287c194,215
<   /// of `LoopId` (effectively constructing a larger "intersection" of those
<   /// loops) with a newly constructed tensor (sub)expression of given kind.
<   /// Returns the identifier of the new lattice point.
<   LatPointId conjLat(TensorExp::Kind kind, LatPointId p0, LatPointId p1,
<                      Operation *op = nullptr);
< 
<   /// Conjunctive merge of two lattice sets: `(s0 /\_op s1)`.
<   /// Returns the identifier of the new set.
<   LatSetId conjSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                    Operation *op = nullptr);
< 
<   /// Disjunctive merge of two lattice sets: `(s0 /\_op s1, s0, s1)`.
<   /// Returns the identifier of the new set.
<   LatSetId disjSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                    Operation *op = nullptr);
< 
<   /// Disjunctive merge of two lattice sets with custom handling of the
<   /// overlap, left, and right regions.  Any region may be left missing
<   /// in the output.  Returns the identifier of the new set.
<   LatSetId combiSet(TensorExp::Kind kind, LatSetId s0, LatSetId s1,
<                     Operation *orig, bool includeLeft, TensorExp::Kind ltrans,
<                     Operation *opleft, bool includeRight,
<                     TensorExp::Kind rtrans, Operation *opright);
---
>   /// of loop indices (effectively constructing a larger "intersection" of those
>   /// indices) with a newly constructed tensor (sub)expression of given kind.
>   /// Returns the index of the new lattice point.
>   unsigned conjLatPoint(Kind kind, unsigned p0, unsigned p1,
>                         Operation *op = nullptr);
> 
>   /// Conjunctive merge of two lattice sets L0 and L1 is conjunction of
>   /// cartesian product. Returns the index of the new set.
>   unsigned takeConj(Kind kind, unsigned s0, unsigned s1,
>                     Operation *op = nullptr);
> 
>   /// Disjunctive merge of two lattice sets L0 and L1 is (L0 /\_op L1, L0, L1).
>   /// Returns the index of the new set.
>   unsigned takeDisj(Kind kind, unsigned s0, unsigned s1,
>                     Operation *op = nullptr);
> 
>   /// Disjunctive merge of two lattice sets L0 and L1 with custom handling of
>   /// the overlap, left, and right regions. Any region may be left missing in
>   /// the output. Returns the index of the new set.
>   unsigned takeCombi(Kind kind, unsigned s0, unsigned s1, Operation *orig,
>                      bool includeLeft, Kind ltrans, Operation *opleft,
>                      bool includeRight, Kind rtrans, Operation *opright);
291,292c219,220
<   /// as new expression. Returns the identifier of the new set.
<   LatSetId mapSet(TensorExp::Kind kind, LatSetId s, Value v = Value(),
---
>   /// as new expression. Returns the index of the new set.
>   unsigned mapSet(Kind kind, unsigned s0, Value v = Value(),
298c226
<   LatSetId optimizeSet(LatSetId s);
---
>   unsigned optimizeSet(unsigned s0);
304,307c232
<   BitVector simplifyCond(LatSetId s, LatPointId p);
< 
<   /// Returns true if p0 > p1.
<   bool latGT(LatPointId p0, LatPointId p1) const;
---
>   BitVector simplifyCond(unsigned s0, unsigned p0);
309,310c234,235
<   /// Returns true if p0 and p1 only differ in dense.
<   bool onlyDenseDiff(LatPointId p0, LatPointId p1) const;
---
>   /// Returns true if Li > Lj.
>   bool latGT(unsigned i, unsigned j) const;
312,315c237,238
<   /// Gets the tensor-identifier of the `TensorLoopId`.
<   constexpr TensorId tensor(TensorLoopId b) const { return b % numTensors; }
<   /// Gets the loop-identifier of the `TensorLoopId`.
<   constexpr LoopId loop(TensorLoopId b) const { return b / numTensors; }
---
>   /// Returns true if Li and Lj only differ in dense.
>   bool onlyDenseDiff(unsigned i, unsigned j);
317,319c240,243
<   /// Get the total number of tensors (including the output-tensor and
<   /// synthetic-tensor).
<   constexpr unsigned getNumTensors() const { return numTensors; }
---
>   /// Bit translation (get tensor ID).
>   unsigned tensor(unsigned b) const { return b % numTensors; }
>   /// Bit translation (get loop index).
>   unsigned index(unsigned b) const { return b / numTensors; }
321,322c245,246
<   /// Get the total number of loops (native loops + filter loops).
<   constexpr unsigned getNumLoops() const { return numLoops; }
---
>   /// Get the number of total loops (native loops + filter loops).
>   unsigned getNumLoops() const { return numLoops; }
324c248
<   constexpr unsigned getNumNativeLoops() const { return numNativeLoops; }
---
>   unsigned getNumNativeLoops() const { return numNativeLoops; }
326,332c250,252
<   constexpr unsigned getNumFilterLoops() const {
<     return numLoops - numNativeLoops;
<   }
<   /// Get the identifier of the first filter-loop.
<   constexpr LoopId getStartingFilterLoopId() const {
<     return getNumNativeLoops();
<   }
---
>   unsigned getNumFilterLoops() const { return numLoops - numNativeLoops; }
>   /// Get the starting filter loop index.
>   unsigned getFilterLoopStartingIdx() const { return getNumNativeLoops(); }
334,336c254,256
<   /// Returns true if `b` is the `i`th loop of the output tensor.
<   constexpr bool isOutTensor(TensorLoopId b, LoopId i) const {
<     return b == makeTensorLoopId(outTensor, i);
---
>   /// Returns true if bit corresponds to index of output tensor.
>   bool isOutTensor(unsigned b, unsigned i) const {
>     return tensor(b) == outTensor && index(b) == i;
339,343c259,263
<   /// Get the output tensor's identifier.
<   constexpr TensorId getOutTensorID() const { return outTensor; }
<   /// Get the synthetic tensor's identifier (used for all invariant
<   /// tensor expressions).
<   constexpr TensorId getSynTensorID() const { return syntheticTensor; }
---
>   /// Gets tensor ID for the output tensor.
>   unsigned getOutTensorID() const { return outTensor; }
>   /// Gets tensor ID for the synthetic tensor (used for all invariant tensor
>   /// expressions).
>   unsigned getSynTensorID() const { return syntheticTensor; }
345,347c265,267
<   constexpr bool isFilterLoop(LoopId i) const {
<     assert(isValidLoopId(i));
<     return i >= numNativeLoops;
---
>   bool isFilterLoop(unsigned ldx) const {
>     assert(ldx < numLoops);
>     return ldx >= numNativeLoops;
350,357c270,271
<   /// Returns true if the expression is `(kTensor t)`.
<   bool expIsTensor(ExprId e, TensorId t) const {
<     const auto &expr = exp(e);
<     return expr.kind == TensorExp::Kind::kTensor && expr.tensor == t;
<   }
< 
<   /// Returns true if the expression contains the tensor as an operand.
<   bool expContainsTensor(ExprId e, TensorId t) const;
---
>   /// Returns true if the expression contains the `t` as an operand.
>   bool expContainsTensor(unsigned e, unsigned t) const;
363c277
<   bool hasNegateOnOut(ExprId e) const;
---
>   bool hasNegateOnOut(unsigned e) const;
369c283
<   bool isSingleCondition(TensorId t, ExprId e) const;
---
>   bool isSingleCondition(unsigned t, unsigned e) const;
371,372c285
<   /// Returns true if any `TensorLoopId` in the bitvector corresponds
<   /// to sparse level-type.
---
>   /// Returns true if any set bit corresponds to sparse dimension level type.
375,447c288,291
<   /// Returns true if bits contains a dependent index reduction condition on
<   /// sparse levels.
<   bool hasSparseIdxReduction(const BitVector &bits) const;
< 
<   /// Gets the level-type of the `t`th tensor on `i`th loop.
<   DimLevelType getDimLevelType(TensorId t, LoopId i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return lvlTypes[t][i];
<   }
< 
<   /// Gets the level-type of the TensorLoopId.
<   DimLevelType getDimLevelType(TensorLoopId b) const {
<     return getDimLevelType(tensor(b), loop(b));
<   }
< 
<   /// Gets the loop identifier for the `lvl`th level of the `t`th tensor.
<   std::optional<LoopId> getLoopId(TensorId t, Level lvl) const {
<     assert(isValidLevel(t, lvl));
<     return lvlToLoop[t][lvl];
<   }
< 
<   /// Gets the level number of the the `t`th tensor on `i`th loop.
<   std::optional<Level> getLvl(TensorId t, LoopId i) const {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToLvl[t][i];
<   }
<   std::optional<Level> getLvl(TensorLoopId b) const {
<     return getLvl(tensor(b), loop(b));
<   }
< 
<   /// Sets the level number and level-type of the `t`th tensor on
<   /// `i`th loop.
<   void setLevelAndType(TensorId t, LoopId i, Level lvl, DimLevelType dlt) {
<     assert(isValidLevel(t, lvl) && isValidLoopId(i) && isValidDLT(dlt));
<     lvlTypes[t][i] = dlt;
<     loopToLvl[t][i] = lvl;
<     lvlToLoop[t][lvl] = i;
<     // TODO: Maybe we should favor a constant loop bound when there are multiple
<     // choices.
<     loopBounds[i] = std::make_pair(t, lvl);
<   }
< 
<   using ForeachTensorLoopIdCallback = function_ref<void(
<       TensorLoopId, TensorId, std::optional<Level>, DimLevelType, bool)>;
< 
<   /// Iterates over a set of `TensorLoopId`s, invoking the callback
<   /// for each `TensorLoopId` and passing it the corresponding tensor
<   /// identifier, level, and level-type, following with a boolean value
<   /// indicating whether it is a dependent index reduction loop condition.
<   void foreachTensorLoopId(LatPointId p,
<                            ForeachTensorLoopIdCallback callback) const {
<     // TODO: the default ought to be simple=true; but we'll need to make
<     // sure to update all the tests to make sure they do the right thing.
<     foreachTensorLoopId(p, /*simple=*/false, callback);
<   }
<   void foreachTensorLoopId(LatPointId p, bool simple,
<                            ForeachTensorLoopIdCallback callback) const {
<     const auto &point = lat(p);
<     const auto &bits = simple ? point.simple : point.bits;
<     for (const TensorLoopId b : bits.set_bits()) {
<       const TensorId t = tensor(b);
<       const auto optLvl = getLvl(b);
<       const auto lvlTp = getDimLevelType(b);
<       if (isLvlWithNonTrivialIdxExp(b)) {
<         // This must be an undefined level.
<         assert(!optLvl.has_value());
<         // Slice the tid along the dependent level to iterate current loop.
<         callback(b, t, getLoopDependentLevel(b), lvlTp,
<                  /*isIdxReduc=*/true);
<       } else {
<         callback(b, t, optLvl, lvlTp, /*isIdxReduc=*/false);
<       }
<     }
---
>   /// Gets the dimension level type of the `t`th tensor on `i`th loop.
>   DimLevelType getDimLevelType(unsigned t, unsigned i) const {
>     assert(t < numTensors && i < numLoops);
>     return dimTypes[t][i];
450,465c294,296
<   /// Sets whether the output tensor is sparse or not.
<   void setHasSparseOut(bool s) { hasSparseOut = s; }
< 
<   /// Establishes the two-way map that i <-> <t, lvl, dlt>.
<   void setLoopDependentTensorLevel(LoopId i, TensorId t, Level lvl,
<                                    DimLevelType dlt) {
<     assert(isValidLoopId(i) && isValidLevel(t, lvl));
<     assert(!loopToDependencies[i][t].has_value()); // must be the first def
<     loopToDependencies[i][t] = std::make_pair(lvl, dlt);
<     levelToDependentLoop[t][lvl].push_back(i);
<   }
< 
<   /// Whether the loop has dependent slice.
<   bool hasDependentLvl(LoopId i, TensorId t) {
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToDependencies[i][t].has_value();
---
>   /// Gets the dimension level type of `b`.
>   DimLevelType getDimLevelType(unsigned b) const {
>     return getDimLevelType(tensor(b), index(b));
468,472c299,301
<   /// Returns the list of loop indices which appear in the non-trivial index
<   /// expression on t_l, e.g., A[i+j] => {i, j}
<   std::vector<LoopId> &getDependentLoops(TensorId t, Level lvl) {
<     assert(isValidLevel(t, lvl));
<     return levelToDependentLoop[t][lvl];
---
>   std::optional<unsigned> getLoopIdx(unsigned t, unsigned dim) const {
>     assert(t < numTensors && dim < numLoops);
>     return dimToLoopIdx[t][dim];
475,478c304,307
<   /// Returns the defining [tid, lvl] for the loop.
<   std::pair<TensorId, Level> getLoopDefiningLvl(LoopId i) const {
<     assert(isValidLoopId(i));
<     return loopBounds[i];
---
>   /// Gets the dimension number of the the `t`th tensor on `i`th loop.
>   std::optional<unsigned> getDimNum(unsigned t, unsigned i) const {
>     assert(t < numTensors && i < numLoops);
>     return loopIdxToDim[t][i];
481,487c310,312
<   /// Checks whether the TensorLoopId represents a tensor level contains
<   /// non-trivial index expression.
<   bool isLvlWithNonTrivialIdxExp(TensorLoopId b) const {
<     const TensorId t = tensor(b);
<     const LoopId i = loop(b);
<     assert(isValidTensorId(t) && isValidLoopId(i));
<     return loopToDependencies[i][t].has_value();
---
>   /// Gets the dimension number of `b`.
>   std::optional<unsigned> getDimNum(unsigned b) const {
>     return getDimNum(tensor(b), index(b));
490,497c315,323
<   /// Checks whether the TensorLoopId represents a sparse tensor level contains
<   /// non-trivial index expression.
<   bool isSparseLvlWithNonTrivialIdxExp(TensorLoopId b) const {
<     if (isLvlWithNonTrivialIdxExp(b)) {
<       auto dlt = getLoopDependentLevelType(b);
<       return isCompressedDLT(dlt) || isSingletonDLT(dlt);
<     }
<     return false;
---
>   /// Sets the dimension and dimension level type of the `t`th tensor on `i`th
>   /// loop.
>   void setDimAndDimLevelType(unsigned t, unsigned i, unsigned dim,
>                              DimLevelType dlt) {
>     assert(isValidDLT(dlt));
>     dimTypes[t][i] = dlt;
>     loopIdxToDim[t][i] = dim;
>     assert(dim < numLoops);
>     dimToLoopIdx[t][dim] = i;
500,502c326,334
<   Level getLoopDependentLevel(TensorLoopId b) const {
<     assert(isLvlWithNonTrivialIdxExp(b));
<     return loopToDependencies[loop(b)][tensor(b)]->first;
---
>   // Iterates the bits of a lattice, for each set bit, converts it into the
>   // corresponding tensor dimension and invokes the callback.
>   void foreachTidDimPairInBits(
>       const BitVector &bits,
>       function_ref<void(unsigned b, unsigned tid, std::optional<unsigned> dim,
>                         DimLevelType dlt)>
>           cb) {
>     for (unsigned b : bits.set_bits())
>       cb(b, tensor(b), getDimNum(b), getDimLevelType(b));
505,508c337,338
<   DimLevelType getLoopDependentLevelType(TensorLoopId b) const {
<     assert(isLvlWithNonTrivialIdxExp(b));
<     return loopToDependencies[loop(b)][tensor(b)]->second;
<   }
---
>   // Has sparse output tensor setter.
>   void setHasSparseOut(bool s) { hasSparseOut = s; }
511,576c341,346
<   /// These methods return `const&` because the underlying objects must
<   /// not be mutated by client code.  The only exception is for mutating
<   /// the value associated with an expression, for which there are
<   /// dedicated methods below.
<   ///
<   /// NOTE: It is inadvisable to keep the reference alive for a long
<   /// time (e.g., as in `TensorExpr &te = merger.exp(e)`), since insertions
<   /// into the merger can cause data movement which will invalidate the
<   /// underlying memory address.  This isn't just a problem with the `&`
<   /// references, but also applies to the `ArrayRef`.  In particular,
<   /// using `for (LatPointId p : merger.set(s))` will run into the same
<   /// dangling-reference problems if the loop body inserts new sets.
<   const TensorExp &exp(ExprId e) const {
<     assert(isValidExprId(e));
<     return tensorExps[e];
<   }
<   const LatPoint &lat(LatPointId p) const {
<     assert(isValidLatPointId(p));
<     return latPoints[p];
<   }
<   ArrayRef<LatPointId> set(LatSetId s) const {
<     assert(isValidLatSetId(s));
<     return latSets[s];
<   }
< 
<   /// Checks whether the given expression has an associated value.
<   bool hasExprValue(ExprId e) const { return static_cast<bool>(exp(e).val); }
< 
<   /// Sets the expression to have the associated value.  Asserts that
<   /// the new value is defined, and that the expression does not already
<   /// have a value.  If you want to overwrite a previous associated value,
<   /// use `updateExprValue` instead.
<   void setExprValue(ExprId e, Value v) {
<     assert(isValidExprId(e));
<     assert(v && "Got an undefined value");
<     auto &val = tensorExps[e].val;
<     assert(!val && "Expression already has an associated value");
<     val = v;
<   }
< 
<   /// Clears the value associated with the expression.  Asserts that the
<   /// expression does indeed have an associated value before clearing it.
<   /// If you don't want to check for a previous associated value first,
<   /// then use `updateExprValue` instead.
<   void clearExprValue(ExprId e) {
<     assert(isValidExprId(e));
<     auto &val = tensorExps[e].val;
<     assert(val && "Expression does not have an associated value to clear");
<     val = Value();
<   }
< 
<   /// Unilaterally updates the expression to have the associated value.
<   /// That is, unlike `setExprValue` and `clearExprValue`, this method
<   /// does not perform any checks on whether the expression had a
<   /// previously associated value nor whether the new value is defined.
<   //
<   // TODO: The unilateral update semantics are required by the
<   // current implementation of `CodegenEnv::genLoopBoundary`; however,
<   // that implementation seems a bit dubious.  We would much rather have
<   // the semantics `{ clearExprValue(e); setExprValue(e, v); }` or
<   // `{ clearExprValue(e); if (v) setExprValue(e, v); }` since those
<   // provide better invariants.
<   void updateExprValue(ExprId e, Value v) {
<     assert(isValidExprId(e));
<     tensorExps[e].val = v;
<   }
---
>   /// Typically it is inadvisible to keep the reference around, as in
>   /// "TensorExpr &te = merger.exp(e))", since insertions into the merger
>   /// may cause data movement and invalidate the underlying memory address.
>   TensorExp &exp(unsigned e) { return tensorExps[e]; }
>   LatPoint &lat(unsigned l) { return latPoints[l]; }
>   SmallVector<unsigned> &set(unsigned s) { return latSets[s]; }
580,582c350,352
<   void dumpExp(ExprId e) const;
<   void dumpLat(LatPointId p) const;
<   void dumpSet(LatSetId s) const;
---
>   void dumpExp(unsigned e) const;
>   void dumpLat(unsigned p) const;
>   void dumpSet(unsigned s) const;
587,589c357,359
<   /// remaining tensor (sub)expression and the next loop in the iteration
<   /// graph.  Returns the identifier of the root set.
<   LatSetId buildLattices(ExprId e, LoopId i);
---
>   /// remaining tensor (sub)expression and the next loop index in the
>   /// iteration graph. Returns index of the root expression.
>   unsigned buildLattices(unsigned e, unsigned i);
592,593c362,363
<   /// On success, returns the identifier of the root expression.
<   std::optional<ExprId> buildTensorExpFromLinalg(linalg::GenericOp op);
---
>   /// Returns index of the root expression on success.
>   std::optional<unsigned> buildTensorExpFromLinalg(linalg::GenericOp op);
596,597c366,367
<   Value buildExp(RewriterBase &rewriter, Location loc, ExprId e, Value v0,
<                  Value v1) const;
---
>   Value buildExp(RewriterBase &rewriter, Location loc, unsigned e, Value v0,
>                  Value v1);
601,622c371,373
<   constexpr bool isValidTensorId(TensorId t) const { return t < numTensors; }
<   constexpr bool isValidLoopId(LoopId i) const {
<     return i != detail::kInvalidId && i < numLoops;
<   }
<   bool isValidLevel(TensorId t, Level lvl) const {
<     assert(levelToDependentLoop[t].size() == lvlToLoop[t].size());
<     return isValidTensorId(t) && lvl < lvlToLoop[t].size();
<   }
<   bool isValidExprId(ExprId e) const {
<     return e != detail::kInvalidId && e < tensorExps.size();
<   }
<   bool isValidLatPointId(LatPointId p) const {
<     return p != detail::kInvalidId && p < latPoints.size();
<   }
<   bool isValidLatSetId(LatSetId s) const {
<     return s != detail::kInvalidId && s < latSets.size();
<   }
<   bool maybeZero(ExprId e) const;
<   bool isInvariant(ExprId e) const {
<     return exp(e).kind == TensorExp::Kind::kInvariant;
<   }
<   Type inferType(ExprId e, Value src) const;
---
>   bool maybeZero(unsigned e) const;
>   bool isInvariant(unsigned e) const;
>   Type inferType(unsigned e, Value src);
625c376
<   std::optional<ExprId> buildTensorExp(linalg::GenericOp op, Value v);
---
>   std::optional<unsigned> buildTensorExp(linalg::GenericOp op, Value v);
628,629c379,380
<   const TensorId outTensor;
<   const TensorId syntheticTensor;
---
>   const unsigned outTensor;
>   const unsigned syntheticTensor;
635,663c386,392
<   // Below we use `std::vector` for things which have a priori fixed
<   // sizes, whereas we use `llvm::SmallVector` for things with variable
<   // size.  Do beware that these two classes differ in the semantics of
<   // `operator[]`: `SmallVector` performs OOB checks, whereas `std::vector`
<   // does not.
< 
<   // Map that converts pair<TensorId, LoopId> to the corresponding
<   // level-type.
<   std::vector<std::vector<DimLevelType>> lvlTypes;
< 
<   // Map that converts pair<TensorId, LoopId> to the corresponding
<   // level.
<   std::vector<std::vector<std::optional<Level>>> loopToLvl;
< 
<   // Map that converts pair<TensorId, Level> to the corresponding LoopId.
<   std::vector<std::vector<std::optional<LoopId>>> lvlToLoop;
< 
<   // Map from a loop to its dependencies if any.
<   // The dependencies of a loop is a set of (tensor, level) pairs.
<   // It is currently only set for non-trivial index expressions.
<   // E.g., A[i+j] => i and j will have dependencies {A0, dlt(A0)} to indicate
<   // that i and j are used in the non-trivial index expression on A0.
<   std::vector<std::vector<std::optional<std::pair<Level, DimLevelType>>>>
<       loopToDependencies;
< 
<   // The inverse map of ldxToDependencies from tensor level -> dependent loop
<   // E.g., A[i+j], we have A0 => {i, j}, to indicate that A0 uses both {i, j}
<   // to compute its indices.
<   std::vector<std::vector<std::vector<LoopId>>> levelToDependentLoop;
---
>   // Map that converts pair<tensor id, loop id> to the corresponding dimension
>   // level type.
>   std::vector<std::vector<DimLevelType>> dimTypes;
> 
>   // Map that converts pair<tensor id, loop id> to the corresponding
>   // dimension.
>   std::vector<std::vector<std::optional<unsigned>>> loopIdxToDim;
665,666c394,395
<   // Map from a loop to the [tid, lvl] pair that defines the loop boundary.
<   std::vector<std::pair<TensorId, Level>> loopBounds;
---
>   // Map that converts pair<tensor id, dim> to the corresponding loop id.
>   std::vector<std::vector<std::optional<unsigned>>> dimToLoopIdx;
670c399
<   llvm::SmallVector<SmallVector<LatPointId>> latSets;
---
>   llvm::SmallVector<SmallVector<unsigned>> latSets;
--- include/mlir/Dialect/SparseTensor/CMakeLists.txt
--- include/mlir/Dialect/SparseTensor/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.h include/mlir/Dialect/SparseTensor/Transforms/Passes.h
52,53c52,53
<   SparsificationOptions(SparseParallelizationStrategy p, bool idxReduc)
<       : parallelizationStrategy(p), enableIndexReduction(idxReduc) {}
---
>   SparsificationOptions(SparseParallelizationStrategy p)
>       : parallelizationStrategy(p) {}
55c55
<       : SparsificationOptions(SparseParallelizationStrategy::kNone, false) {}
---
>       : SparsificationOptions(SparseParallelizationStrategy::kNone) {}
57d56
<   bool enableIndexReduction;
135d133
<                                          bool createSparseDeallocs,
140,141c138
< createSparseTensorCodegenPass(bool createSparseDeallocs,
<                               bool enableBufferInitialization);
---
> createSparseTensorCodegenPass(bool enableBufferInitialization);
185,187c182,183
<     bool createSparseDeallocs, bool enableRuntimeLibrary,
<     bool enableBufferInitialization, unsigned vectorLength,
<     bool enableVLAVectorization, bool enableSIMDIndex32);
---
>     bool enableRuntimeLibrary, bool enableBufferInitialization,
>     unsigned vectorLength, bool enableVLAVectorization, bool enableSIMDIndex32);
205,210d200
< 
< void populateSparseGPUCodegenPatterns(RewritePatternSet &patterns,
<                                       unsigned numThreads);
< 
< std::unique_ptr<Pass> createSparseGPUCodegenPass();
< std::unique_ptr<Pass> createSparseGPUCodegenPass(unsigned numThreads);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.td include/mlir/Dialect/SparseTensor/Transforms/Passes.td
73c73
<     "affine::AffineDialect",
---
>     "AffineDialect",
84,86d83
<     Option<"enableIndexReduction", "enable-index-reduction", "bool",
<            "false",
<            "Enable dependent index reduction based algorithm to handle non-trivial index expressions on sparse inputs (experimental features)">,
223,228d219
<     Option<"createSparseDeallocs", "create-sparse-deallocs", "bool",
<            "true", "Specify if the temporary buffers created by the sparse "
<                    "compiler should be deallocated. For compatibility with core "
<                    "bufferization passes. "
<                    "This option is only used when enable-runtime-library=false. "
<                    "See also create-deallocs for BufferizationOption.">,
310,329d300
<   ];
< }
< 
< def SparseGPUCodegen : Pass<"sparse-gpu-codegen", "ModuleOp"> {
<   let summary = "Generates GPU code during sparsification";
<   let description = [{
<     Enables sparse compiler to use GPU acceleration.
<   }];
<   let constructor = "mlir::createSparseGPUCodegenPass()";
<   let dependentDialects = [
<     "arith::ArithDialect",
<     "bufferization::BufferizationDialect",
<     "gpu::GPUDialect",
<     "linalg::LinalgDialect",
<     "memref::MemRefDialect",
<     "scf::SCFDialect",
<     "sparse_tensor::SparseTensorDialect",
<   ];
<   let options = [
<     Option<"numThreads", "num_threads", "int32_t", "1024", "Sets the number of GPU threads">,
--- include/mlir/Dialect/SparseTensor/Transforms/Passes.h
52,53c52,53
<   SparsificationOptions(SparseParallelizationStrategy p, bool idxReduc)
<       : parallelizationStrategy(p), enableIndexReduction(idxReduc) {}
---
>   SparsificationOptions(SparseParallelizationStrategy p)
>       : parallelizationStrategy(p) {}
55c55
<       : SparsificationOptions(SparseParallelizationStrategy::kNone, false) {}
---
>       : SparsificationOptions(SparseParallelizationStrategy::kNone) {}
57d56
<   bool enableIndexReduction;
135d133
<                                          bool createSparseDeallocs,
140,141c138
< createSparseTensorCodegenPass(bool createSparseDeallocs,
<                               bool enableBufferInitialization);
---
> createSparseTensorCodegenPass(bool enableBufferInitialization);
185,187c182,183
<     bool createSparseDeallocs, bool enableRuntimeLibrary,
<     bool enableBufferInitialization, unsigned vectorLength,
<     bool enableVLAVectorization, bool enableSIMDIndex32);
---
>     bool enableRuntimeLibrary, bool enableBufferInitialization,
>     unsigned vectorLength, bool enableVLAVectorization, bool enableSIMDIndex32);
205,210d200
< 
< void populateSparseGPUCodegenPatterns(RewritePatternSet &patterns,
<                                       unsigned numThreads);
< 
< std::unique_ptr<Pass> createSparseGPUCodegenPass();
< std::unique_ptr<Pass> createSparseGPUCodegenPass(unsigned numThreads);
--- include/mlir/Dialect/SparseTensor/Transforms/Passes.td
73c73
<     "affine::AffineDialect",
---
>     "AffineDialect",
84,86d83
<     Option<"enableIndexReduction", "enable-index-reduction", "bool",
<            "false",
<            "Enable dependent index reduction based algorithm to handle non-trivial index expressions on sparse inputs (experimental features)">,
223,228d219
<     Option<"createSparseDeallocs", "create-sparse-deallocs", "bool",
<            "true", "Specify if the temporary buffers created by the sparse "
<                    "compiler should be deallocated. For compatibility with core "
<                    "bufferization passes. "
<                    "This option is only used when enable-runtime-library=false. "
<                    "See also create-deallocs for BufferizationOption.">,
310,329d300
<   ];
< }
< 
< def SparseGPUCodegen : Pass<"sparse-gpu-codegen", "ModuleOp"> {
<   let summary = "Generates GPU code during sparsification";
<   let description = [{
<     Enables sparse compiler to use GPU acceleration.
<   }];
<   let constructor = "mlir::createSparseGPUCodegenPass()";
<   let dependentDialects = [
<     "arith::ArithDialect",
<     "bufferization::BufferizationDialect",
<     "gpu::GPUDialect",
<     "linalg::LinalgDialect",
<     "memref::MemRefDialect",
<     "scf::SCFDialect",
<     "sparse_tensor::SparseTensorDialect",
<   ];
<   let options = [
<     Option<"numThreads", "num_threads", "int32_t", "1024", "Sets the number of GPU threads">,
--- include/mlir/Dialect/SparseTensor/Transforms/BufferizableOpInterfaceImpl.h
--- include/mlir/Dialect/SparseTensor/Transforms/CMakeLists.txt
--- include/mlir/Dialect/SparseTensor/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/Enums.h include/mlir/Dialect/SparseTensor/IR/Enums.h
43c43
< /// type is 64-bit, but targets with different "index" bitwidths should
---
> /// type is 64-bit, but targets with different "index" bit widths should
48c48
< /// Encoding of overhead types (both position overhead and coordinate
---
> /// Encoding of overhead types (both pointer overhead and indices
95,99d94
< // TODO: We currently split out the non-variadic version from the variadic
< // version. Using ##__VA_ARGS__ to avoid the split gives
< //   warning: token pasting of ',' and __VA_ARGS__ is a GNU extension
< //   [-Wgnu-zero-variadic-macro-arguments]
< // and __VA_OPT__(, ) __VA_ARGS__ requires c++20.
112,132d106
< // This x-macro includes all `V` types and supports variadic arguments.
< #define MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, ...)                              \
<   DO(F64, double, __VA_ARGS__)                                                 \
<   DO(F32, float, __VA_ARGS__)                                                  \
<   DO(F16, f16, __VA_ARGS__)                                                    \
<   DO(BF16, bf16, __VA_ARGS__)                                                  \
<   DO(I64, int64_t, __VA_ARGS__)                                                \
<   DO(I32, int32_t, __VA_ARGS__)                                                \
<   DO(I16, int16_t, __VA_ARGS__)                                                \
<   DO(I8, int8_t, __VA_ARGS__)                                                  \
<   DO(C64, complex64, __VA_ARGS__)                                              \
<   DO(C32, complex32, __VA_ARGS__)
< 
< // This x-macro calls its argument on every pair of overhead and `V` types.
< #define MLIR_SPARSETENSOR_FOREVERY_V_O(DO)                                     \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 64, uint64_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 32, uint32_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 16, uint16_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 8, uint8_t)                             \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 0, index_type)
< 
175,188c149,158
<   Undef = 0,                 // 0b0000_00
<   Dense = 4,                 // 0b0001_00
<   Compressed = 8,            // 0b0010_00
<   CompressedNu = 9,          // 0b0010_01
<   CompressedNo = 10,         // 0b0010_10
<   CompressedNuNo = 11,       // 0b0010_11
<   Singleton = 16,            // 0b0100_00
<   SingletonNu = 17,          // 0b0100_01
<   SingletonNo = 18,          // 0b0100_10
<   SingletonNuNo = 19,        // 0b0100_11
<   CompressedWithHi = 32,     // 0b1000_00
<   CompressedWithHiNu = 33,   // 0b1000_01
<   CompressedWithHiNo = 34,   // 0b1000_10
<   CompressedWithHiNuNo = 35, // 0b1000_11
---
>   Undef = 0,           // 0b000_00
>   Dense = 4,           // 0b001_00
>   Compressed = 8,      // 0b010_00
>   CompressedNu = 9,    // 0b010_01
>   CompressedNo = 10,   // 0b010_10
>   CompressedNuNo = 11, // 0b010_11
>   Singleton = 16,      // 0b100_00
>   SingletonNu = 17,    // 0b100_01
>   SingletonNo = 18,    // 0b100_10
>   SingletonNuNo = 19,  // 0b100_11
194,197c164,166
<   Dense = 4,             // 0b0001_00
<   Compressed = 8,        // 0b0010_00
<   Singleton = 16,        // 0b0100_00
<   CompressedWithHi = 32, // 0b1000_00
---
>   Dense = 4,      // 0b001_00
>   Compressed = 8, // 0b010_00
>   Singleton = 16, // 0b100_00
224,231d192
<   case DimLevelType::CompressedWithHi:
<     return "compressed-hi";
<   case DimLevelType::CompressedWithHiNu:
<     return "compressed-hi-nu";
<   case DimLevelType::CompressedWithHiNo:
<     return "compressed-hi-no";
<   case DimLevelType::CompressedWithHiNuNo:
<     return "compressed-hi-nu-no";
242,244c203,204
<   return (formatBits <= 1)
<              ? (propertyBits == 0)
<              : (formatBits == 2 || formatBits == 4 || formatBits == 8);
---
>   return (formatBits <= 1) ? (propertyBits == 0)
>                            : (formatBits == 2 || formatBits == 4);
267,272d226
< /// Check if the `DimLevelType` is compressed (regardless of properties).
< constexpr bool isCompressedWithHiDLT(DimLevelType dlt) {
<   return (static_cast<uint8_t>(dlt) & ~3) ==
<          static_cast<uint8_t>(DimLevelType::CompressedWithHi);
< }
< 
356,360c310
<                isValidDLT(DimLevelType::SingletonNuNo) &&
<                isValidDLT(DimLevelType::CompressedWithHi) &&
<                isValidDLT(DimLevelType::CompressedWithHiNu) &&
<                isValidDLT(DimLevelType::CompressedWithHiNo) &&
<                isValidDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                isValidDLT(DimLevelType::SingletonNuNo)),
374,384d323
< static_assert((!isCompressedWithHiDLT(DimLevelType::Dense) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHi) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNu) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNo) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNuNo) &&
<                !isCompressedWithHiDLT(DimLevelType::Singleton) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNu) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNo) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNuNo)),
<               "isCompressedWithHiDLT definition is broken");
< 
404,408c343
<                !isOrderedDLT(DimLevelType::SingletonNuNo) &&
<                isOrderedDLT(DimLevelType::CompressedWithHi) &&
<                isOrderedDLT(DimLevelType::CompressedWithHiNu) &&
<                !isOrderedDLT(DimLevelType::CompressedWithHiNo) &&
<                !isOrderedDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                !isOrderedDLT(DimLevelType::SingletonNuNo)),
419,423c354
<                !isUniqueDLT(DimLevelType::SingletonNuNo) &&
<                isUniqueDLT(DimLevelType::CompressedWithHi) &&
<                !isUniqueDLT(DimLevelType::CompressedWithHiNu) &&
<                isUniqueDLT(DimLevelType::CompressedWithHiNo) &&
<                !isUniqueDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                !isUniqueDLT(DimLevelType::SingletonNuNo)),
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensorAttrDefs.td include/mlir/Dialect/SparseTensor/IR/SparseTensorAttrDefs.td
23,57d22
< // Type aliases.
< //
< // These attributes are just like `IndexAttr` (include/mlir/IR/OpBase.td),
< // except that:
< // (1) the `summary` is more specific (i.e., the fourth parameter to
< //     `TypedAttrBase`), which helps tablegen provide better error messages.
< // (2) tablegen-generated getters will have the given `returnType`, in
< //     lieu of the `APInt` that `IndexAttr` uses.  This avoids the boilerplate
< //     of needing to say `get{FOO}().getZExtValue()`, as well as using
< //     C++ types which better document intent.
< //===----------------------------------------------------------------------===//
< 
< def DimensionAttr :
<     TypedAttrBase<
<       Index, "IntegerAttr",
<       And<[CPred<"$_self.isa<::mlir::IntegerAttr>()">,
<            CPred<"$_self.cast<::mlir::IntegerAttr>().getType()"
<                  ".isa<::mlir::IndexType>()">]>,
<       "dimension attribute"> {
<   let returnType = [{::mlir::sparse_tensor::Dimension}];
<   let convertFromStorage = [{$_self.getValue().getZExtValue()}];
< }
< 
< def LevelAttr :
<     TypedAttrBase<
<       Index, "IntegerAttr",
<       And<[CPred<"$_self.isa<::mlir::IntegerAttr>()">,
<            CPred<"$_self.cast<::mlir::IntegerAttr>().getType()"
<                  ".isa<::mlir::IndexType>()">]>,
<       "level attribute"> {
<   let returnType = [{::mlir::sparse_tensor::Level}];
<   let convertFromStorage = [{$_self.getValue().getZExtValue()}];
< }
< 
< //===----------------------------------------------------------------------===//
130,139c95,105
<     - Level-type for each level of a tensor type:
<         - **dense** : all entries along this level are stored.
<         - **compressed** : only nonzeros along this level are stored.
<         - **singleton** : a variant of the compressed level-format,
<           for when coordinates are guaranteed to have no siblings at this level.
<       By default, each level-type has the property of being unique (no
<       duplicates at that level) and ordered (coordinates appear sorted
<       at that level).  The following two suffixes can be used to specify
<       that the level should instead be non-unique (duplicates may appear)
<       and/or non-ordered (coordinates may appear unsorted).
---
>     - Dimension level type for each dimension of a tensor type:
>         - **dense** : dimension is dense, all entries along this dimension
>           are stored
>         - **compressed** : dimension is sparse, only nonzeros along this dimensions
>           are stored
>         - **singleton** : dimension stores individual indices with no siblings
>       By default, each dimension level types has the property of being unique
>       (no duplicates at that level) and ordered (indices appear sorted at that
>       level). The following two suffixes can be used to make the last two
>       dimension level types not-unique (duplicates may appear) and not-ordered
>       (indices may appear unsorted).
142,206c108,158
<       Currently, these suffixes (if present) must appear in this order.
<       In the future, we may introduce additional level-types and
<       properties, and split up how the level-format and properties are
<       specified rather than using this suffix mechanism.
< 
<       TODO: This field is called "dimLevelType" for historical reasons,
<       even though the types are per-level rather than per-dimension.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<     - An optional permutation which maps (higher-ordering)-coordinates
<       to level-coordinates; defaulting to the identity permutation.
<       For example, given a 2-d tensor with the default higher-ordering,
<       `(i, j) -> (i, j)` specifies row-wise storage and `(i, j) ->
<       (j, i)` specifies column-wise storage.
< 
<       TODO: this field is called "dimOrdering" for historical reasons,
<       even though it actually operates on level-coordinates rather than
<       dimension-coordinates.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<     - An optional higher-order mapping from dimension-coordinates to
<       a higher-order coordinate space; defaulting to the identity map.
<       This is applied before the `dimOrdering`, thus we have the composite:
<       dimCoords --higherOrdering--> hoCoords --dimOrdering--> lvlCoords.
<       The higher-order mapping is used to define block-sparse storage,
<       jagged-diagonal (JDS/ELL/ITPACK) storage, etc.
< 
<       For example, given a 2-d tensor, the mapping
<       `(i, j) -> (i floordiv 2, j floordiv 3, i mod 2, j mod 3)`
<       imposes an higher-order partitioning into 2x3 blocks along the
<       matrix layout.  For block-sparsity, blocks are typically stored
<       with compression while dense storage is used within each block
<       (although hybrid schemes are possible as well).
< 
<       TODO: the following example is out-of-date and will be implemented
<       in a different manner than described here.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<       The higher-order mapping also provides a notion of "counting a
<       dimension", where every stored element with the same coordinate
<       is mapped to a new slice.  For instance, ELL storage of a 2-d
<       tensor can be defined with the mapping `(i, j) -> (#i, i, j)`
<       using the notation of [Chou20].  Lacking the `#` symbol in MLIR's
<       affine mapping, we use a free symbol `c` to define such counting,
<       together with a constant that denotes the number of resulting
<       slices.  For example, the mapping `(i, j)[c] -> (c * 3 * i, i, j)`
<       with the level-types `["dense", "dense", "compressed"]` denotes ELL
<       storage with three jagged diagonals that count the dimension `i`.
< 
<     - The required bitwidth for "position" storage (integral offsets
<       into the sparse storage scheme).  A narrow width reduces the memory
<       footprint of overhead storage, as long as the width suffices to
<       define the total required range (viz. the maximum number of stored
<       entries over all indirection levels).  The choices are `8`, `16`,
<       `32`, `64`, or, the default, `0` to indicate the native bitwidth.
< 
<     - The required bitwidth for "coordinate" storage (the coordinates
<       of stored entries).  A narrow width reduces the memory footprint
<       of overhead storage, as long as the width suffices to define
<       the total required range (viz. the maximum value of each tensor
<       coordinate over all levels).  The choices are `8`, `16`, `32`,
<       `64`, or, the default, `0` to indicate a native bitwidth.
---
>       Currently, these suffixes, is present, should appear in this order.
>       In the future, we may introduce many more dimension level types and
>       properties, and separate specifying the two completely rather than
>       using this suffix mechanism.
> 
>     - An optional dimension ordering on the indices of this tensor type. Unlike
>       dense storage, most sparse storage schemes do not provide fast random
>       access.  This affine map specifies the order of dimensions that should be
>       supported by the sparse storage scheme. For example, for a 2-d tensor,
>       `(i, j) -> (i, j)` requests row-wise storage and `(i, j) -> (j, i)`
>       requests column-wise storage.  By default, an identify mapping is used,
>       which implies that the original indices directly correspond to stored
>       indices.
> 
>     - An optional higher-ordering mapping from the original index space of
>       the tensor to a higher-order index space, used to define block-sparse
>       storage or ELL (jagged diagonal) storage. For example, for a 2-d tensor,
>       the mapping `(i, j) -> (i floordiv 2, j floordiv 3, i mod 2, j mod 3)`
>       imposes an higher-order partitioning into 2x3 blocks along the matrix
>       layout. A dimension ordering can be used to define a desired ordering
>       on this higher-order index space. Likewise, the dimension level types
>       define dense or compressed storage along this higher-order index space.
>       For block-sparse, blocks are typically stored with compression while
>       dense storage is used within each block (although hybrid schemes are
>       possible as well). The higher-order mapping also provides a notion of
>       "counting a dimension", where every stored element with the same index
>       is mapped to a new slice. For instance, ELL storage of a 2-d tensor can
>       be defined with the mapping `(i, j) -> (#i, i, j)` using the notation
>       of [Chou20]. Lacking the `#` symbol in MLIR's affine mapping, we use
>       a free symbol `c` to define such counting, together with a constant
>       that denotes the number of resulting slices. For example, the mapping
>       `(i, j)[c] -> (c * 3 * i, i, j)` with the first two higher-order indices
>       stored dense and the innermost compressed denotes ELL storage with
>       three jagged diagonals that count the dimension `i`.
> 
>       TODO: introduce a real counting symbol to MLIR's mapping, since an
>             expression like 3*c*i has no direct interpretation?
> 
>     - The required bit width for "pointer" storage (integral offsets into
>       the sparse storage scheme). A narrow width reduces the memory footprint
>       of overhead storage, as long as the width suffices to define the total
>       required range (viz. the maximum number of stored entries over all indirection
>       dimensions). The choices are `8`, `16`, `32`, `64`, or, the default, `0` to
>       indicate the native bit width.
> 
>     - The required bit width for "index" storage (elements of the coordinates of
>       stored entries). A narrow width reduces the memory footprint of overhead
>       storage, as long as the width suffices to define the total required range
>       (viz. the maximum value of each tensor index over all dimensions). The
>       choices are `8`, `16`, `32`, `64`, or, the default, `0` to indicate a
>       native bit width.
208,209c160,161
<     - An optional array of `SparseTensorDimSliceAttr`, which specifies
<       how the sparse tensor is partitioned on each dimension.
---
>     - An optional array of SparseTensorDimSliceAttr, which specifies how the sparse
>       tensor is partitioned on each level.
230,231c182,183
<       posWidth = 32,
<       crdWidth = 8
---
>       pointerBitWidth = 32,
>       indexBitWidth = 8
265c217
<     // A level-type for each level of the sparse storage.
---
>     // A dimension level type for each dimension of the tensor type.
268c220
<       "level-types"
---
>       "per dimension level type"
270c222
<     // A permutation from (higher-ordering)-coordinates to level-coordinates.
---
>     // A dimension order on the indices of this tensor type.
272c224
<     // A mapping from dimension-coordinates to (higher-ordering)-coordinates.
---
>     // A mapping between the original and higher-ordering index space.
274,278c226,230
<     // The required bitwidth for position storage.
<     "unsigned":$posWidth,
<     // The required bitwidth for coordinate storage.
<     "unsigned":$crdWidth,
<     // A slice attribute for each dimension of the tensor type.
---
>     // The required bit width for pointer storage.
>     "unsigned":$pointerBitWidth,
>     // The required bit width for index storage.
>     "unsigned":$indexBitWidth,
>     // A dimension level type for each dimension of the tensor type.
289,290c241,242
<                      "unsigned":$posWidth,
<                      "unsigned":$crdWidth), [{
---
>                      "unsigned":$pointerBitWidth,
>                      "unsigned":$indexBitWidth), [{
294,295c246,247
<                          posWidth,
<                          crdWidth,
---
>                          pointerBitWidth,
>                          indexBitWidth,
301,302c253,254
<     /// Returns the type for position storage based on posWidth
<     Type getPosType() const;
---
>     /// Returns the type for pointer storage based on pointerBitWidth
>     Type getPointerType() const;
304,305c256,257
<     /// Returns the type for coordinate storage based on crdWidth
<     Type getCrdType() const;
---
>     /// Returns the type for index storage based on indexBitWidth
>     Type getIndexType() const;
311,316c263
<     /// Constructs a new encoding with the pointer and index bitwidth
<     /// reset to the default.
<     SparseTensorEncodingAttr withoutBitWidths() const;
< 
<     /// Returns true if every level is dense.  Also returns true for
<     /// the null encoding (since dense-tensors are always all-dense).
---
>     /// Return true if every level is dense in the encoding.
319,325c266
<     /// Returns true if every level is ordered.  Also returns true for
<     /// the null encoding (since dense-tensors are always all-ordered).
<     bool isAllOrdered() const;
< 
<     /// Returns true if the encoding has an identity dimension ordering.
<     /// Also returns true for the null encoding (since dense-tensors
<     /// always have the identity ordering).
---
>     /// Return true if the encoding has an identity dimension ordering.
328,344d268
<     /// Returns the number of storage levels.  Asserts that the encoding
<     /// is non-null (since there is no fixed result that's valid for
<     /// every dense-tensor).
<     ::mlir::sparse_tensor::Level getLvlRank() const;
< 
<     /// Safely looks up the level-type for the requested level.  (Returns
<     /// `DimLevelType::Dense` for the null encoding, since dense-tensors
<     /// are always all-dense.)
<     ::mlir::sparse_tensor::DimLevelType getLvlType(::mlir::sparse_tensor::Level l) const;
< 
<     bool isDenseLvl(::mlir::sparse_tensor::Level l) const { return isDenseDLT(getLvlType(l)); }
<     bool isCompressedLvl(::mlir::sparse_tensor::Level l) const { return isCompressedDLT(getLvlType(l)); }
<     bool isCompressedWithHiLvl(::mlir::sparse_tensor::Level l) const { return isCompressedWithHiDLT(getLvlType(l)); }
<     bool isSingletonLvl(::mlir::sparse_tensor::Level l) const { return isSingletonDLT(getLvlType(l)); }
<     bool isOrderedLvl(::mlir::sparse_tensor::Level l) const { return isOrderedDLT(getLvlType(l)); }
<     bool isUniqueLvl(::mlir::sparse_tensor::Level l) const { return isUniqueDLT(getLvlType(l)); }
< 
349,354c273,278
<     std::optional<uint64_t> getStaticDimSliceOffset(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticDimSliceSize(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticDimSliceStride(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticLvlSliceOffset(::mlir::sparse_tensor::Level lvl) const;
<     std::optional<uint64_t> getStaticLvlSliceSize(::mlir::sparse_tensor::Level lvl) const;
<     std::optional<uint64_t> getStaticLvlSliceStride(::mlir::sparse_tensor::Level lvl) const;
---
>     std::optional<uint64_t> getStaticDimSliceOffset(unsigned dim) const;
>     std::optional<uint64_t> getStaticDimSliceSize(unsigned dim) const;
>     std::optional<uint64_t> getStaticDimSliceStride(unsigned dim) const;
>     std::optional<uint64_t> getStaticLvlSliceOffset(unsigned lvl) const;
>     std::optional<uint64_t> getStaticLvlSliceSize(unsigned lvl) const;
>     std::optional<uint64_t> getStaticLvlSliceStride(unsigned lvl) const;
368,370c292,294
<         I32EnumAttrCase<"LvlSize",    0, "lvl_sz">,
<         I32EnumAttrCase<"PosMemSize", 1, "pos_mem_sz">,
<         I32EnumAttrCase<"CrdMemSize", 2, "crd_mem_sz">,
---
>         I32EnumAttrCase<"DimSize",    0, "dim_sz">,
>         I32EnumAttrCase<"PtrMemSize", 1, "ptr_mem_sz">,
>         I32EnumAttrCase<"IdxMemSize", 2, "idx_mem_sz">,
372,373d295
<         I32EnumAttrCase<"DimOffset",  4, "dim_offset">,
<         I32EnumAttrCase<"DimStride",  5, "dim_stride">,
393,396d314
< def IsSparseTensorSlicePred
<   : CPred<"!!::mlir::sparse_tensor::getSparseTensorEncoding($_self) && "
<           "  ::mlir::sparse_tensor::getSparseTensorEncoding($_self).isSlice()">;
< 
403,405d320
< class SparseTensorSliceOf<list<Type> allowedTypes>
<   : TensorOf<allowedTypes, [IsSparseTensorSlicePred], "sparse tensor slice">;
< 
407d321
< def AnySparseTensorSlice : SparseTensorSliceOf<[AnyType]>;
413,447d326
< 
< //===----------------------------------------------------------------------===//
< // Sparse Tensor Sorting Algorithm Attribute.
< //===----------------------------------------------------------------------===//
< 
< // TODO: Currently, we only provide four implementations, and expose the
< // implementations via attribute algorithm. In the future, if we will need
< // to support both stable and non-stable quick sort, we may add
< // quick_sort_nonstable enum to the attribute. Alternative, we may use two
< // attributes, (stable|nonstable, algorithm), to specify a sorting
< // implementation.
< //
< // --------------------------------------------------------------------------
< // |           | hybrid_qsort| insertion_sort | qsort       | heap_sort.    |
< // |non-stable | Impl        | X              |  Impl       | Impl          |
< // |stable     | X           | Impl           |  Not Impl   | X             |
< // --------------------------------------------------------------------------
< 
< // The C++ enum for sparse tensor sort kind.
< def SparseTensorSortKindEnum
<     : I32EnumAttr<"SparseTensorSortKind", "sparse tensor sort algorithm", [
<         I32EnumAttrCase<"HybridQuickSort",    0, "hybrid_quick_sort">,
<         I32EnumAttrCase<"InsertionSortStable", 1, "insertion_sort_stable">,
<         I32EnumAttrCase<"QuickSort", 2, "quick_sort">,
<         I32EnumAttrCase<"HeapSort", 3, "heap_sort">,
<       ]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = SparseTensor_Dialect.cppNamespace;
< }
< 
< // Define the enum sparse tensor sort kind attribute.
< def SparseTensorSortKindAttr
<     : EnumAttr<SparseTensor_Dialect, SparseTensorSortKindEnum,
<                "SparseTensorSortAlgorithm"> {
< }
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensorBase.td include/mlir/Dialect/SparseTensor/IR/SparseTensorBase.td
23c23
<     schemes consisting of positions, coordinates, and values. Lower-level
---
>     schemes consisting of pointers, indices, and values. Lower-level
34,56c34,54
<     The MLIR implementation [Biketal22] closely follows the "sparse
<     iteration theory" that forms the foundation of TACO.  A rewriting
<     rule is applied to each tensor expression in the Linalg dialect
<     (MLIR's tensor index notation) where the sparsity of tensors is
<     indicated using the per-level level-types (e.g., dense, compressed,
<     singleton) together with a specification of the order on the levels
<     (see [Chou18] for an in-depth discussions and possible extensions
<     to these level-types).  Subsequently, a topologically sorted
<     iteration graph, reflecting the required order on coordinates with
<     respect to the levels of each tensor, is constructed to ensure
<     that all tensors are visited in natural level-coordinate order.
<     Next, iteration lattices are constructed for the tensor expression
<     for every index in topological order.  Each iteration lattice point
<     consists of a conjunction of tensor coordinates together with a tensor
<     (sub)expression that needs to be evaluated for that conjunction.
<     Within the lattice, iteration points are ordered according to
<     the way coordinates are exhausted.  As such these iteration
<     lattices drive actual sparse code generation, which consists of
<     a relatively straightforward one-to-one mapping from iteration
<     lattices to combinations of for-loops, while-loops, and if-statements.
<     Sparse tensor outputs that materialize uninitialized are handled with
<     direct insertions if all parallel loops are outermost or insertions
<     that indirectly go through a 1-dimensional access pattern expansion
---
>     The MLIR implementation [Biketal22] closely follows the "sparse iteration
>     theory" that forms the foundation of TACO. A rewriting rule is applied to
>     each tensor expression in the Linalg dialect (MLIR's tensor index notation)
>     where the sparsity of tensors is indicated using the per-dimension level
>     types dense/compressed together with a specification of the order on the
>     dimensions (see [Chou18] for an in-depth discussions and possible
>     extensions to these level types). Subsequently, a topologically sorted
>     iteration graph, reflecting the required order on indices with respect
>     to the dimensions of each tensor, is constructed to ensure that all tensors
>     are visited in natural index order. Next, iteration lattices are
>     constructed for the tensor expression for every index in topological
>     order. Each iteration lattice point consists of a conjunction of tensor
>     indices together with a tensor (sub)expression that needs to be evaluated
>     for that conjunction.  Within the lattice, iteration points are ordered
>     according to the way indices are exhausted. As such these iteration
>     lattices drive actual sparse code generation, which consists of a
>     relatively straightforward one-to-one mapping from iteration lattices
>     to combinations of for-loops, while-loops, and if-statements. Sparse
>     tensor outputs that materialize uninitialized are handled with direct
>     insertions if all parallel loops are outermost or insertions that
>     indirectly go through a 1-dimensional access pattern expansion
87a86
>   let useFoldAPI = kEmitFoldAdaptorFolder;
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensor.h include/mlir/Dialect/SparseTensor/IR/SparseTensor.h
21,63d20
< //===----------------------------------------------------------------------===//
< //
< // Type aliases to help code be more self-documenting.  Unfortunately
< // these are not type-checked, so they only provide documentation rather
< // than doing anything to prevent mixups.
< //
< // We must include these here (rather than in "SparseTensorType.h")
< // because they are used by methods declared in the tablegen files.
< //
< //===----------------------------------------------------------------------===//
< 
< namespace mlir {
< namespace sparse_tensor {
< 
< /// The type of dimension identifiers, and dimension-ranks.  We use the
< /// same type for both identifiers and ranks because the latter are used
< /// mainly for ordering-comparisons against the former (just like how the
< /// one-past-the-end iterators are used).
< using Dimension = uint64_t;
< 
< /// The type of level identifiers, and level-ranks.  We use the same
< /// type for both identifiers and ranks because the latter are used
< /// mainly for ordering-comparisons against the former (just like how
< /// the one-past-the-end iterators are used).
< using Level = uint64_t;
< 
< /// The type for individual components of a compile-time shape.  We avoid
< /// calling this "size" because we use the term "sizes" to indicate the
< /// actual run-time sizes, whereas this type also allows the value
< /// `ShapedType::kDynamic`.
< using DynSize = int64_t;
< 
< /// The type for individual components of a compile-time shape which
< /// are known not to be `ShapedType::kDynamic`.
< using StaticSize = int64_t;
< 
< } // namespace sparse_tensor
< } // namespace mlir
< 
< //===----------------------------------------------------------------------===//
< // TableGen-defined classes
< //===----------------------------------------------------------------------===//
< 
81,84d37
< //===----------------------------------------------------------------------===//
< // Additional convenience methods.
< //===----------------------------------------------------------------------===//
< 
88,91c41,69
< /// Convenience method to abbreviate casting `getType()`.
< template <typename T>
< inline RankedTensorType getRankedTensorType(T t) {
<   return t.getType().template cast<RankedTensorType>();
---
> /// Convenience method to get a sparse encoding attribute from a type.
> /// Returns null-attribute for any type without an encoding.
> SparseTensorEncodingAttr getSparseTensorEncoding(Type type);
> 
> /// Returns true iff the given type is a type for a COO tensor with the last
> /// dimension level type being unique.
> bool isUniqueCOOType(RankedTensorType tp);
> 
> /// Returns the starting dimension for a trailing COO region that spans across
> /// at least two dimensions. If no such COO region is found, returns the rank
> /// of the tensor.
> unsigned getCOOStart(SparseTensorEncodingAttr enc);
> 
> //
> // Dimension level types.
> //
> 
> // MSVC does not allow this function to be constexpr, because
> // `SparseTensorEncodingAttr::operator bool` isn't declared constexpr.
> // And therefore all functions calling it cannot be constexpr either.
> // TODO: since Clang does allow these to be constexpr, perhaps we should
> // define a macro to abstract over `inline` vs `constexpr` annotations.
> inline DimLevelType getDimLevelType(SparseTensorEncodingAttr enc, uint64_t d) {
>   if (enc) {
>     auto types = enc.getDimLevelType();
>     assert(d < types.size() && "Dimension out of bounds");
>     return types[d];
>   }
>   return DimLevelType::Dense; // unannotated tensor is dense
94,97c72,73
< /// Convenience method to abbreviate casting `getType()`.
< template <typename T>
< inline MemRefType getMemRefType(T t) {
<   return t.getType().template cast<MemRefType>();
---
> inline DimLevelType getDimLevelType(RankedTensorType type, uint64_t d) {
>   return getDimLevelType(getSparseTensorEncoding(type), d);
100,102c76,99
< /// Convenience method to get a sparse encoding attribute from a type.
< /// Returns null-attribute for any type without an encoding.
< SparseTensorEncodingAttr getSparseTensorEncoding(Type type);
---
> /// Convenience function to test for dense dimension (0 <= d < rank).
> inline bool isDenseDim(RankedTensorType type, uint64_t d) {
>   return isDenseDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for compressed dimension (0 <= d < rank).
> inline bool isCompressedDim(RankedTensorType type, uint64_t d) {
>   return isCompressedDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for singleton dimension (0 <= d < rank).
> inline bool isSingletonDim(RankedTensorType type, uint64_t d) {
>   return isSingletonDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for dense dimension (0 <= d < rank).
> inline bool isDenseDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isDenseDLT(getDimLevelType(enc, d));
> }
> 
> /// Convenience function to test for compressed dimension (0 <= d < rank).
> inline bool isCompressedDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isCompressedDLT(getDimLevelType(enc, d));
> }
104,131c101,119
< /// Returns true iff the given sparse tensor encoding attribute has a trailing
< /// COO region starting at the given level.
< bool isCOOType(SparseTensorEncodingAttr enc, Level startLvl, bool isUnique);
< 
< /// Returns true iff the given type is a COO type where the last level
< /// is unique.
< bool isUniqueCOOType(Type tp);
< 
< /// Returns the starting level for a trailing COO region that spans
< /// at least two levels.  If no such COO region is found, then returns
< /// the level-rank.
< Level getCOOStart(SparseTensorEncodingAttr enc);
< 
< /// Helpers to setup a COO type.
< RankedTensorType getCOOFromTypeWithOrdering(RankedTensorType src,
<                                             AffineMap ordering, bool ordered);
< 
< RankedTensorType getCOOFromType(RankedTensorType src, bool ordered);
< 
< /// Returns true iff MLIR operand has any sparse operand or result.
< inline bool hasAnySparseOperandOrResult(Operation *op) {
<   bool anySparseIn = llvm::any_of(op->getOperands().getTypes(), [](Type t) {
<     return getSparseTensorEncoding(t) != nullptr;
<   });
<   bool anySparseOut = llvm::any_of(op->getResults().getTypes(), [](Type t) {
<     return getSparseTensorEncoding(t) != nullptr;
<   });
<   return anySparseIn || anySparseOut;
---
> /// Convenience function to test for singleton dimension (0 <= d < rank).
> inline bool isSingletonDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isSingletonDLT(getDimLevelType(enc, d));
> }
> 
> //
> // Dimension level properties.
> //
> 
> /// Convenience function to test for ordered property in the
> /// given dimension (0 <= d < rank).
> inline bool isOrderedDim(RankedTensorType type, uint64_t d) {
>   return isOrderedDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for unique property in the
> /// given dimension (0 <= d < rank).
> inline bool isUniqueDim(RankedTensorType type, uint64_t d) {
>   return isUniqueDLT(getDimLevelType(type, d));
138,164c126,135
< // This CPP guard is to disable deprecation warnings for the LLVM
< // build-bot, while making it easy to re-enable it for local development.
< #if 0
< #define DEPRECATED                                                             \
<   LLVM_DEPRECATED("The toOrigDim/toStoredDim functions are deprecated "        \
<                   "because they only work for permutations; therefore any "    \
<                   "code using them cannot support non-permutations.",          \
<                   "")
< #else
< #define DEPRECATED
< #endif
< 
< /// [deprecated] Convenience method to translate the given level to the
< /// corresponding dimension.  Requires: `0 <= l < lvlRank`.
< DEPRECATED Dimension toOrigDim(SparseTensorEncodingAttr enc, Level l);
< DEPRECATED Dimension toOrigDim(RankedTensorType type, Level l);
< 
< /// [deprecated] Convenience method to translate the given dimension to
< /// the corresponding level.  Requires: `0 <= d < dimRank`.
< DEPRECATED Level toStoredDim(SparseTensorEncodingAttr enc, Dimension d);
< DEPRECATED Level toStoredDim(RankedTensorType type, Dimension d);
< 
< #undef DEPRECATED
< 
< namespace detail {
< Type getIntegerOrIndexType(MLIRContext *ctx, unsigned bitwidth);
< } // namespace detail
---
> uint64_t toOrigDim(SparseTensorEncodingAttr enc, uint64_t d);
> uint64_t toStoredDim(SparseTensorEncodingAttr enc, uint64_t d);
> 
> /// Convenience method to translate the given stored dimension
> /// to the original dimension (0 <= d < rank).
> uint64_t toOrigDim(RankedTensorType type, uint64_t d);
> 
> /// Convenience method to translate the given original dimension
> /// to the stored dimension (0 <= d < rank).
> uint64_t toStoredDim(RankedTensorType type, uint64_t d);
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensorOps.td include/mlir/Dialect/SparseTensor/IR/SparseTensorOps.td
30c30
<     Arguments<(ins AnyType:$source)>,
---
>     Arguments<(ins AnyType:$source, UnitAttr:$expandSymmetry)>,
43,45c43,48
<     Reading in a symmetric matrix will result in just the lower/upper triangular
<     part of the matrix (so that only relevant information is stored). Proper
<     symmetry support for operating on symmetric matrices is still TBD.
---
>     An optional attribute `expandSymmetry` can be used to extend this operation
>     to make symmetry in external formats explicit in the storage. That is, when
>     the attribute presents and a non-zero value is discovered at (i, j) where
>     i!=j, we add the same value to (j, i). This claims more storage than a pure
>     symmetric storage, and thus may cause a bad performance hit. True symmetric
>     storage is planned for the future.
53,167c56,57
<   let assemblyFormat = "$source attr-dict `:` type($source) `to` type($result)";
< }
< 
< def SparseTensor_PackOp : SparseTensor_Op<"pack", [Pure]>,
<     Arguments<(ins TensorOf<[AnyType]>:$values,
<                    TensorOf<[AnySignlessIntegerOrIndex]>:$coordinates,
<                    OptionalAttr<IndexAttr>:$batched_lvls)>,
<     Results<(outs AnySparseTensor: $result)> {
<   let summary = "Returns a sparse tensor from the given (values, coordinates) pair";
< 
<   let description = [{
<     Packs the values/coordinates into a COO sparse tensor.  The length
<     of `values` must match the outer-length of `coordinates`, since these
<     two tensors are "zipped" together.  The `coordinates` argument provides
<     level-coords for each value, therefore, the inner-length of `coordinates`
<     must match the level-rank of the returned tensor, and each level-coords
<     must be valid for the level-sizes of the returned tensor.  Note that
<     the returned tensor must be statically shaped because it is impossible
<     to infer the dimension-shape from level-coordinates alone.
< 
<     TODO: The returned tensor is allowed (in principle) to have non-identity
<     dimOrdering/higherOrdering mappings.  However, the current implementation
<     does not yet support them.
< 
<     - `coordinates : tensor<NSE x lvlRank x iType>`
<       supplies the level-coords for each element in `values`.
<     - `values : tensor<NSE x V>`
<       supplies the corresponding values for each entry in `coordinates`.
<     - `batched_lvls : optional<index>`
<       supplies the number of leading levels that are batched.
< 
<     This operation can be used to materialize a sparse tensor from external
<     sources; e.g., when passing two numpy arrays from Python.
< 
<     Example:
< 
<     ```mlir
<     %values      = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
<     %coordinates = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
<     %st = sparse_tensor.pack %values, %coordinates
<         : tensor<3xf64>, tensor<3x2xindex> to tensor<3x4xf64, #COO>
<     // yields COO format |1.1, 0.0, 0.0, 0.0|
<     //     of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
<     //                   |0.0, 0.0, 0.0, 0.0|
<     ```
< 
<     If `batched_lvls` is provided, the operation materializes a batched sparse tensor.
<     Example:
< 
<     ```mlir
<     %values      = arith.constant dense<[[ 1.1,   2.2,   3.3 ],
<                                          [ 1.2,   2.3,   0.0 ]]> : tensor<2x3xf64>
<     %coordinates = arith.constant dense<[[ [0],   [1],   [2] ],
<                                          [ [1],   [2],   [3] ]> : tensor<2x3x1xindex>
<     %st = sparse_tensor.pack %values, %coordinates batched_lvls=1
<         : tensor<2x3xf64>, tensor<2x3x1xindex> to tensor<2x4xf64, #BCOO>
<     // yields BCOO format |1.1, 2.2, 3.3, 0.0|
<     //      of 2x4 matrix |0.0, 1.2, 2.3, 0.0|
<     ```
<   }];
< 
<   let extraClassDeclaration = [{
<     /// Returns the number of leading levels that are batched.
<     unsigned getNumBatchedLvls();
<   }];
< 
<   let assemblyFormat =
<     "$values `,` $coordinates (`batched_lvls` `=` $batched_lvls^)? attr-dict"
<     "`:` type($values) `,` type($coordinates) `to` type($result)";
< 
<   let hasVerifier = 1;
< }
< 
< def SparseTensor_UnpackOp : SparseTensor_Op<"unpack">,
<     Arguments<(ins AnySparseTensor:$tensor)>,
<     Results<(outs 1DTensorOf<[AnyType]>:$values,
<                   2DTensorOf<[AnySignlessIntegerOrIndex]>:$coordinates,
<                   AnySignlessIntegerOrIndex:$nse)> {
<   let summary = "Returns the (values, coordinates) pair unpacked from the input tensor";
< 
<   let description = [{
<     The unpack operation is the inverse of `sparse_tensor::pack`.  It returns
<     the values, level-coordinates, and number-of-stored-entries extracted
<     from the sparse tensor.  The source tensor is allowed (in principle)
<     to have non-identity dimOrdering/higherOrdering mappings.  Regardless
<     of the mappings, the returned `coordinates` are always level-coordinates,
<     because this is what we mean by "unpacking" as opposed to other forms
<     of exposing sparse tensors to external clients.  This operation can be
<     used for returning an unpacked MLIR sparse tensor to frontend; e.g.,
<     returning two numpy arrays to Python.
< 
<     TODO: the current implementation does not yet support non-identity mappings.
< 
<     This operation ends the lifetime of the sparse tensor, and using
<     the tensor after the unpack is undefined behavior.
< 
<     Example:
< 
<     ```mlir
<     // input COO format |1.1, 0.0, 0.0, 0.0|
<     //    of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
<     //                  |0.0, 0.0, 0.0, 0.0|
<     %values, %coordinates, %nse
<       = sparse_tensor.unpack %st
<       : tensor<3x4xf64, #COO> to tensor<2xf64>, tensor<2x2xindex>, index
<     // %values      = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
<     // %coordinates = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
<     // %nse = 3
<     ```
<   }];
< 
<   let assemblyFormat =
<     "$tensor attr-dict `:` type($tensor)"
<     "`to` type($values) `,` type($coordinates) `,` type($nse)";
< 
---
>   let assemblyFormat = "(`expand_symmetry` $expandSymmetry^)? $source attr-dict"
>                        "`:` type($source) `to` type($result)";
172c62
<   [Pure]>,
---
>   [Pure, SameOperandsAndResultElementType]>,
219,220c109,110
< def SparseTensor_ToPositionsOp : SparseTensor_Op<"positions", [Pure]>,
<     Arguments<(ins AnySparseTensor:$tensor, LevelAttr:$level)>,
---
> def SparseTensor_ToPointersOp : SparseTensor_Op<"pointers", [Pure]>,
>     Arguments<(ins AnySparseTensor:$tensor, IndexAttr:$dimension)>,
222c112
<   let summary = "Extracts the `level`-th positions array of the `tensor`";
---
>   let summary = "Extracts pointers array at given dimension from a tensor";
224,232c114,120
<     Returns the positions array of the tensor's storage at the given
<     level.  This is similar to the `bufferization.to_memref` operation
<     in the sense that it provides a bridge between a tensor world view
<     and a bufferized world view.  Unlike the `bufferization.to_memref`
<     operation, however, this sparse operation actually lowers into code
<     that extracts the positions array from the sparse storage itself
<     (either by calling a support library or through direct code).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the pointers array of the sparse storage format at the
>     given dimension for the given sparse tensor. This is similar to the
>     `bufferization.to_memref` operation in the sense that it provides a bridge
>     between a tensor world view and a bufferized world view. Unlike the
>     `bufferization.to_memref` operation, however, this sparse operation actually
>     lowers into code that extracts the pointers array from the sparse storage
>     scheme (either by calling a support library or through direct code).
237c125
<     %1 = sparse_tensor.positions %0 { level = 1 : index }
---
>     %1 = sparse_tensor.pointers %0 { dimension = 1 : index }
245,246c133,134
< def SparseTensor_ToCoordinatesOp : SparseTensor_Op<"coordinates", [Pure]>,
<     Arguments<(ins AnySparseTensor:$tensor, LevelAttr:$level)>,
---
> def SparseTensor_ToIndicesOp : SparseTensor_Op<"indices", [Pure]>,
>     Arguments<(ins AnySparseTensor:$tensor, IndexAttr:$dimension)>,
248c136
<   let summary = "Extracts the `level`-th coordinates array of the `tensor`";
---
>   let summary = "Extracts indices array at given dimension from a tensor";
250,258c138,144
<     Returns the coordinates array of the tensor's storage at the given
<     level.  This is similar to the `bufferization.to_memref` operation
<     in the sense that it provides a bridge between a tensor world view
<     and a bufferized world view.  Unlike the `bufferization.to_memref`
<     operation, however, this sparse operation actually lowers into code
<     that extracts the coordinates array from the sparse storage itself
<     (either by calling a support library or through direct code).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the indices array of the sparse storage format at the
>     given dimension for the given sparse tensor. This is similar to the
>     `bufferization.to_memref` operation in the sense that it provides a bridge
>     between a tensor world view and a bufferized world view. Unlike the
>     `bufferization.to_memref` operation, however, this sparse operation actually
>     lowers into code that extracts the indices array from the sparse storage
>     scheme (either by calling a support library or through direct code).
263c149
<     %1 = sparse_tensor.coordinates %0 { level = 1 : index }
---
>     %1 = sparse_tensor.indices %0 { dimension = 1 : index }
271c157
< def SparseTensor_ToCoordinatesBufferOp : SparseTensor_Op<"coordinates_buffer", [Pure]>,
---
> def SparseTensor_ToIndicesBufferOp : SparseTensor_Op<"indices_buffer", [Pure]>,
274c160
<   let summary = "Extracts the linear coordinates array from a tensor";
---
>   let summary = "Extracts the linear indices array from a tensor";
276,289c162,172
<     Returns the linear coordinates array for a sparse tensor with
<     a trailing COO region with at least two levels.  It is an error
<     if the tensor doesn't contain such a COO region.  This is similar
<     to the `bufferization.to_memref` operation in the sense that it
<     provides a bridge between a tensor world view and a bufferized
<     world view.  Unlike the `bufferization.to_memref` operation,
<     however, this operation actually lowers into code that extracts
<     the linear coordinates array from the sparse storage scheme that
<     stores the coordinates for the COO region as an array of structures.
<     For example, a 2D COO sparse tensor with two non-zero elements at
<     coordinates (1, 3) and (4, 6) are stored in a linear buffer as
<     (1, 4, 3, 6) instead of two buffer as (1, 4) and (3, 6).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the linear indices array for a sparse tensor with a trailing COO
>     region with at least two dimensions. It is an error if the tensor doesn't
>     contain such a COO region. This is similar to the `bufferization.to_memref`
>     operation in the sense that it provides a bridge between a tensor world view
>     and a bufferized world view. Unlike the `bufferization.to_memref` operation,
>     however, this sparse operation actually lowers into code that extracts the
>     linear indices array from the sparse storage scheme that stores the indices
>     for the COO region as an array of structures. For example, a 2D COO sparse
>     tensor with two non-zero elements at coordinates (1, 3) and (4, 6) are
>     stored in a linear buffer as (1, 4, 3, 6) instead of two buffer as (1, 4)
>     and (3, 6).
294,295c177,178
<     %1 = sparse_tensor.coordinates_buffer %0
<        : tensor<64x64xf64, #COO> to memref<?xindex>
---
>     %1 = sparse_tensor.indices_buffer %0
>     : tensor<64x64xf64, #COO> to memref<?xindex>
315,316d197
<     Writing into the result of this operation is undefined behavior.
< 
327,381d207
< def SparseTensor_ToSliceOffsetOp : SparseTensor_Op<"slice.offset", [Pure]>,
<     Arguments<(ins AnySparseTensorSlice:$slice, IndexAttr:$dim)>,
<     Results<(outs Index:$offset)> {
<   let summary = "Extracts the offset of the sparse tensor slice at the given dimension";
<   let description = [{
<     Extracts the offset of the sparse tensor slice at the given dimension.
< 
<     Currently, sparse tensor slices are still a work in progress, and only
<     works when runtime library is disabled (i.e., running sparse compiler
<     with `enable-runtime-library=false`).
< 
<     Example:
< 
<     ```mlir
<     %0 = tensor.extract_slice %s[%v1, %v2][64, 64][1, 1] : tensor<128x128xf64, #DCSR>
<                                                         to tensor<64x64xf64, #Slice>
< 
<     %1 = sparse_tensor.slice.offset %0 at 0 : tensor<64x64xf64, #Slice>
<     %2 = sparse_tensor.slice.offset %0 at 1 : tensor<64x64xf64, #Slice>
<     // %1 = %v1
<     // %2 = %v2
<     ```
<   }];
<   let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";
<   let hasVerifier = 1;
< }
< 
< def SparseTensor_ToSliceStrideOp : SparseTensor_Op<"slice.stride", [Pure]>,
<     Arguments<(ins AnySparseTensorSlice:$slice, IndexAttr:$dim)>,
<     Results<(outs Index:$stride)> {
<   let summary = "Extracts the stride of the sparse tensor slice at the given dimension";
<   let description = [{
<     Extracts the stride of the sparse tensor slice at the given dimension.
< 
<     Currently, sparse tensor slices are still a work in progress, and only
<     works when runtime library is disabled (i.e., running sparse compiler
<     with `enable-runtime-library=false`).
< 
<     Example:
< 
<     ```mlir
<     %0 = tensor.extract_slice %s[%v1, %v2][64, 64][%s1, %s2] : tensor<128x128xf64, #DCSR>
<                                                             to tensor<64x64xf64, #Slice>
< 
<     %1 = sparse_tensor.slice.stride %0 at 0 : tensor<64x64xf64, #Slice>
<     %2 = sparse_tensor.slice.stride %0 at 1 : tensor<64x64xf64, #Slice>
<     // %1 = %s1
<     // %2 = %s2
< 
<     ```
<   }];
<   let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";
<   let hasVerifier = 1;
< }
< 
383d208
<     Arguments<(ins Optional<SparseTensorStorageSpecifier>:$source)>,
387,394c212,213
<     Returns an initial storage specifier value.  A storage specifier
<     value holds the level-sizes, position arrays, coordinate arrays,
<     and the value array.
<     If this is a specifier for slices, it also holds the extra strides/offsets
<     for each tensor dimension.
< 
<     TODO: The sparse tensor slice support is currently in a unstable state, and
<     is subject to change in the future.
---
>     Returns an initial storage specifier value. A storage specifier value holds
>     the sizes for tensor dimensions, pointer arrays, index arrays, and the value array.
399,404d217
<     #CSR = #sparse_tensor.encoding<{ dimLevelType = [ "dense", "compressed" ]}>
<     #CSR_SLICE = #sparse_tensor.encoding<{
<       dimLevelType = [ "dense", "compressed" ],
<       slice = [ (1, 4, 1), (1, 4, 2) ]
<     }>
< 
406,408d218
<     %1 = sparse_tensor.storage_specifier.init with %src
<          : !sparse_tensor.storage_specifier<#CSR> to
<            !sparse_tensor.storage_specifier<#CSR_SLICE>
412,420c222
<   let builders = [
<     OpBuilder<(ins "Type":$result),
<     [{
<       build($_builder, $_state, result, Value());
<     }]>
<   ];
< 
<   let assemblyFormat = "attr-dict (`with` $source^)? `:` (`from` qualified(type($source))^ `to`)?"
<                                                         " qualified(type($result))";
---
>   let assemblyFormat = "attr-dict `:` qualified(type($result))";
426,427c228,229
<                    OptionalAttr<LevelAttr>:$level)>,
<     Results<(outs Index:$result)> {
---
>                    OptionalAttr<IndexAttr>:$dim)>,
>     Results<(outs AnyType:$result)> {
432c234
<     Example of querying the size of the coordinates array for level 0:
---
>     Example of querying the size of the index array for level 0:
435,436c237,238
<     %0 = sparse_tensor.storage_specifier.get %arg0 crd_mem_sz at 0
<          : !sparse_tensor.storage_specifier<#COO>
---
>     %0 = sparse_tensor.storage_specifier.get %arg0 idx_mem_sz at 0
>          : !sparse_tensor.storage_specifier<#COO> to i64
440,441c242,243
<   let assemblyFormat = "$specifier $specifierKind (`at` $level^)? attr-dict"
<                        "`:` qualified(type($specifier))";
---
>   let assemblyFormat = "$specifier $specifierKind (`at` $dim^)? attr-dict `:` "
>                        "qualified(type($specifier)) `to` type($result)";
450,451c252,253
<                    OptionalAttr<LevelAttr>:$level,
<                    Index:$value)>,
---
>                    OptionalAttr<IndexAttr>:$dim,
>                    AnyType:$value)>,
458c260
<     Example of updating the sizes of the coordinates array for level 0:
---
>     Example of updating the sizes of the index array for level 0:
461,462c263,265
<     %0 = sparse_tensor.storage_specifier.set %arg0 crd_mem_sz at 0 with %new_sz
<        : !sparse_tensor.storage_specifier<#COO>
---
>     %0 = sparse_tensor.storage_specifier.set %arg0 idx_mem_sz at 0 with %new_sz
>        : i32, !sparse_tensor.storage_specifier<#COO>
> 
465,466c268,269
<   let assemblyFormat = "$specifier $specifierKind (`at` $level^)? `with` $value"
<                        " attr-dict `:` qualified(type($result))";
---
>   let assemblyFormat = "$specifier $specifierKind (`at` $dim^)? `with` $value attr-dict `:` "
>                        "type($value) `,` qualified(type($result))";
490c293
<     Arguments<(ins Variadic<AnyRankedTensor>:$inputs, DimensionAttr:$dimension)>,
---
>     Arguments<(ins Variadic<AnyRankedTensor>:$inputs, IndexAttr:$dimension)>,
495,499c298,302
<      Concatenates a list input tensors and the output tensor with the same
<      dimension-rank.  The concatenation happens on the specified `dimension`
<      (0 <= dimension < dimRank).  The resulting `dimension` size is the
<      sum of all the input sizes for that dimension, while all the other
<      dimensions should have the same size in the input and output tensors.
---
>      Concatenates a list input tensors and the output tensor with the same rank.
>      The concatenation happens on the specified `dimension` (0<= dimension < rank).
>      The resulting `dimension` size is the sum of all the input dimension sizes,
>      while all the other dimensions should have the same size in the input and
>      output tensors.
531c334
<                Variadic<Index>:$lvlCoords)>,
---
>                Variadic<Index>:$indices)>,
533c336
<   string summary = "Inserts a value into the sparse tensor";
---
>   string summary = "Inserts a value into given sparse tensor";
535,540c338,344
<     Inserts the value into the underlying storage of the tensor at the
<     given level-coordinates.  The arity of `lvlCoords` must match the
<     level-rank of the tensor.  This operation can only be applied when
<     the tensor materializes unintialized from a `bufferization.alloc_tensor`
<     operation and the final tensor is constructed with a `load` operation
<     which has the `hasInserts` attribute set.
---
>     Inserts the given value at given indices into the underlying
>     sparse storage format of the given tensor with the given indices.
>     The arity of indices must match the rank of the tensor. This
>     operation can only be applied when a tensor materializes unintialized
>     with a `bufferization.alloc_tensor` operation and the final tensor
>     is constructed with a `load` operation that has the `hasInserts`
>     attribute set.
542,543c346,347
<     The level-properties of the sparse tensor type fully describe what
<     kind of insertion order is allowed.  When all levels have "unique"
---
>     Properties in the sparse tensor type fully describe what kind
>     of insertion order is allowed. When all dimensions have "unique"
545,547c349,351
<     strict lexicographical level-coordinate order.  Other properties
<     define different insertion regimens.  Inserting in a way contrary
<     to these properties results in undefined behavior.
---
>     strict lexicographical index order. Other properties define
>     different insertion regimens. Inserting in a way contrary to
>     these properties results in undefined behavior.
561,562c365
<   let assemblyFormat = "$value `into` $tensor `[` $lvlCoords `]` attr-dict"
<                        "`:` type($tensor)";
---
>   let assemblyFormat = "$value `into` $tensor `[` $indices `]` attr-dict `:` type($tensor)";
641c444
<     Performs an access pattern expansion for the innermost levels of the
---
>     Performs an access pattern expansion for the innermost dimensions of the
649,659c452,460
<     The `values` and `filled` arrays must have lengths equal to the
<     level-size of the innermost level (i.e., as if the innermost level
<     were *dense*).  The `added` array and `count` are used to store new
<     level-coordinates when a false value is encountered in the `filled`
<     array.  All arrays should be allocated before the loop (possibly even
<     shared between loops in a future optimization) so that their *dense*
<     initialization can be amortized over many iterations.  Setting and
<     resetting the dense arrays in the loop nest itself is kept *sparse*
<     by only iterating over set elements through an indirection using
<     the added array, so that the operations are kept proportional to
<     the number of nonzeros.
---
>     The values and filled array have sizes that suffice for a *dense* innermost
>     dimension (e.g. a full row for matrices). The added array and count are used
>     to store new indices when a false value is encountered in the filled array.
>     All arrays should be allocated before the loop (possibly even shared between
>     loops in a future optimization) so that their *dense* initialization can be
>     amortized over many iterations. Setting and resetting the dense arrays in
>     the loop nest itself is kept *sparse* by only iterating over set elements
>     through an indirection using the added array, so that the operations are
>     kept proportional to the number of nonzeros.
683c484
<                    Variadic<Index>:$lvlCoords)>,
---
>                    Variadic<Index>:$indices)>,
689,695c490,495
<     level-coordinates.  The arity of `lvlCoords` is one less than the
<     level-rank of the tensor, with the coordinate of the innermost
<     level defined through the `added` array.  The `values` and `filled`
<     arrays are reset in a *sparse* fashion by only iterating over set
<     elements through an indirection using the `added` array, so that
<     the operations are kept proportional to the number of nonzeros.
<     See the `sparse_tensor.expand` operation for more details.
---
>     indices. The arity of indices is one less than the rank of the tensor,
>     with the remainder innermost indices defined through the added array.
>     The values and filled array are reset in a *sparse* fashion by only
>     iterating over set elements through an indirection using the added
>     array, so that the operations are kept proportional to the number of
>     nonzeros. See the `sparse_tensor.expand` operation for more details.
709c509
<                        " `into` $tensor `[` $lvlCoords `]` attr-dict"
---
>                        " `into` $tensor `[` $indices `]` attr-dict"
776,780d575
<     //
<     // TODO: Currently tablegen doesn't support the assembly syntax when
<     // `algorithm` is an optional enum attribute. We may want to use an optional
<     // enum attribute when this is fixed in tablegen.
<     //
784c579
<                SparseTensorSortKindAttr:$algorithm)>  {
---
>                UnitAttr:$stable)>  {
806,808c601,602
<     The enum attribute `algorithm` indicates the sorting algorithm used to
<     implement the operator: hybrid_quick_sort, insertion_sort_stable,
<     quick_sort, or heap_sort.
---
>     The `stable` attribute indicates whether a stable sorting algorithm should
>     be used to implement the operator.
816c610
<     sparse_tensor.sort insertion_sort_stable %n, %x1, %x2 jointly y1, %y2
---
>     sparse_tensor.sort %n, %x1, %x2 jointly y1, %y2
821,822c615
<     sparse_tensor.sort hybrid_quick_sort %n, %x1, %x2 jointly y1, %y2
<       { alg=1 : index}
---
>     sparse_tensor.sort stable %n, %x1, %x2 jointly y1, %y2
826c619,620
<   let assemblyFormat = "$algorithm $n `,` $xs (`jointly` $ys^)? attr-dict"
---
>   let assemblyFormat = "(`stable` $stable^)? $n"
>                        "`,`$xs (`jointly` $ys^)? attr-dict"
835c629
<                SparseTensorSortKindAttr:$algorithm)>  {
---
>                UnitAttr:$stable)>  {
854c648
<     sparse_tensor.sort_coo insertion_sort_stable %n, %x { nx = 2 : index}
---
>     sparse_tensor.sort_coo %n, %x { nx = 2 : index}
859,860c653
<     sparse_tensor.sort hybrid_quick_sort %n, %xy jointly %y1
<       { nx = 2 : index, ny = 2 : index}
---
>     sparse_tensor.sort %n, %xy jointly %y1 { nx = 2 : index, ny = 2 : index}
865c658
<   let assemblyFormat = "$algorithm $n"
---
>   let assemblyFormat = "(`stable` $stable^)? $n"
1211,1212c1004
<                     Variadic<AnyType>:$initArgs,
<                     OptionalAttr<AffineMapAttr>:$order)>,
---
>                     Variadic<AnyType>:$initArgs)>,
1219,1229c1011,1016
<      `tensor`: the input tensor to iterate over.
<      `initArgs`: the initial loop argument to carry and update during each iteration.
<      `order`: an optional permutation affine map that specifies the order in which
<      the dimensions are visited (e.g., row first or column first). This is only
<      applicable when the input tensor is a non-annotated dense tensor.
< 
<      For an input tensor with dim-rank `n`, the block must take `n + 1`
<      arguments (plus additional loop-carried variables as described below).
<      The first `n` arguments provide the dimension-coordinates of the element
<      being visited, and must all have `index` type.  The `(n+1)`-th argument
<      provides the element's value, and must have the tensor's element type.
---
>      For an input tensor with rank n, the block must take n + 1 (and additional loop
>      carried variables as described below) arguments. The first n arguments must be
>      Index type, together indicating the current coordinates of the element being visited.
>      The last argument must have the same type as the
>      tensor's element type, representing the actual value loaded from the input
>      tensor at the given coordinates.
1234c1021
<      SSA values mentioned above (n coordinates and 1 value).
---
>      SSA values mentioned above (n coordinate and 1 value).
1251,1254c1038,1040
<      It is important to note that the generated loop iterates over
<      elements in their storage order.  However, regardless of the
<      storage scheme used by the tensor, the block is always given
<      the dimension-coordinates.
---
>      It is important to note that foreach generated loop iterates over the stored elements
>      in the storage order. However, no matter what storage order is used, the indices passed
>      to the block always obey the original dimension order.
1279,1284d1064
<      // foreach on a row-major dense tensor but visit column first
<      sparse_tensor.foreach in %0 {order=affine_map<(i,j)->(j,i)>}: tensor<2x3xf64> do {
<       ^bb0(%row: index, %col: index, %arg3: f64):
<          // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]
<      }
< 
1289c1069
<     OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs, "AffineMapAttr":$order,
---
>     OpBuilder<(ins "Value":$tensor,
1291,1295d1070
<     OpBuilder<(ins "Value":$tensor, "AffineMapAttr":$order,
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, ValueRange(), order, bodyBuilder);
<     }]>,
1297,1305c1072,1073
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, ValueRange(), nullptr, bodyBuilder);
<     }]>,
<     OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs,
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, iterArgs, nullptr, bodyBuilder);
<     }]>
---
>       "ValueRange":$iterArgs,
>       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>")>
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR: SparseTensorType.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensorTypes.td include/mlir/Dialect/SparseTensor/IR/SparseTensorTypes.td
40,44c40,44
<     Values with storage_specifier types represent aggregated storage scheme
<     metadata for the given sparse tensor encoding.  It currently holds
<     a set of values for level-sizes, coordinate arrays, position arrays,
<     and value array.  Note that the type is not yet stable and subject to
<     change in the near future.
---
>     Values with storage_specifier types represent aggregated storage scheme metadata
>     for the given sparse tensor encoding.
>     It currently holds a set of values for sizes of sparse tensor dimension, index array,
>     pointer array and value array.
>     Note that the type is not yet stable and subject to change in the near future.
66a67,73
> 
>   let extraClassDeclaration = [{
>     // Get the integer type used to store memory and dimension sizes.
>     IntegerType getSizesType() const;
>     Type getFieldType(StorageSpecifierKind kind, std::optional<unsigned> dim) const;
>     Type getFieldType(StorageSpecifierKind kind, std::optional<APInt> dim) const;
>   }];
--- include/mlir/Dialect/SparseTensor/IR/SparseTensor.h
21,63d20
< //===----------------------------------------------------------------------===//
< //
< // Type aliases to help code be more self-documenting.  Unfortunately
< // these are not type-checked, so they only provide documentation rather
< // than doing anything to prevent mixups.
< //
< // We must include these here (rather than in "SparseTensorType.h")
< // because they are used by methods declared in the tablegen files.
< //
< //===----------------------------------------------------------------------===//
< 
< namespace mlir {
< namespace sparse_tensor {
< 
< /// The type of dimension identifiers, and dimension-ranks.  We use the
< /// same type for both identifiers and ranks because the latter are used
< /// mainly for ordering-comparisons against the former (just like how the
< /// one-past-the-end iterators are used).
< using Dimension = uint64_t;
< 
< /// The type of level identifiers, and level-ranks.  We use the same
< /// type for both identifiers and ranks because the latter are used
< /// mainly for ordering-comparisons against the former (just like how
< /// the one-past-the-end iterators are used).
< using Level = uint64_t;
< 
< /// The type for individual components of a compile-time shape.  We avoid
< /// calling this "size" because we use the term "sizes" to indicate the
< /// actual run-time sizes, whereas this type also allows the value
< /// `ShapedType::kDynamic`.
< using DynSize = int64_t;
< 
< /// The type for individual components of a compile-time shape which
< /// are known not to be `ShapedType::kDynamic`.
< using StaticSize = int64_t;
< 
< } // namespace sparse_tensor
< } // namespace mlir
< 
< //===----------------------------------------------------------------------===//
< // TableGen-defined classes
< //===----------------------------------------------------------------------===//
< 
81,84d37
< //===----------------------------------------------------------------------===//
< // Additional convenience methods.
< //===----------------------------------------------------------------------===//
< 
88,91c41,69
< /// Convenience method to abbreviate casting `getType()`.
< template <typename T>
< inline RankedTensorType getRankedTensorType(T t) {
<   return t.getType().template cast<RankedTensorType>();
---
> /// Convenience method to get a sparse encoding attribute from a type.
> /// Returns null-attribute for any type without an encoding.
> SparseTensorEncodingAttr getSparseTensorEncoding(Type type);
> 
> /// Returns true iff the given type is a type for a COO tensor with the last
> /// dimension level type being unique.
> bool isUniqueCOOType(RankedTensorType tp);
> 
> /// Returns the starting dimension for a trailing COO region that spans across
> /// at least two dimensions. If no such COO region is found, returns the rank
> /// of the tensor.
> unsigned getCOOStart(SparseTensorEncodingAttr enc);
> 
> //
> // Dimension level types.
> //
> 
> // MSVC does not allow this function to be constexpr, because
> // `SparseTensorEncodingAttr::operator bool` isn't declared constexpr.
> // And therefore all functions calling it cannot be constexpr either.
> // TODO: since Clang does allow these to be constexpr, perhaps we should
> // define a macro to abstract over `inline` vs `constexpr` annotations.
> inline DimLevelType getDimLevelType(SparseTensorEncodingAttr enc, uint64_t d) {
>   if (enc) {
>     auto types = enc.getDimLevelType();
>     assert(d < types.size() && "Dimension out of bounds");
>     return types[d];
>   }
>   return DimLevelType::Dense; // unannotated tensor is dense
94,97c72,73
< /// Convenience method to abbreviate casting `getType()`.
< template <typename T>
< inline MemRefType getMemRefType(T t) {
<   return t.getType().template cast<MemRefType>();
---
> inline DimLevelType getDimLevelType(RankedTensorType type, uint64_t d) {
>   return getDimLevelType(getSparseTensorEncoding(type), d);
100,102c76,99
< /// Convenience method to get a sparse encoding attribute from a type.
< /// Returns null-attribute for any type without an encoding.
< SparseTensorEncodingAttr getSparseTensorEncoding(Type type);
---
> /// Convenience function to test for dense dimension (0 <= d < rank).
> inline bool isDenseDim(RankedTensorType type, uint64_t d) {
>   return isDenseDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for compressed dimension (0 <= d < rank).
> inline bool isCompressedDim(RankedTensorType type, uint64_t d) {
>   return isCompressedDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for singleton dimension (0 <= d < rank).
> inline bool isSingletonDim(RankedTensorType type, uint64_t d) {
>   return isSingletonDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for dense dimension (0 <= d < rank).
> inline bool isDenseDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isDenseDLT(getDimLevelType(enc, d));
> }
> 
> /// Convenience function to test for compressed dimension (0 <= d < rank).
> inline bool isCompressedDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isCompressedDLT(getDimLevelType(enc, d));
> }
104,131c101,119
< /// Returns true iff the given sparse tensor encoding attribute has a trailing
< /// COO region starting at the given level.
< bool isCOOType(SparseTensorEncodingAttr enc, Level startLvl, bool isUnique);
< 
< /// Returns true iff the given type is a COO type where the last level
< /// is unique.
< bool isUniqueCOOType(Type tp);
< 
< /// Returns the starting level for a trailing COO region that spans
< /// at least two levels.  If no such COO region is found, then returns
< /// the level-rank.
< Level getCOOStart(SparseTensorEncodingAttr enc);
< 
< /// Helpers to setup a COO type.
< RankedTensorType getCOOFromTypeWithOrdering(RankedTensorType src,
<                                             AffineMap ordering, bool ordered);
< 
< RankedTensorType getCOOFromType(RankedTensorType src, bool ordered);
< 
< /// Returns true iff MLIR operand has any sparse operand or result.
< inline bool hasAnySparseOperandOrResult(Operation *op) {
<   bool anySparseIn = llvm::any_of(op->getOperands().getTypes(), [](Type t) {
<     return getSparseTensorEncoding(t) != nullptr;
<   });
<   bool anySparseOut = llvm::any_of(op->getResults().getTypes(), [](Type t) {
<     return getSparseTensorEncoding(t) != nullptr;
<   });
<   return anySparseIn || anySparseOut;
---
> /// Convenience function to test for singleton dimension (0 <= d < rank).
> inline bool isSingletonDim(SparseTensorEncodingAttr enc, uint64_t d) {
>   return isSingletonDLT(getDimLevelType(enc, d));
> }
> 
> //
> // Dimension level properties.
> //
> 
> /// Convenience function to test for ordered property in the
> /// given dimension (0 <= d < rank).
> inline bool isOrderedDim(RankedTensorType type, uint64_t d) {
>   return isOrderedDLT(getDimLevelType(type, d));
> }
> 
> /// Convenience function to test for unique property in the
> /// given dimension (0 <= d < rank).
> inline bool isUniqueDim(RankedTensorType type, uint64_t d) {
>   return isUniqueDLT(getDimLevelType(type, d));
138,164c126,135
< // This CPP guard is to disable deprecation warnings for the LLVM
< // build-bot, while making it easy to re-enable it for local development.
< #if 0
< #define DEPRECATED                                                             \
<   LLVM_DEPRECATED("The toOrigDim/toStoredDim functions are deprecated "        \
<                   "because they only work for permutations; therefore any "    \
<                   "code using them cannot support non-permutations.",          \
<                   "")
< #else
< #define DEPRECATED
< #endif
< 
< /// [deprecated] Convenience method to translate the given level to the
< /// corresponding dimension.  Requires: `0 <= l < lvlRank`.
< DEPRECATED Dimension toOrigDim(SparseTensorEncodingAttr enc, Level l);
< DEPRECATED Dimension toOrigDim(RankedTensorType type, Level l);
< 
< /// [deprecated] Convenience method to translate the given dimension to
< /// the corresponding level.  Requires: `0 <= d < dimRank`.
< DEPRECATED Level toStoredDim(SparseTensorEncodingAttr enc, Dimension d);
< DEPRECATED Level toStoredDim(RankedTensorType type, Dimension d);
< 
< #undef DEPRECATED
< 
< namespace detail {
< Type getIntegerOrIndexType(MLIRContext *ctx, unsigned bitwidth);
< } // namespace detail
---
> uint64_t toOrigDim(SparseTensorEncodingAttr enc, uint64_t d);
> uint64_t toStoredDim(SparseTensorEncodingAttr enc, uint64_t d);
> 
> /// Convenience method to translate the given stored dimension
> /// to the original dimension (0 <= d < rank).
> uint64_t toOrigDim(RankedTensorType type, uint64_t d);
> 
> /// Convenience method to translate the given original dimension
> /// to the stored dimension (0 <= d < rank).
> uint64_t toStoredDim(RankedTensorType type, uint64_t d);
--- include/mlir/Dialect/SparseTensor/IR/Enums.h
43c43
< /// type is 64-bit, but targets with different "index" bitwidths should
---
> /// type is 64-bit, but targets with different "index" bit widths should
48c48
< /// Encoding of overhead types (both position overhead and coordinate
---
> /// Encoding of overhead types (both pointer overhead and indices
95,99d94
< // TODO: We currently split out the non-variadic version from the variadic
< // version. Using ##__VA_ARGS__ to avoid the split gives
< //   warning: token pasting of ',' and __VA_ARGS__ is a GNU extension
< //   [-Wgnu-zero-variadic-macro-arguments]
< // and __VA_OPT__(, ) __VA_ARGS__ requires c++20.
112,132d106
< // This x-macro includes all `V` types and supports variadic arguments.
< #define MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, ...)                              \
<   DO(F64, double, __VA_ARGS__)                                                 \
<   DO(F32, float, __VA_ARGS__)                                                  \
<   DO(F16, f16, __VA_ARGS__)                                                    \
<   DO(BF16, bf16, __VA_ARGS__)                                                  \
<   DO(I64, int64_t, __VA_ARGS__)                                                \
<   DO(I32, int32_t, __VA_ARGS__)                                                \
<   DO(I16, int16_t, __VA_ARGS__)                                                \
<   DO(I8, int8_t, __VA_ARGS__)                                                  \
<   DO(C64, complex64, __VA_ARGS__)                                              \
<   DO(C32, complex32, __VA_ARGS__)
< 
< // This x-macro calls its argument on every pair of overhead and `V` types.
< #define MLIR_SPARSETENSOR_FOREVERY_V_O(DO)                                     \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 64, uint64_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 32, uint32_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 16, uint16_t)                           \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 8, uint8_t)                             \
<   MLIR_SPARSETENSOR_FOREVERY_V_VAR(DO, 0, index_type)
< 
175,188c149,158
<   Undef = 0,                 // 0b0000_00
<   Dense = 4,                 // 0b0001_00
<   Compressed = 8,            // 0b0010_00
<   CompressedNu = 9,          // 0b0010_01
<   CompressedNo = 10,         // 0b0010_10
<   CompressedNuNo = 11,       // 0b0010_11
<   Singleton = 16,            // 0b0100_00
<   SingletonNu = 17,          // 0b0100_01
<   SingletonNo = 18,          // 0b0100_10
<   SingletonNuNo = 19,        // 0b0100_11
<   CompressedWithHi = 32,     // 0b1000_00
<   CompressedWithHiNu = 33,   // 0b1000_01
<   CompressedWithHiNo = 34,   // 0b1000_10
<   CompressedWithHiNuNo = 35, // 0b1000_11
---
>   Undef = 0,           // 0b000_00
>   Dense = 4,           // 0b001_00
>   Compressed = 8,      // 0b010_00
>   CompressedNu = 9,    // 0b010_01
>   CompressedNo = 10,   // 0b010_10
>   CompressedNuNo = 11, // 0b010_11
>   Singleton = 16,      // 0b100_00
>   SingletonNu = 17,    // 0b100_01
>   SingletonNo = 18,    // 0b100_10
>   SingletonNuNo = 19,  // 0b100_11
194,197c164,166
<   Dense = 4,             // 0b0001_00
<   Compressed = 8,        // 0b0010_00
<   Singleton = 16,        // 0b0100_00
<   CompressedWithHi = 32, // 0b1000_00
---
>   Dense = 4,      // 0b001_00
>   Compressed = 8, // 0b010_00
>   Singleton = 16, // 0b100_00
224,231d192
<   case DimLevelType::CompressedWithHi:
<     return "compressed-hi";
<   case DimLevelType::CompressedWithHiNu:
<     return "compressed-hi-nu";
<   case DimLevelType::CompressedWithHiNo:
<     return "compressed-hi-no";
<   case DimLevelType::CompressedWithHiNuNo:
<     return "compressed-hi-nu-no";
242,244c203,204
<   return (formatBits <= 1)
<              ? (propertyBits == 0)
<              : (formatBits == 2 || formatBits == 4 || formatBits == 8);
---
>   return (formatBits <= 1) ? (propertyBits == 0)
>                            : (formatBits == 2 || formatBits == 4);
267,272d226
< /// Check if the `DimLevelType` is compressed (regardless of properties).
< constexpr bool isCompressedWithHiDLT(DimLevelType dlt) {
<   return (static_cast<uint8_t>(dlt) & ~3) ==
<          static_cast<uint8_t>(DimLevelType::CompressedWithHi);
< }
< 
356,360c310
<                isValidDLT(DimLevelType::SingletonNuNo) &&
<                isValidDLT(DimLevelType::CompressedWithHi) &&
<                isValidDLT(DimLevelType::CompressedWithHiNu) &&
<                isValidDLT(DimLevelType::CompressedWithHiNo) &&
<                isValidDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                isValidDLT(DimLevelType::SingletonNuNo)),
374,384d323
< static_assert((!isCompressedWithHiDLT(DimLevelType::Dense) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHi) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNu) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNo) &&
<                isCompressedWithHiDLT(DimLevelType::CompressedWithHiNuNo) &&
<                !isCompressedWithHiDLT(DimLevelType::Singleton) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNu) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNo) &&
<                !isCompressedWithHiDLT(DimLevelType::SingletonNuNo)),
<               "isCompressedWithHiDLT definition is broken");
< 
404,408c343
<                !isOrderedDLT(DimLevelType::SingletonNuNo) &&
<                isOrderedDLT(DimLevelType::CompressedWithHi) &&
<                isOrderedDLT(DimLevelType::CompressedWithHiNu) &&
<                !isOrderedDLT(DimLevelType::CompressedWithHiNo) &&
<                !isOrderedDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                !isOrderedDLT(DimLevelType::SingletonNuNo)),
419,423c354
<                !isUniqueDLT(DimLevelType::SingletonNuNo) &&
<                isUniqueDLT(DimLevelType::CompressedWithHi) &&
<                !isUniqueDLT(DimLevelType::CompressedWithHiNu) &&
<                isUniqueDLT(DimLevelType::CompressedWithHiNo) &&
<                !isUniqueDLT(DimLevelType::CompressedWithHiNuNo)),
---
>                !isUniqueDLT(DimLevelType::SingletonNuNo)),
--- include/mlir/Dialect/SparseTensor/IR/SparseTensorBase.td
23c23
<     schemes consisting of positions, coordinates, and values. Lower-level
---
>     schemes consisting of pointers, indices, and values. Lower-level
34,56c34,54
<     The MLIR implementation [Biketal22] closely follows the "sparse
<     iteration theory" that forms the foundation of TACO.  A rewriting
<     rule is applied to each tensor expression in the Linalg dialect
<     (MLIR's tensor index notation) where the sparsity of tensors is
<     indicated using the per-level level-types (e.g., dense, compressed,
<     singleton) together with a specification of the order on the levels
<     (see [Chou18] for an in-depth discussions and possible extensions
<     to these level-types).  Subsequently, a topologically sorted
<     iteration graph, reflecting the required order on coordinates with
<     respect to the levels of each tensor, is constructed to ensure
<     that all tensors are visited in natural level-coordinate order.
<     Next, iteration lattices are constructed for the tensor expression
<     for every index in topological order.  Each iteration lattice point
<     consists of a conjunction of tensor coordinates together with a tensor
<     (sub)expression that needs to be evaluated for that conjunction.
<     Within the lattice, iteration points are ordered according to
<     the way coordinates are exhausted.  As such these iteration
<     lattices drive actual sparse code generation, which consists of
<     a relatively straightforward one-to-one mapping from iteration
<     lattices to combinations of for-loops, while-loops, and if-statements.
<     Sparse tensor outputs that materialize uninitialized are handled with
<     direct insertions if all parallel loops are outermost or insertions
<     that indirectly go through a 1-dimensional access pattern expansion
---
>     The MLIR implementation [Biketal22] closely follows the "sparse iteration
>     theory" that forms the foundation of TACO. A rewriting rule is applied to
>     each tensor expression in the Linalg dialect (MLIR's tensor index notation)
>     where the sparsity of tensors is indicated using the per-dimension level
>     types dense/compressed together with a specification of the order on the
>     dimensions (see [Chou18] for an in-depth discussions and possible
>     extensions to these level types). Subsequently, a topologically sorted
>     iteration graph, reflecting the required order on indices with respect
>     to the dimensions of each tensor, is constructed to ensure that all tensors
>     are visited in natural index order. Next, iteration lattices are
>     constructed for the tensor expression for every index in topological
>     order. Each iteration lattice point consists of a conjunction of tensor
>     indices together with a tensor (sub)expression that needs to be evaluated
>     for that conjunction.  Within the lattice, iteration points are ordered
>     according to the way indices are exhausted. As such these iteration
>     lattices drive actual sparse code generation, which consists of a
>     relatively straightforward one-to-one mapping from iteration lattices
>     to combinations of for-loops, while-loops, and if-statements. Sparse
>     tensor outputs that materialize uninitialized are handled with direct
>     insertions if all parallel loops are outermost or insertions that
>     indirectly go through a 1-dimensional access pattern expansion
87a86
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/SparseTensor/IR/CMakeLists.txt
--- include/mlir/Dialect/SparseTensor/IR/SparseTensorTypes.td
40,44c40,44
<     Values with storage_specifier types represent aggregated storage scheme
<     metadata for the given sparse tensor encoding.  It currently holds
<     a set of values for level-sizes, coordinate arrays, position arrays,
<     and value array.  Note that the type is not yet stable and subject to
<     change in the near future.
---
>     Values with storage_specifier types represent aggregated storage scheme metadata
>     for the given sparse tensor encoding.
>     It currently holds a set of values for sizes of sparse tensor dimension, index array,
>     pointer array and value array.
>     Note that the type is not yet stable and subject to change in the near future.
66a67,73
> 
>   let extraClassDeclaration = [{
>     // Get the integer type used to store memory and dimension sizes.
>     IntegerType getSizesType() const;
>     Type getFieldType(StorageSpecifierKind kind, std::optional<unsigned> dim) const;
>     Type getFieldType(StorageSpecifierKind kind, std::optional<APInt> dim) const;
>   }];
--- include/mlir/Dialect/SparseTensor/IR/SparseTensorOps.td
30c30
<     Arguments<(ins AnyType:$source)>,
---
>     Arguments<(ins AnyType:$source, UnitAttr:$expandSymmetry)>,
43,45c43,48
<     Reading in a symmetric matrix will result in just the lower/upper triangular
<     part of the matrix (so that only relevant information is stored). Proper
<     symmetry support for operating on symmetric matrices is still TBD.
---
>     An optional attribute `expandSymmetry` can be used to extend this operation
>     to make symmetry in external formats explicit in the storage. That is, when
>     the attribute presents and a non-zero value is discovered at (i, j) where
>     i!=j, we add the same value to (j, i). This claims more storage than a pure
>     symmetric storage, and thus may cause a bad performance hit. True symmetric
>     storage is planned for the future.
53,167c56,57
<   let assemblyFormat = "$source attr-dict `:` type($source) `to` type($result)";
< }
< 
< def SparseTensor_PackOp : SparseTensor_Op<"pack", [Pure]>,
<     Arguments<(ins TensorOf<[AnyType]>:$values,
<                    TensorOf<[AnySignlessIntegerOrIndex]>:$coordinates,
<                    OptionalAttr<IndexAttr>:$batched_lvls)>,
<     Results<(outs AnySparseTensor: $result)> {
<   let summary = "Returns a sparse tensor from the given (values, coordinates) pair";
< 
<   let description = [{
<     Packs the values/coordinates into a COO sparse tensor.  The length
<     of `values` must match the outer-length of `coordinates`, since these
<     two tensors are "zipped" together.  The `coordinates` argument provides
<     level-coords for each value, therefore, the inner-length of `coordinates`
<     must match the level-rank of the returned tensor, and each level-coords
<     must be valid for the level-sizes of the returned tensor.  Note that
<     the returned tensor must be statically shaped because it is impossible
<     to infer the dimension-shape from level-coordinates alone.
< 
<     TODO: The returned tensor is allowed (in principle) to have non-identity
<     dimOrdering/higherOrdering mappings.  However, the current implementation
<     does not yet support them.
< 
<     - `coordinates : tensor<NSE x lvlRank x iType>`
<       supplies the level-coords for each element in `values`.
<     - `values : tensor<NSE x V>`
<       supplies the corresponding values for each entry in `coordinates`.
<     - `batched_lvls : optional<index>`
<       supplies the number of leading levels that are batched.
< 
<     This operation can be used to materialize a sparse tensor from external
<     sources; e.g., when passing two numpy arrays from Python.
< 
<     Example:
< 
<     ```mlir
<     %values      = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
<     %coordinates = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
<     %st = sparse_tensor.pack %values, %coordinates
<         : tensor<3xf64>, tensor<3x2xindex> to tensor<3x4xf64, #COO>
<     // yields COO format |1.1, 0.0, 0.0, 0.0|
<     //     of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
<     //                   |0.0, 0.0, 0.0, 0.0|
<     ```
< 
<     If `batched_lvls` is provided, the operation materializes a batched sparse tensor.
<     Example:
< 
<     ```mlir
<     %values      = arith.constant dense<[[ 1.1,   2.2,   3.3 ],
<                                          [ 1.2,   2.3,   0.0 ]]> : tensor<2x3xf64>
<     %coordinates = arith.constant dense<[[ [0],   [1],   [2] ],
<                                          [ [1],   [2],   [3] ]> : tensor<2x3x1xindex>
<     %st = sparse_tensor.pack %values, %coordinates batched_lvls=1
<         : tensor<2x3xf64>, tensor<2x3x1xindex> to tensor<2x4xf64, #BCOO>
<     // yields BCOO format |1.1, 2.2, 3.3, 0.0|
<     //      of 2x4 matrix |0.0, 1.2, 2.3, 0.0|
<     ```
<   }];
< 
<   let extraClassDeclaration = [{
<     /// Returns the number of leading levels that are batched.
<     unsigned getNumBatchedLvls();
<   }];
< 
<   let assemblyFormat =
<     "$values `,` $coordinates (`batched_lvls` `=` $batched_lvls^)? attr-dict"
<     "`:` type($values) `,` type($coordinates) `to` type($result)";
< 
<   let hasVerifier = 1;
< }
< 
< def SparseTensor_UnpackOp : SparseTensor_Op<"unpack">,
<     Arguments<(ins AnySparseTensor:$tensor)>,
<     Results<(outs 1DTensorOf<[AnyType]>:$values,
<                   2DTensorOf<[AnySignlessIntegerOrIndex]>:$coordinates,
<                   AnySignlessIntegerOrIndex:$nse)> {
<   let summary = "Returns the (values, coordinates) pair unpacked from the input tensor";
< 
<   let description = [{
<     The unpack operation is the inverse of `sparse_tensor::pack`.  It returns
<     the values, level-coordinates, and number-of-stored-entries extracted
<     from the sparse tensor.  The source tensor is allowed (in principle)
<     to have non-identity dimOrdering/higherOrdering mappings.  Regardless
<     of the mappings, the returned `coordinates` are always level-coordinates,
<     because this is what we mean by "unpacking" as opposed to other forms
<     of exposing sparse tensors to external clients.  This operation can be
<     used for returning an unpacked MLIR sparse tensor to frontend; e.g.,
<     returning two numpy arrays to Python.
< 
<     TODO: the current implementation does not yet support non-identity mappings.
< 
<     This operation ends the lifetime of the sparse tensor, and using
<     the tensor after the unpack is undefined behavior.
< 
<     Example:
< 
<     ```mlir
<     // input COO format |1.1, 0.0, 0.0, 0.0|
<     //    of 3x4 matrix |0.0, 0.0, 2.2, 3.3|
<     //                  |0.0, 0.0, 0.0, 0.0|
<     %values, %coordinates, %nse
<       = sparse_tensor.unpack %st
<       : tensor<3x4xf64, #COO> to tensor<2xf64>, tensor<2x2xindex>, index
<     // %values      = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>
<     // %coordinates = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>
<     // %nse = 3
<     ```
<   }];
< 
<   let assemblyFormat =
<     "$tensor attr-dict `:` type($tensor)"
<     "`to` type($values) `,` type($coordinates) `,` type($nse)";
< 
---
>   let assemblyFormat = "(`expand_symmetry` $expandSymmetry^)? $source attr-dict"
>                        "`:` type($source) `to` type($result)";
172c62
<   [Pure]>,
---
>   [Pure, SameOperandsAndResultElementType]>,
219,220c109,110
< def SparseTensor_ToPositionsOp : SparseTensor_Op<"positions", [Pure]>,
<     Arguments<(ins AnySparseTensor:$tensor, LevelAttr:$level)>,
---
> def SparseTensor_ToPointersOp : SparseTensor_Op<"pointers", [Pure]>,
>     Arguments<(ins AnySparseTensor:$tensor, IndexAttr:$dimension)>,
222c112
<   let summary = "Extracts the `level`-th positions array of the `tensor`";
---
>   let summary = "Extracts pointers array at given dimension from a tensor";
224,232c114,120
<     Returns the positions array of the tensor's storage at the given
<     level.  This is similar to the `bufferization.to_memref` operation
<     in the sense that it provides a bridge between a tensor world view
<     and a bufferized world view.  Unlike the `bufferization.to_memref`
<     operation, however, this sparse operation actually lowers into code
<     that extracts the positions array from the sparse storage itself
<     (either by calling a support library or through direct code).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the pointers array of the sparse storage format at the
>     given dimension for the given sparse tensor. This is similar to the
>     `bufferization.to_memref` operation in the sense that it provides a bridge
>     between a tensor world view and a bufferized world view. Unlike the
>     `bufferization.to_memref` operation, however, this sparse operation actually
>     lowers into code that extracts the pointers array from the sparse storage
>     scheme (either by calling a support library or through direct code).
237c125
<     %1 = sparse_tensor.positions %0 { level = 1 : index }
---
>     %1 = sparse_tensor.pointers %0 { dimension = 1 : index }
245,246c133,134
< def SparseTensor_ToCoordinatesOp : SparseTensor_Op<"coordinates", [Pure]>,
<     Arguments<(ins AnySparseTensor:$tensor, LevelAttr:$level)>,
---
> def SparseTensor_ToIndicesOp : SparseTensor_Op<"indices", [Pure]>,
>     Arguments<(ins AnySparseTensor:$tensor, IndexAttr:$dimension)>,
248c136
<   let summary = "Extracts the `level`-th coordinates array of the `tensor`";
---
>   let summary = "Extracts indices array at given dimension from a tensor";
250,258c138,144
<     Returns the coordinates array of the tensor's storage at the given
<     level.  This is similar to the `bufferization.to_memref` operation
<     in the sense that it provides a bridge between a tensor world view
<     and a bufferized world view.  Unlike the `bufferization.to_memref`
<     operation, however, this sparse operation actually lowers into code
<     that extracts the coordinates array from the sparse storage itself
<     (either by calling a support library or through direct code).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the indices array of the sparse storage format at the
>     given dimension for the given sparse tensor. This is similar to the
>     `bufferization.to_memref` operation in the sense that it provides a bridge
>     between a tensor world view and a bufferized world view. Unlike the
>     `bufferization.to_memref` operation, however, this sparse operation actually
>     lowers into code that extracts the indices array from the sparse storage
>     scheme (either by calling a support library or through direct code).
263c149
<     %1 = sparse_tensor.coordinates %0 { level = 1 : index }
---
>     %1 = sparse_tensor.indices %0 { dimension = 1 : index }
271c157
< def SparseTensor_ToCoordinatesBufferOp : SparseTensor_Op<"coordinates_buffer", [Pure]>,
---
> def SparseTensor_ToIndicesBufferOp : SparseTensor_Op<"indices_buffer", [Pure]>,
274c160
<   let summary = "Extracts the linear coordinates array from a tensor";
---
>   let summary = "Extracts the linear indices array from a tensor";
276,289c162,172
<     Returns the linear coordinates array for a sparse tensor with
<     a trailing COO region with at least two levels.  It is an error
<     if the tensor doesn't contain such a COO region.  This is similar
<     to the `bufferization.to_memref` operation in the sense that it
<     provides a bridge between a tensor world view and a bufferized
<     world view.  Unlike the `bufferization.to_memref` operation,
<     however, this operation actually lowers into code that extracts
<     the linear coordinates array from the sparse storage scheme that
<     stores the coordinates for the COO region as an array of structures.
<     For example, a 2D COO sparse tensor with two non-zero elements at
<     coordinates (1, 3) and (4, 6) are stored in a linear buffer as
<     (1, 4, 3, 6) instead of two buffer as (1, 4) and (3, 6).
< 
<     Writing into the result of this operation is undefined behavior.
---
>     Returns the linear indices array for a sparse tensor with a trailing COO
>     region with at least two dimensions. It is an error if the tensor doesn't
>     contain such a COO region. This is similar to the `bufferization.to_memref`
>     operation in the sense that it provides a bridge between a tensor world view
>     and a bufferized world view. Unlike the `bufferization.to_memref` operation,
>     however, this sparse operation actually lowers into code that extracts the
>     linear indices array from the sparse storage scheme that stores the indices
>     for the COO region as an array of structures. For example, a 2D COO sparse
>     tensor with two non-zero elements at coordinates (1, 3) and (4, 6) are
>     stored in a linear buffer as (1, 4, 3, 6) instead of two buffer as (1, 4)
>     and (3, 6).
294,295c177,178
<     %1 = sparse_tensor.coordinates_buffer %0
<        : tensor<64x64xf64, #COO> to memref<?xindex>
---
>     %1 = sparse_tensor.indices_buffer %0
>     : tensor<64x64xf64, #COO> to memref<?xindex>
315,316d197
<     Writing into the result of this operation is undefined behavior.
< 
327,381d207
< def SparseTensor_ToSliceOffsetOp : SparseTensor_Op<"slice.offset", [Pure]>,
<     Arguments<(ins AnySparseTensorSlice:$slice, IndexAttr:$dim)>,
<     Results<(outs Index:$offset)> {
<   let summary = "Extracts the offset of the sparse tensor slice at the given dimension";
<   let description = [{
<     Extracts the offset of the sparse tensor slice at the given dimension.
< 
<     Currently, sparse tensor slices are still a work in progress, and only
<     works when runtime library is disabled (i.e., running sparse compiler
<     with `enable-runtime-library=false`).
< 
<     Example:
< 
<     ```mlir
<     %0 = tensor.extract_slice %s[%v1, %v2][64, 64][1, 1] : tensor<128x128xf64, #DCSR>
<                                                         to tensor<64x64xf64, #Slice>
< 
<     %1 = sparse_tensor.slice.offset %0 at 0 : tensor<64x64xf64, #Slice>
<     %2 = sparse_tensor.slice.offset %0 at 1 : tensor<64x64xf64, #Slice>
<     // %1 = %v1
<     // %2 = %v2
<     ```
<   }];
<   let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";
<   let hasVerifier = 1;
< }
< 
< def SparseTensor_ToSliceStrideOp : SparseTensor_Op<"slice.stride", [Pure]>,
<     Arguments<(ins AnySparseTensorSlice:$slice, IndexAttr:$dim)>,
<     Results<(outs Index:$stride)> {
<   let summary = "Extracts the stride of the sparse tensor slice at the given dimension";
<   let description = [{
<     Extracts the stride of the sparse tensor slice at the given dimension.
< 
<     Currently, sparse tensor slices are still a work in progress, and only
<     works when runtime library is disabled (i.e., running sparse compiler
<     with `enable-runtime-library=false`).
< 
<     Example:
< 
<     ```mlir
<     %0 = tensor.extract_slice %s[%v1, %v2][64, 64][%s1, %s2] : tensor<128x128xf64, #DCSR>
<                                                             to tensor<64x64xf64, #Slice>
< 
<     %1 = sparse_tensor.slice.stride %0 at 0 : tensor<64x64xf64, #Slice>
<     %2 = sparse_tensor.slice.stride %0 at 1 : tensor<64x64xf64, #Slice>
<     // %1 = %s1
<     // %2 = %s2
< 
<     ```
<   }];
<   let assemblyFormat = "$slice `at` $dim attr-dict `:` type($slice)";
<   let hasVerifier = 1;
< }
< 
383d208
<     Arguments<(ins Optional<SparseTensorStorageSpecifier>:$source)>,
387,394c212,213
<     Returns an initial storage specifier value.  A storage specifier
<     value holds the level-sizes, position arrays, coordinate arrays,
<     and the value array.
<     If this is a specifier for slices, it also holds the extra strides/offsets
<     for each tensor dimension.
< 
<     TODO: The sparse tensor slice support is currently in a unstable state, and
<     is subject to change in the future.
---
>     Returns an initial storage specifier value. A storage specifier value holds
>     the sizes for tensor dimensions, pointer arrays, index arrays, and the value array.
399,404d217
<     #CSR = #sparse_tensor.encoding<{ dimLevelType = [ "dense", "compressed" ]}>
<     #CSR_SLICE = #sparse_tensor.encoding<{
<       dimLevelType = [ "dense", "compressed" ],
<       slice = [ (1, 4, 1), (1, 4, 2) ]
<     }>
< 
406,408d218
<     %1 = sparse_tensor.storage_specifier.init with %src
<          : !sparse_tensor.storage_specifier<#CSR> to
<            !sparse_tensor.storage_specifier<#CSR_SLICE>
412,420c222
<   let builders = [
<     OpBuilder<(ins "Type":$result),
<     [{
<       build($_builder, $_state, result, Value());
<     }]>
<   ];
< 
<   let assemblyFormat = "attr-dict (`with` $source^)? `:` (`from` qualified(type($source))^ `to`)?"
<                                                         " qualified(type($result))";
---
>   let assemblyFormat = "attr-dict `:` qualified(type($result))";
426,427c228,229
<                    OptionalAttr<LevelAttr>:$level)>,
<     Results<(outs Index:$result)> {
---
>                    OptionalAttr<IndexAttr>:$dim)>,
>     Results<(outs AnyType:$result)> {
432c234
<     Example of querying the size of the coordinates array for level 0:
---
>     Example of querying the size of the index array for level 0:
435,436c237,238
<     %0 = sparse_tensor.storage_specifier.get %arg0 crd_mem_sz at 0
<          : !sparse_tensor.storage_specifier<#COO>
---
>     %0 = sparse_tensor.storage_specifier.get %arg0 idx_mem_sz at 0
>          : !sparse_tensor.storage_specifier<#COO> to i64
440,441c242,243
<   let assemblyFormat = "$specifier $specifierKind (`at` $level^)? attr-dict"
<                        "`:` qualified(type($specifier))";
---
>   let assemblyFormat = "$specifier $specifierKind (`at` $dim^)? attr-dict `:` "
>                        "qualified(type($specifier)) `to` type($result)";
450,451c252,253
<                    OptionalAttr<LevelAttr>:$level,
<                    Index:$value)>,
---
>                    OptionalAttr<IndexAttr>:$dim,
>                    AnyType:$value)>,
458c260
<     Example of updating the sizes of the coordinates array for level 0:
---
>     Example of updating the sizes of the index array for level 0:
461,462c263,265
<     %0 = sparse_tensor.storage_specifier.set %arg0 crd_mem_sz at 0 with %new_sz
<        : !sparse_tensor.storage_specifier<#COO>
---
>     %0 = sparse_tensor.storage_specifier.set %arg0 idx_mem_sz at 0 with %new_sz
>        : i32, !sparse_tensor.storage_specifier<#COO>
> 
465,466c268,269
<   let assemblyFormat = "$specifier $specifierKind (`at` $level^)? `with` $value"
<                        " attr-dict `:` qualified(type($result))";
---
>   let assemblyFormat = "$specifier $specifierKind (`at` $dim^)? `with` $value attr-dict `:` "
>                        "type($value) `,` qualified(type($result))";
490c293
<     Arguments<(ins Variadic<AnyRankedTensor>:$inputs, DimensionAttr:$dimension)>,
---
>     Arguments<(ins Variadic<AnyRankedTensor>:$inputs, IndexAttr:$dimension)>,
495,499c298,302
<      Concatenates a list input tensors and the output tensor with the same
<      dimension-rank.  The concatenation happens on the specified `dimension`
<      (0 <= dimension < dimRank).  The resulting `dimension` size is the
<      sum of all the input sizes for that dimension, while all the other
<      dimensions should have the same size in the input and output tensors.
---
>      Concatenates a list input tensors and the output tensor with the same rank.
>      The concatenation happens on the specified `dimension` (0<= dimension < rank).
>      The resulting `dimension` size is the sum of all the input dimension sizes,
>      while all the other dimensions should have the same size in the input and
>      output tensors.
531c334
<                Variadic<Index>:$lvlCoords)>,
---
>                Variadic<Index>:$indices)>,
533c336
<   string summary = "Inserts a value into the sparse tensor";
---
>   string summary = "Inserts a value into given sparse tensor";
535,540c338,344
<     Inserts the value into the underlying storage of the tensor at the
<     given level-coordinates.  The arity of `lvlCoords` must match the
<     level-rank of the tensor.  This operation can only be applied when
<     the tensor materializes unintialized from a `bufferization.alloc_tensor`
<     operation and the final tensor is constructed with a `load` operation
<     which has the `hasInserts` attribute set.
---
>     Inserts the given value at given indices into the underlying
>     sparse storage format of the given tensor with the given indices.
>     The arity of indices must match the rank of the tensor. This
>     operation can only be applied when a tensor materializes unintialized
>     with a `bufferization.alloc_tensor` operation and the final tensor
>     is constructed with a `load` operation that has the `hasInserts`
>     attribute set.
542,543c346,347
<     The level-properties of the sparse tensor type fully describe what
<     kind of insertion order is allowed.  When all levels have "unique"
---
>     Properties in the sparse tensor type fully describe what kind
>     of insertion order is allowed. When all dimensions have "unique"
545,547c349,351
<     strict lexicographical level-coordinate order.  Other properties
<     define different insertion regimens.  Inserting in a way contrary
<     to these properties results in undefined behavior.
---
>     strict lexicographical index order. Other properties define
>     different insertion regimens. Inserting in a way contrary to
>     these properties results in undefined behavior.
561,562c365
<   let assemblyFormat = "$value `into` $tensor `[` $lvlCoords `]` attr-dict"
<                        "`:` type($tensor)";
---
>   let assemblyFormat = "$value `into` $tensor `[` $indices `]` attr-dict `:` type($tensor)";
641c444
<     Performs an access pattern expansion for the innermost levels of the
---
>     Performs an access pattern expansion for the innermost dimensions of the
649,659c452,460
<     The `values` and `filled` arrays must have lengths equal to the
<     level-size of the innermost level (i.e., as if the innermost level
<     were *dense*).  The `added` array and `count` are used to store new
<     level-coordinates when a false value is encountered in the `filled`
<     array.  All arrays should be allocated before the loop (possibly even
<     shared between loops in a future optimization) so that their *dense*
<     initialization can be amortized over many iterations.  Setting and
<     resetting the dense arrays in the loop nest itself is kept *sparse*
<     by only iterating over set elements through an indirection using
<     the added array, so that the operations are kept proportional to
<     the number of nonzeros.
---
>     The values and filled array have sizes that suffice for a *dense* innermost
>     dimension (e.g. a full row for matrices). The added array and count are used
>     to store new indices when a false value is encountered in the filled array.
>     All arrays should be allocated before the loop (possibly even shared between
>     loops in a future optimization) so that their *dense* initialization can be
>     amortized over many iterations. Setting and resetting the dense arrays in
>     the loop nest itself is kept *sparse* by only iterating over set elements
>     through an indirection using the added array, so that the operations are
>     kept proportional to the number of nonzeros.
683c484
<                    Variadic<Index>:$lvlCoords)>,
---
>                    Variadic<Index>:$indices)>,
689,695c490,495
<     level-coordinates.  The arity of `lvlCoords` is one less than the
<     level-rank of the tensor, with the coordinate of the innermost
<     level defined through the `added` array.  The `values` and `filled`
<     arrays are reset in a *sparse* fashion by only iterating over set
<     elements through an indirection using the `added` array, so that
<     the operations are kept proportional to the number of nonzeros.
<     See the `sparse_tensor.expand` operation for more details.
---
>     indices. The arity of indices is one less than the rank of the tensor,
>     with the remainder innermost indices defined through the added array.
>     The values and filled array are reset in a *sparse* fashion by only
>     iterating over set elements through an indirection using the added
>     array, so that the operations are kept proportional to the number of
>     nonzeros. See the `sparse_tensor.expand` operation for more details.
709c509
<                        " `into` $tensor `[` $lvlCoords `]` attr-dict"
---
>                        " `into` $tensor `[` $indices `]` attr-dict"
776,780d575
<     //
<     // TODO: Currently tablegen doesn't support the assembly syntax when
<     // `algorithm` is an optional enum attribute. We may want to use an optional
<     // enum attribute when this is fixed in tablegen.
<     //
784c579
<                SparseTensorSortKindAttr:$algorithm)>  {
---
>                UnitAttr:$stable)>  {
806,808c601,602
<     The enum attribute `algorithm` indicates the sorting algorithm used to
<     implement the operator: hybrid_quick_sort, insertion_sort_stable,
<     quick_sort, or heap_sort.
---
>     The `stable` attribute indicates whether a stable sorting algorithm should
>     be used to implement the operator.
816c610
<     sparse_tensor.sort insertion_sort_stable %n, %x1, %x2 jointly y1, %y2
---
>     sparse_tensor.sort %n, %x1, %x2 jointly y1, %y2
821,822c615
<     sparse_tensor.sort hybrid_quick_sort %n, %x1, %x2 jointly y1, %y2
<       { alg=1 : index}
---
>     sparse_tensor.sort stable %n, %x1, %x2 jointly y1, %y2
826c619,620
<   let assemblyFormat = "$algorithm $n `,` $xs (`jointly` $ys^)? attr-dict"
---
>   let assemblyFormat = "(`stable` $stable^)? $n"
>                        "`,`$xs (`jointly` $ys^)? attr-dict"
835c629
<                SparseTensorSortKindAttr:$algorithm)>  {
---
>                UnitAttr:$stable)>  {
854c648
<     sparse_tensor.sort_coo insertion_sort_stable %n, %x { nx = 2 : index}
---
>     sparse_tensor.sort_coo %n, %x { nx = 2 : index}
859,860c653
<     sparse_tensor.sort hybrid_quick_sort %n, %xy jointly %y1
<       { nx = 2 : index, ny = 2 : index}
---
>     sparse_tensor.sort %n, %xy jointly %y1 { nx = 2 : index, ny = 2 : index}
865c658
<   let assemblyFormat = "$algorithm $n"
---
>   let assemblyFormat = "(`stable` $stable^)? $n"
1211,1212c1004
<                     Variadic<AnyType>:$initArgs,
<                     OptionalAttr<AffineMapAttr>:$order)>,
---
>                     Variadic<AnyType>:$initArgs)>,
1219,1229c1011,1016
<      `tensor`: the input tensor to iterate over.
<      `initArgs`: the initial loop argument to carry and update during each iteration.
<      `order`: an optional permutation affine map that specifies the order in which
<      the dimensions are visited (e.g., row first or column first). This is only
<      applicable when the input tensor is a non-annotated dense tensor.
< 
<      For an input tensor with dim-rank `n`, the block must take `n + 1`
<      arguments (plus additional loop-carried variables as described below).
<      The first `n` arguments provide the dimension-coordinates of the element
<      being visited, and must all have `index` type.  The `(n+1)`-th argument
<      provides the element's value, and must have the tensor's element type.
---
>      For an input tensor with rank n, the block must take n + 1 (and additional loop
>      carried variables as described below) arguments. The first n arguments must be
>      Index type, together indicating the current coordinates of the element being visited.
>      The last argument must have the same type as the
>      tensor's element type, representing the actual value loaded from the input
>      tensor at the given coordinates.
1234c1021
<      SSA values mentioned above (n coordinates and 1 value).
---
>      SSA values mentioned above (n coordinate and 1 value).
1251,1254c1038,1040
<      It is important to note that the generated loop iterates over
<      elements in their storage order.  However, regardless of the
<      storage scheme used by the tensor, the block is always given
<      the dimension-coordinates.
---
>      It is important to note that foreach generated loop iterates over the stored elements
>      in the storage order. However, no matter what storage order is used, the indices passed
>      to the block always obey the original dimension order.
1279,1284d1064
<      // foreach on a row-major dense tensor but visit column first
<      sparse_tensor.foreach in %0 {order=affine_map<(i,j)->(j,i)>}: tensor<2x3xf64> do {
<       ^bb0(%row: index, %col: index, %arg3: f64):
<          // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]
<      }
< 
1289c1069
<     OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs, "AffineMapAttr":$order,
---
>     OpBuilder<(ins "Value":$tensor,
1291,1295d1070
<     OpBuilder<(ins "Value":$tensor, "AffineMapAttr":$order,
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, ValueRange(), order, bodyBuilder);
<     }]>,
1297,1305c1072,1073
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, ValueRange(), nullptr, bodyBuilder);
<     }]>,
<     OpBuilder<(ins "Value":$tensor, "ValueRange":$iterArgs,
<       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>":$bodyBuilder),
<     [{
<       build($_builder, $_state, tensor, iterArgs, nullptr, bodyBuilder);
<     }]>
---
>       "ValueRange":$iterArgs,
>       "function_ref<void(OpBuilder &, Location, ValueRange, Value, ValueRange)>")>
--- include/mlir/Dialect/SparseTensor/IR/SparseTensorAttrDefs.td
23,57d22
< // Type aliases.
< //
< // These attributes are just like `IndexAttr` (include/mlir/IR/OpBase.td),
< // except that:
< // (1) the `summary` is more specific (i.e., the fourth parameter to
< //     `TypedAttrBase`), which helps tablegen provide better error messages.
< // (2) tablegen-generated getters will have the given `returnType`, in
< //     lieu of the `APInt` that `IndexAttr` uses.  This avoids the boilerplate
< //     of needing to say `get{FOO}().getZExtValue()`, as well as using
< //     C++ types which better document intent.
< //===----------------------------------------------------------------------===//
< 
< def DimensionAttr :
<     TypedAttrBase<
<       Index, "IntegerAttr",
<       And<[CPred<"$_self.isa<::mlir::IntegerAttr>()">,
<            CPred<"$_self.cast<::mlir::IntegerAttr>().getType()"
<                  ".isa<::mlir::IndexType>()">]>,
<       "dimension attribute"> {
<   let returnType = [{::mlir::sparse_tensor::Dimension}];
<   let convertFromStorage = [{$_self.getValue().getZExtValue()}];
< }
< 
< def LevelAttr :
<     TypedAttrBase<
<       Index, "IntegerAttr",
<       And<[CPred<"$_self.isa<::mlir::IntegerAttr>()">,
<            CPred<"$_self.cast<::mlir::IntegerAttr>().getType()"
<                  ".isa<::mlir::IndexType>()">]>,
<       "level attribute"> {
<   let returnType = [{::mlir::sparse_tensor::Level}];
<   let convertFromStorage = [{$_self.getValue().getZExtValue()}];
< }
< 
< //===----------------------------------------------------------------------===//
130,139c95,105
<     - Level-type for each level of a tensor type:
<         - **dense** : all entries along this level are stored.
<         - **compressed** : only nonzeros along this level are stored.
<         - **singleton** : a variant of the compressed level-format,
<           for when coordinates are guaranteed to have no siblings at this level.
<       By default, each level-type has the property of being unique (no
<       duplicates at that level) and ordered (coordinates appear sorted
<       at that level).  The following two suffixes can be used to specify
<       that the level should instead be non-unique (duplicates may appear)
<       and/or non-ordered (coordinates may appear unsorted).
---
>     - Dimension level type for each dimension of a tensor type:
>         - **dense** : dimension is dense, all entries along this dimension
>           are stored
>         - **compressed** : dimension is sparse, only nonzeros along this dimensions
>           are stored
>         - **singleton** : dimension stores individual indices with no siblings
>       By default, each dimension level types has the property of being unique
>       (no duplicates at that level) and ordered (indices appear sorted at that
>       level). The following two suffixes can be used to make the last two
>       dimension level types not-unique (duplicates may appear) and not-ordered
>       (indices may appear unsorted).
142,206c108,158
<       Currently, these suffixes (if present) must appear in this order.
<       In the future, we may introduce additional level-types and
<       properties, and split up how the level-format and properties are
<       specified rather than using this suffix mechanism.
< 
<       TODO: This field is called "dimLevelType" for historical reasons,
<       even though the types are per-level rather than per-dimension.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<     - An optional permutation which maps (higher-ordering)-coordinates
<       to level-coordinates; defaulting to the identity permutation.
<       For example, given a 2-d tensor with the default higher-ordering,
<       `(i, j) -> (i, j)` specifies row-wise storage and `(i, j) ->
<       (j, i)` specifies column-wise storage.
< 
<       TODO: this field is called "dimOrdering" for historical reasons,
<       even though it actually operates on level-coordinates rather than
<       dimension-coordinates.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<     - An optional higher-order mapping from dimension-coordinates to
<       a higher-order coordinate space; defaulting to the identity map.
<       This is applied before the `dimOrdering`, thus we have the composite:
<       dimCoords --higherOrdering--> hoCoords --dimOrdering--> lvlCoords.
<       The higher-order mapping is used to define block-sparse storage,
<       jagged-diagonal (JDS/ELL/ITPACK) storage, etc.
< 
<       For example, given a 2-d tensor, the mapping
<       `(i, j) -> (i floordiv 2, j floordiv 3, i mod 2, j mod 3)`
<       imposes an higher-order partitioning into 2x3 blocks along the
<       matrix layout.  For block-sparsity, blocks are typically stored
<       with compression while dense storage is used within each block
<       (although hybrid schemes are possible as well).
< 
<       TODO: the following example is out-of-date and will be implemented
<       in a different manner than described here.
<       (This will be corrected in an upcoming change that completely
<       overhauls the syntax of this attribute.)
< 
<       The higher-order mapping also provides a notion of "counting a
<       dimension", where every stored element with the same coordinate
<       is mapped to a new slice.  For instance, ELL storage of a 2-d
<       tensor can be defined with the mapping `(i, j) -> (#i, i, j)`
<       using the notation of [Chou20].  Lacking the `#` symbol in MLIR's
<       affine mapping, we use a free symbol `c` to define such counting,
<       together with a constant that denotes the number of resulting
<       slices.  For example, the mapping `(i, j)[c] -> (c * 3 * i, i, j)`
<       with the level-types `["dense", "dense", "compressed"]` denotes ELL
<       storage with three jagged diagonals that count the dimension `i`.
< 
<     - The required bitwidth for "position" storage (integral offsets
<       into the sparse storage scheme).  A narrow width reduces the memory
<       footprint of overhead storage, as long as the width suffices to
<       define the total required range (viz. the maximum number of stored
<       entries over all indirection levels).  The choices are `8`, `16`,
<       `32`, `64`, or, the default, `0` to indicate the native bitwidth.
< 
<     - The required bitwidth for "coordinate" storage (the coordinates
<       of stored entries).  A narrow width reduces the memory footprint
<       of overhead storage, as long as the width suffices to define
<       the total required range (viz. the maximum value of each tensor
<       coordinate over all levels).  The choices are `8`, `16`, `32`,
<       `64`, or, the default, `0` to indicate a native bitwidth.
---
>       Currently, these suffixes, is present, should appear in this order.
>       In the future, we may introduce many more dimension level types and
>       properties, and separate specifying the two completely rather than
>       using this suffix mechanism.
> 
>     - An optional dimension ordering on the indices of this tensor type. Unlike
>       dense storage, most sparse storage schemes do not provide fast random
>       access.  This affine map specifies the order of dimensions that should be
>       supported by the sparse storage scheme. For example, for a 2-d tensor,
>       `(i, j) -> (i, j)` requests row-wise storage and `(i, j) -> (j, i)`
>       requests column-wise storage.  By default, an identify mapping is used,
>       which implies that the original indices directly correspond to stored
>       indices.
> 
>     - An optional higher-ordering mapping from the original index space of
>       the tensor to a higher-order index space, used to define block-sparse
>       storage or ELL (jagged diagonal) storage. For example, for a 2-d tensor,
>       the mapping `(i, j) -> (i floordiv 2, j floordiv 3, i mod 2, j mod 3)`
>       imposes an higher-order partitioning into 2x3 blocks along the matrix
>       layout. A dimension ordering can be used to define a desired ordering
>       on this higher-order index space. Likewise, the dimension level types
>       define dense or compressed storage along this higher-order index space.
>       For block-sparse, blocks are typically stored with compression while
>       dense storage is used within each block (although hybrid schemes are
>       possible as well). The higher-order mapping also provides a notion of
>       "counting a dimension", where every stored element with the same index
>       is mapped to a new slice. For instance, ELL storage of a 2-d tensor can
>       be defined with the mapping `(i, j) -> (#i, i, j)` using the notation
>       of [Chou20]. Lacking the `#` symbol in MLIR's affine mapping, we use
>       a free symbol `c` to define such counting, together with a constant
>       that denotes the number of resulting slices. For example, the mapping
>       `(i, j)[c] -> (c * 3 * i, i, j)` with the first two higher-order indices
>       stored dense and the innermost compressed denotes ELL storage with
>       three jagged diagonals that count the dimension `i`.
> 
>       TODO: introduce a real counting symbol to MLIR's mapping, since an
>             expression like 3*c*i has no direct interpretation?
> 
>     - The required bit width for "pointer" storage (integral offsets into
>       the sparse storage scheme). A narrow width reduces the memory footprint
>       of overhead storage, as long as the width suffices to define the total
>       required range (viz. the maximum number of stored entries over all indirection
>       dimensions). The choices are `8`, `16`, `32`, `64`, or, the default, `0` to
>       indicate the native bit width.
> 
>     - The required bit width for "index" storage (elements of the coordinates of
>       stored entries). A narrow width reduces the memory footprint of overhead
>       storage, as long as the width suffices to define the total required range
>       (viz. the maximum value of each tensor index over all dimensions). The
>       choices are `8`, `16`, `32`, `64`, or, the default, `0` to indicate a
>       native bit width.
208,209c160,161
<     - An optional array of `SparseTensorDimSliceAttr`, which specifies
<       how the sparse tensor is partitioned on each dimension.
---
>     - An optional array of SparseTensorDimSliceAttr, which specifies how the sparse
>       tensor is partitioned on each level.
230,231c182,183
<       posWidth = 32,
<       crdWidth = 8
---
>       pointerBitWidth = 32,
>       indexBitWidth = 8
265c217
<     // A level-type for each level of the sparse storage.
---
>     // A dimension level type for each dimension of the tensor type.
268c220
<       "level-types"
---
>       "per dimension level type"
270c222
<     // A permutation from (higher-ordering)-coordinates to level-coordinates.
---
>     // A dimension order on the indices of this tensor type.
272c224
<     // A mapping from dimension-coordinates to (higher-ordering)-coordinates.
---
>     // A mapping between the original and higher-ordering index space.
274,278c226,230
<     // The required bitwidth for position storage.
<     "unsigned":$posWidth,
<     // The required bitwidth for coordinate storage.
<     "unsigned":$crdWidth,
<     // A slice attribute for each dimension of the tensor type.
---
>     // The required bit width for pointer storage.
>     "unsigned":$pointerBitWidth,
>     // The required bit width for index storage.
>     "unsigned":$indexBitWidth,
>     // A dimension level type for each dimension of the tensor type.
289,290c241,242
<                      "unsigned":$posWidth,
<                      "unsigned":$crdWidth), [{
---
>                      "unsigned":$pointerBitWidth,
>                      "unsigned":$indexBitWidth), [{
294,295c246,247
<                          posWidth,
<                          crdWidth,
---
>                          pointerBitWidth,
>                          indexBitWidth,
301,302c253,254
<     /// Returns the type for position storage based on posWidth
<     Type getPosType() const;
---
>     /// Returns the type for pointer storage based on pointerBitWidth
>     Type getPointerType() const;
304,305c256,257
<     /// Returns the type for coordinate storage based on crdWidth
<     Type getCrdType() const;
---
>     /// Returns the type for index storage based on indexBitWidth
>     Type getIndexType() const;
311,316c263
<     /// Constructs a new encoding with the pointer and index bitwidth
<     /// reset to the default.
<     SparseTensorEncodingAttr withoutBitWidths() const;
< 
<     /// Returns true if every level is dense.  Also returns true for
<     /// the null encoding (since dense-tensors are always all-dense).
---
>     /// Return true if every level is dense in the encoding.
319,325c266
<     /// Returns true if every level is ordered.  Also returns true for
<     /// the null encoding (since dense-tensors are always all-ordered).
<     bool isAllOrdered() const;
< 
<     /// Returns true if the encoding has an identity dimension ordering.
<     /// Also returns true for the null encoding (since dense-tensors
<     /// always have the identity ordering).
---
>     /// Return true if the encoding has an identity dimension ordering.
328,344d268
<     /// Returns the number of storage levels.  Asserts that the encoding
<     /// is non-null (since there is no fixed result that's valid for
<     /// every dense-tensor).
<     ::mlir::sparse_tensor::Level getLvlRank() const;
< 
<     /// Safely looks up the level-type for the requested level.  (Returns
<     /// `DimLevelType::Dense` for the null encoding, since dense-tensors
<     /// are always all-dense.)
<     ::mlir::sparse_tensor::DimLevelType getLvlType(::mlir::sparse_tensor::Level l) const;
< 
<     bool isDenseLvl(::mlir::sparse_tensor::Level l) const { return isDenseDLT(getLvlType(l)); }
<     bool isCompressedLvl(::mlir::sparse_tensor::Level l) const { return isCompressedDLT(getLvlType(l)); }
<     bool isCompressedWithHiLvl(::mlir::sparse_tensor::Level l) const { return isCompressedWithHiDLT(getLvlType(l)); }
<     bool isSingletonLvl(::mlir::sparse_tensor::Level l) const { return isSingletonDLT(getLvlType(l)); }
<     bool isOrderedLvl(::mlir::sparse_tensor::Level l) const { return isOrderedDLT(getLvlType(l)); }
<     bool isUniqueLvl(::mlir::sparse_tensor::Level l) const { return isUniqueDLT(getLvlType(l)); }
< 
349,354c273,278
<     std::optional<uint64_t> getStaticDimSliceOffset(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticDimSliceSize(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticDimSliceStride(::mlir::sparse_tensor::Dimension dim) const;
<     std::optional<uint64_t> getStaticLvlSliceOffset(::mlir::sparse_tensor::Level lvl) const;
<     std::optional<uint64_t> getStaticLvlSliceSize(::mlir::sparse_tensor::Level lvl) const;
<     std::optional<uint64_t> getStaticLvlSliceStride(::mlir::sparse_tensor::Level lvl) const;
---
>     std::optional<uint64_t> getStaticDimSliceOffset(unsigned dim) const;
>     std::optional<uint64_t> getStaticDimSliceSize(unsigned dim) const;
>     std::optional<uint64_t> getStaticDimSliceStride(unsigned dim) const;
>     std::optional<uint64_t> getStaticLvlSliceOffset(unsigned lvl) const;
>     std::optional<uint64_t> getStaticLvlSliceSize(unsigned lvl) const;
>     std::optional<uint64_t> getStaticLvlSliceStride(unsigned lvl) const;
368,370c292,294
<         I32EnumAttrCase<"LvlSize",    0, "lvl_sz">,
<         I32EnumAttrCase<"PosMemSize", 1, "pos_mem_sz">,
<         I32EnumAttrCase<"CrdMemSize", 2, "crd_mem_sz">,
---
>         I32EnumAttrCase<"DimSize",    0, "dim_sz">,
>         I32EnumAttrCase<"PtrMemSize", 1, "ptr_mem_sz">,
>         I32EnumAttrCase<"IdxMemSize", 2, "idx_mem_sz">,
372,373d295
<         I32EnumAttrCase<"DimOffset",  4, "dim_offset">,
<         I32EnumAttrCase<"DimStride",  5, "dim_stride">,
393,396d314
< def IsSparseTensorSlicePred
<   : CPred<"!!::mlir::sparse_tensor::getSparseTensorEncoding($_self) && "
<           "  ::mlir::sparse_tensor::getSparseTensorEncoding($_self).isSlice()">;
< 
403,405d320
< class SparseTensorSliceOf<list<Type> allowedTypes>
<   : TensorOf<allowedTypes, [IsSparseTensorSlicePred], "sparse tensor slice">;
< 
407d321
< def AnySparseTensorSlice : SparseTensorSliceOf<[AnyType]>;
413,447d326
< 
< //===----------------------------------------------------------------------===//
< // Sparse Tensor Sorting Algorithm Attribute.
< //===----------------------------------------------------------------------===//
< 
< // TODO: Currently, we only provide four implementations, and expose the
< // implementations via attribute algorithm. In the future, if we will need
< // to support both stable and non-stable quick sort, we may add
< // quick_sort_nonstable enum to the attribute. Alternative, we may use two
< // attributes, (stable|nonstable, algorithm), to specify a sorting
< // implementation.
< //
< // --------------------------------------------------------------------------
< // |           | hybrid_qsort| insertion_sort | qsort       | heap_sort.    |
< // |non-stable | Impl        | X              |  Impl       | Impl          |
< // |stable     | X           | Impl           |  Not Impl   | X             |
< // --------------------------------------------------------------------------
< 
< // The C++ enum for sparse tensor sort kind.
< def SparseTensorSortKindEnum
<     : I32EnumAttr<"SparseTensorSortKind", "sparse tensor sort algorithm", [
<         I32EnumAttrCase<"HybridQuickSort",    0, "hybrid_quick_sort">,
<         I32EnumAttrCase<"InsertionSortStable", 1, "insertion_sort_stable">,
<         I32EnumAttrCase<"QuickSort", 2, "quick_sort">,
<         I32EnumAttrCase<"HeapSort", 3, "heap_sort">,
<       ]> {
<   let genSpecializedAttr = 0;
<   let cppNamespace = SparseTensor_Dialect.cppNamespace;
< }
< 
< // Define the enum sparse tensor sort kind attribute.
< def SparseTensorSortKindAttr
<     : EnumAttr<SparseTensor_Dialect, SparseTensorSortKindEnum,
<                "SparseTensorSortAlgorithm"> {
< }
--- include/mlir/Dialect/SparseTensor/Pipelines
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h include/mlir/Dialect/SparseTensor/Pipelines/Passes.h
56,62d55
<   PassOptions::Option<bool> enableIndexReduction{
<       *this, "enable-index-reduction",
<       desc("Enable dependent index reduction based algorithm to handle "
<            "non-trivial index expressions on sparse inputs (experimental "
<            "features)"),
<       init(false)};
< 
76,84d68
<   PassOptions::Option<bool> createSparseDeallocs{
<       *this, "create-sparse-deallocs",
<       desc("Specify if the temporary buffers created by the sparse "
<            "compiler should be deallocated. For compatibility with core "
<            "bufferization passes. "
<            "This option is only used when enable-runtime-library=false. "
<            "See also create-deallocs for BufferizationOption."),
<       init(true)};
< 
125,132d108
<   /// These options are used to enable GPU code generation.
<   PassOptions::Option<std::string> gpuTriple{*this, "gpu-triple",
<                                              desc("GPU target triple")};
<   PassOptions::Option<std::string> gpuChip{*this, "gpu-chip",
<                                            desc("GPU target architecture")};
<   PassOptions::Option<std::string> gpuFeatures{*this, "gpu-features",
<                                                desc("GPU target features")};
< 
135c111
<     return SparsificationOptions(parallelization, enableIndexReduction);
---
>     return SparsificationOptions(parallelization);
145,152c121,128
<   ConvertVectorToLLVMPassOptions lowerVectorToLLVMOptions() const {
<     ConvertVectorToLLVMPassOptions opts{};
<     opts.reassociateFPReductions = reassociateFPReductions;
<     opts.force32BitVectorIndices = force32BitVectorIndices;
<     opts.armNeon = armNeon;
<     opts.armSVE = armSVE;
<     opts.amx = amx;
<     opts.x86Vector = x86Vector;
---
>   LowerVectorToLLVMOptions lowerVectorToLLVMOptions() const {
>     LowerVectorToLLVMOptions opts{};
>     opts.enableReassociateFPReductions(reassociateFPReductions);
>     opts.enableIndexOptimizations(force32BitVectorIndices);
>     opts.enableArmNeon(armNeon);
>     opts.enableArmSVE(armSVE);
>     opts.enableAMX(amx);
>     opts.enableX86Vector(x86Vector);
--- include/mlir/Dialect/SparseTensor/Pipelines/Passes.h
56,62d55
<   PassOptions::Option<bool> enableIndexReduction{
<       *this, "enable-index-reduction",
<       desc("Enable dependent index reduction based algorithm to handle "
<            "non-trivial index expressions on sparse inputs (experimental "
<            "features)"),
<       init(false)};
< 
76,84d68
<   PassOptions::Option<bool> createSparseDeallocs{
<       *this, "create-sparse-deallocs",
<       desc("Specify if the temporary buffers created by the sparse "
<            "compiler should be deallocated. For compatibility with core "
<            "bufferization passes. "
<            "This option is only used when enable-runtime-library=false. "
<            "See also create-deallocs for BufferizationOption."),
<       init(true)};
< 
125,132d108
<   /// These options are used to enable GPU code generation.
<   PassOptions::Option<std::string> gpuTriple{*this, "gpu-triple",
<                                              desc("GPU target triple")};
<   PassOptions::Option<std::string> gpuChip{*this, "gpu-chip",
<                                            desc("GPU target architecture")};
<   PassOptions::Option<std::string> gpuFeatures{*this, "gpu-features",
<                                                desc("GPU target features")};
< 
135c111
<     return SparsificationOptions(parallelization, enableIndexReduction);
---
>     return SparsificationOptions(parallelization);
145,152c121,128
<   ConvertVectorToLLVMPassOptions lowerVectorToLLVMOptions() const {
<     ConvertVectorToLLVMPassOptions opts{};
<     opts.reassociateFPReductions = reassociateFPReductions;
<     opts.force32BitVectorIndices = force32BitVectorIndices;
<     opts.armNeon = armNeon;
<     opts.armSVE = armSVE;
<     opts.amx = amx;
<     opts.x86Vector = x86Vector;
---
>   LowerVectorToLLVMOptions lowerVectorToLLVMOptions() const {
>     LowerVectorToLLVMOptions opts{};
>     opts.enableReassociateFPReductions(reassociateFPReductions);
>     opts.enableIndexOptimizations(force32BitVectorIndices);
>     opts.enableArmNeon(armNeon);
>     opts.enableArmSVE(armSVE);
>     opts.enableAMX(amx);
>     opts.enableX86Vector(x86Vector);
--- include/mlir/Dialect/Func
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/IR and include/mlir/Dialect/Func/IR
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/Transforms and include/mlir/Dialect/Func/Transforms
--- include/mlir/Dialect/Func/CMakeLists.txt
--- include/mlir/Dialect/Func/Transforms
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/Transforms/DecomposeCallGraphTypes.h include/mlir/Dialect/Func/Transforms/DecomposeCallGraphTypes.h
56,57c56,57
<   template <typename FnT, typename T = typename llvm::function_traits<
<                               std::decay_t<FnT>>::template arg_t<2>>
---
>   template <typename FnT,
>             typename T = typename llvm::function_traits<FnT>::template arg_t<2>>
Only in ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/Transforms: OneToNFuncConversions.h
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/Transforms/Passes.h include/mlir/Dialect/Func/Transforms/Passes.h
35,37d34
< /// Pass to deduplicate functions.
< std::unique_ptr<Pass> createDuplicateFunctionEliminationPass();
< 
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/Transforms/Passes.td include/mlir/Dialect/Func/Transforms/Passes.td
43,53d42
< def DuplicateFunctionEliminationPass : Pass<"duplicate-function-elimination",
<     "ModuleOp"> {
<   let summary = "Deduplicate functions";
<   let description = [{
<     Deduplicate functions that are equivalent in all aspects but their symbol
<     name. The pass chooses one representative per equivalence class, erases
<     the remainder, and updates function calls accordingly.
<   }];
<   let constructor = "mlir::func::createDuplicateFunctionEliminationPass()";
< }
< 
--- include/mlir/Dialect/Func/Transforms/Passes.h
35,37d34
< /// Pass to deduplicate functions.
< std::unique_ptr<Pass> createDuplicateFunctionEliminationPass();
< 
--- include/mlir/Dialect/Func/Transforms/Passes.td
43,53d42
< def DuplicateFunctionEliminationPass : Pass<"duplicate-function-elimination",
<     "ModuleOp"> {
<   let summary = "Deduplicate functions";
<   let description = [{
<     Deduplicate functions that are equivalent in all aspects but their symbol
<     name. The pass chooses one representative per equivalence class, erases
<     the remainder, and updates function calls accordingly.
<   }];
<   let constructor = "mlir::func::createDuplicateFunctionEliminationPass()";
< }
< 
--- include/mlir/Dialect/Func/Transforms/CMakeLists.txt
--- include/mlir/Dialect/Func/Transforms/FuncConversions.h
--- include/mlir/Dialect/Func/Transforms/DecomposeCallGraphTypes.h
56,57c56,57
<   template <typename FnT, typename T = typename llvm::function_traits<
<                               std::decay_t<FnT>>::template arg_t<2>>
---
>   template <typename FnT,
>             typename T = typename llvm::function_traits<FnT>::template arg_t<2>>
--- include/mlir/Dialect/Func/IR
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/Func/IR/FuncOps.td include/mlir/Dialect/Func/IR/FuncOps.td
11a12
> include "mlir/IR/AttrTypeBase.td"
25a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
37c39
<     [DeclareOpInterfaceMethods<CallOpInterface>, MemRefsNormalizable,
---
>     [CallOpInterface, MemRefsNormalizable,
79a82,94
> 
>     /// Get the argument operands to the called function.
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
> 
>     operand_iterator arg_operand_begin() { return operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
>     /// Return the callee of this operation.
>     CallInterfaceCallable getCallableForCallee() {
>       return (*this)->getAttrOfType<SymbolRefAttr>("callee");
>     }
92c107
<       DeclareOpInterfaceMethods<CallOpInterface>,
---
>       CallOpInterface,
128a144,146
>     // TODO: Remove once migrated callers.
>     ValueRange operands() { return getCalleeOperands(); }
> 
130,131c148,149
<     operand_range getCallOperands() {
<       return {++operand_begin(), operand_end()};
---
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
133a152,154
>     operand_iterator arg_operand_begin() { return ++operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
283,294d303
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
316a326,335
> 
>     //===------------------------------------------------------------------===//
>     // FuncOp Methods
>     //===------------------------------------------------------------------===//
> 
>     /// Arg attributes for access modes
>     /// These unit attributes may be added to function args to denote how a
>     /// memref pointer is accessed by the function.
>     static StringRef getReadAccessAttrName() { return "func.read_access"; }
>     static StringRef getWriteAccessAttrName() { return "func.write_access"; }
--- include/mlir/Dialect/Func/IR/CMakeLists.txt
--- include/mlir/Dialect/Func/IR/FuncOps.h
--- include/mlir/Dialect/Func/IR/FuncOps.td
11a12
> include "mlir/IR/AttrTypeBase.td"
25a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
37c39
<     [DeclareOpInterfaceMethods<CallOpInterface>, MemRefsNormalizable,
---
>     [CallOpInterface, MemRefsNormalizable,
79a82,94
> 
>     /// Get the argument operands to the called function.
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
>     }
> 
>     operand_iterator arg_operand_begin() { return operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
>     /// Return the callee of this operation.
>     CallInterfaceCallable getCallableForCallee() {
>       return (*this)->getAttrOfType<SymbolRefAttr>("callee");
>     }
92c107
<       DeclareOpInterfaceMethods<CallOpInterface>,
---
>       CallOpInterface,
128a144,146
>     // TODO: Remove once migrated callers.
>     ValueRange operands() { return getCalleeOperands(); }
> 
130,131c148,149
<     operand_range getCallOperands() {
<       return {++operand_begin(), operand_end()};
---
>     operand_range getArgOperands() {
>       return {arg_operand_begin(), arg_operand_end()};
133a152,154
>     operand_iterator arg_operand_begin() { return ++operand_begin(); }
>     operand_iterator arg_operand_end() { return operand_end(); }
> 
283,294d303
<     /// Returns the argument attributes for all callable region arguments or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableArgAttrs() {
<       return getArgAttrs().value_or(nullptr);
<     }
< 
<     /// Returns the result attributes for all callable region results or
<     /// null if there are none.
<     ::mlir::ArrayAttr getCallableResAttrs() {
<       return getResAttrs().value_or(nullptr);
<     }
< 
316a326,335
> 
>     //===------------------------------------------------------------------===//
>     // FuncOp Methods
>     //===------------------------------------------------------------------===//
> 
>     /// Arg attributes for access modes
>     /// These unit attributes may be added to function args to denote how a
>     /// memref pointer is accessed by the function.
>     static StringRef getReadAccessAttrName() { return "func.read_access"; }
>     static StringRef getWriteAccessAttrName() { return "func.write_access"; }
--- include/mlir/Dialect/X86Vector
diff ../../../../llvm-project.0/mlir/include/mlir/Dialect/X86Vector/X86Vector.td include/mlir/Dialect/X86Vector/X86Vector.td
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/X86Vector/CMakeLists.txt
--- include/mlir/Dialect/X86Vector/X86VectorDialect.h
--- include/mlir/Dialect/X86Vector/X86Vector.td
26a27
>   let useFoldAPI = kEmitFoldAdaptorFolder;
--- include/mlir/Dialect/X86Vector/Transforms.h
--- include/mlir/Rewrite
diff ../../../../llvm-project.0/mlir/include/mlir/Rewrite/PatternApplicator.h include/mlir/Rewrite/PatternApplicator.h
19,20d18
< #include "mlir/IR/Action.h"
< 
27,45d24
< 
< /// This is the type of Action that is dispatched when a pattern is applied.
< /// It captures the pattern to apply on top of the usual context.
< class ApplyPatternAction : public tracing::ActionImpl<ApplyPatternAction> {
< public:
<   using Base = tracing::ActionImpl<ApplyPatternAction>;
<   ApplyPatternAction(ArrayRef<IRUnit> irUnits, const Pattern &pattern)
<       : Base(irUnits), pattern(pattern) {}
<   static constexpr StringLiteral tag = "apply-pattern";
<   static constexpr StringLiteral desc =
<       "Encapsulate the application of rewrite patterns";
< 
<   void print(raw_ostream &os) const override {
<     os << "`" << tag << " pattern: " << pattern.getDebugName();
<   }
< 
< private:
<   const Pattern &pattern;
< };
--- include/mlir/Rewrite/PatternApplicator.h
19,20d18
< #include "mlir/IR/Action.h"
< 
27,45d24
< 
< /// This is the type of Action that is dispatched when a pattern is applied.
< /// It captures the pattern to apply on top of the usual context.
< class ApplyPatternAction : public tracing::ActionImpl<ApplyPatternAction> {
< public:
<   using Base = tracing::ActionImpl<ApplyPatternAction>;
<   ApplyPatternAction(ArrayRef<IRUnit> irUnits, const Pattern &pattern)
<       : Base(irUnits), pattern(pattern) {}
<   static constexpr StringLiteral tag = "apply-pattern";
<   static constexpr StringLiteral desc =
<       "Encapsulate the application of rewrite patterns";
< 
<   void print(raw_ostream &os) const override {
<     os << "`" << tag << " pattern: " << pattern.getDebugName();
<   }
< 
< private:
<   const Pattern &pattern;
< };
--- include/mlir/Rewrite/FrozenRewritePatternSet.h
--- include/mlir/Rewrite/PassUtil.td
--- include/mlir-c
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir-c/Bindings and include/mlir-c/Bindings
diff ../../../../llvm-project.0/mlir/include/mlir-c/BuiltinAttributes.h include/mlir-c/BuiltinAttributes.h
29,34d28
< // Location attribute.
< //===----------------------------------------------------------------------===//
< 
< MLIR_CAPI_EXPORTED bool mlirAttributeIsALocation(MlirAttribute attr);
< 
< //===----------------------------------------------------------------------===//
diff ../../../../llvm-project.0/mlir/include/mlir-c/BuiltinTypes.h include/mlir-c/BuiltinTypes.h
98,104d97
< /// Checks whether the given type is an f8E4M3B11FNUZ type.
< MLIR_CAPI_EXPORTED bool mlirTypeIsAFloat8E4M3B11FNUZ(MlirType type);
< 
< /// Creates an f8E4M3B11FNUZ type in the given context. The type is owned by the
< /// context.
< MLIR_CAPI_EXPORTED MlirType mlirFloat8E4M3B11FNUZTypeGet(MlirContext ctx);
< 
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir-c/Dialect and include/mlir-c/Dialect
diff ../../../../llvm-project.0/mlir/include/mlir-c/IR.h include/mlir-c/IR.h
51d50
< DEFINE_C_API_STRUCT(MlirBytecodeWriterConfig, void);
229,236d227
< /// Returns the underlying location attribute of this location.
< MLIR_CAPI_EXPORTED MlirAttribute
< mlirLocationGetAttribute(MlirLocation location);
< 
< /// Creates a location from a location attribute.
< MLIR_CAPI_EXPORTED MlirLocation
< mlirLocationFromAttribute(MlirAttribute attribute);
< 
408,429d398
< /// Do not verify the operation when using custom operation printers.
< MLIR_CAPI_EXPORTED void
< mlirOpPrintingFlagsAssumeVerified(MlirOpPrintingFlags flags);
< 
< //===----------------------------------------------------------------------===//
< // Bytecode printing flags API.
< //===----------------------------------------------------------------------===//
< 
< /// Creates new printing flags with defaults, intended for customization.
< /// Must be freed with a call to mlirBytecodeWriterConfigDestroy().
< MLIR_CAPI_EXPORTED MlirBytecodeWriterConfig
< mlirBytecodeWriterConfigCreate(void);
< 
< /// Destroys printing flags created with mlirBytecodeWriterConfigCreate.
< MLIR_CAPI_EXPORTED void
< mlirBytecodeWriterConfigDestroy(MlirBytecodeWriterConfig config);
< 
< /// Sets the version to emit in the writer config.
< MLIR_CAPI_EXPORTED void
< mlirBytecodeWriterConfigDesiredEmitVersion(MlirBytecodeWriterConfig flags,
<                                            int64_t version);
< 
444,453d412
< /// Parses an operation, giving ownership to the caller. If parsing fails a null
< /// operation will be returned, and an error diagnostic emitted.
< ///
< /// `sourceStr` may be either the text assembly format, or binary bytecode
< /// format. `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name.
< MLIR_CAPI_EXPORTED MlirOperation mlirOperationCreateParse(
<     MlirContext context, MlirStringRef sourceStr, MlirStringRef sourceName);
< 
568c527
< /// Same as mlirOperationPrint but writing the bytecode format.
---
> /// Same as mlirOperationPrint but writing the bytecode format out.
573,578d531
< /// Same as mlirOperationWriteBytecode but with writer config and returns
< /// failure only if desired bytecode could not be honored.
< MLIR_CAPI_EXPORTED MlirLogicalResult mlirOperationWriteBytecodeWithConfig(
<     MlirOperation op, MlirBytecodeWriterConfig config,
<     MlirStringCallback callback, void *userData);
< 
782,787d734
< 
< /// Replace all uses of 'of' value with the 'with' value, updating anything in
< /// the IR that uses 'of' to use the other value instead.  When this returns
< /// there are zero uses of 'of'.
< MLIR_CAPI_EXPORTED void mlirValueReplaceAllUsesOfWith(MlirValue of,
<                                                       MlirValue with);
diff ../../../../llvm-project.0/mlir/include/mlir-c/Pass.h include/mlir-c/Pass.h
73c73
< /// Run the provided `passManager` on the given `op`.
---
> /// Run the provided `passManager` on the given `module`.
75c75
< mlirPassManagerRunOnOp(MlirPassManager passManager, MlirOperation op);
---
> mlirPassManagerRun(MlirPassManager passManager, MlirModule module);
--- include/mlir-c/Debug.h
--- include/mlir-c/RegisterEverything.h
--- include/mlir-c/BuiltinAttributes.h
29,34d28
< // Location attribute.
< //===----------------------------------------------------------------------===//
< 
< MLIR_CAPI_EXPORTED bool mlirAttributeIsALocation(MlirAttribute attr);
< 
< //===----------------------------------------------------------------------===//
--- include/mlir-c/Bindings
Common subdirectories: ../../../../llvm-project.0/mlir/include/mlir-c/Bindings/Python and include/mlir-c/Bindings/Python
--- include/mlir-c/Bindings/Python
--- include/mlir-c/Bindings/Python/Interop.h
--- include/mlir-c/AffineMap.h
--- include/mlir-c/Diagnostics.h
--- include/mlir-c/Conversion.h
--- include/mlir-c/AffineExpr.h
--- include/mlir-c/ExecutionEngine.h
--- include/mlir-c/Interfaces.h
--- include/mlir-c/Support.h
--- include/mlir-c/IntegerSet.h
--- include/mlir-c/Transforms.h
--- include/mlir-c/Pass.h
73c73
< /// Run the provided `passManager` on the given `op`.
---
> /// Run the provided `passManager` on the given `module`.
75c75
< mlirPassManagerRunOnOp(MlirPassManager passManager, MlirOperation op);
---
> mlirPassManagerRun(MlirPassManager passManager, MlirModule module);
--- include/mlir-c/IR.h
51d50
< DEFINE_C_API_STRUCT(MlirBytecodeWriterConfig, void);
229,236d227
< /// Returns the underlying location attribute of this location.
< MLIR_CAPI_EXPORTED MlirAttribute
< mlirLocationGetAttribute(MlirLocation location);
< 
< /// Creates a location from a location attribute.
< MLIR_CAPI_EXPORTED MlirLocation
< mlirLocationFromAttribute(MlirAttribute attribute);
< 
408,429d398
< /// Do not verify the operation when using custom operation printers.
< MLIR_CAPI_EXPORTED void
< mlirOpPrintingFlagsAssumeVerified(MlirOpPrintingFlags flags);
< 
< //===----------------------------------------------------------------------===//
< // Bytecode printing flags API.
< //===----------------------------------------------------------------------===//
< 
< /// Creates new printing flags with defaults, intended for customization.
< /// Must be freed with a call to mlirBytecodeWriterConfigDestroy().
< MLIR_CAPI_EXPORTED MlirBytecodeWriterConfig
< mlirBytecodeWriterConfigCreate(void);
< 
< /// Destroys printing flags created with mlirBytecodeWriterConfigCreate.
< MLIR_CAPI_EXPORTED void
< mlirBytecodeWriterConfigDestroy(MlirBytecodeWriterConfig config);
< 
< /// Sets the version to emit in the writer config.
< MLIR_CAPI_EXPORTED void
< mlirBytecodeWriterConfigDesiredEmitVersion(MlirBytecodeWriterConfig flags,
<                                            int64_t version);
< 
444,453d412
< /// Parses an operation, giving ownership to the caller. If parsing fails a null
< /// operation will be returned, and an error diagnostic emitted.
< ///
< /// `sourceStr` may be either the text assembly format, or binary bytecode
< /// format. `sourceName` is used as the file name of the source; any IR without
< /// locations will get a `FileLineColLoc` location with `sourceName` as the file
< /// name.
< MLIR_CAPI_EXPORTED MlirOperation mlirOperationCreateParse(
<     MlirContext context, MlirStringRef sourceStr, MlirStringRef sourceName);
< 
568c527
< /// Same as mlirOperationPrint but writing the bytecode format.
---
> /// Same as mlirOperationPrint but writing the bytecode format out.
573,578d531
< /// Same as mlirOperationWriteBytecode but with writer config and returns
< /// failure only if desired bytecode could not be honored.
< MLIR_CAPI_EXPORTED MlirLogicalResult mlirOperationWriteBytecodeWithConfig(
<     MlirOperation op, MlirBytecodeWriterConfig config,
<     MlirStringCallback callback, void *userData);
< 
782,787d734
< 
< /// Replace all uses of 'of' value with the 'with' value, updating anything in
< /// the IR that uses 'of' to use the other value instead.  When this returns
< /// there are zero uses of 'of'.
< MLIR_CAPI_EXPORTED void mlirValueReplaceAllUsesOfWith(MlirValue of,
<                                                       MlirValue with);
--- include/mlir-c/BuiltinTypes.h
98,104d97
< /// Checks whether the given type is an f8E4M3B11FNUZ type.
< MLIR_CAPI_EXPORTED bool mlirTypeIsAFloat8E4M3B11FNUZ(MlirType type);
< 
< /// Creates an f8E4M3B11FNUZ type in the given context. The type is owned by the
< /// context.
< MLIR_CAPI_EXPORTED MlirType mlirFloat8E4M3B11FNUZTypeGet(MlirContext ctx);
< 
--- include/mlir-c/Dialect
diff ../../../../llvm-project.0/mlir/include/mlir-c/Dialect/SparseTensor.h include/mlir-c/Dialect/SparseTensor.h
29,41c29,37
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_DENSE = 4,                     // 0b0001_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED = 8,                // 0b0010_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU = 9,             // 0b0010_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NO = 10,            // 0b0010_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU_NO = 11,         // 0b0010_11
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON = 16,                // 0b0100_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU = 17,             // 0b0100_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NO = 18,             // 0b0100_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU_NO = 19,          // 0b0100_11
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI = 32,       // 0b1000_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NU = 33,    // 0b1000_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NO = 34,    // 0b1000_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NU_NO = 35, // 0b1000_11
---
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_DENSE = 4,             // 0b001_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED = 8,        // 0b010_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU = 9,     // 0b010_01
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NO = 10,    // 0b010_10
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU_NO = 11, // 0b010_11
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON = 16,        // 0b100_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU = 17,     // 0b100_01
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NO = 18,     // 0b100_10
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU_NO = 19,  // 0b100_11
48c44
< /// Checks whether the given attribute is a `sparse_tensor.encoding` attribute.
---
> /// Checks whether the given attribute is a sparse_tensor.encoding attribute.
52c48
< /// Creates a `sparse_tensor.encoding` attribute with the given parameters.
---
> /// Creates a sparse_tensor.encoding attribute with the given parameters.
54c50
<     MlirContext ctx, intptr_t lvlRank,
---
>     MlirContext ctx, intptr_t numDimLevelTypes,
56,57c52,53
<     MlirAffineMap dimOrdering, MlirAffineMap higherOrdering, int posWidth,
<     int crdWidth);
---
>     MlirAffineMap dimOrdering, MlirAffineMap higherOrdering,
>     int pointerBitWidth, int indexBitWidth);
59c55
< /// Returns the level-rank of the `sparse_tensor.encoding` attribute.
---
> /// Returns the number of dim level types in a sparse_tensor.encoding attribute.
61c57
< mlirSparseTensorEncodingGetLvlRank(MlirAttribute attr);
---
> mlirSparseTensorEncodingGetNumDimLevelTypes(MlirAttribute attr);
63c59
< /// Returns a specified level-type of the `sparse_tensor.encoding` attribute.
---
> /// Returns a specified dim level type in a sparse_tensor.encoding attribute.
65c61
< mlirSparseTensorEncodingAttrGetDimLevelType(MlirAttribute attr, intptr_t lvl);
---
> mlirSparseTensorEncodingAttrGetDimLevelType(MlirAttribute attr, intptr_t pos);
67c63
< /// Returns the dimension-ordering of the `sparse_tensor.encoding` attribute.
---
> /// Returns the dimension ordering in a sparse_tensor.encoding attribute.
71c67
< /// Returns the higher-ordering of the `sparse_tensor.encoding` attribute.
---
> /// Returns the higher ordering in a sparse_tensor.encoding attribute.
75c71
< /// Returns the position bitwidth of the `sparse_tensor.encoding` attribute.
---
> /// Returns the pointer bit width in a sparse_tensor.encoding attribute.
77c73
< mlirSparseTensorEncodingAttrGetPosWidth(MlirAttribute attr);
---
> mlirSparseTensorEncodingAttrGetPointerBitWidth(MlirAttribute attr);
79c75
< /// Returns the coordinate bitwidth of the `sparse_tensor.encoding` attribute.
---
> /// Returns the index bit width in a sparse_tensor.encoding attribute.
81c77
< mlirSparseTensorEncodingAttrGetCrdWidth(MlirAttribute attr);
---
> mlirSparseTensorEncodingAttrGetIndexBitWidth(MlirAttribute attr);
--- include/mlir-c/Dialect/GPU.h
--- include/mlir-c/Dialect/Async.h
--- include/mlir-c/Dialect/SparseTensor.h
29,41c29,37
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_DENSE = 4,                     // 0b0001_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED = 8,                // 0b0010_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU = 9,             // 0b0010_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NO = 10,            // 0b0010_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU_NO = 11,         // 0b0010_11
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON = 16,                // 0b0100_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU = 17,             // 0b0100_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NO = 18,             // 0b0100_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU_NO = 19,          // 0b0100_11
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI = 32,       // 0b1000_00
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NU = 33,    // 0b1000_01
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NO = 34,    // 0b1000_10
<   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_WITH_HI_NU_NO = 35, // 0b1000_11
---
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_DENSE = 4,             // 0b001_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED = 8,        // 0b010_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU = 9,     // 0b010_01
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NO = 10,    // 0b010_10
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_COMPRESSED_NU_NO = 11, // 0b010_11
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON = 16,        // 0b100_00
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU = 17,     // 0b100_01
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NO = 18,     // 0b100_10
>   MLIR_SPARSE_TENSOR_DIM_LEVEL_SINGLETON_NU_NO = 19,  // 0b100_11
48c44
< /// Checks whether the given attribute is a `sparse_tensor.encoding` attribute.
---
> /// Checks whether the given attribute is a sparse_tensor.encoding attribute.
52c48
< /// Creates a `sparse_tensor.encoding` attribute with the given parameters.
---
> /// Creates a sparse_tensor.encoding attribute with the given parameters.
54c50
<     MlirContext ctx, intptr_t lvlRank,
---
>     MlirContext ctx, intptr_t numDimLevelTypes,
56,57c52,53
<     MlirAffineMap dimOrdering, MlirAffineMap higherOrdering, int posWidth,
<     int crdWidth);
---
>     MlirAffineMap dimOrdering, MlirAffineMap higherOrdering,
>     int pointerBitWidth, int indexBitWidth);
59c55
< /// Returns the level-rank of the `sparse_tensor.encoding` attribute.
---
> /// Returns the number of dim level types in a sparse_tensor.encoding attribute.
61c57
< mlirSparseTensorEncodingGetLvlRank(MlirAttribute attr);
---
> mlirSparseTensorEncodingGetNumDimLevelTypes(MlirAttribute attr);
63c59
< /// Returns a specified level-type of the `sparse_tensor.encoding` attribute.
---
> /// Returns a specified dim level type in a sparse_tensor.encoding attribute.
65c61
< mlirSparseTensorEncodingAttrGetDimLevelType(MlirAttribute attr, intptr_t lvl);
---
> mlirSparseTensorEncodingAttrGetDimLevelType(MlirAttribute attr, intptr_t pos);
67c63
< /// Returns the dimension-ordering of the `sparse_tensor.encoding` attribute.
---
> /// Returns the dimension ordering in a sparse_tensor.encoding attribute.
71c67
< /// Returns the higher-ordering of the `sparse_tensor.encoding` attribute.
---
> /// Returns the higher ordering in a sparse_tensor.encoding attribute.
75c71
< /// Returns the position bitwidth of the `sparse_tensor.encoding` attribute.
---
> /// Returns the pointer bit width in a sparse_tensor.encoding attribute.
77c73
< mlirSparseTensorEncodingAttrGetPosWidth(MlirAttribute attr);
---
> mlirSparseTensorEncodingAttrGetPointerBitWidth(MlirAttribute attr);
79c75
< /// Returns the coordinate bitwidth of the `sparse_tensor.encoding` attribute.
---
> /// Returns the index bit width in a sparse_tensor.encoding attribute.
81c77
< mlirSparseTensorEncodingAttrGetCrdWidth(MlirAttribute attr);
---
> mlirSparseTensorEncodingAttrGetIndexBitWidth(MlirAttribute attr);
--- include/mlir-c/Dialect/LLVM.h
--- include/mlir-c/Dialect/SCF.h
--- include/mlir-c/Dialect/Linalg.h
--- include/mlir-c/Dialect/PDL.h
--- include/mlir-c/Dialect/Shape.h
--- include/mlir-c/Dialect/Quant.h
--- include/mlir-c/Dialect/Tensor.h
--- include/mlir-c/Dialect/ControlFlow.h
--- include/mlir-c/Dialect/MLProgram.h
--- include/mlir-c/Dialect/Transform.h
--- include/mlir-c/Dialect/Func.h

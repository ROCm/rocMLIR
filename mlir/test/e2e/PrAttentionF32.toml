directory = "PrAttentionF32"
prefix = "rocmlir-gen"
suffix = "--operation attention -t f32 --arch %arch -pv %random_data %rocmlir_gen_flags -relDiff_threshold 0.000005 | rocmlir-driver -c | mlir-cpu-runner -O2 --shared-libs=%linalg_test_lib_dir/libmlir_rocm_runtime%shlibext,%conv_validation_wrapper_library_dir/libconv-validation-wrappers%shlibext,%linalg_test_lib_dir/libmlir_runner_utils%shlibext,%linalg_test_lib_dir/libmlir_float16_utils%shlibext --entry-point-result=void | FileCheck %s --check-prefix="

[[axis]]
name = "operation"
values = ["attention"]
prefix = "--operation "

[[axis]]
name = "transK"
values = ["true", "false"]
prefix = "--transK="

[[axis]]
name = "scale"
values = ["true", "false"]
prefix = "--with-attn-scale="

[[axis]]
name = "bias"
values = ["true", "false"]
prefix = "--with-attn-bias="

## attention variant
[[suite]]
name = "pr_attention_f32"

[[suite.test]]
config = "-seq_len 384 -head_dim 64"

[[suite.test]]
config = "-seq_len 64 -head_dim 64 -perf_config 32,64,32,32,32,4,1,1"

## This one test kPerBlock (16 x 4) == head_dim case
[[suite.test]]
config = "-seq_len 64 -head_dim 64 -perf_config 64,64,16,32,32,4,1,1"

[[suite.test]]
config = "-seq_len 64 -head_dim 64"


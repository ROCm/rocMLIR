directory = "PrAttentionF16"
prefix = "rocmlir-gen"
suffix = "--operation attention -t f16 --arch %arch -pv %random_data %rocmlir_gen_flags -relDiff_threshold 0.02 -absDiff_threshold 0.02 -RMS_threshold 0.015 | rocmlir-driver -c | mlir-cpu-runner -O2 --shared-libs=%linalg_test_lib_dir/libmlir_rocm_runtime%shlibext,%conv_validation_wrapper_library_dir/libconv-validation-wrappers%shlibext,%linalg_test_lib_dir/libmlir_runner_utils%shlibext,%linalg_test_lib_dir/libmlir_float16_utils%shlibext --entry-point-result=void | FileCheck %s --check-prefix="

[[axis]]
name = "operation"
values = ["attention"]
prefix = "--operation "

[[axis]]
name = "transK"
values = ["true", "false"]
prefix = "--transK="

## attention variant
[[suite]]
name = "pr_attention_f16"

[[suite.test]]
config = "-seq_len 384 -head_dim 64"

[[suite.test]]
config = "-seq_len 64 -head_dim 64 -perf_config 32,64,32,32,32,4,1,1"

## This one test kPerBlock (16 x 4) == head_dim case
## Also it has drepeats > 1 to check correct fetching
[[suite.test]]
config = "-seq_len 256 -head_dim 128 -perf_config 128,128,32,64,64,4,1,1"

[[suite.test]]
config = "-seq_len 64 -head_dim 64"

# Check a padding config
[[suite.test]]
config = "-seq_len 64 -head_dim 64 -perf_config 64,256,4,64,64,8,1,1"

# check scale
[[suite.test]]
config = "-seq_len 384 -head_dim 64 --with-attn-scale"

# check bias
[[suite.test]]
config = "-seq_len 384 -head_dim 64 --with-attn-bias"

# check scale and bias together
[[suite.test]]
config = "-seq_len 384 -head_dim 64 --with-attn-scale --with-attn-bias"

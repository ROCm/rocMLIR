//===- FoldTranspose.cpp - rewrites to allow MIOpen kernel fusion  ------===//
//
// Copyright 2022 Advanced Micro Devices.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// ============================================================
#include "PassDetail.h"

#include "mlir/Dialect/Linalg/IR/Linalg.h"

#include "mlir/Dialect/MIOpen/MIOpen.h"
#include "mlir/Dialect/MIOpen/Passes.h"
#include "mlir/Dialect/MIOpen/TransformMapBuilder.h"

#include "mlir/Transforms/GreedyPatternRewriteDriver.h"

#include "llvm/Support/Debug.h"

#define DEBUG_TYPE "miopen-fold-transpose"

using namespace mlir;
using namespace mlir::miopen;

namespace {
struct MIOpenFoldTransposePass
    : public MIOpenFoldTransposePassBase<MIOpenFoldTransposePass> {
  void runOnOperation() override;
};
} // end namespace

/// If there is a chain of operations that leads from `v` to
/// a miopen.conv2d* op, return that convolution.
static Operation *getConvUser(Value v) {
  for (Operation *user : v.getUsers()) {
    if (isa<Conv2DOp, Conv2DBwdDataOp, Conv2DBwdWeightOp>(user))
      return user;
    if (auto transform = dyn_cast<TransformOp>(user))
      if (Operation *upstream = getConvUser(transform.output()))
        return upstream;
  }
  return nullptr;
}

namespace {
// MITPRewritePattern
// Fold linarg.generic and memref.alloc generated by transpose op into
// miopen.transform
struct RemoveTrivialTransposePattern
    : public OpRewritePattern<linalg::GenericOp> {
  // Explicit constructor to set a higher pattern benefic than the more general
  // pattern below.
  explicit RemoveTrivialTransposePattern(MLIRContext *ctx)
      : OpRewritePattern<linalg::GenericOp>(ctx, /*benefit=*/2) {}

  miopen::TransformOp makeTranspose(PatternRewriter &b, Value inp,
                                    const AffineMapAttr &inMap,
                                    const AffineMapAttr &outMap) const {
    AffineMap inpIdxMap = inMap.getAffineMap();
    AffineMap outpIdxMap = outMap.getAffineMap();
    Location loc = inp.getLoc();
    MemRefType inpType = inp.getType().template cast<MemRefType>();
    ArrayRef<int64_t> inpShape = inpType.getShape();

    SmallVector<uint32_t, 8> endDims;
    SmallVector<uint32_t, 8> startDims;
    for (uint32_t i = 0, e = inpShape.size(); i < e; ++i) {
      startDims.push_back(i);
      uint32_t inMapped = inpIdxMap.getDimPosition(i);
      endDims.push_back(outpIdxMap.getDimPosition(inMapped));
    }
    miopen::BottomUpTMBuilder transform(b, inpShape, loc);
    transform.passThrough(endDims, startDims);
    auto tfOp = b.create<miopen::TransformOp>(loc, inp, transform.get());
    return tfOp;
  }

  LogicalResult matchAndRewrite(linalg::GenericOp laGeneric,
                                PatternRewriter &b) const override {
    // 0. Test compatibility
    // 0.0. Only fully parallel for now
    for (StringRef itr :
         laGeneric.iterator_types().getAsValueRange<StringAttr>()) {
      if (itr != "parallel") {
        return failure();
      }
    }

    bool bPassing = false;
    laGeneric.getRegion().walk([&](linalg::YieldOp yieldOp) {
      Value laReturn = yieldOp->getOperand(0);
      bPassing = (laReturn == laGeneric.getRegion().getArgument(0));
    });

    // 0.1. Test it only passes through 1:1 and no other calculation
    if (laGeneric.inputs().size() != 1 || laGeneric.outputs().size() != 1 ||
        !bPassing) {
      return failure();
    }

    // 0.2. linalg.generic lowered from tosa.transpose should have memref.alloc
    Value out = *laGeneric.outputs().begin();
    auto allocToDel = out.getDefiningOp<memref::AllocOp>();
    if (!allocToDel) {
      return failure();
    }

    // get maps to construct a transforming map for the transpose
    auto idxMaps =
        laGeneric->template getAttrOfType<ArrayAttr>("indexing_maps");
    AffineMapAttr inIdxMap = idxMaps[0].cast<AffineMapAttr>();
    AffineMapAttr outIdxMap = idxMaps[1].cast<AffineMapAttr>();
    auto tpTransform =
        makeTranspose(b, laGeneric->getOperand(0), inIdxMap, outIdxMap);

    b.replaceOp(allocToDel, {tpTransform});
    b.eraseOp(laGeneric);
    return success();
  }
};

/// If the input to a linalg.generic is the output of a conv2d and the indexing
/// map for that input is a non-trivial permutation of an identity, convert that
/// indexing map to a transpose. This must happen before gridwise gemm
/// conversion because all the transforms on the convolution output are
/// collected at that time.
struct FoldTransposingConvAccess : OpRewritePattern<linalg::GenericOp> {
  using OpRewritePattern<linalg::GenericOp>::OpRewritePattern;

  LogicalResult matchAndRewrite(linalg::GenericOp laGeneric,
                                PatternRewriter &b) const override {
    Location loc = laGeneric->getLoc();
    // We do this out-of-line so as to not invalidate our iterator
    SmallVector<
        std::tuple<unsigned, Value, AffineMap, SmallVector<uint32_t, 4>>>
        toReplace;

    for (OpOperand &operand : laGeneric->getOpOperands()) {
      Value opValue = operand.get();
      Operation *convUser = getConvUser(opValue);
      if (!convUser)
        continue;

      for (Operation *user : opValue.getUsers()) {
        if (isa<linalg::GenericOp>(user) && user != laGeneric) {
          LLVM_DEBUG(llvm::dbgs() << "Multiple generics on same conv output\n");
          return failure();
        }
      }

      if (!isa_and_nonnull<memref::AllocOp>(opValue.getDefiningOp()))
        continue;

      AffineMap idxMap = laGeneric.getTiedIndexingMap(&operand);
      if (idxMap.isMinorIdentityWithBroadcasting())
        continue;
      SmallVector<uint32_t, 4> permutation;
      if (!idxMap.isPermutationOfMinorIdentityWithBroadcasting(permutation))
        continue;

      unsigned opIndex = operand.getOperandNumber();
      toReplace.emplace_back(opIndex, opValue, idxMap, permutation);
    }

    // Actually do the rewrites, if any
    for (auto &tuple : toReplace) {
      unsigned opIndex = std::get<0>(tuple);
      Value opValue = std::get<1>(tuple);
      AffineMap idxMap = std::get<2>(tuple);
      SmallVector<uint32_t, 4> permutation = std::get<3>(tuple);
      LLVM_DEBUG(llvm::dbgs() << "Replacing index map with permutation ");
      LLVM_DEBUG(llvm::interleaveComma(permutation, llvm::dbgs()));
      LLVM_DEBUG(llvm::dbgs() << "\n");

      auto allocation = cast<memref::AllocOp>(opValue.getDefiningOp());
      // Swap out the allocation for the form it needs to take in order to
      // eliminate the non-trivial map.
      ArrayRef<int64_t> shape = opValue.getType().cast<MemRefType>().getShape();
      SmallVector<int64_t, 4> newShape(shape.size(), -1LL);
      SmallVector<uint32_t, 4> endIdentity;
      for (uint32_t i = 0, e = shape.size(); i < e; ++i) {
        endIdentity.push_back(i);
        newShape[permutation[i]] = shape[i];
      }

      // All this new stuff needs to go where the old memref.alloc was
      PatternRewriter::InsertionGuard guard(b);
      b.setInsertionPointAfterValue(allocation);
      auto newAllocType = allocation.getType()
                              .cast<MemRefType>()
                              .clone(newShape)
                              .cast<MemRefType>();
      Value newAlloc =
          b.replaceOpWithNewOp<memref::AllocOp>(allocation, newAllocType);

      miopen::BottomUpTMBuilder permuteMapBuilder(b, newShape, loc);
      permuteMapBuilder.passThrough(endIdentity, permutation);
      TransformMapAttr permuteMapAttr = permuteMapBuilder.get();
      auto permuted =
          b.create<miopen::TransformOp>(loc, newAlloc, permuteMapAttr);
      llvm::SmallPtrSet<Operation *, 2> skips = {laGeneric, permuted};
      newAlloc.replaceAllUsesExcept(permuted, skips);

      // Correct indexing maps
      AffineMap composed =
          idxMap.compose(permuteMapAttr.getMap().getAffineMap());
      Attribute newMaps =
          laGeneric.indexing_maps().replaceImmediateSubAttribute(
              {{opIndex, AffineMapAttr::get(composed)}});
      laGeneric->setAttr(laGeneric.indexing_mapsAttrName(), newMaps);
    }

    return success(!toReplace.empty());
  }
};
} // end namespace

void MIOpenFoldTransposePass::runOnOperation() {
  if (getOperation()->hasAttr("original_func") && !getOperation()->hasAttr("kernel")) return;

  MLIRContext *ctx = &getContext();

  RewritePatternSet patternsTP(ctx);
  patternsTP.add<RemoveTrivialTransposePattern, FoldTransposingConvAccess>(ctx);
  if (failed(
          applyPatternsAndFoldGreedily(getOperation(), std::move(patternsTP))))
    signalPassFailure();
}

std::unique_ptr<Pass> mlir::miopen::createMIOpenFoldTransposePass() {
  return std::make_unique<MIOpenFoldTransposePass>();
}

//===- loweringUtil.h - functions that often come up during lowering or turing
//---------------===//
//
// Part of the MLIR Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
#ifndef MIOPEN_LOWERING_UTIL_H
#define MIOPEN_LOWERING_UTIL_H

#include "mlir/Dialect/MIOpen/utility/math.h"

#include "mlir/Dialect/MIOpen/MIOpen.h"
#include "mlir/IR/Attributes.h"
#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/Support/LogicalResult.h"

#include "llvm/ADT/SmallSet.h"
#include "llvm/ADT/SmallVector.h"

namespace mlir {
namespace miopen {

// Heuristic logic to compute KBlock for backward weight atomic add kernel.
// The logic is adopted from MIOpen.
//
// The logic searches within the range of [1, 20 * number of CUs / gridSize],
// where gridSize is the original number of workgroups required for the
// convolution, and find the largest KBlock number which preserves the 2
// contraints:
// - GemmK (before splitting) = KBlock * KPerBlock * KPack * GemmK (after
// splitting).
// - n (batch size) is divisible by KBlock.
//
// 20 is a magic number obtained in MIOpen after empirical testing. It offers a
// reasonable reduction of GemmK after splitting, without incurring too much
// overheads on atomic adds. One potential future work is to make this value be
// tunable.
inline int64_t calculateKBlockNum(int64_t n, int64_t ho, int64_t wo, int64_t g,
                                  int64_t k, int64_t c, int64_t y, int64_t x,
                                  int64_t MPerBlock, int64_t NPerBlock,
                                  int64_t KPerBlock, int64_t KPack,
                                  int64_t num_cu) {
  const int64_t KPerGroup = k / g;
  const int64_t CPerGroup = c / g;
  const int64_t gemmM = KPerGroup;
  const int64_t gemmN = CPerGroup * y * x;

  const int64_t gemmK = n * ho * wo;
  int64_t gemmKBlock = 1;

  assert(gemmM % MPerBlock == 0);
  assert(gemmN % NPerBlock == 0);
  assert(gemmK % (KPerBlock * KPack) == 0);

  const int64_t gridSize = g * (gemmM / MPerBlock) * (gemmN / NPerBlock);
  const int64_t maxGridSize = 20 * num_cu;

  gemmKBlock = std::max(maxGridSize / gridSize, static_cast<int64_t>(1));
  gemmKBlock = std::min(gemmKBlock, n);

  for (; gemmKBlock > 1; --gemmKBlock) {
    if (n % gemmKBlock != 0)
      continue;

    if (gemmK % (gemmKBlock * KPerBlock * KPack) != 0)
      continue;

    break;
  }

  // not more than n
  gemmKBlock = std::min(n, gemmKBlock);
  // not less than 1
  gemmKBlock = std::max(static_cast<int64_t>(1), gemmKBlock);

  // llvm::errs() << "\n gemmKBlock: " << gemmKBlock << " gemmK: " << gemmK
  //               << " ho: " << ho << " wo: " << wo << "\n";
  return gemmKBlock;
}

/// Unwrap a value from the transforms surrounding it, gathering up the
/// transforms.
/// Given a Value `v` that is generated by
///   %v1 = miopen.transform [#transform1] %v0
///   %v = miopen.transform [#transform2] %v1
/// this method will return %v0 and an ArrayAttr equal to [#transform2,
/// #transform1]. If `existing` is passed in, it must be an array of
/// `TransformMapAttr`s which will be prepended to the returned `ArrayAttr`.

std::tuple<Value, ArrayAttr> untransform(OpBuilder &b, Value transformed,
                                         ArrayAttr existing = nullptr);

/// Given an array of transform_maps `transforms` (to be composed left to
/// right), returns the array of dimensions in the lowest space of these
/// transforms that need to be checked for out of bounds stores on the left
/// (checking for indices less than 0) and on the right (indices greater than
/// the dimension on the memref)
std::tuple<ArrayAttr, ArrayAttr> computeOobFromTransforms(Builder &b,
                                                          ArrayAttr transforms);

/// Populate a vector of gemm IDs to be used by a backward data convolution
/// algorithm. In the current v4r1 algorithm, several kernels may be needed to
/// realize a complete backward data convolution.
///
/// The values of gemm IDs would be 0, or a positive integer to denote the IDs
/// of the actual implicit GEMM kernels to partipate the backward data
/// convolution, or it could be -1 in case a zero initialization utility kernel
/// is needed. The zero initialization kernel, if needed, would be placed in the
/// front of the vector.
SmallVector<int64_t>
populateBackwardDataGemmIds(int64_t strideHeight, int64_t strideWidth,
                            int64_t dilationHeight, int64_t dilationWidth,
                            int64_t filterHeight, int64_t filterWidth);

/// Obtain convolution direction given a Convolution Op.
/// TODO(whchung): apply ConvolutionOp OpTrait check after supporting PR is in.
miopen::ConvOpType obtainConvDirection(Operation *op);

/// Obtain convolution input data type given a Convolution Op.
/// TODO(whchung): apply ConvolutionOp OpTrait check after supporting PR is in.
mlir::Type obtainConvDataType(Operation *op);

/// Create a map of dimension names to indices in a destination coordinate space
/// using the expansion map [original name] -> [expanded names] to
/// replace one dimension with multiple ones.
/// For example, expandNamesInPlace(["a", "b", "c"], {"b": ["x", "y"]})
/// will return the mapping {"a": 0, "x": 1, "y": 2, "c": 3}
llvm::StringMap<uint32_t>
expandNamesInPlace(ArrayRef<StringRef> original,
                   const llvm::StringMap<SmallVector<StringRef, 2>> expansion);
llvm::StringMap<uint32_t>
expandNamesInPlace(CoordTransformsBuilder &builder,
                   const llvm::StringMap<SmallVector<StringRef, 2>> expansion);

} // end namespace miopen
} // end namespace mlir
#endif

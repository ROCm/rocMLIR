//===- loweringUtil.h - functions that often come up during lowering or turing
//---------------===//
//
// Part of the MLIR Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
#ifndef MIOPEN_LOWERING_UTIL_H
#define MIOPEN_LOWERING_UTIL_H

#include "mlir/Dialect/MIOpen/utility/math.h"

#include "mlir/Dialect/MIOpen/MIOpen.h"
#include "mlir/IR/Attributes.h"
#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/Support/LogicalResult.h"

#include "llvm/ADT/SmallSet.h"
#include "llvm/ADT/SmallVector.h"

namespace mlir {
namespace miopen {

// Heuristic logic to compute KBlock for backward weight atomic add kernel.
// The logic is adopted from MIOpen.
//
// The logic searches within the range of [1, 20 * number of CUs / gridSize],
// where gridSize is the original number of workgroups required for the
// convolution, and find the largest KBlock number which preserves the 2
// contraints:
// - GemmK (before splitting) = KBlock * KPerBlock * KPack * GemmK (after
// splitting).
// - n (batch size) is divisible by KBlock.
//
// 20 is a magic number obtained in MIOpen after empirical testing. It offers a
// reasonable reduction of GemmK after splitting, without incurring too much
// overheads on atomic adds. One potential future work is to make this value be
// tunable.
inline LogicalResult calculateKBlockNum(int64_t n, int64_t ho, int64_t wo,
                                        int64_t g, int64_t k, int64_t c,
                                        int64_t y, int64_t x, int64_t MPerBlock,
                                        int64_t NPerBlock, int64_t KPerBlock,
                                        int64_t KPack, int64_t num_cu,
                                        int64_t *nKBlock) {
  const int64_t gemmM = k;
  const int64_t gemmN = c * y * x;
  const int64_t gemmK = n * ho * wo;

  int64_t gemmKBlock = 1;

  if ((gemmM % MPerBlock != 0) || (gemmN % NPerBlock != 0) ||
      (gemmK % (KPerBlock * KPack) != 0))
    return failure();

  const int64_t gridSize = g * (gemmM / MPerBlock) * (gemmN / NPerBlock);
  const int64_t maxGridSize = 20 * num_cu;

  gemmKBlock = std::max(maxGridSize / gridSize, static_cast<int64_t>(1));
  gemmKBlock = std::min(gemmKBlock, n);

  for (; gemmKBlock > 1; --gemmKBlock) {
    if (n % gemmKBlock != 0)
      continue;

    if (gemmK % (gemmKBlock * KPerBlock * KPack) != 0)
      continue;

    break;
  }
  // not more than n
  gemmKBlock = std::min(n, gemmKBlock);
  // not less than 1
  gemmKBlock = std::max((__int64_t)1, gemmKBlock);

  *nKBlock = gemmKBlock;
  return success();
}

/// Unwrap a value from the transforms surrounding it, gathering up the
/// transforms.
/// Given a Value `v` that is generated by
///   %v1 = miopen.transform [#transform1] %v0
///   %v = miopen.transform [#transform2] %v1
/// this method will return %v0 and an ArrayAttr equal to [#transform2,
/// #transform1]. If `existing` is passed in, it must be an array of
/// `TransformMapAttr`s which will be prepended to the returned `ArrayAttr`.

std::tuple<Value, ArrayAttr> untransform(OpBuilder &b, Value transformed,
                                         ArrayAttr existing = nullptr);

/// Given an array of transform_maps `transforms` (to be composed left to
/// right), returns the array of dimensions in the lowest space of these
/// transforms that need to be checked for out of bounds stores on the left
/// (checking for indices less than 0) and on the right (indices greater than
/// the dimension on the memref)
std::tuple<ArrayAttr, ArrayAttr> computeOobFromTransforms(Builder &b,
                                                          ArrayAttr transforms);

/// Populate a vector of gemm IDs to be used by a backward data convolution
/// algorithm. In the current v4r1 algorithm, several kernels may be needed to
/// realize a complete backward data convolution.
///
/// The values of gemm IDs would be 0, or a positive integer to denote the IDs
/// of the actual implicit GEMM kernels to partipate the backward data
/// convolution, or it could be -1 in case a zero initialization utility kernel
/// is needed. The zero initialization kernel, if needed, would be placed in the
/// front of the vector.
SmallVector<int64_t>
populateBackwardDataGemmIds(int64_t strideHeight, int64_t strideWidth,
                            int64_t dilationHeight, int64_t dilationWidth,
                            int64_t filterHeight, int64_t filterWidth);

/// Obtain convolution direction given a Convolution Op.
/// TODO(whchung): apply ConvolutionOp OpTrait check after supporting PR is in.
miopen::ConvOpType obtainConvDirection(Operation *op);

/// Obtain convolution input data type given a Convolution Op.
/// TODO(whchung): apply ConvolutionOp OpTrait check after supporting PR is in.
mlir::Type obtainConvDataType(Operation *op);
} // end namespace miopen
} // end namespace mlir
#endif

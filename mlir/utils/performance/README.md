# Quick Tuner

## Overview

The 3 quick tuner scripts allow users to generate new quick tuner perf config listings from exhaustive tuning data. These three scripts are:
- `quickTunerPreproc.py`, accepts debug files from `tuningRunner.py` and builds a single combined dataframe with additional normed TFlop column
- `quickTunerGen.py`, accepts single dataframe file generated above and then builds a perf config list for all datatypes specified, using specified methods
- `quickTunerStat.py`, verifies performance of the perf config list using data or using `perfRunner.py` functions

## `quickTunerPreproc.py`

Running `tuningRunner.py` with the `--debug` flag saves the tuning output to a dataframe with the following columns:

```
DataType    OutDataType Chip    numCU   TransA  TransB  G   M   K   N   PerfConfig  LDSBankConflict TFlops
```

saved to the file specified by `--output` with the suffix `.debug`

This data contains all data relevant to tuning a certain gemm config. Note: since PerfConfig is a single string it needs to be properly processed to extract the proper perf config parameters.

### Example:

Suppose we have the following directory structure with data generated from `tuningRunner.py`:
```
/data/run1.debug
/data/run2.debug
/data/run3.debug
```

and running
```
python3 quickTunerPreproc.py --input-dir /home/root/data --output combined_data
```
`--input-dir` specifies the directory that contains the *.debug files. These files will be globbed, read, combined into a single dataframe and finally saved to the filename specified by `--output`. Additionally a column `NormalizedTFlops` will added. This is generated by grouping the data by GEMM config (`DataType`, `OutDataType`, `TransA`, `TransB`, `G`, `M`, `K`, and `N`) then using a min-max normalizer to place the `TFlops` on the range of [0.0,1.0]. This is used for generating the perf configs in the next step: generation.

## `quickTunerGen.py`

After running `quickTuningPreproc.py` we are left with a dataframe that contains all GEMM-perf config data. This can be used as direct input for `quickTunerGen.py`.

### Example:

```
python3 quickTunerGen.py --input-file combined data --method {default,fairSelect} --save
```
`--input-file` specifies the input file, the dataframe generated above. This will then run the selected methods on the data, in this `default` and `fairSelect`. The results will then be saved by using the `--save` argument, saving to `METHOD_NAME.DATATYPE.qt`:
```
# Assuming combined data had f16 and f32 GEMMs
default.f16.qt
default.f32.qt
fairSelect.f16.qt
fairSelect.f32.qt
```
Normally there are about 20 or values saved, referred to as N, in each `.qt` file

These results can then be used in the verifier `quickTunerStat.py`. This lets us test to ensure the perf configs are the in the top percentage.

### Methods
The main metric that is being rated is the 'TFlops'. From these 'TFlops' the aforementioned 'NormalizedTFlops' come in. It is possible that other metrics could be used such as cache misses, and resource usage. This could be a future direction for the tuning engine. For each set of data, we generate perf configs on a per-datatype basis, so only GEMMs with the same `DataType` will be considered.

#### `default`

The default method is the method currently used to generate perf configs. It uses an efficient method of extract the most performant perf config for each GEMM and then extracting from that list, the top N most performant perf configs.

#### 'fairSelect'

The fair select method first orders each GEMM based on their TFlop performance and then takes the top 10% of the dataset. From here it attempts to pick common perf configs from each GEMM, since these common perf configs are in the top 10% of their respective GEMMs we have now represented (at least once) every GEMM that shares that perf config in their top 10%. We continue until every GEMM has at least one perf config from its top 10% selected. In the case where we haven't selected enough perf configs to make up the target N but we have picked common perf configs for every GEMM we simply follow the best performers in a large sorted list and pop from it until it is full.

In the opposite case, where we have no picked common perf configs for every GEMM but we have selected N or more already, we can slice until we are at N configs. Since the selection values perf configs that are most common first, we are only cutting configs that are not as common as those selected first.

#### Other Methods

There are other methods like `topNSelect`, `topMode`, `takeNEach` & some clustering methods. These methods generated far less performant perf configs than both the `default` and `fairSelect` methods and thus are not enabled by default.

*topNSelect*
This method splits each GEMM's data into sections, then takes the top n performers from each section, generating an "average" set from each GEMM dataset.

*topMode*
This method takes the most common perf config the top 15% of the runs and pads with the highest performers until we get N values in total.

*takeNEach*
This method operates similarly to `default` except it takes the top n (n = N // len(GEMMs)) from each GEMM instead of just a single value.

## `quickTunerStat.py`

The last script is a verifier that allows us to pass in the quick tuning config files (*.qt files) generated by `quickTunerGen.py`, as well as a set of GEMM configs to test against. Finally, you can either pass in a database or let the script run tuning on the provided quick tuning files and compare the scores.

### Example:

```
# to verify with data
python3 quickTunerStat.py --input-dir . --gemm-configs ../../mlir/utils/performance/gemm-configs.test  --method data --data combined_data --rank 
# OR
python3 quickTunerStat.py --input-dir . --gemm-configs ../../mlir/utils/performance/gemm-configs.test  --method tuning --data combined_data --rank 
```

Which will print something like this:
```
            f16  f32  i8
default      10    7   0
fairSelect    0    0   0
```
The columns are labeled by data type and rows by method. This is the ranking of each perf config generation method with the method (data/tuning) validation data.

`--input-dir` is the directory containing the `*.qt` files. `--gemm-configs` is the file containing the actual GEMM configs we want to test against. Now, there are two different methods shown `data` and `tuning`.

`--method data` requires `--data` to be passed in, this is just the data that was previously preprocessed using `quickTunerPreproc.py`. This data is used as a lookup table to ensure the data is performing in the proper threshold (90th percentile).
`--method tuning` requires no extra data to be passed in, instead it uses soem functionality borrowed from `perfRunner.py` to run the `romclir-tuning-driver` and collect data for the provided `--gemm-configs`.

Providing `--rank` will output files for each datatype of the form `quick_tuning_DATATYPE`. Which is the "winning" method's perf config. Running rank will also generate a file `quick_tuning_datatype_cpp` which is a C++ vector style initializer array of the data so the user can easily copy and paste into the appropriate file, the winning perf config.

